from sqlalchemy import create_engine
import pandas as pd

def create_conn():
    engine = None
    try:
                engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
        print("Connection successful")
    except Exception as error:
        print(error)

    return engine

def fetch_data(engine, table_name):
    df = None
    try:
                df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)
    except Exception as error:
        print(error)

    return df
import  script

import pandas as pd
engine=script.create_conn()

data=script.fetch_data(engine, "xdr_data")
data
missing_fields = data.isnull().sum()

print(missing_fields)
column_data_types = data.dtypes

print(column_data_types)
float_columns = data.select_dtypes(include=['float64'])

float_columns
filled_data = float_columns.fillna(float_columns.mean())

print(filled_data)
missing_fields = filled_data.isnull().sum()

print(missing_fields)
import pandas as pd




non_float_columns = data.select_dtypes(exclude=['float64'])





cleand_data = pd.concat([filled_data, non_float_columns], axis=1)



print(cleand_data)
from sqlalchemy import create_engine




engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')





cleand_data.to_sql('xdr_cleaned_data', engine, if_exists='replace', index=False)
from sqlalchemy import create_engine




engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')





float_columns.to_sql('xdr_float_data', engine, if_exists='replace', index=False)
import  script
engine=script.create_conn()

data=script.fetch_data(engine, "xdr_data")
data
column_names_list = data.columns.tolist()

print(column_names_list)
print(data['Handset Type'].head(20))
top_10_handsets = data['Handset Type'].value_counts().head(10)

print(top_10_handsets)
top_3_manufactures = data['Handset Manufacturer'].value_counts().head(3)

print(top_3_manufactures)


top_3_manufacturers = data['Handset Manufacturer'].value_counts().head(3).index

filtered_data = data[data['Handset Manufacturer'].isin(top_3_manufacturers)]





top_5_handsets_per_manufacturer = filtered_data.groupby('Handset Manufacturer')['Handset Type'].value_counts().groupby(level=0, group_keys=False).nlargest(5)

print(top_5_handsets_per_manufacturer)
import  script
engine=script.create_conn()

cleaned_data=script.fetch_data(engine, "xdr_cleaned_data")

sessions_per_user = cleaned_data.groupby('MSISDN/Number')['Bearer Id'].nunique()
sessions_per_user
import pandas as pd

cleaned_data['Start'] = pd.to_datetime(cleaned_data['Start'])

cleaned_data['End'] = pd.to_datetime(cleaned_data['End'])

cleaned_data['Session Duration'] = (cleaned_data['End'] - cleaned_data['Start']).dt.total_seconds()

total_session_duration = cleaned_data.groupby('MSISDN/Number')['Session Duration'].sum()
total_session_duration
import pandas as pd





total_download_per_user = cleaned_data.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()

total_upload_per_user = cleaned_data.groupby('MSISDN/Number')['Total UL (Bytes)'].sum()




total_data_per_user = pd.DataFrame({

    'Total Download (Bytes)': total_download_per_user,

    'Total Upload (Bytes)': total_upload_per_user

}).reset_index()
total_data_per_user
column_headers = cleaned_data.columns

column_headers
import pandas as pd





download_columns = ['Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

upload_columns = ['Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']




total_data_per_user_per_app = cleaned_data.groupby('MSISDN/Number')[download_columns + upload_columns].sum().reset_index()

total_data_per_user_per_app
import  script
engine=script.create_conn()

float_data=script.fetch_data(engine, "xdr_float_data")
float_data
import pandas as pd






selected_columns =float_data.drop(['Bearer Id', 'IMSI', 'IMEI', 'MSISDN/Number', 'Start ms', 'End ms'], axis=1)



results = pd.DataFrame(index=['mean', 'median', 'mode', 'variance', 'std_dev', 'range', 'skewness', 'kurtosis'])



for column in selected_columns:

    mean = selected_columns[column].mean()

    median = selected_columns[column].median()

    mode = selected_columns[column].mode()[0]

    variance = selected_columns[column].var()

    std_dev = selected_columns[column].std()

    data_range = selected_columns[column].max() - selected_columns[column].min()

    skewness = selected_columns[column].skew()

    kurtosis = selected_columns[column].kurtosis()



    results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]






results
float_data.head(10)
import matplotlib.pyplot as plt




columns_for_analysis = ["Dur. (ms)", "Avg RTT DL (ms)", "Avg RTT UL (ms)", "Avg Bearer TP DL (kbps)", "Total UL (Bytes)"]




for column in columns_for_analysis:

    plt.figure(figsize=(8, 5))

    plt.hist(float_data[column].dropna(), bins=30, edgecolor='black', color='skyblue')

    plt.title(f'Histogram of {column}')

    plt.xlabel(column)

    plt.ylabel('Frequency')

    plt.show()

import seaborn as sns




columns_for_analysis = ["Total UL (Bytes)"]




for column in columns_for_analysis:

    plt.figure(figsize=(8, 5))

    sns.boxplot(y=column, data=float_data, color='skyblue')

    plt.title(f'Box Plot of {column}')

    plt.show()

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt





columns_for_analysis = ["Social Media DL (Bytes)", "Social Media UL (Bytes)", "Google DL (Bytes)", "Google UL (Bytes)",

                         "Email DL (Bytes)", "Email UL (Bytes)", "Youtube DL (Bytes)", "Youtube UL (Bytes)",

                         "Netflix DL (Bytes)", "Netflix UL (Bytes)", "Gaming DL (Bytes)", "Gaming UL (Bytes)",

                         "Other DL (Bytes)", "Other UL (Bytes)", "Total DL (Bytes)", "Total UL (Bytes)"]




selected_data = float_data[columns_for_analysis]




correlation_matrix = selected_data.corr()




plt.figure(figsize=(12, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

plt.title('Correlation Heatmap - Applications vs Total DL+UL Data')

plt.show()
import  script
engine=script.create_conn()

float_data=script.fetch_data(engine, "xdr_float_data")
float_data
float_data
import pandas as pd






selected_columns =float_data.drop(['Bearer Id', 'IMSI', 'IMEI', 'MSISDN/Number', 'Start ms', 'End ms'], axis=1)



results = pd.DataFrame(index=['mean', 'median', 'mode', 'variance', 'std_dev', 'range', 'skewness', 'kurtosis'])



for column in selected_columns:

    mean = selected_columns[column].mean()

    median = selected_columns[column].median()

    mode = selected_columns[column].mode()[0]

    variance = selected_columns[column].var()

    std_dev = selected_columns[column].std()

    data_range = selected_columns[column].max() - selected_columns[column].min()

    skewness = selected_columns[column].skew()

    kurtosis = selected_columns[column].kurtosis()



    results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]






results
float_data.head(10)
import matplotlib.pyplot as plt




columns_for_analysis = ["Dur. (ms)", "Avg RTT DL (ms)", "Avg RTT UL (ms)", "Avg Bearer TP DL (kbps)", "Total UL (Bytes)"]




for column in columns_for_analysis:

    plt.figure(figsize=(8, 5))

    plt.hist(float_data[column].dropna(), bins=50, edgecolor='black', color='skyblue')

    plt.title(f'Histogram of {column}')

    plt.xlabel(column)

    plt.ylabel('Frequency')

    plt.show()

import numpy as np





dur_percentile_12 = np.percentile(float_data["Dur. (ms)"].dropna(), 12)

print(f"The value below which 12% of observations fall is: {dur_percentile_12}")

import seaborn as sns




columns_for_analysis = ["Total UL (Bytes)"]




for column in columns_for_analysis:

    plt.figure(figsize=(8, 5))

    sns.boxplot(y=column, data=float_data, color='skyblue')

    plt.title(f'Box Plot of {column}')

    plt.show()

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt





columns_for_analysis = ["Social Media DL (Bytes)", "Social Media UL (Bytes)", "Google DL (Bytes)", "Google UL (Bytes)",

                         "Email DL (Bytes)", "Email UL (Bytes)", "Youtube DL (Bytes)", "Youtube UL (Bytes)",

                         "Netflix DL (Bytes)", "Netflix UL (Bytes)", "Gaming DL (Bytes)", "Gaming UL (Bytes)",

                         "Other DL (Bytes)", "Other UL (Bytes)", "Total DL (Bytes)", "Total UL (Bytes)"]




selected_data = float_data[columns_for_analysis]




correlation_matrix = selected_data.corr()




plt.figure(figsize=(12, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

plt.title('Correlation Heatmap - Applications vs Total DL+UL Data')

plt.show()
import  script

import pandas as pd
engine=script.create_conn()

data=script.fetch_data(engine, "xdr_data")
data
missing_fields = data.isnull().sum()

print(missing_fields)
column_data_types = data.dtypes

print(column_data_types)
float_columns = data.select_dtypes(include=['float64'])

float_columns
filled_data = float_columns.fillna(float_columns.mean())

print(filled_data)
missing_fields = filled_data.isnull().sum()

print(missing_fields)
import pandas as pd




non_float_columns = data.select_dtypes(exclude=['float64'])





cleand_data = pd.concat([filled_data, non_float_columns], axis=1)



print(cleand_data)
from sqlalchemy import create_engine




engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')





cleand_data.to_sql('xdr_cleaned_data', engine, if_exists='replace', index=False)
from sqlalchemy import create_engine




engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')





float_columns.to_sql('xdr_float_data', engine, if_exists='replace', index=False)
from sqlalchemy import create_engine
import pandas as pd
"""
a function that connect to the local database
"""
def create_conn():
    engine = None
    try:
                engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
        print("Connection successful")
    except Exception as error:
        print(error)

    return engine

"""
a function that that accept engine, and table_name as an argument and return pandas data fream
"""
def fetch_data(engine, table_name):
    df = None
    try:
                df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)
    except Exception as error:
        print(error)

    return df
import  script
engine=script.create_conn()

cleaned_data=script.fetch_data(engine, "xdr_cleaned_data")

sessions_per_user = cleaned_data.groupby('MSISDN/Number')['Bearer Id'].nunique()
sessions_per_user
import pandas as pd

cleaned_data['Start'] = pd.to_datetime(cleaned_data['Start'])

cleaned_data['End'] = pd.to_datetime(cleaned_data['End'])

cleaned_data['Session Duration'] = (cleaned_data['End'] - cleaned_data['Start']).dt.total_seconds()

total_session_duration = cleaned_data.groupby('MSISDN/Number')['Session Duration'].sum()
total_session_duration
import pandas as pd





total_download_per_user = cleaned_data.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()

total_upload_per_user = cleaned_data.groupby('MSISDN/Number')['Total UL (Bytes)'].sum()




total_data_per_user = pd.DataFrame({

    'Total Download (Bytes)': total_download_per_user,

    'Total Upload (Bytes)': total_upload_per_user

}).reset_index()
total_data_per_user
column_headers = cleaned_data.columns

column_headers
import pandas as pd





download_columns = ['Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

upload_columns = ['Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']




total_data_per_user_per_app = cleaned_data.groupby('MSISDN/Number')[download_columns + upload_columns].sum().reset_index()

total_data_per_user_per_app
import  script
engine=script.create_conn()

cleaned_data=script.fetch_data(engine, "xdr_cleaned_data")
cleaned_data.columns

import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt






engagement_metrics = cleaned_data[['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)']]




agg_engagement = engagement_metrics.groupby('MSISDN/Number').agg({

    'Dur. (ms)': 'sum',

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()





agg_engagement['Total Engagement'] = agg_engagement['Dur. (ms)'] + agg_engagement['Total UL (Bytes)'] + agg_engagement['Total DL (Bytes)']





top_10_customers = agg_engagement.sort_values('Total Engagement', ascending=False).head(10)



top_10_customers
import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import MinMaxScaler





cluster_data = agg_engagement[['Dur. (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)']]




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(cluster_data)










kmeans = KMeans(n_clusters=3, random_state=42)

agg_engagement['Cluster'] = kmeans.fit_predict(normalized_data)





cluster_summary = agg_engagement.groupby('Cluster').agg({

    'Dur. (ms)': ['min', 'max', 'mean', 'sum'],

    'Total UL (Bytes)': ['min', 'max', 'mean', 'sum'],

    'Total DL (Bytes)': ['min', 'max', 'mean', 'sum']

}).reset_index()







cluster_summary
import pandas as pd








cleaned_data['Social Media Traffic'] = cleaned_data['Social Media DL (Bytes)'] + cleaned_data['Social Media UL (Bytes)']

cleaned_data['Google Traffic'] = cleaned_data['Google DL (Bytes)'] + cleaned_data['Google UL (Bytes)']

cleaned_data['Email Traffic'] = cleaned_data['Email DL (Bytes)'] + cleaned_data['Email UL (Bytes)']

cleaned_data['Youtube Traffic'] = cleaned_data['Youtube DL (Bytes)'] + cleaned_data['Youtube UL (Bytes)']

cleaned_data['Netflix Traffic'] = cleaned_data['Netflix DL (Bytes)'] + cleaned_data['Netflix UL (Bytes)']

cleaned_data['Gaming Traffic'] = cleaned_data['Gaming DL (Bytes)'] + cleaned_data['Gaming UL (Bytes)']

cleaned_data['Other Traffic'] = cleaned_data['Other DL (Bytes)'] + cleaned_data['Other UL (Bytes)']




agg_user_app_traffic = cleaned_data.groupby('MSISDN/Number')[['Social Media Traffic', 'Google Traffic', 'Email Traffic',

                                                   'Youtube Traffic', 'Netflix Traffic', 'Gaming Traffic', 'Other Traffic']].sum().reset_index()




top_10_social_media_users = agg_user_app_traffic.nlargest(10, 'Social Media Traffic')

top_10_google_users = agg_user_app_traffic.nlargest(10, 'Google Traffic')

top_10_email_users = agg_user_app_traffic.nlargest(10, 'Email Traffic')

top_10_youtube_users = agg_user_app_traffic.nlargest(10, 'Youtube Traffic')

top_10_netflix_users = agg_user_app_traffic.nlargest(10, 'Netflix Traffic')

top_10_gaming_users = agg_user_app_traffic.nlargest(10, 'Gaming Traffic')

top_10_other_users = agg_user_app_traffic.nlargest(10, 'Other Traffic')





top_10_social_media_users

 

top_3_apps = agg_user_app_traffic[['Social Media Traffic', 'Google Traffic', 'Email Traffic',

                                   'Youtube Traffic', 'Netflix Traffic', 'Gaming Traffic', 'Other Traffic']].sum().nlargest(3)




top_3_apps.plot(kind='bar', rot=0, color='skyblue')

plt.title('Top 3 Most Used Applications')

plt.xlabel('Application')

plt.ylabel('Total Traffic')

plt.show()
import streamlit as st
from over_view import over_view
from user_engagment import engagement
from experience_analytics import experiencd
from satisfaction_analytics import satisfaction

page_options = ["Over view", "User Engagment", "Experience Analytics", "Satisfaction Analytics"]
selected_page = st.sidebar.selectbox("Select a page", page_options)

if selected_page == "Over view":
    over_view()
elif selected_page == "User Engagment":
    engagement()
elif selected_page == "Experience Analytics":
    experiencd()
elif selected_page == "Satisfaction Analytics":
    satisfaction()
from sqlalchemy import create_engine
import pandas as pd
"""
a function that connect to the local database
"""
def create_conn():
    engine = None
    try:
                engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
        print("Connection successful")
    except Exception as error:
        print(error)

    return engine

"""
a function that that accept engine, and table_name as an argument and return pandas data fream
"""
def fetch_data(engine, table_name):
    df = None
    try:
                df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)
    except Exception as error:
        print(error)

    return df
import streamlit as st
from other_module import fetch_data, create_conn
from visualization import create_top_10_handsets_chart, create_top_3_manufacturers_chart, create_top_5_handsets_per_manufacturer_chart

def over_view():
    st.title("Over view")
    st.write("This is Over view analysis TellCo's company")

    engine = create_conn()
    data = fetch_data(engine, "xdr_data")
    cleaned_data = fetch_data(engine, "xdr_cleaned_data")
    
    
    st.write("    create_top_10_handsets_chart(data)
    st.write("    create_top_3_manufacturers_chart(data)

    st.write("    create_top_5_handsets_per_manufacturer_chart(data)
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from other_module import fetch_data, create_conn
"""
    function that plot top 10 handsets type and it's count
"""
def create_top_10_handsets_chart(data):
    top_10_handsets = data['Handset Type'].value_counts().head(10)
    fig = px.bar(top_10_handsets, x=top_10_handsets.index, y=top_10_handsets.values, labels={'x': 'Handset Type', 'y': 'Count'})
    fig.update_layout(title='Top 10 Handsets')
    st.plotly_chart(fig)
"""
    function that plot top 3 manufacturers with it's count 
"""
def create_top_3_manufacturers_chart(data):
    top_3_manufactures = data['Handset Manufacturer'].value_counts().head(3)
    fig = px.bar(top_3_manufactures, x=top_3_manufactures.index, y=top_3_manufactures.values, labels={'x': 'Manufacturer', 'y': 'Count'})
    fig.update_layout(title='Top 3 Handset Manufacturers')
    st.plotly_chart(fig)
"""
    function that plot top 5 handsets per manufacturer with it's type
"""
def create_top_5_handsets_per_manufacturer_chart(data):
    top_3_manufacturers = data['Handset Manufacturer'].value_counts().head(3).index
    filtered_data = data[data['Handset Manufacturer'].isin(top_3_manufacturers)]

    top_5_handsets_per_manufacturer = filtered_data.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')

    fig = go.Figure(data=go.Heatmap(
        z=top_5_handsets_per_manufacturer['Count'],
        x=top_5_handsets_per_manufacturer['Handset Manufacturer'],
        y=top_5_handsets_per_manufacturer['Handset Type'],
        colorscale='Viridis',
        colorbar=dict(title='Count'),
    ))

    fig.update_layout(title='Top 5 Handsets per Manufacturer',
                      xaxis=dict(title='Manufacturer'),
                      yaxis=dict(title='Handset Type'))

    st.plotly_chart(fig)
from sqlalchemy import create_engine


print(__name__)

def connect_to_postgres():

    database_name = 'telecom'
    table_name= 'xdr_data'

    connection_params = { "host": "localhost", "user": "postgres", "password": "postgres",
                        "port": "5432", "database": database_name}

    engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

    return engine


if __name__ == "__main__":
    connect_to_postgres()
import pandas as pd
from postgres_connection import connect_to_postgres


def get_df():
    
        df = pd.read_csv('data.csv')

    return df
import pandas as pd

from process_data import  get_df

import numpy as np

import seaborn as sns

pd.set_option('display.float_format', lambda x: '%.0f' % x)

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler



df = get_df()






Handset_counts = df['Handset Type'].value_counts()

Handset_counts = Handset_counts .reset_index()

Handset_counts.columns = ['Handset Type', 'Count']

Handset_counts.head(10)

manufacturers_counts = df['Handset Manufacturer'].value_counts()

manufacturers_counts = manufacturers_counts .reset_index()

manufacturers_counts.columns = ['top 3 handset manufacturers', 'Count']

manufacturers_counts.head(3)
df.columns


filtered_df = df[df['Handset Manufacturer'].isin(['Apple', 'Samsung', 'Huawei'])]

filtered_df.iloc[0]

Handset_counts = filtered_df['Handset Type'].value_counts()

Handset_counts = Handset_counts .reset_index()

Handset_counts.columns = ['top 5 handsets', 'Count']

Handset_counts.head(5)








df['Start'] = pd.to_datetime(df['Start'])

df['End'] = pd.to_datetime(df['End'])




df['Session Duration (s)'] = (df['End'] - df['Start']).dt.total_seconds()




applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']




user_aggregated = df.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',  
    'Session Duration (s)': 'sum',  
    'Total DL (Bytes)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    **{f'{app} DL (Bytes)': 'sum' for app in applications},  
    **{f'{app} UL (Bytes)': 'sum' for app in applications}  
})




for app in applications:

    user_aggregated[f'{app} (Total Bytes)'] = user_aggregated[f'{app} DL (Bytes)'] + user_aggregated[f'{app} UL (Bytes)']

    user_aggregated.drop([f'{app} DL (Bytes)', f'{app} UL (Bytes)'], axis=1, inplace=True)




user_aggregated.rename(columns={'Bearer Id': 'Number of xDR sessions'}, inplace=True)




user_aggregated.head(10)
missing_values = df.isnull()

missing_values

missing_values_count = df.isnull().sum()

missing_values_count




non_numeric_columns = df.select_dtypes(exclude=['number']).columns



df_numeric = df.drop(non_numeric_columns, axis=1)



cleaned_data = df_numeric.fillna(df_numeric.mean())


selected_columns = [

    "Dur. (ms)",

    "Avg Bearer TP DL (kbps)",

    "Avg Bearer TP UL (kbps)",

    "Social Media DL (Bytes)",

    "Social Media UL (Bytes)",

    "Google DL (Bytes)",

    "Google UL (Bytes)",

    "Email DL (Bytes)",

    "Email UL (Bytes)",

    "Youtube DL (Bytes)",

    "Youtube UL (Bytes)",

    "Netflix DL (Bytes)",

    "Netflix UL (Bytes)",

    "Gaming DL (Bytes)",

    "Gaming UL (Bytes)",

    "Total UL (Bytes)",

    "Total DL (Bytes)",

    "Other DL (Bytes)",

    "Other UL (Bytes)"

]




displayed_data = cleaned_data[selected_columns]




displayed_data.describe()







quantitative_columns = displayed_data.select_dtypes(include=['number']).columns



dispersion_data = pd.DataFrame(index=quantitative_columns, columns=['Range', 'Variance', 'Standard Deviation', 'IQR'])



for column in quantitative_columns:

    data = displayed_data[column].dropna() 

   

    data_range = data.max() - data.min()

    

    data_variance = data.var()

    

    data_std_dev = data.std()

    

    
    data_iqr = data.quantile(0.75) - data.quantile(0.25)

    

    
    dispersion_data.loc[column] = [data_range, data_variance, data_std_dev, data_iqr]



dispersion_data
import matplotlib.pyplot as plt




numeric_columns = displayed_data.select_dtypes(include='number').columns



for column in numeric_columns:

    plt.figure(figsize=(8, 5))

    plt.hist(df[column].dropna(), bins=30, color='skyblue', edgecolor='black')

    plt.title(f'Histogram of {column}')

    plt.xlabel(column)

    plt.ylabel('Frequency')

    plt.show()


import seaborn as sns

import matplotlib.pyplot as plt




applications = [

    'Social Media',

    'Google',

    'Email',

    'Youtube',

    'Netflix',

    'Gaming',

    'Other',

]




cleaned_data['Total Data (DL+UL)'] = cleaned_data['Total UL (Bytes)'] + cleaned_data['Total DL (Bytes)']




plt.figure(figsize=(14, 10))

for app in applications:

    sns.scatterplot(x=cleaned_data[app + ' DL (Bytes)'], y=cleaned_data[app + ' UL (Bytes)'], label=app)



plt.xlabel('DL Data (Bytes)')

plt.ylabel('UL Data (Bytes)')

plt.title('Scatter Plot of DL vs. UL Data for Each Application')

plt.legend()

plt.show()



df['Start'] = pd.to_datetime(df['Start'])

df['End'] = pd.to_datetime(df['End'])




df['Session Duration (s)'] = (df['End'] - df['Start']).dt.total_seconds()




df['Total Data (DL+UL)'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']




df['Duration Decile'] = pd.qcut(df.groupby('MSISDN/Number')['Session Duration (s)'].transform('sum'), q=10, labels=False, duplicates='drop')




decile_data = df.groupby('Duration Decile')['Total Data (DL+UL)'].sum().reset_index()




decile_data = decile_data.sort_values(by='Total Data (DL+UL)', ascending=False)




print(decile_data)



columns_of_interest = [

    'Social Media DL (Bytes)',

    'Google DL (Bytes)',

    'Email DL (Bytes)',

    'Youtube DL (Bytes)',

    'Netflix DL (Bytes)',

    'Gaming DL (Bytes)',

    'Other DL (Bytes)',

    'Social Media UL (Bytes)',

    'Google UL (Bytes)',

    'Email UL (Bytes)',

    'Youtube UL (Bytes)',

    'Netflix UL (Bytes)',

    'Gaming UL (Bytes)',

    'Other UL (Bytes)',

]




correlation_data = cleaned_data[columns_of_interest]




correlation_matrix = correlation_data.corr()






correlation_matrix

app_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

               'Google DL (Bytes)', 'Google UL (Bytes)',

               'Email DL (Bytes)', 'Email UL (Bytes)',

               'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

               'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

               'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

               'Other DL (Bytes)', 'Other UL (Bytes)']




for app in ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other', 'Total']:

    cleaned_data[f'{app} Total Bytes'] = cleaned_data[f'{app} DL (Bytes)'] + cleaned_data[f'{app} UL (Bytes)']




total_bytes_columns = [f'{app} Total Bytes' for app in ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']]

total_bytes_data = cleaned_data[total_bytes_columns]




corr_matrix_total_bytes = total_bytes_data.corr()




plt.figure(figsize=(12, 10))

sns.heatmap(corr_matrix_total_bytes, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

plt.title('Correlation Matrix for Total Bytes')

plt.show()






all_columns_for_pca = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                       'Google DL (Bytes)', 'Google UL (Bytes)',

                       'Email DL (Bytes)', 'Email UL (Bytes)',

                       'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                       'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                       'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                       'Other DL (Bytes)', 'Other UL (Bytes)',

                       'Total DL (Bytes)', 'Total UL (Bytes)',

                       'Avg RTT DL (ms)', 'Avg RTT UL (ms)',

                       'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                       'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',

                       'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)',

                       '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)',

                       'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)',

                       '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)',

                       'HTTP DL (Bytes)', 'HTTP UL (Bytes)',

                       'Activity Duration DL (ms)', 'Activity Duration UL (ms)',

                       'Nb of sec with 125000B < Vol DL', 'Nb of sec with 1250B < Vol UL < 6250B',

                       'Nb of sec with 31250B < Vol DL < 125000B', 'Nb of sec with 37500B < Vol UL',

                       'Nb of sec with 6250B < Vol DL < 31250B', 'Nb of sec with 6250B < Vol UL < 37500B',

                       'Nb of sec with Vol DL < 6250B', 'Nb of sec with Vol UL < 1250B']




data_for_pca = cleaned_data[all_columns_for_pca]




scaler = StandardScaler()

scaled_data = scaler.fit_transform(data_for_pca)




pca = PCA(10)

principal_components = pca.fit_transform(scaled_data)




explained_variance_ratio = pca.explained_variance_ratio_




plt.figure(figsize=(12, 6))

plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.8, align='center')

plt.step(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), where='mid')

plt.title('Explained Variance vs. Number of Principal Components')

plt.xlabel('Number of Principal Components')

plt.ylabel('Explained Variance Ratio')

plt.show()



principal_components  = pd.DataFrame(principal_components)

principal_components
! pip install scikit-learn
import streamlit as st
from streamlit.logger import get_logger
from process_data import get_df
import matplotlib.pyplot as plt
import seaborn as sns 


LOGGER = get_logger(__name__)
df = get_df()

df.to_csv('data.csv')
def run():
    st.set_page_config(
        page_title="10Academy",
        page_icon="👋",
    )

    st.write("
    Handset_counts = df['Handset Type'].value_counts()
    Handset_counts = Handset_counts .reset_index()
    Handset_counts.columns = ['Handset Type', 'Count']
    manufacturers_counts = df['Handset Manufacturer'].value_counts()
    manufacturers_counts = manufacturers_counts .reset_index()
    manufacturers_counts.columns = ['top 3 handset manufacturers', 'Count']
    filtered_df = df[df['Handset Manufacturer'].isin(['Apple', 'Samsung', 'Huawei'])]
    Handset_counts = filtered_df['Handset Type'].value_counts()
    Handset_counts = Handset_counts .reset_index()
    Handset_counts.columns = ['top 5 handsets', 'Count']
    numeric_columns = df.select_dtypes(include=['number']).columns


    for column in numeric_columns:
                fig, ax = plt.subplots(figsize=(8, 5))

                ax.hist(df[column].dropna(), bins=30, color='skyblue', edgecolor='black')

                ax.set_title(f'Histogram of {column}')
        ax.set_xlabel(column)
        ax.set_ylabel('Frequency')

                st.pyplot(fig)

    


   




if __name__ == "__main__":
    run()
import pandas as pd

from process_data import  get_df

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans



pd.set_option('display.float_format', lambda x: '%.0f' % x)



df = get_df()

df






analytics_columns = [

    'MSISDN/Number',

    'Avg RTT DL (ms)',

    'Avg RTT UL (ms)',

    'TCP DL Retrans. Vol (Bytes)',

    'TCP UL Retrans. Vol (Bytes)',

    'Handset Type',



    
    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

]



num_columns = [

    'Avg RTT DL (ms)',

    'Avg RTT UL (ms)',

    'TCP DL Retrans. Vol (Bytes)',

    'TCP UL Retrans. Vol (Bytes)',

    
    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

]



df = df[analytics_columns]


df_cleaned_handset=df.dropna(subset=['Handset Type'])



df_cleaned_handset 


lower_percentile = 1

upper_percentile = 99



lower_bounds = df_cleaned_handset[num_columns].quantile(lower_percentile / 100)

upper_bounds = df_cleaned_handset[num_columns].quantile(upper_percentile / 100)



for col in num_columns:

    outliers = (df_cleaned_handset[col] < lower_bounds[col]) | (df_cleaned_handset[col] > upper_bounds[col])

    if col  == 'Avg Bearer TP DL (kbps)':

    

     df_cleaned_handset[col] = df_cleaned_handset[col].mask(outliers, df_cleaned_handset[col].mean())



df_cleaned_handset




df_clean_nan = df_cleaned_handset.copy()

df_clean_nan[num_columns] = df_clean_nan[num_columns].fillna(df_clean_nan[num_columns].mean())

df_clean_nan



df_clean_nan['Sum RTT'] = df_clean_nan['Avg RTT DL (ms)'] + df_clean_nan['Avg RTT UL (ms)']

df_clean_nan['Sum TCP Retrans. Vol (Bytes)'] = df_clean_nan['TCP DL Retrans. Vol (Bytes)'] + df_clean_nan['TCP UL Retrans. Vol (Bytes)']

df_clean_nan['Sum Bearer TP'] = df_clean_nan['Avg Bearer TP DL (kbps)'] + df_clean_nan['Avg Bearer TP UL (kbps)']




df_clean_nan = df_clean_nan.rename(columns={

    'Avg RTT DL (ms)': 'Avg RTT DL',

    'Avg RTT UL (ms)': 'Avg RTT UL',

    'TCP DL Retrans. Vol (Bytes)': 'TCP Retrans. Vol DL',

    'TCP UL Retrans. Vol (Bytes)': 'TCP Retrans. Vol UL',

    'Avg Bearer TP DL (kbps)': 'Avg Bearer TP DL',

    'Avg Bearer TP UL (kbps)': 'Avg Bearer TP UL',

})



df_clean_nan

important_columns = [

    'MSISDN/Number',

    'Handset Type',

    'Sum RTT',

    'Sum TCP Retrans. Vol (Bytes)',

    'Sum Bearer TP'

]



num_important_columns  = [

    'Sum RTT',

    'Sum TCP Retrans. Vol (Bytes)',

    'Sum Bearer TP'

]



df_important = df_clean_nan[important_columns]

df_important








num_important_columns = [

    'Sum RTT',

    'Sum TCP Retrans. Vol (Bytes)',

    'Sum Bearer TP'

]




grouped_df = df_important.groupby('MSISDN/Number')[num_important_columns].sum().reset_index()




grouped_df.head(10)


top_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].nlargest(10)

bottom_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].nsmallest(10)

most_frequent_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].mode()



print("Top TCP Values:")

print(top_tcp_values)



print("\nBottom TCP Values:")

print(bottom_tcp_values)



print("\nMost Frequent TCP Values:")

print(most_frequent_tcp_values)






top_rtt_values = df_important['Sum RTT'].nlargest(10)

bottom_rtt_values = df_important['Sum RTT'].nsmallest(10)

most_frequent_rtt_values = df_important['Sum RTT'].mode()



print("Top RTT Values:")

print(top_rtt_values)



print("\nBottom RTT Values:")

print(bottom_rtt_values)



print("\nMost Frequent RTT Values:")

print(most_frequent_rtt_values)




top_throughput_values = df_important['Sum Bearer TP'].nlargest(10)

bottom_throughput_values = df_important['Sum Bearer TP'].nsmallest(10)

most_frequent_throughput_values = df_important['Sum Bearer TP'].mode()



print("Top Throughput Values:")

print(top_throughput_values)



print("\nBottom Throughput Values:")

print(bottom_throughput_values)



print("\nMost Frequent Throughput Values:")

print(most_frequent_throughput_values)




average_throughput_per_handset = df_important.groupby('Handset Type')['Sum Bearer TP'].mean().reset_index()

print("Distribution of Average Throughput per Handset Type:")

print(average_throughput_per_handset)




avg_tcp_r_per_handset = df_important.groupby('Handset Type')['Sum TCP Retrans. Vol (Bytes)'].mean().reset_index()

print("Distribution of Average TCP Retransmission per Handset Type:")

avg_tcp_r_per_handset








experience_metrics = [

    'Sum RTT',

    'Sum TCP Retrans. Vol (Bytes)',

    'Sum Bearer TP',

]




df_cluster = grouped_df[experience_metrics]




scaler = StandardScaler()

scaled_data = scaler.fit_transform(df_cluster)




kmeans = KMeans(n_clusters=3, random_state=42)

df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)




cluster_means = df_cluster.groupby('Cluster').mean()

print(cluster_means)




experience_metrics = [

    'Sum RTT',

    'Sum TCP Retrans. Vol (Bytes)',

    'Sum Bearer TP',

]




df_cluster = grouped_df[experience_metrics].dropna()




scaler = StandardScaler()

scaled_data = scaler.fit_transform(df_cluster)




kmeans = KMeans(n_clusters=3, random_state=42)

df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)




sns.set(style="whitegrid")




sns.pairplot(df_cluster, hue="Cluster", palette="Set1", height=3, diag_kind="kde")

plt.show()


experience_metrics = [

    'Sum RTT',

    'Sum TCP Retrans. Vol (Bytes)',

    'Sum Bearer TP',

]




df_cluster = grouped_df[experience_metrics].dropna()




scaler = StandardScaler()

scaled_data = scaler.fit_transform(df_cluster)




kmeans = KMeans(n_clusters=3, random_state=42)

df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)




fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



scatter = ax.scatter(

    df_cluster['Sum RTT'],

    df_cluster['Sum TCP Retrans. Vol (Bytes)'],

    df_cluster['Sum Bearer TP'],

    c=df_cluster['Cluster'],

    cmap='viridis',

    s=50,

    alpha=0.6,

    edgecolors='w'

)



ax.set_xlabel('Sum RTT')

ax.set_ylabel('Sum TCP Retrans. Vol (Bytes)')

ax.set_zlabel('Sum Bearer TP')

ax.set_title('K-Means Clustering of User Experiences')



legend1 = ax.legend(*scatter.legend_elements(), title='Clusters')

ax.add_artist(legend1)



plt.show()
%reload_ext autoreload

%autoreload 2
import os, sys


rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)





import pandas as pd

import numpy as np

from src.dbdata_loader import load_data

from src.dbdata_loader import write_to_sql

import src.data_cleaner as cleaner

import src.plotting as plotting

import matplotlib.pyplot as plt

import seaborn as sns

import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans


df = load_data(credentials_file='env_vars.txt')




df.head()

new_df=df.copy()
new_df.info()


unique_imeis_df = new_df.drop_duplicates(subset='IMEI')



top_handsets = unique_imeis_df['Handset Type'].value_counts().head(10)

print(f'The top 10 handsets used by customers based on unique IMEI values:\n{top_handsets}')




top_manufacturers = new_df['Handset Manufacturer'].value_counts().head(3)

print(f'the top 3 handset manufacturers \n {top_manufacturers}')
top_handsets_per_manufacturer = []




for manufacturer in top_manufacturers.index[:3]:

    
    manufacturer_df = new_df[new_df['Handset Manufacturer'] == manufacturer]

    

    
    unique_imeis_manufacturer_df = manufacturer_df.drop_duplicates(subset='IMEI')

    

    
    top_handsets = unique_imeis_manufacturer_df['Handset Type'].value_counts().head(5)

    

    
    top_handsets_per_manufacturer.append((manufacturer, top_handsets))




for manufacturer, top_handsets in top_handsets_per_manufacturer:

    print(f'The top 5 handsets for {manufacturer}:\n{top_handsets}\n')

clean_df=cleaner.convert_bytes_to_megabytes(new_df,'Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)', 'Total UL (Bytes)', 'Total DL (Bytes)')

new_df=clean_df.copy()

clean_df=cleaner.convert_ms_to_s(new_df,'Start ms','End ms', 'Dur. (ms)','Avg RTT DL (ms)', 'Avg RTT UL (ms)','Activity Duration DL (ms)', 'Activity Duration UL (ms)')

new_df=clean_df.copy()

clean_df= cleaner.convert_kbps_to_mbps(new_df, 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)')

new_df=clean_df.copy()


column_mapping = {'DL TP < 50 Kbps (%)': 'DL TP < 0.05 mbps (%)', '50 Kbps < DL TP < 250 Kbps (%)': '0.05 mbps < DL TP < 0.25 mbps (%)', '250 Kbps < DL TP < 1 Mbps (%)': '0.25 mbps < DL TP < 1 Mbps (%)', 'UL TP < 10 Kbps (%)': 'UL TP < 0.01 mbps (%)', '10 Kbps < UL TP < 50 Kbps (%)': '0.01 mbps < UL TP < 0.05 mbps (%)', '50 Kbps < UL TP < 300 Kbps (%)': '0.05 mbps < UL TP < 0.3 mbps (%)', 'UL TP > 300 Kbps (%)': 'UL TP > 0.3 mbps (%)'}

new_df = new_df.rename(columns=column_mapping)



new_df.columns


grouped_data = new_df.groupby(['MSISDN/Number', 'Bearer Id']).agg({

    'Dur. (s)': 'sum',

    'Social Media DL (MB)': 'sum',

    'Social Media UL (MB)': 'sum',

    'Google DL (MB)': 'sum',

    'Google UL (MB)': 'sum',

    'Email DL (MB)': 'sum',

    'Email UL (MB)': 'sum',

    'Youtube DL (MB)': 'sum',

    'Youtube UL (MB)': 'sum',

    'Netflix DL (MB)': 'sum',

    'Netflix UL (MB)': 'sum',

    'Gaming DL (MB)': 'sum',

    'Gaming UL (MB)': 'sum',

    'Other DL (MB)': 'sum',

    'Other UL (MB)': 'sum',

    'Total UL (MB)': 'sum',

    'Total DL (MB)': 'sum'

}).reset_index()




grouped_data['Number of xDR sessions'] = grouped_data.groupby('MSISDN/Number')['Bearer Id'].transform('nunique')




grouped_data = grouped_data.drop_duplicates(subset='MSISDN/Number')




selected_columns = [

    'MSISDN/Number', 'Bearer Id', 'Number of xDR sessions', 'Dur. (s)',

    'Social Media DL (MB)', 'Social Media UL (MB)',

    'Google DL (MB)', 'Google UL (MB)',

    'Email DL (MB)', 'Email UL (MB)',

    'Youtube DL (MB)', 'Youtube UL (MB)',

    'Netflix DL (MB)', 'Netflix UL (MB)',

    'Gaming DL (MB)', 'Gaming UL (MB)',

    'Other DL (MB)', 'Other UL (MB)',

    'Total UL (MB)', 'Total DL (MB)',

]




beh_selected_data = grouped_data[selected_columns]




beh_selected_data = beh_selected_data.rename(columns={'Dur. (s)': 'Session duration (s)'})



beh_selected_data.head()
cleaner.percent_missing(beh_selected_data)
cleaner.missing_values_table(beh_selected_data)

beh_selected_data.describe()

beh_selected_data.skew()

beh_selected_data.kurt()

beh_selected_data['Number of xDR sessions'].min()
beh_selected_data.columns


plotting.plot_hist(beh_selected_data.head(100), "Number of xDR sessions", "brown")

plotting.plot_hist(beh_selected_data.head(100), "Session duration (s)", "orange")

plotting.plot_hist(beh_selected_data.head(100), "Social Media DL (MB)", "green")

plotting.plot_hist(beh_selected_data.head(100), "Social Media UL (MB)", "blue")

plotting.plot_hist(beh_selected_data.head(100), "Google DL (MB)", "green")

plotting.plot_hist(beh_selected_data.head(100), "Google UL (MB)", "blue")

plotting.plot_hist(beh_selected_data.head(100), "Email DL (MB)", "green")

plotting.plot_hist(beh_selected_data.head(100), "Email UL (MB)", "blue")

plotting.plot_hist(beh_selected_data.head(100), "Youtube DL (MB)", "green")

plotting.plot_hist(beh_selected_data.head(100), "Youtube UL (MB)", "blue")

plotting.plot_hist(beh_selected_data.head(100), "Netflix DL (MB)", "green")

plotting.plot_hist(beh_selected_data.head(100), "Netflix UL (MB)", "blue")

plotting.plot_hist(beh_selected_data.head(100), "Gaming DL (MB)", "green")

plotting.plot_hist(beh_selected_data.head(100), "Gaming UL (MB)", "blue")

plotting.plot_hist(beh_selected_data.head(100), "Other DL (MB)", "green")

plotting.plot_hist(beh_selected_data.head(100), "Other UL (MB)", "blue")

plotting.plot_hist(beh_selected_data.head(100), "Total DL (MB)", "
plotting.plot_hist(beh_selected_data.head(100), "Total UL (MB)", "
plotting.plot_box(beh_selected_data.head(1000), "Number of xDR sessions", "Number of xDR sessions Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Session duration (s)", "Session duration (s) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Social Media DL (MB)", "Social Media DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Social Media UL (MB)", "Social Media UL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Google DL (MB)", "Google DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Google UL (MB)", "Google UL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Email DL (MB)", "Email DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Email UL (MB)", "Email UL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Youtube DL (MB)", "Youtube DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Youtube UL (MB)", "Youtube UL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Netflix DL (MB)", "Netflix DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Netflix UL (MB)", "Netflix UL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Gaming DL (MB)", "Gaming DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Gaming UL (MB)", "Gaming UL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Other DL (MB)", "Other DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Other UL (MB)", "Other UL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Total DL (MB)", "Total DL (MB) Outliers")

plotting.plot_box(beh_selected_data.head(1000), "Total UL (MB)", "Total UL (MB) Outliers")

beh_selected_data.columns
beh_selected_data["Number of xDR sessions"].max()
columns_to_fix = ['Number of xDR sessions',

       'Session duration (s)', 'Social Media DL (MB)', 'Social Media UL (MB)',

       'Google DL (MB)', 'Google UL (MB)', 'Email DL (MB)', 'Email UL (MB)',

       'Youtube DL (MB)', 'Youtube UL (MB)', 'Netflix DL (MB)',

       'Netflix UL (MB)', 'Gaming DL (MB)', 'Gaming UL (MB)', 'Other DL (MB)',

       'Other UL (MB)', 'Total UL (MB)', 'Total DL (MB)']

beh_selected_data = cleaner.fix_multi_outliers(beh_selected_data, columns_to_fix)
beh_selected_data["Number of xDR sessions"].max()
beh_selected_data.describe()


plotting.plot_box(beh_selected_data.head(100), "Number of xDR sessions", "Number of xDR sessions Outliers")

beh_selected_data.columns
plotting.plot_scatter(beh_selected_data.head(1000), x_col="Social Media DL (MB)", y_col="Total DL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Social Media DL (MB) vs Total DL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Social Media UL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Social Media UL (MB) vs Total UL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Google DL (MB)", y_col="Total DL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Google DL (MB) vs Total DL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Google UL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Google UL (MB) vs Total UL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Email DL (MB)", y_col="Total DL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Email DL (MB) vs Total DL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Email UL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Email UL (MB) vs Total UL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Youtube DL (MB)", y_col="Total DL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Youtube DL (MB) vs Total DL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Youtube UL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Youtube DL (MB) vs Total UL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Netflix DL (MB)", y_col="Total DL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Netflix DL (MB) vs Total DL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Netflix UL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Netflix UL (MB) vs Total UL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Gaming DL (MB)", y_col="Total DL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Gaming DL (MB) vs Total DL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Gaming UL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Gaming UL (MB) vs Total UL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Other DL (MB)", y_col="Total DL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Other DL (MB) vs Total DL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Other UL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Other UL (MB) Media UL (MB) vs Total UL (MB)")



plotting.plot_scatter(beh_selected_data.head(1000), x_col="Total DL (MB)", y_col="Total UL (MB)", hue="Session duration (s)",

             style="Number of xDR sessions", title="Total DL (MB) Media UL (MB) vs Total UL (MB)")
beh_selected_data.columns

total_duration_per_user = beh_selected_data.groupby('MSISDN/Number')['Session duration (s)'].sum()




total_data_columns = ['Social Media DL (MB)', 'Social Media UL (MB)',

                       'Google DL (MB)', 'Google UL (MB)',

                       'Email DL (MB)', 'Email UL (MB)',

                       'Youtube DL (MB)', 'Youtube UL (MB)',

                       'Netflix DL (MB)', 'Netflix UL (MB)',

                       'Gaming DL (MB)', 'Gaming UL (MB)',

                       'Other DL (MB)', 'Other UL (MB)',

                       'Total UL (MB)', 'Total DL (MB)']



beh_selected_data['Total Data (MB)'] = beh_selected_data[total_data_columns].sum(axis=1)




user_data = pd.DataFrame({

    'MSISDN/Number': total_duration_per_user.index,

    'Total Duration (s)': total_duration_per_user.values,

    'Total Data (MB)': beh_selected_data.groupby('MSISDN/Number')['Total Data (MB)'].sum().values

})




user_data['Duration Decile'] = pd.qcut(user_data['Total Duration (s)'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False, duplicates='drop')

user_data['Data Decile'] = pd.qcut(user_data['Total Data (MB)'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False, duplicates='drop')




user_data['Decile'] = user_data.apply(lambda row: f"Decile {row['Duration Decile']*20 + 1} ({row['Duration Decile']*20}% - {(row['Duration Decile']+1)*20}%)", axis=1)




decile_data = user_data.groupby('Decile').agg({

    'MSISDN/Number': 'count',

    'Total Data (MB)': 'sum'

}).reset_index()




decile_data = decile_data.rename(columns={'MSISDN/Number': 'Number of Users', 'Total Data (MB)': 'Total Data (DL+UL) per Decile (MB)'})




decile_data.set_index('Decile', inplace=True)



decile_data.head()

correlation_columns = [

    'Social Media DL (MB)', 'Social Media UL (MB)',

       'Google DL (MB)', 'Google UL (MB)', 'Email DL (MB)', 'Email UL (MB)',

       'Youtube DL (MB)', 'Youtube UL (MB)', 'Netflix DL (MB)',

       'Netflix UL (MB)', 'Gaming DL (MB)', 'Gaming UL (MB)', 'Other DL (MB)',

       'Other UL (MB)'

]




correlation_data = beh_selected_data[correlation_columns]




correlation_matrix = correlation_data.corr()




print("Correlation Matrix:")

print(correlation_matrix)




print("\nInterpretation:")

print("The correlation matrix shows the correlation coefficients between different data usage categories.")

print("Values closer to 1 indicate a strong positive correlation, values closer to -1 indicate a strong negative correlation, and values near 0 indicate little to no correlation.")
plotting.plot_heatmap(correlation_matrix,"correlation between the different applications")
dfPair = beh_selected_data[['Session duration (s)', 'Social Media DL (MB)', 'Social Media UL (MB)',

       'Google DL (MB)', 'Google UL (MB)', 'Email DL (MB)', 'Email UL (MB)',

       'Youtube DL (MB)', 'Youtube UL (MB)', 'Netflix DL (MB)',

       'Netflix UL (MB)', 'Gaming DL (MB)', 'Gaming UL (MB)', 'Other DL (MB)',

       'Other UL (MB)']]
sns.pairplot(dfPair.head(50), hue ='Session duration (s)', diag_kind = 'kde',

             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},

             height=4)
beh_selected_data.dtypes

cleaner.normalizer(beh_selected_data,columns_to_exclude=['MSISDN/Number','Bearer Id'])
beh_selected_data.describe()
beh_selected_data.columns
new_df.columns

eng_adf = pd.merge(new_df, beh_selected_data[['MSISDN/Number', 'Number of xDR sessions']], 

                       on='MSISDN/Number', how='left')
eng_adf.columns

engagement_metrics = eng_adf.groupby('MSISDN/Number').agg({

    'Number of xDR sessions': 'count',

    'Dur. (s)': 'sum',

    'Total DL (MB)': 'sum',

    'Total UL (MB)': 'sum'

}).reset_index()




top_10_sessions_frequency = engagement_metrics.nlargest(10, 'Number of xDR sessions')[['MSISDN/Number', 'Number of xDR sessions']]

top_10_session_duration = engagement_metrics.nlargest(10, 'Dur. (s)')[['MSISDN/Number', 'Dur. (s)']]

top_10_total_dl_traffic = engagement_metrics.nlargest(10, 'Total DL (MB)')[['MSISDN/Number', 'Total DL (MB)']]

top_10_total_ul_traffic = engagement_metrics.nlargest(10, 'Total UL (MB)')[['MSISDN/Number', 'Total UL (MB)']]




print("Top 10 Customers by Sessions Frequency:")

print(top_10_sessions_frequency)



print("\nTop 10 Customers by Session Duration:")

print(top_10_session_duration)



print("\nTop 10 Customers by Total DL Traffic:")

print(top_10_total_dl_traffic)



print("\nTop 10 Customers by Total UL Traffic:")

print(top_10_total_ul_traffic)
engagement_metrics.head()

unnormalized_engagement_metrics = engagement_metrics.copy()




engagement_metrics_columns = ['Number of xDR sessions', 'Dur. (s)', 'Total DL (MB)', 'Total UL (MB)']




normalized_engagement_metrics = cleaner.normalizer(engagement_metrics, columns_to_exclude=['MSISDN/Number'])




normalized_engagement_metrics_data = normalized_engagement_metrics[engagement_metrics_columns]




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_clusters = kmeans.fit_predict(normalized_engagement_metrics_data)




engagement_metrics['Engagement Cluster'] = engagement_clusters




print("Unnormalized Engagement Metrics:")

print(unnormalized_engagement_metrics)



print("\nNormalized Engagement Metrics with Clusters:")

print(engagement_metrics[['MSISDN/Number', 'Number of xDR sessions', 'Dur. (s)', 'Total DL (MB)', 'Total UL (MB)', 'Engagement Cluster']])
engagement_metrics.head()
normalized_engagement_metrics.head()

cluster_metrics = engagement_metrics.groupby('Engagement Cluster').agg({

    'Number of xDR sessions': ['min', 'max', 'mean', 'sum'],

    'Dur. (s)': ['min', 'max', 'mean', 'sum'],

    'Total DL (MB)': ['min', 'max', 'mean', 'sum'],

    'Total UL (MB)': ['min', 'max', 'mean', 'sum']

}).reset_index()




print("Metrics for Each Cluster:")

print(cluster_metrics)
cleaner.missing_values_table(eng_adf)

ceng_adf=eng_adf.copy()

ceng_adf = ceng_adf.dropna(subset=['MSISDN/Number'])

eng_adf=ceng_adf.copy()




user_columns = ['MSISDN/Number']

applications_columns = [

    'Social Media', 'Google', 'Email', 'Youtube',

    'Netflix', 'Gaming', 'Other'

]




total_traffic_columns = [f"{app} Total (MB)" for app in applications_columns]




total_traffic_per_application = pd.DataFrame()



for app in applications_columns:

    dl_column = f"{app} DL (MB)"

    ul_column = f"{app} UL (MB)"

    

    
    total_traffic_per_application[f"{app} Total (MB)"] = eng_adf[dl_column] + eng_adf[ul_column]




total_traffic_per_application = pd.concat([eng_adf['MSISDN/Number'], total_traffic_per_application], axis=1)




print("Total Traffic per Application per User:")

print(total_traffic_per_application)



total_traffic_per_application.columns

traffic_columns = ['Social Media Total (MB)', 'Google Total (MB)', 'Email Total (MB)',

                    'Youtube Total (MB)', 'Netflix Total (MB)', 'Gaming Total (MB)', 'Other Total (MB)']




top_10_tables = {}



for column in traffic_columns:

    
    top_10_users = total_traffic_per_application.nlargest(10, column)[['MSISDN/Number', column]]

    

    
    top_10_tables[column] = top_10_users




for column, table in top_10_tables.items():

    print(f"\nTop 10 users for '{column}':")

    print(table)


total_usage = total_traffic_per_application[traffic_columns].sum()




top_3_applications = total_usage.nlargest(3)







plt.figure(figsize=(10, 6))




top_3_applications.plot(kind='bar', color='skyblue')




plt.title('Top 3 Most Used Applications')

plt.xlabel('Application')

plt.ylabel('Total Data Usage (MB)')




plt.show()




engagement_columns = ['Social Media Total (MB)', 'Google Total (MB)', 'Email Total (MB)',

                       'Youtube Total (MB)', 'Netflix Total (MB)', 'Gaming Total (MB)', 'Other Total (MB)']




engagement_data = total_traffic_per_application[engagement_columns]




inertia = []

for k in range(1, 11):  
    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(engagement_data)

    inertia.append(kmeans.inertia_)




plt.figure(figsize=(8, 6))

plt.plot(range(1, 11), inertia, marker='o')

plt.title('Elbow Method for Optimal k')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Sum of Squared Distances (Inertia)')

plt.show()

normalized_engagement_metrics_data = total_traffic_per_application[engagement_columns]




inertia = []

for k in range(1, 11):  
    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(engagement_data)

    inertia.append(kmeans.inertia_)




plt.figure(figsize=(8, 6))

plt.plot(range(1, 11), inertia, marker='o')

plt.title('Elbow Method for Optimal k')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Sum of Squared Distances (Inertia)')

plt.show()
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler



def percent_missing(df):

        totalCells = np.product(df.shape)

        missingCount = df.isnull().sum()

        totalMissing = missingCount.sum()

        print("The dataset contains", round(
        ((totalMissing/totalCells) * 100), 2), "%", "missing values.")


def missing_values_table(df):
        mis_val = df.isnull().sum()

        mis_val_percent = 100 * df.isnull().sum() / len(df)

        mis_val_dtype = df.dtypes

        mis_val_table = pd.concat(
        [mis_val, mis_val_percent, mis_val_dtype], axis=1)

        mis_val_table_ren_columns = mis_val_table.rename(
        columns={0: 'Missing Values', 1: '% of Total Values', 2: 'Dtype'})

        mis_val_table_ren_columns = mis_val_table_ren_columns[
        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

        print("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
          "There are " + str(mis_val_table_ren_columns.shape[0]) +
          " columns that have missing values.")

        return mis_val_table_ren_columns


def drop_columns_by_missing_percentage(df, percentage):
    """
    Drop columns in the DataFrame based on the given percentage threshold of missing values.

    Parameters:
    - df: DataFrame
        The input DataFrame.
    - percentage: float
        The threshold percentage for dropping columns.

    Returns:
    - df_cleaned: DataFrame
        The DataFrame with columns dropped based on the threshold.
    """
        missing_values_info = missing_values_table(df)

        columns_to_drop = missing_values_info[missing_values_info['% of Total Values']
                                          > percentage].index

        df_cleaned = df.drop(columns=columns_to_drop)

        print(
        f"\nDropped {len(columns_to_drop)} columns based on the {percentage}% missing value threshold.")
    print("Remaining columns:", df_cleaned.shape[1])

    return df_cleaned


def fill_missing_values(df, column_names, fill_method='mean'):
    """
    Fill missing values in specified columns of the DataFrame based on the provided fill method.

    Parameters:
    - df: DataFrame
        The input DataFrame.
    - column_names: list
        List of column names with missing values to fill.
    - fill_method: str
        The fill method to use (default is 'mean'). Options: 'mean', 'median', 'mode', 'bfill', 'ffill', or a specific value.

    Returns:
    - df_filled: DataFrame
        The DataFrame with missing values filled in the specified columns.
    """
        df_filled = df.copy()

        if fill_method == 'mean':
        fill_values = df_filled[column_names].mean()
    elif fill_method == 'median':
        fill_values = df_filled[column_names].median()
    elif fill_method == 'mode':
        fill_values = df_filled[column_names].mode().iloc[0]
    elif fill_method == 'bfill':
        fill_values = df_filled[column_names].bfill()
    elif fill_method == 'ffill':
        fill_values = df_filled[column_names].ffill()
    else:
        fill_values = fill_method

        df_filled[column_names] = df_filled[column_names].fillna(fill_values)

        print(
        f"\nFilled missing values in columns {column_names} using '{fill_method}' method.")

    return df_filled


def get_numeric_columns(df):
        numeric_cols = df.select_dtypes(include=['float64']).columns
    return list(numeric_cols)


def fix_multi_outliers(df, columns, lower_percentile=0.5, upper_percentile=95):
        df = df.copy()

        for col in columns:
                if col not in ["Bearer Id", "IMSI", "MSISDN/Number", "IMEI"]:
                        lower_threshold = df[col].quantile(lower_percentile / 100)
            upper_threshold = df[col].quantile(upper_percentile / 100)

                        df.loc[df[col] < lower_threshold, col] = df[col].median()

                        df.loc[df[col] > upper_threshold, col] = df[col].median()

    return df


def fix_outliers(df, columns):
    for column in columns:
        df[column] = np.where(df[column] > df[column].quantile(
            0.95), df[column].median(), df[column])
    return df[columns]


def convert_bytes_to_megabytes(df, *bytes_columns):
    """
    Convert specified columns from bytes to megabytes and rename columns.

    Parameters:
    - df: DataFrame
        The input DataFrame.
    - *bytes_columns: str
        Variable-length list of column names with bytes values.

    """
        df_result = df.copy()

    megabyte = 1 * 10e+5  
        for col in bytes_columns:
                df_result[col] = df_result[col] / megabyte

                new_col_name = col.replace('Bytes', 'MB')
        df_result.rename(columns={col: new_col_name}, inplace=True)

    return df_result


def convert_ms_to_s(df, *ms_columns):
    """
    Convert specified columns from milliseconds to seconds and rename columns.

    Parameters:
    - df: DataFrame
        The input DataFrame.
    - *ms_columns: str
        Variable-length list of column names with milliseconds values.

    """
        df_result = df.copy()

        for col in ms_columns:
                df_result[col] = df_result[col] / 1000

                new_col_name = col.replace('ms', 's')
        df_result.rename(columns={col: new_col_name}, inplace=True)

    return df_result


def convert_kbps_to_mbps(df, *kbps_columns):
    """
    Convert specified columns from kilobits per second (kbps) to megabits per second (mbps)
    and rename columns.

    Parameters:
    - df: DataFrame
        The input DataFrame.
    - *kbps_columns: str
        Variable-length list of column names with kbps values.

    Returns:
    - DataFrame
        A new DataFrame with specified columns converted to mbps and renamed.
    """
        df_result = df.copy()

        for col in kbps_columns:
                df_result[col] = df_result[col] / 1000

                new_col_name = col.replace('kbps', 'mbps')
        df_result.rename(columns={col: new_col_name}, inplace=True)

    return df_result


def normalizer(df, columns_to_exclude=[]):
        normalized_df = df.copy()

        columns_to_normalize = [
        col for col in df.columns if col not in columns_to_exclude]

        if columns_to_normalize:
        minmax_scaler = MinMaxScaler()
        normalized_values = minmax_scaler.fit_transform(
            df[columns_to_normalize])

                normalized_df[columns_to_normalize] = normalized_values

                print("Min Value: ", normalized_values.min())
        print("Max value: ", normalized_values.max())

    return normalized_df
import pandas as pd
from sqlalchemy import create_engine
import os


def load_data(credentials_file='env_vars.txt', database_name='telecom', table_name='xdr_data'):
        script_dir = os.path.dirname(os.path.abspath(__file__))

        credentials_file_path = os.path.join(script_dir, '..', credentials_file)

        user, password, host, port = read_db_credentials(credentials_file_path)

        connection_params = {"host": host, "user": user,
                         "password": password, "port": port, "database": database_name}
    engine = create_engine(
        f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

        sql_query = f'SELECT * FROM {table_name}'

        df = pd.read_sql(sql_query, con=engine)

    return df


def write_to_sql(df, table_name, credentials_file='env_vars.txt', database_name='telecom'):
        script_dir = os.path.dirname(__file__)

        credentials_file_path = os.path.join(script_dir, '..', credentials_file)

        user, password, host, port = read_db_credentials(credentials_file_path)

        connection_params = {"host": host, "user": user,
                         "password": password, "port": port, "database": database_name}
    engine = create_engine(
        f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

        df.to_sql(name=table_name, con=engine, index=False, if_exists='replace')


def read_db_credentials(credentials_file):
        with open(credentials_file, 'r') as file:
        lines = file.readlines()

        user = lines[0].strip().split('=')[1]
    password = lines[1].strip().split('=')[1]
    host = lines[2].strip().split('=')[1]
    port = lines[3].strip().split('=')[1]

    return user, password, host, port
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

def set_seaborn_style():
    """
    Set a custom Seaborn style.
    """
    sns.set(style="whitegrid")

def plot_histogram_kde(data, title, x_label, y_label, color='skyblue'):
    """
    Plot a histogram with KDE.

    Parameters:
    - data: Series or array-like
        Data to be plotted.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - y_label: str
        Label for the y-axis.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(10, 6))
    sns.histplot(data, kde=True, color=color, edgecolor='black')
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.ylabel(y_label, fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def plot_boxplot(data, title, x_label, color='lightcoral'):
    """
    Plot a boxplot.

    Parameters:
    - data: Series or array-like
        Data to be plotted.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=data, color=color)
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def plot_time_series(data, time_column, title, x_label, y_label, color='skyblue'):
    """
    Plot a time series using Matplotlib.

    Parameters:
    - data: DataFrame
        Data containing a time series.
    - time_column: str
        Column representing the time.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - y_label: str
        Label for the y-axis.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(12, 6))
    sns.lineplot(data=data.resample('D').size(), color=color, marker='o')
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.ylabel(y_label, fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def plot_countplot(data, x_column, title, x_label, rotation=45, color='skyblue'):
    """
    Plot a countplot using Seaborn.

    Parameters:
    - data: DataFrame
        Data to be plotted.
    - x_column: str
        Column for the x-axis.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - rotation: int, optional
        Rotation angle for x-axis labels.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(10, 6))
    sns.countplot(x=data[x_column], color=color)
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.xticks(rotation=rotation, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def create_table(table_data):
    """
    Create a table using Plotly.

    Parameters:
    - table_data: DataFrame
        Data for the table.

    Returns:
    - None
    """
    table = go.Figure(data=[go.Table(
        header=dict(values=list(table_data.columns),
                    fill_color='lightblue',
                    align='center',
                    font=dict(color='black', size=14)),
        cells=dict(values=[table_data['Handset Manufacturer'], table_data['Handset Type'], table_data['count']],
                   fill=dict(color=['white', 'lightcyan', 'lightcyan']),
                   align='center',
                   font=dict(color='black', size=12)))
    ])

    table.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=0, b=0))

    table.show()
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as pl

from sqlalchemy import create_engine, text

from scipy.stats import zscore

import psycopg2

import os

import plotly.graph_objects as go

import plotly.express as px
os.chdir('..')
from db.connection import DatabaseConnection

from db.sql_preprocessor import DBFilter

from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot
db_connection = DatabaseConnection()
db_connection.connect()
query = "SELECT * FROM xdr_data"

df = db_connection.execute_query(query)
df.head()
df.columns
missing_values = df.isnull().sum()

print(missing_values)
duplicates = df.duplicated()

print("Number of duplicate rows:", duplicates.sum())
top_handsets = df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')

top_handsets = top_handsets.sort_values(by='count', ascending=False).head(10)
table_data = top_handsets[['Handset Manufacturer', 'Handset Type', 'count']]




table = go.Figure(data=[go.Table(

    header=dict(values=list(table_data.columns),

                fill_color='lightblue',

                align='center',

                font=dict(color='black', size=14)),

    cells=dict(values=[table_data['Handset Manufacturer'], table_data['Handset Type'], table_data['count']],

               fill=dict(color=['lightcyan', 'lightcyan', 'lightcyan']),

               align='center',

               font=dict(color='black', size=12)))

])




table.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=0, b=0))




table.show()
top_handsets = df.query("`Handset Manufacturer` != 'undefined' and `Handset Type` != 'undefined'")[['Handset Manufacturer', 'Handset Type']]
top_handsets = top_handsets.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')

top_handsets = top_handsets.sort_values(by='count', ascending=False).head(10)
top_handsets[['Handset Manufacturer', 'Handset Type', 'count']]

fig = px.bar(top_handsets, x='Handset Type', y='count', color='Handset Manufacturer',

             labels={'count': 'Count', 'Handset Type': 'Handset Type'},

             title='Top Handsets by Manufacturer and Type',

             template='plotly_white',  
             color_discrete_sequence=px.colors.qualitative.Set1)  



fig.update_layout(

    xaxis=dict(title='Handset Type'),

    yaxis=dict(title='Count'),

    legend=dict(title='Manufacturer'),

    barmode='group',

    showlegend=True

)




fig.show()
top_manufacturers = df[df['Handset Manufacturer'] != 'undefined']
top_manufacturers = top_manufacturers['Handset Manufacturer'].value_counts().head(3).reset_index(name='count')
top_manufacturers.columns = ['Handset Manufacturer', 'count']

top_manufacturers
len(top_manufacturers)

total_rows = len(df)

top_manufacturers['percentage'] = (top_manufacturers['count'] / total_rows) * 100




print(top_manufacturers)

total_handsets = df['Handset Type'].count()




print("Total number of handsets:", total_handsets)

top_manufacturers = df[df['Handset Manufacturer'] != 'undefined']
top_manufacturers = top_manufacturers['Handset Manufacturer'].value_counts().head(3).index
filtered_df = df[df['Handset Manufacturer'].isin(top_manufacturers)]
top_handsets_per_manufacturer = (

    filtered_df.groupby(['Handset Manufacturer', 'Handset Type'])

    .size()

    .reset_index(name='count')

    .sort_values(by=['Handset Manufacturer', 'count'], ascending=[True, False])

    .groupby('Handset Manufacturer')

    .head(5)

)
top_handsets_per_manufacturer
import pandas as pd
import numpy as np
import sqlite3

class DBFilter:
    def __init__(self, dataframe):
        self.df = dataframe

    def filter_numeric_columns(self, threshold=0):
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        filtered_df = self.df[numeric_columns].apply(lambda x: x[x > threshold])

        return filtered_df
    
    def load_data_from_db(self, db_path, sql_query):
        connection = sqlite3.connect(db_path)
        df = pd.read_sql_query(sql_query, connection)
        connection.close()
        return df

    def get_unique_values(self, column):
        unique_values = self.df[column].unique()
        return unique_values
    
    def most_repeated_value(self, column):
        return self.df[column].mode().values[0]

    def calculate_average(self, column):
        return self.df[column].mean()

    def close_connection(self):
                pass
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv
import pandas as pd

class DatabaseConnection:
    def __init__(self):
                load_dotenv()

                self.username = os.getenv("DB_USERNAME")
        self.password = os.getenv("DB_PASSWORD")
        self.host = os.getenv("DB_HOST")
        self.port = os.getenv("DB_PORT")
        self.database = os.getenv("DB_DATABASE")

                if None in (self.username, self.password, self.host, self.port, self.database):
            raise ValueError("One or more database credentials are missing.")

        self.connection_url = f"postgresql+psycopg2://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}"
        self.engine = None
        self.connection = None

    def connect(self):
        try:
            self.engine = create_engine(self.connection_url)
            self.connection = self.engine.connect()
            print("Connected to the database.")
        except Exception as e:
            print(f"Error connecting to the database: {str(e)}")

    def execute_query(self, query):
        try:
            df = pd.read_sql_query(query, self.connection)
            return df
        except Exception as e:
            print(f"Error executing query: {str(e)}")
            
    def execute_update_query(self, query):
        try:
            self.connection.execute(query)
            print("Query executed successfully.")
        except Exception as e:
            print(f"Error executing query: {str(e)}")

    def close_connection(self):
        try:
            self.connection.close()
            print("Connection closed.")
        except Exception as e:
            print(f"Error closing connection: {str(e)}")
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from sqlalchemy import create_engine, text

from scipy.stats import zscore

import psycopg2

import os

import plotly.express as px

import random

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

import tabulate

import plotly.graph_objects as go

from scipy.spatial import distance

from sklearn.metrics import pairwise_distances_argmin_min

from functools import reduce

from tabulate import tabulate

from scipy.stats.mstats import winsorize
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.linear_model import Ridge

from sklearn.model_selection import cross_val_score

from sklearn.linear_model import RidgeCV

from sklearn.ensemble import RandomForestRegressor

from sklearn.ensemble import GradientBoostingRegressor
os.chdir('..')
from db.connection import DatabaseConnection

from db.sql_preprocessor import DBFilter

from src.utils import missing_values_table, find_agg, convert_bytes_to_megabytes, fix_outlier, remove_outliers

from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot, create_table
db_connection = DatabaseConnection()

set_seaborn_style()
db_connection.connect()
query = "SELECT * FROM xdr_data"

df = db_connection.execute_query(query)
df.columns

features = ['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']

df['Start'] = pd.to_datetime(df['Start'])

df['End'] = pd.to_datetime(df['End'])




df['Session Duration (ms)'] = (df['End'] - df['Start']).dt.total_seconds() * 1000




df['Total Traffic (Bytes)'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']




user_engagement = df.groupby('MSISDN/Number').agg({

    'Start': 'count',                           
    'Session Duration (ms)': 'mean',           
    'Total Traffic (Bytes)': 'sum'             
})




user_engagement = user_engagement.rename(columns={

    'Start': 'Sessions Frequency',

    'Session Duration (ms)': 'Average Session Duration (ms)',

    'Total Traffic (Bytes)': 'Total Traffic (Bytes)'

})




print(user_engagement)

scaler = StandardScaler()

normalized_engagement = scaler.fit_transform(user_engagement)




kmeans = KMeans(n_clusters=3, random_state=42)

user_engagement['Cluster'] = kmeans.fit_predict(normalized_engagement)




print("Customers Classified into 3 Groups of Engagement:")

print(user_engagement)

kmeans = KMeans(n_clusters=3, random_state=42)

user_engagement['Cluster'] = kmeans.fit_predict(normalized_engagement)




less_engaged_cluster = user_engagement.groupby('Cluster')['Sessions Frequency'].mean().idxmin()




distances = pairwise_distances_argmin_min(normalized_engagement, kmeans.cluster_centers_)[1]




user_engagement.reset_index(inplace=True)




user_engagement['Engagement Score'] = distances if less_engaged_cluster == 0 else -distances

print("Customers with Engagement Scores:")

print(user_engagement[['MSISDN/Number', 'Engagement Score']])

top_10_engaged_customers = user_engagement['Engagement Score'].sort_values(ascending=False).head(10)




top_10_df = pd.DataFrame({'Engagement Score': top_10_engaged_customers.values})




top_10_df.reset_index(inplace=True, drop=True)




fig = px.bar(top_10_df, x=top_10_df.index + 1, y='Engagement Score', title='Top 10 Engaged Customers')




fig.update_layout(xaxis_title='Rank', yaxis_title='Engagement Score')




fig.show()
top_10_engaged_customers

grouped_data_throughput = df.groupby('MSISDN/Number').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




grouped_data_throughput['Avg Bearer TP DL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP DL (kbps)'].mean(), inplace=True)

grouped_data_throughput['Avg Bearer TP UL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP UL (kbps)'].mean(), inplace=True)




print(grouped_data_throughput)
grouped_data_rtt = df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean().reset_index()




grouped_data_rtt['Avg RTT DL (ms)'].fillna(grouped_data_rtt['Avg RTT DL (ms)'].mean(), inplace=True)




mean_value_rtt = grouped_data_rtt['Avg RTT DL (ms)'].mean()

std_dev_rtt = grouped_data_rtt['Avg RTT DL (ms)'].std()




outlier_threshold_rtt = 3




grouped_data_rtt['Avg RTT DL (ms)'] = grouped_data_rtt['Avg RTT DL (ms)'].apply(

    lambda x: mean_value_rtt if abs(x - mean_value_rtt) > outlier_threshold_rtt * std_dev_rtt else x

)




print(grouped_data_rtt)
grouped_data_retrans = df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].fillna(grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].mean(), inplace=True)




mean_value = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].mean()

std_dev = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].std()




outlier_threshold = 3




grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'] = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].apply(

    lambda x: mean_value if abs(x - mean_value) > outlier_threshold * std_dev else x

)




print(grouped_data_retrans)

grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()




grouped_data_tcp = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




merged_data = pd.merge(grouped_data_handset, grouped_data_tcp, on='Handset Type')




print(merged_data)

consolidated_data = pd.merge(grouped_data_handset, grouped_data_retrans, on='MSISDN/Number')

consolidated_data = pd.merge(consolidated_data, grouped_data_rtt, on='MSISDN/Number')

consolidated_data = pd.merge(consolidated_data, grouped_data_throughput, on='MSISDN/Number')

consolidated_data = pd.merge(consolidated_data, user_engagement, on='MSISDN/Number')



print(consolidated_data.dtypes)


features_for_clustering = ['TCP DL Retrans. Vol (Bytes)', 'Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']




consolidated_data.dropna(subset=features_for_clustering, inplace=True)




scaler = StandardScaler()

consolidated_data_scaled = scaler.fit_transform(consolidated_data[features_for_clustering])




n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters, random_state=42)

consolidated_data['cluster'] = kmeans.fit_predict(consolidated_data_scaled)




consolidated_data['euclidean_distance'] = consolidated_data.apply(

    lambda row: distance.euclidean(row[features_for_clustering], kmeans.cluster_centers_[row['cluster']]),

    axis=1

)




consolidated_data['experience_score'] = 1 / (1 + consolidated_data['euclidean_distance'])




print(consolidated_data[['MSISDN/Number', 'experience_score']])


top_10_experience_customers = consolidated_data.sort_values(by='experience_score', ascending=False).head(10)




top_10_experience_df = pd.DataFrame({'experience_score': top_10_experience_customers['experience_score'].values})




top_10_experience_df.reset_index(inplace=True, drop=True)




fig = px.bar(top_10_experience_df, x=top_10_experience_df.index + 1, y='experience_score', title='Top 10 Customers by Experience Score')




fig.update_layout(xaxis_title='Rank', yaxis_title='Experience Score')




fig.show()
top_10_experience_customers = consolidated_data.sort_values(by='experience_score', ascending=False).head(10)




columns_to_include = [

    'MSISDN/Number',

    'experience_score',

]




result_df = top_10_experience_customers[columns_to_include]




print(result_df)
top_10_experience_scores

columns_to_handle_outliers = ['Engagement Score', 'experience_score']




def handle_outliers_iqr(consolidated_data, columns):

    for column in columns:

        Q1 = consolidated_data[column].quantile(0.25)

        Q3 = consolidated_data[column].quantile(0.75)

        IQR = Q3 - Q1

        lower_limit = Q1 - 1.5 * IQR

        upper_limit = Q3 + 1.5 * IQR

        consolidated_data[column] = consolidated_data[column].apply(lambda x: max(lower_limit, min(x, upper_limit)))

    return consolidated_data




consolidated_data = handle_outliers_iqr(consolidated_data, columns_to_handle_outliers)

consolidated_data['satisfaction_score'] = (user_engagement['Engagement Score'] + consolidated_data['experience_score']) / 2




top_satisfied_customers = consolidated_data.sort_values(by='satisfaction_score', ascending=False)




top_10_satisfied_customers = top_satisfied_customers.head(10)

print("Top 10 Satisfied Customers:")

print(top_10_satisfied_customers[['MSISDN/Number', 'satisfaction_score']])

consolidated_data

sorted_data = consolidated_data.sort_values(by='satisfaction_score', ascending=False)




top_10 = sorted_data.head(10)




fig = go.Figure(data=[go.Table(

    header=dict(values=list(top_10.columns),

                fill_color='paleturquoise',

                align='left'),

    cells=dict(values=[top_10[col] for col in top_10.columns],

               fill_color='lavender',

               align='left'))

])




fig.show()

fig1 = px.scatter(consolidated_data, x='satisfaction_score', y='Engagement Score',

                  title='Satisfaction Score vs Engagement Score',

                  labels={'satisfaction_score': 'Satisfaction Score',

                          'engagement_score': 'Engagement Score'})




fig2 = px.scatter(consolidated_data, x='satisfaction_score', y='experience_score',

                  title='Satisfaction Score vs Experience Score',

                  labels={'satisfaction_score': 'Satisfaction Score',

                          'experience_score': 'Experience Score'})




fig3 = px.scatter(consolidated_data, x='Engagement Score', y='experience_score',

                  title='Engagement Score vs Experience Score',

                  labels={'engagement_score': 'Engagement Score',

                          'experience_score': 'Experience Score'})




fig1.show()

fig2.show()

fig3.show()
consolidated_data

max_satisfaction_score = consolidated_data['satisfaction_score'].max()




low_satisfaction_threshold = 0.1







































0 * max_satisfaction_score

moderate_satisfaction_threshold = 0.25 * max_satisfaction_score

high_satisfaction_threshold = 0.50 * max_satisfaction_score




consolidated_data['Satisfaction Level'] = pd.cut(consolidated_data['satisfaction_score'],

                                  bins=[-float('inf'), low_satisfaction_threshold, moderate_satisfaction_threshold, high_satisfaction_threshold, float('inf')],

                                  labels=['Low Satisfied', 'Moderately Satisfied', 'Satisfied', 'Highly Satisfied'])




satisfaction_counts = consolidated_data['Satisfaction Level'].value_counts()




satisfaction_percentage = (satisfaction_counts / len(df)) * 100




fig = px.pie(satisfaction_percentage, names=satisfaction_percentage.index, values=satisfaction_percentage.values,

             title='Percentage Distribution of Satisfaction Levels',

             labels={'index': 'Satisfaction Level', 'values': 'Percentage of Individuals'})




fig.show()

columns_to_handle_outliers = ['Engagement Score', 'experience_score', 'satisfaction_score']




def handle_outliers_iqr(consolidated_data, columns):

    for column in columns:

        Q1 = consolidated_data[column].quantile(0.25)

        Q3 = consolidated_data[column].quantile(0.75)

        IQR = Q3 - Q1

        lower_limit = Q1 - 1.5 * IQR

        upper_limit = Q3 + 1.5 * IQR

        consolidated_data[column] = consolidated_data[column].apply(lambda x: max(lower_limit, min(x, upper_limit)))

    return consolidated_data




consolidated_data = handle_outliers_iqr(consolidated_data, columns_to_handle_outliers)

consolidated_data.to_csv('consolidated_data.csv', index=False)

top_satisfied_customers = consolidated_data.sort_values(by='satisfaction_score', ascending=False)




top_10_satisfied_customers = top_satisfied_customers.head(10)

print("Top 10 Satisfied Customers:")

print(top_10_satisfied_customers[['MSISDN/Number', 'satisfaction_score']])

max_satisfaction_score = consolidated_data['satisfaction_score'].max()




low_satisfaction_threshold = 0.25 * max_satisfaction_score

moderate_satisfaction_threshold = 0.50 * max_satisfaction_score

high_satisfaction_threshold = 0.75 * max_satisfaction_score




consolidated_data['Satisfaction Level'] = pd.cut(consolidated_data['satisfaction_score'],

                                  bins=[-float('inf'), low_satisfaction_threshold, moderate_satisfaction_threshold, high_satisfaction_threshold, float('inf')],

                                  labels=['Low Satisfied', 'Moderately Satisfied', 'Satisfied', 'Highly Satisfied'])




satisfaction_counts = consolidated_data['Satisfaction Level'].value_counts()




satisfaction_percentage = (satisfaction_counts / len(df)) * 100




fig = px.pie(satisfaction_percentage, names=satisfaction_percentage.index, values=satisfaction_percentage.values,

             title='Percentage Distribution of Satisfaction Levels',

             labels={'index': 'Satisfaction Level', 'values': 'Percentage of Individuals'})




fig.show()

regression_features = ['Engagement Score', 'experience_score']




regression_data = consolidated_data.dropna(subset=regression_features)




X_train, X_test, y_train, y_test = train_test_split(

    regression_data[regression_features],

    regression_data['satisfaction_score'],

    test_size=0.2, random_state=42

)




scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)




model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)




model.fit(X_train_scaled, y_train)




y_pred = model.predict(X_test_scaled)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)




cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)



consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])



print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])

print(f'Cross-validated R-squared scores: {cv_scores}')

print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')

plt.scatter(y_test, y_pred)

plt.xlabel('Actual Satisfaction Score')

plt.ylabel('Predicted Satisfaction Score')

plt.title('Actual vs. Predicted Satisfaction Score')

plt.show()

scores = cross_val_score(model, X_train_scaled, y_train, cv=5)

print(f'Cross-validated R-squared scores: {scores}')

regression_features = ['Engagement Score', 'experience_score']




regression_data = consolidated_data.dropna(subset=regression_features)




X_train, X_test, y_train, y_test = train_test_split(

    regression_data[regression_features],

    regression_data['satisfaction_score'],

    test_size=0.2, random_state=42

)




model = RandomForestRegressor(n_estimators=100, random_state=42)




model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])



print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])




cv_scores = cross_val_score(model, X_train, y_train, cv=5)

print(f'Cross-validated R-squared scores: {cv_scores}')

print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')

plt.scatter(y_test, y_pred)

plt.xlabel('Actual Satisfaction Score')

plt.ylabel('Predicted Satisfaction Score')

plt.title('Actual vs. Predicted Satisfaction Score')

plt.show()

regression_features = ['Engagement Score', 'experience_score']




regression_data = consolidated_data.dropna(subset=regression_features)




X_train, X_test, y_train, y_test = train_test_split(

    regression_data[regression_features],

    regression_data['satisfaction_score'],

    test_size=0.2, random_state=42

)




model = GradientBoostingRegressor(n_estimators=100, random_state=42)




model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])



print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])




cv_scores = cross_val_score(model, X_train, y_train, cv=5)

print(f'Cross-validated R-squared scores: {cv_scores}')

print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')

clustering_features = ['Engagement Score', 'experience_score']




clustering_data = consolidated_data.dropna(subset=clustering_features)




scaler = StandardScaler()

clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])




kmeans = KMeans(n_clusters=2, random_state=42)

clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)




print("Results of K-means Clustering (k=2):")

print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])

clustering_features = ['Engagement Score', 'experience_score']




clustering_data = consolidated_data.dropna(subset=clustering_features)




scaler = StandardScaler()

clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])




kmeans = KMeans(n_clusters=3, random_state=42)

clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)




print("Results of K-means Clustering (k=2):")

print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])

clustering_features = ['Engagement Score', 'experience_score']




clustering_data = consolidated_data.dropna(subset=clustering_features)




scaler = StandardScaler()

clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])




kmeans = KMeans(n_clusters=4, random_state=42)

clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)




print("Results of K-means Clustering (k=2):")

print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])

cluster_aggregation = clustering_data.groupby('cluster').agg({

    'satisfaction_score': 'mean',

    'experience_score': 'mean'

}).reset_index()




print("Average Scores per Cluster:")

print(cluster_aggregation)

create_table_query = """

CREATE TABLE user_scores (

    user_id VARCHAR(255),

    engagement_score FLOAT,

    experience_score FLOAT,

    satisfaction_score FLOAT

);

"""

db_connection.execute_query(create_table_query)
user_scores_df = consolidated_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'satisfaction_score']]

user_scores_df.to_sql('user_scores', con=db_connection.engine, index=False, if_exists='append')




db_connection.close_connection()
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from sqlalchemy import create_engine, text

from scipy.stats import zscore

import psycopg2

import os

import plotly.express as px

import random

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

import tabulate

import plotly.graph_objects as go
os.chdir('..')
from db.connection import DatabaseConnection

from db.sql_preprocessor import DBFilter

from src.utils import missing_values_table, find_agg, convert_bytes_to_megabytes, fix_outlier, remove_outliers

from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot, create_table
db_connection = DatabaseConnection()

set_seaborn_style()
db_connection.connect()
query = "SELECT * FROM xdr_data"

df = db_connection.execute_query(query)

missing_values_df = missing_values_table(df)

print("Missing Values in df:")

print(missing_values_df)
grouped_data = df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




grouped_data['TCP DL Retrans. Vol (Bytes)'].fillna(grouped_data['TCP DL Retrans. Vol (Bytes)'].mean(), inplace=True)




mean_value = grouped_data['TCP DL Retrans. Vol (Bytes)'].mean()

std_dev = grouped_data['TCP DL Retrans. Vol (Bytes)'].std()




outlier_threshold = 3




grouped_data['TCP DL Retrans. Vol (Bytes)'] = grouped_data['TCP DL Retrans. Vol (Bytes)'].apply(

    lambda x: mean_value if abs(x - mean_value) > outlier_threshold * std_dev else x

)




print(grouped_data)
 grouped_data_rtt = df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean().reset_index()




grouped_data_rtt['Avg RTT DL (ms)'].fillna(grouped_data_rtt['Avg RTT DL (ms)'].mean(), inplace=True)




mean_value_rtt = grouped_data_rtt['Avg RTT DL (ms)'].mean()

std_dev_rtt = grouped_data_rtt['Avg RTT DL (ms)'].std()




outlier_threshold_rtt = 3




grouped_data_rtt['Avg RTT DL (ms)'] = grouped_data_rtt['Avg RTT DL (ms)'].apply(

    lambda x: mean_value_rtt if abs(x - mean_value_rtt) > outlier_threshold_rtt * std_dev_rtt else x

)




print(grouped_data_rtt)
grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()




grouped_data_handset['Handset Type'].fillna(grouped_data_handset['Handset Type'].mode().iat[0], inplace=True)




print(grouped_data_handset)

grouped_data_throughput = df.groupby('MSISDN/Number').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




grouped_data_throughput['Avg Bearer TP DL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP DL (kbps)'].mean(), inplace=True)

grouped_data_throughput['Avg Bearer TP UL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP UL (kbps)'].mean(), inplace=True)




print(grouped_data_throughput)

top_10_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].nlargest(10)




bottom_10_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)




most_frequent_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)




print("Top 10 TCP values:")

print(top_10_tcp_values)



print("\nBottom 10 TCP values:")

print(bottom_10_tcp_values)



print("\nMost frequent TCP values:")

print(most_frequent_tcp_values)





top_10_data = pd.DataFrame({'Top 10 TCP Values': top_10_tcp_values.values}, index=top_10_tcp_values.index)

bottom_10_data = pd.DataFrame({'Bottom 10 TCP Values': bottom_10_tcp_values.values}, index=bottom_10_tcp_values.index)

most_frequent_data = pd.DataFrame({'Frequency': most_frequent_tcp_values.values}, index=most_frequent_tcp_values.index)


most_frequent_tcp_values = most_frequent_tcp_values[~(most_frequent_tcp_values.index == "16853393.739320666")]


fig, axes = plt.subplots(3, 1, figsize=(12, 15))






bottom_10_data.plot(kind='bar', ax=axes[1], legend=False)

axes[1].set_ylabel('TCP DL Retrans. Vol (Bytes)')

axes[1].set_title('Bottom 10 TCP Values')






plt.show()

bottom_10_data.plot(kind='bar', ax=axes[1], legend=False)

axes[1].set_ylabel('TCP DL Retrans. Vol (Bytes)')

axes[1].set_title('Bottom 10 TCP Values')




most_frequent_data.plot(kind='bar', ax=axes[2], legend=False)

axes[2].set_xlabel('Index')

axes[2].set_ylabel('Frequency')

axes[2].set_title('Most Frequent TCP Values')




plt.show()
data

top_10_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].nlargest(10)




bottom_10_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].nsmallest(10)




most_frequent_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].value_counts().head(10)




print("Top 10 RTT values:")

print(top_10_rtt_values)



print("\nBottom 10 RTT values:")

print(bottom_10_rtt_values)



print("\nMost frequent RTT values:")

print(most_frequent_rtt_values)

top_10_rtt_data = pd.DataFrame({'Top 10 RTT Values': top_10_rtt_values.values}, index=top_10_rtt_values.index)

bottom_10_rtt_data = pd.DataFrame({'Bottom 10 RTT Values': bottom_10_rtt_values.values}, index=bottom_10_rtt_values.index)




fig, axes = plt.subplots(2, 1, figsize=(12, 10))






bottom_10_rtt_data.plot(kind='bar', ax=axes[1], legend=False, color='skyblue')

axes[1].set_xlabel('Index')

axes[1].set_ylabel('Avg RTT DL (ms)')

axes[1].set_title('Bottom 10 RTT Values')




plt.show()

top_10_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].nlargest(10)




bottom_10_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].nsmallest(10)




most_frequent_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].value_counts().head(10)




top_10_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].nlargest(10)




bottom_10_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].nsmallest(10)




most_frequent_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].value_counts().head(10)




print("Top 10 Throughput values (DL):")

print(top_10_throughput_dl_values)



print("\nBottom 10 Throughput values (DL):")

print(bottom_10_throughput_dl_values)



print("\nMost frequent Throughput values (DL):")

print(most_frequent_throughput_dl_values)



print("\nTop 10 Throughput values (UL):")

print(top_10_throughput_ul_values)



print("\nBottom 10 Throughput values (UL):")

print(bottom_10_throughput_ul_values)



print("\nMost frequent Throughput values (UL):")

print(most_frequent_throughput_ul_values)

top_10_throughput_dl_data = pd.DataFrame({'Top 10 Throughput DL Values': top_10_throughput_dl_values.values}, index=top_10_throughput_dl_values.index)

bottom_10_throughput_dl_data = pd.DataFrame({'Bottom 10 Throughput DL Values': bottom_10_throughput_dl_values.values}, index=bottom_10_throughput_dl_values.index)




fig, axes = plt.subplots(2, 1, figsize=(12, 10))




bottom_10_throughput_dl_data.plot(kind='bar', ax=axes[1], legend=False, color='purple')

axes[1].set_xlabel('Index')

axes[1].set_ylabel('Avg Bearer TP DL (kbps)')

axes[1].set_title('Bottom 10 Throughput DL Values')




plt.show()

grouped_throughput = df.groupby('Handset Type').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




print(grouped_throughput)

grouped_throughput_handset = pd.merge(grouped_data_handset, grouped_data_throughput, on='MSISDN/Number')




print(grouped_throughput_handset)



grouped_throughput_handset = grouped_throughput_handset.groupby('Handset Type').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




print(grouped_throughput_handset)

merged_data = pd.merge(grouped_data_handset, grouped_data, on='MSISDN/Number')




print(merged_data)




grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()




grouped_data_tcp = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




merged_data = pd.merge(grouped_data_handset, grouped_data_tcp, on='Handset Type')




print(merged_data)

top_10_mean_tcp_values = merged_data.nlargest(10, 'TCP DL Retrans. Vol (Bytes)')




plt.figure(figsize=(12, 6))




plt.bar(top_10_mean_tcp_values['Handset Type'], top_10_mean_tcp_values['TCP DL Retrans. Vol (Bytes)'], color='blue')




plt.xlabel('Handset Type')

plt.ylabel('Mean TCP DL Retrans. Vol (Bytes)')

plt.title('Top 10 Mean TCP DL Retrans. Vol (Bytes) for Each Handset Type')




plt.xticks(rotation=90)




plt.show()
top_10_mean_tcp_values
%pip install sqlalchemy

%pip install psycopg2

%pip install pandas
import pandas as pd

from sqlalchemy import create_engine

from urllib.parse import quote




username = 'postgres'

password = 'nati@postgres'

hostname = 'localhost'

port = '5432'

database_name = 'TellCo'




escaped_password = quote(password, safe='')




engine = create_engine(f'postgresql://{username}:{escaped_password}@{hostname}:{port}/{database_name}')




with engine.connect() as connection:

    
    query = """

        SELECT *

        FROM xdr_data

    """

    df = pd.read_sql(query, connection)


df.head()

handset_counts = df['Handset Type'].value_counts()




top_10_handsets = handset_counts.head(10)




print(top_10_handsets)

manufacturer_counts = df['Handset Manufacturer'].value_counts()




top_3_manufacturers = manufacturer_counts.head(3)




print(top_3_manufacturers)
top_3_manufacturers = manufacturer_counts.head(3).index




for manufacturer in top_3_manufacturers:

    
    filtered_df = df[df['Handset Manufacturer'] == manufacturer]

    

    
    handset_counts = filtered_df['Handset Type'].value_counts()



    
    top_5_handsets = handset_counts.head(5)



    
    print(f"Top 5 handsets for {manufacturer}:")

    print(top_5_handsets)

aggregated_data = df.groupby('IMSI').agg({

    'Dur. (ms)': 'count',

    'Dur. (ms).1': 'sum',

    'Total DL (Bytes)': 'sum',

    'Total UL (Bytes)': 'sum',

    'Social Media DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

})




aggregated_data['Social Media (Bytes)'] = aggregated_data['Social Media UL (Bytes)'] + aggregated_data['Social Media DL (Bytes)']

aggregated_data['Youtube (Bytes)'] = aggregated_data['Youtube UL (Bytes)'] + aggregated_data['Youtube DL (Bytes)']

aggregated_data['Google (Bytes)'] = aggregated_data['Google UL (Bytes)'] + aggregated_data['Google DL (Bytes)']

aggregated_data['Email (Bytes)'] = aggregated_data['Email UL (Bytes)'] + aggregated_data['Email DL (Bytes)']

aggregated_data['Netflix (Bytes)'] = aggregated_data['Netflix UL (Bytes)'] + aggregated_data['Netflix DL (Bytes)']

aggregated_data['Gaming (Bytes)'] = aggregated_data['Gaming UL (Bytes)'] + aggregated_data['Gaming DL (Bytes)']

aggregated_data['Other (Bytes)'] = aggregated_data['Other UL (Bytes)'] + aggregated_data['Other DL (Bytes)']






aggregated_data.drop(columns=['Other UL (Bytes)', 'Other DL (Bytes)', 'Gaming UL (Bytes)', 'Gaming DL (Bytes)', 'Netflix UL (Bytes)', 'Netflix DL (Bytes)', 'Email UL (Bytes)', 'Email DL (Bytes)', 'Social Media UL (Bytes)', 'Social Media DL (Bytes)', 'Youtube UL (Bytes)', 'Youtube DL (Bytes)', 'Google UL (Bytes)', 'Google DL (Bytes)'], inplace=True)




aggregated_data = aggregated_data.rename(columns={

    'Dur. (ms)': 'Number of xDR sessions',

    'Dur. (ms).1': 'Total Session duration (ms)'

})


aggregated_data
import pandas as pd

import numpy as np

import matplotlib

from sqlalchemy import create_engine

from urllib.parse import quote



import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler, MinMaxScaler

from sklearn.cluster import KMeans
def plot_hist(df:pd.DataFrame, column:str, color:str)->None:

    
    
    sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)

    plt.title(f'Distribution of {column}', size=20, fontweight='bold')

    plt.show()



def plot_count(df:pd.DataFrame, column:str) -> None:

    plt.figure(figsize=(12, 7))

    sns.countplot(data=df, x=column)

    plt.title(f'Distribution of {column}', size=20, fontweight='bold')

    plt.show()



def plot_bar(df:pd.DataFrame, x_col:str, y_col:str, title:str, xlabel:str, ylabel:str)->None:

    plt.figure(figsize=(12, 7))

    sns.barplot(data = df, x=x_col, y=y_col)

    plt.title(title, size=20)

    plt.xticks(rotation=75, fontsize=14)

    plt.yticks( fontsize=14)

    plt.xlabel(xlabel, fontsize=16)

    plt.ylabel(ylabel, fontsize=16)

    plt.show()



def plot_heatmap(df:pd.DataFrame, title:str, cbar=False)->None:

    plt.figure(figsize=(12, 7))

    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )

    plt.title(title, size=18, fontweight='bold')

    plt.show()



def plot_box(df:pd.DataFrame, x_col:str, title:str) -> None:

    plt.figure(figsize=(12, 7))

    sns.boxplot(data = df, x=x_col)

    plt.title(title, size=20)

    plt.xticks(rotation=75, fontsize=14)

    plt.show()



def plot_box_multi(df:pd.DataFrame, x_col:str, y_col:str, title:str) -> None:

    plt.figure(figsize=(12, 7))

    sns.boxplot(data = df, x=x_col, y=y_col)

    plt.title(title, size=20)

    plt.xticks(rotation=75, fontsize=14)

    plt.yticks( fontsize=14)

    plt.show()



def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:

    plt.figure(figsize=(12, 7))

    sns.scatterplot(data = df, x=x_col, y=y_col, hue=hue, style=style)

    plt.title(title, size=20)

    plt.xticks(fontsize=14)

    plt.yticks( fontsize=14)

    plt.show()
username = 'postgres'

password = 'nati@postgres'

hostname = 'localhost'

port = '5432'

database_name = 'TellCo'




escaped_password = quote(password, safe='')




engine = create_engine(f'postgresql://{username}:{escaped_password}@{hostname}:{port}/{database_name}')




with engine.connect() as connection:

    
    query = """

        SELECT *

        FROM xdr_data

    """

    df = pd.read_sql(query, connection)



df.head()









session_frequency = df.groupby('MSISDN/Number')['IMEI'].nunique()




engagement_metrics = df.groupby('MSISDN/Number').agg({

    'IMEI': 'nunique',                             
    'Dur. (ms)': 'sum',                            
    'Total UL (Bytes)': 'sum',                      
    'Total DL (Bytes)': 'sum'                       
})




engagement_metrics['Session Frequency'] = session_frequency

engagement_metrics['Total UL (GB)'] = engagement_metrics['Total UL (Bytes)'] / 1e9

engagement_metrics['Total DL (GB)'] = engagement_metrics['Total DL (Bytes)'] / 1e9






engagement_metrics['Total Data Volume (GB)'] = engagement_metrics['Total UL (GB)'] + engagement_metrics['Total DL (GB)']




engagement_metrics = engagement_metrics.drop(['Total DL (Bytes)', 'Total UL (Bytes)'], axis=1)




top_10_frequency = engagement_metrics.sort_values('IMEI', ascending=False).head(10)

top_10_duration = engagement_metrics.sort_values('Dur. (ms)', ascending=False).head(10)

top_10_traffic = engagement_metrics.sort_values('Total Data Volume (GB)', ascending=False).head(10)




print("Top 10 Customers by Sessions Frequency:")

print(top_10_frequency[['IMEI', 'Session Frequency']])



print("\nTop 10 Customers by Session Duration:")

print(top_10_duration[['IMEI', 'Dur. (ms)']])



print("\nTop 10 Customers by Sessions Total Traffic:")

print(top_10_traffic[['IMEI', 'Total Data Volume (GB)']])

kmeans = KMeans(n_clusters=3)

kmeans.fit(engagement_metrics)




cluster_labels = {0: 'High Engagement', 1: 'Medium Engagement', 2: 'Low Engagement'}




engagement_metrics['Cluster'] = kmeans.labels_

engagement_metrics['Cluster'] = engagement_metrics['Cluster'].map(cluster_labels)






grouped_metrics = engagement_metrics.groupby('Cluster')




cluster_stats = grouped_metrics.agg({

    'Session Frequency': ['min', 'max', 'mean', 'sum'],

    'Dur. (ms)': ['min', 'max', 'mean', 'sum'],

    'Total Data Volume (GB)': ['min', 'max', 'mean', 'sum']

})




cluster_stats.columns = ['Min Session Frequency', 'Max Session Frequency', 'Average Session Frequency', 'Total Session Frequency',

                         'Min Duration', 'Max Duration', 'Average Duration', 'Total Duration',

                         'Min Data Volume', 'Max Data Volume', 'Average Data Volume', 'Total Data Volume']




print("\nCluster Metrics:")

cluster_stats

scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics)




kmeans = KMeans(n_clusters=3)

kmeans.fit(normalized_metrics)




cluster_labels = {0: 'High Engagement', 1: 'Medium Engagement', 2: 'Low Engagement'}




engagement_metrics['Cluster'] = kmeans.labels_

engagement_metrics['Cluster'] = engagement_metrics['Cluster'].map(cluster_labels)




clustered_metrics = engagement_metrics[['Session Frequency', 'Dur. (ms)', 'Total Data Volume (GB)', 'Cluster']]




print("\nClustering Results:")

clustered_metrics.head(20)




application_columns = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']




total_data = {}

for app in application_columns:

    total_data[app] = df[app + ' DL (Bytes)'].sum() + df[app + ' UL (Bytes)'].sum()




total_data_df = pd.DataFrame.from_dict(total_data, orient='index', columns=['Total Data Volume'])

total_data_df.index.name = 'Applications'




top_3_apps = total_data_df.nlargest(3, 'Total Data Volume')



plot_bar(df=top_3_apps,

         x_col= top_3_apps.index,

         y_col='Total Data Volume',

         title='Total Upload + Download Data Volume vs. Application',

         xlabel='Applications',

         ylabel='Total Data Volume')

plt.pie(top_3_apps['Total Data Volume'], labels=top_3_apps.index, autopct='%1.1f%%')

plt.title('Top 3 Most Used Applications')

plt.axis('equal')

plt.show()
import pandas as pd

import numpy as np

import matplotlib

from sqlalchemy import create_engine

from urllib.parse import quote



import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler, MinMaxScaler

from sklearn.cluster import KMeans

from sklearn.metrics import pairwise_distances_argmin_min
username = 'postgres'

password = 'nati@postgres'

hostname = 'localhost'

port = '5432'

database_name = 'TellCo'




escaped_password = quote(password, safe='')




engine = create_engine(f'postgresql://{username}:{escaped_password}@{hostname}:{port}/{database_name}')




with engine.connect() as connection:

    
    query = """

        SELECT *

        FROM xdr_data

    """

    df = pd.read_sql(query, connection)



df.head()
selected_fields = ['MSISDN/Number', 'TCP DL Retrans. Vol (Bytes)', 'Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)']

duplicates = df[df.duplicated(subset=selected_fields, keep=False)].copy()

duplicates['Count'] = duplicates.groupby(selected_fields)['MSISDN/Number'].transform('count')

duplicates.loc[:, ['MSISDN/Number', 'TCP DL Retrans. Vol (Bytes)', 'Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'Count']]
df.columns
def detect_outliers(df):

    Q1 = df.quantile(0.25)

    Q3 = df.quantile(0.75)

    IQR = Q3 - Q1

    outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))

    return outliers

for column in df.columns:

    
    data = df[column]



    
    if np.issubdtype(data.dtype, np.number):

        
        outliers = detect_outliers(data)



        
        column_mean = data.mean()



        
        df.loc[outliers, column] = column_mean

        df.loc[data.isnull(), column] = column_mean

    else:

        
        column_mode = data.mode().values[0]

        

        
        df.loc[data.isnull(), column] = column_mode

result_df = df.copy()




experience_metrics = result_df[['TCP DL Retrans. Vol (Bytes)', 'Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)']]




scaler = MinMaxScaler()

normalized_experience_metrics = pd.DataFrame(scaler.fit_transform(experience_metrics), columns=experience_metrics.columns)




kmeans = KMeans(n_clusters=3, random_state=42)

result_df['Experience Cluster'] = kmeans.fit_predict(normalized_experience_metrics)




worst_experience_cluster_label = np.argmax(kmeans.transform(normalized_experience_metrics), axis=1)

worst_experience_cluster_samples = normalized_experience_metrics.iloc[worst_experience_cluster_label == worst_experience_cluster_label.max()]




worst_experience_centroid = worst_experience_cluster_samples.mean()




result_df['Experience Score'] = pairwise_distances_argmin_min(normalized_experience_metrics, worst_experience_centroid.values.reshape(1, -1))[1]




print("User Data with Experience Cluster and Experience Score:")

result_df[['TCP DL Retrans. Vol (Bytes)', 'Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'Experience Cluster', 'Experience Score']].head(20)

result_df_engagement = df.copy()




result_df_engagement = result_df_engagement.rename(columns={'Dur. (ms)': 'Session Duration (ms)'})




session_frequency = result_df_engagement.groupby('MSISDN/Number').size().reset_index(name='Session Frequency')




result_df_engagement = pd.merge(result_df_engagement, session_frequency, on='MSISDN/Number', how='left')




result_df_engagement['Total UL + DL (Bytes)'] = result_df_engagement['Total UL (Bytes)'] + result_df_engagement['Total DL (Bytes)']




engagement_metrics = result_df_engagement[['Session Duration (ms)', 'Total UL + DL (Bytes)', 'Session Frequency']]




scaler_engagement = MinMaxScaler()

normalized_engagement_metrics = pd.DataFrame(scaler_engagement.fit_transform(engagement_metrics), columns=engagement_metrics.columns)




kmeans_engagement = KMeans(n_clusters=3, random_state=42)

result_df_engagement['Engagement Cluster'] = kmeans_engagement.fit_predict(normalized_engagement_metrics)




least_engaged_cluster_label = np.argmin(kmeans_engagement.transform(normalized_engagement_metrics), axis=1)

least_engaged_cluster_samples = normalized_engagement_metrics.iloc[least_engaged_cluster_label == least_engaged_cluster_label.min()]




least_engaged_centroid = least_engaged_cluster_samples.mean()




result_df_engagement['Engagement Score'] = pairwise_distances_argmin_min(normalized_engagement_metrics, least_engaged_centroid.values.reshape(1, -1))[1]




print("User Data with Engagement Cluster and Engagement Score:")

result_df_engagement[['Session Duration (ms)', 'Total UL + DL (Bytes)', 'Session Frequency', 'Engagement Cluster', 'Engagement Score']].head(20)
result_df_combined = result_df.copy()




result_df_combined['Engagement Score'] = result_df_engagement['Engagement Score']




result_df_combined['Satisfaction Score'] = (result_df_combined['Experience Score'] + result_df_combined['Engagement Score']) / 2




top_satisfied_customers = result_df_combined.sort_values(by='Satisfaction Score', ascending=False).head(10)




print("Top 10 Satisfied Customers:")

top_satisfied_customers[['MSISDN/Number', 'Experience Score', 'Engagement Score', 'Satisfaction Score']]

engagement_score = result_df_combined[['Engagement Score']]




kmeans_engagement_score = KMeans(n_clusters=2, random_state=42)

result_df_combined['Engagement Cluster'] = kmeans_engagement_score.fit_predict(engagement_score)




print("User Data with Engagement Cluster (k=2):")

result_df_combined[['MSISDN/Number', 'Engagement Score', 'Engagement Cluster']].head(20)

experience_score = result_df_combined[['Experience Score']]




kmeans_experience_score = KMeans(n_clusters=2, random_state=42)

result_df_combined['Experience Cluster'] = kmeans_experience_score.fit_predict(experience_score)




print("User Data with Experience Cluster (k=2):")

result_df_combined[['MSISDN/Number', 'Experience Score', 'Experience Cluster']].head(20)

satisfaction_result_df = result_df_combined[['MSISDN/Number', 'Experience Score', 'Engagement Score', 'Satisfaction Score']].copy()



satisfaction_result_df.to_sql('satisfaction_result_df', con=engine, if_exists='replace', index=False)

with engine.connect() as connection:

    
    query = """

        SELECT *

        FROM satisfaction_result_df

    """

    satisfaction_result_query_df = pd.read_sql(query, connection)



satisfaction_result_query_df.head()
import streamlit as st
import os, sys, pandas

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

from Dashboard import general_analysis
from Dashboard import overview_analysis
from Dashboard import engagment_analysis
from Dashboard import experience_analysis
from Dashboard import satisfaction_analysis

st.set_page_config(page_title="TellCo Data Analysis")

st.title("TellCo Data Analysis")

menu_options = [
    "General Analysis",
    "User Overview Analysis",
    "User Engagement Analysis",
    "User Experience Analysis",
    "User Satisfaction Analysis",
]

st.sidebar.markdown("<h1 style='text-align: center;'>Navigation</h1>", unsafe_allow_html=True)

selected_page = st.sidebar.radio("", menu_options, index=0)

if selected_page == "General Analysis":
    general_analysis.show()

elif selected_page == "User Overview Analysis":
    overview_analysis.show()

elif selected_page == "User Engagement Analysis":
    engagment_analysis.show()

elif selected_page == "User Experience Analysis":
    experience_analysis.show()

elif selected_page == "User Satisfaction Analysis":
    satisfaction_analysis.show()
import streamlit as st
import pandas as pd
import os, sys
from sqlalchemy import create_engine
from urllib.parse import quote
import matplotlib.pyplot as plt
import seaborn as sns

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

username = 'postgres'
password = 'nati@postgres'
hostname = 'localhost'
port = '5432'
database_name = 'TellCo'

escaped_password = quote(password, safe='')

engine = create_engine(f'postgresql://{username}:{escaped_password}@{hostname}:{port}/{database_name}')

def show():
    st.header("User Engagement Analysis Page")

        engagement_option = st.selectbox(
        'Select an analysis:',
        [
         'Top 3 Most Used Applications',
         'Top 10 Users by Engagment Score',
         'Engagement Score Changes for the Last 20,000 xDR Sessions']
    )


    if engagement_option == 'Top 3 Most Used Applications':
        show_top_3_applications()

    elif engagement_option == 'Top 10 Users by Engagment Score':
        show_top_10_users_by_engagement_score()

    elif engagement_option == 'Engagement Score Changes for the Last 20,000 xDR Sessions':
        show_engagement_score_changes()


def show_top_3_applications():
        query_xdr = """
        SELECT *
        FROM xdr_data
    """
    df = pd.read_sql(query_xdr, engine)

        application_columns = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']

        total_data = {}
    for app in application_columns:
        total_data[app] = df[app + ' DL (Bytes)'].sum() + df[app + ' UL (Bytes)'].sum()

        total_data_df = pd.DataFrame.from_dict(total_data, orient='index', columns=['Total Data Volume'])
    total_data_df.index.name = 'Applications'

        top_3_apps = total_data_df.nlargest(3, 'Total Data Volume')

        fig, ax = plt.subplots()
    sns.barplot(x=top_3_apps.index, y='Total Data Volume', data=top_3_apps, ax=ax)
    ax.set_title('Total Upload + Download Data Volume vs. Application')
    ax.set_xlabel('Applications')
    ax.set_ylabel('Total Data Volume')
    st.pyplot(fig)

def show_top_10_users_by_engagement_score():
    """
    This function displays the top 10 users based on their engagement scores.
    """
        query = """
    SELECT *
    FROM satisfaction_result_df;
    """

        data = pd.read_sql(query, engine)

        data_sorted = data.sort_values(by='Engagement Score', ascending=False)

        top_10_users = data_sorted.head(10)
        st.subheader("Top 10 Users by Engagement Score")
    st.table(top_10_users[['MSISDN/Number', 'Engagement Score']])


def show_engagement_score_changes():
        
        query = """
        SELECT srdf."MSISDN/Number", srdf."Engagement Score", xd."Start"
        FROM satisfaction_result_df srdf
        LEFT JOIN xdr_data xd ON srdf."MSISDN/Number" = xd."MSISDN/Number"
    """
    df_combined = pd.read_sql(query, engine)

        df_filtered = df_combined.dropna(subset=['Engagement Score', 'Start'])

        df_sorted = df_filtered.sort_values(by='Start', ascending=True)

        df_recent = df_sorted.tail(20000)

        df_recent['Start'] = pd.to_datetime(df_recent['Start'])

        df_recent_sorted = df_recent.sort_values(by='Start')

        window_size = 1000      df_recent_sorted['Engagement Score MA'] = df_recent_sorted['Engagement Score'].rolling(window=window_size).mean()

        fig, ax = plt.subplots(figsize=(12, 6))

        sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Engagement Score', ax=ax, label='Engagement Score')
    sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Engagement Score MA', ax=ax, label='Engagement Score MA', color='red')

        ax.set_title('Time Series Analysis of Engagement Scores (Most Recent 20,000 Sessions) with Moving Average')
    ax.set_xlabel('Start')
    ax.set_ylabel('Engagement Score')
    ax.tick_params(rotation=45)
    ax.legend()

        st.pyplot(fig)
import streamlit as st
import pandas as pd
import os, sys
from sqlalchemy import create_engine
from urllib.parse import quote
import matplotlib.pyplot as plt
import seaborn as sns

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

username = 'postgres'
password = 'nati@postgres'
hostname = 'localhost'
port = '5432'
database_name = 'TellCo'

escaped_password = quote(password, safe='')

engine = create_engine(f'postgresql://{username}:{escaped_password}@{hostname}:{port}/{database_name}')


def show():
    st.header("General User Score Analysis Page")

        main_option = st.selectbox(
        'Select an analysis:',
        ['Distribution of user analysis scores', 'Distribution of user analysis scores with Handset groups', 'Time series analysis of user analysis scores']
    )

    if main_option == 'Distribution of user analysis scores':
        show_distribution_of_scores()

    elif main_option == 'Distribution of user analysis scores with Handset groups':
        show_distribution_with_handset_groups()

    elif main_option == 'Time series analysis of user analysis scores':
        show_time_series_analysis()
    

def show_distribution_of_scores():
    st.subheader("Distribution of User Analysis Scores")

        query = """
        SELECT "Experience Score", "Engagement Score", "Satisfaction Score"
        FROM satisfaction_result_df
    """
    df_scores = pd.read_sql(query, engine)

        top_25_exp = df_scores["Experience Score"].quantile(0.75)
    top_50_exp = df_scores["Experience Score"].quantile(0.5)
    top_75_exp = df_scores["Experience Score"].quantile(0.25)
    all_users_exp = df_scores["Experience Score"].mean()

    top_25_eng = df_scores["Engagement Score"].quantile(0.75)
    top_50_eng = df_scores["Engagement Score"].quantile(0.5)
    top_75_eng = df_scores["Engagement Score"].quantile(0.25)
    all_users_eng = df_scores["Engagement Score"].mean()

    top_25_sat = df_scores["Satisfaction Score"].quantile(0.75)
    top_50_sat = df_scores["Satisfaction Score"].quantile(0.5)
    top_75_sat = df_scores["Satisfaction Score"].quantile(0.25)
    all_users_sat = df_scores["Satisfaction Score"].mean()

        result_df = pd.DataFrame({
        'Experience Score': [top_25_exp, top_50_exp, top_75_exp, all_users_exp],
        'Engagement Score': [top_25_eng, top_50_eng, top_75_eng, all_users_eng],
        'Satisfaction Score': [top_25_sat, top_50_sat, top_75_sat, all_users_sat],
    }, index=['Top 25%', 'Top 50%', 'Top 75%', 'All Users'])

        color_scale = sns.light_palette("seagreen", as_cmap=True)

        st.table(result_df.style.background_gradient(cmap=color_scale, axis=None))


    
def show_distribution_with_handset_groups():
            
        query = """
        SELECT srdf."MSISDN/Number", srdf."Experience Score", srdf."Engagement Score", srdf."Satisfaction Score", xd."Handset Manufacturer"
        FROM satisfaction_result_df srdf
        LEFT JOIN xdr_data xd ON srdf."MSISDN/Number" = xd."MSISDN/Number"
    """
    df_combined = pd.read_sql(query, engine)

        valid_handset_values = ['Apple', 'Samsung', 'Huawei']
    df_filtered = df_combined[df_combined['Handset Manufacturer'].isin(valid_handset_values)]

        avg_scores = df_filtered.groupby('Handset Manufacturer').agg({
        'Experience Score': 'mean',
        'Engagement Score': 'mean',
        'Satisfaction Score': 'mean'
    }).transpose()

        color_scale = sns.light_palette("seagreen", as_cmap=True)

        st.subheader("Distribution of User Analysis Scores with Handset Groups")
    st.table(avg_scores.style.background_gradient(cmap=color_scale, axis=None))



def show_time_series_analysis():
        sub_option = st.selectbox(
        'Select a sub-analysis:',
        ['Time series analysis of experiance scores', 'Time series analysis of engagment scores', 'Time series analysis of satisfaction scores']
    )

    if sub_option == 'Time series analysis of experiance scores':
        show_time_series_experience_scores()

    elif sub_option == 'Time series analysis of engagment scores':
        show_time_series_engagement_scores()

    elif sub_option == 'Time series analysis of satisfaction scores':
        show_time_series_satisfaction_scores()

def show_time_series_experience_scores():
        
        query = """
        SELECT srdf."MSISDN/Number", srdf."Experience Score", xd."Start"
        FROM satisfaction_result_df srdf
        LEFT JOIN xdr_data xd ON srdf."MSISDN/Number" = xd."MSISDN/Number"
    """
    df_combined = pd.read_sql(query, engine)

        df_filtered = df_combined.dropna(subset=['Experience Score', 'Start'])

        df_sorted = df_filtered.sort_values(by='Start', ascending=True)

        df_recent = df_sorted.tail(20000)

        df_recent['Start'] = pd.to_datetime(df_recent['Start'])

        df_recent_sorted = df_recent.sort_values(by='Start')

        window_size = 1000      df_recent_sorted['Experience Score MA'] = df_recent_sorted['Experience Score'].rolling(window=window_size).mean()

        fig, ax = plt.subplots(figsize=(12, 6))

        sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Experience Score', ax=ax, label='Experience Score')
    sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Experience Score MA', ax=ax, label='Experience Score MA', color='red')

        ax.set_title('Time Series Analysis of Experience Scores (Most Recent 20,000 Sessions) with Moving Average')
    ax.set_xlabel('Start')
    ax.set_ylabel('Experience Score')
    ax.tick_params(rotation=45)
    ax.legend()

        st.pyplot(fig)
    
def show_time_series_engagement_scores():
        
        query = """
        SELECT srdf."MSISDN/Number", srdf."Engagement Score", xd."Start"
        FROM satisfaction_result_df srdf
        LEFT JOIN xdr_data xd ON srdf."MSISDN/Number" = xd."MSISDN/Number"
    """
    df_combined = pd.read_sql(query, engine)

        df_filtered = df_combined.dropna(subset=['Engagement Score', 'Start'])

        df_sorted = df_filtered.sort_values(by='Start', ascending=True)

        df_recent = df_sorted.tail(20000)

        df_recent['Start'] = pd.to_datetime(df_recent['Start'])

        df_recent_sorted = df_recent.sort_values(by='Start')

        window_size = 1000      df_recent_sorted['Engagement Score MA'] = df_recent_sorted['Engagement Score'].rolling(window=window_size).mean()

        fig, ax = plt.subplots(figsize=(12, 6))

        sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Engagement Score', ax=ax, label='Engagement Score')
    sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Engagement Score MA', ax=ax, label='Engagement Score MA', color='red')

        ax.set_title('Time Series Analysis of Engagement Scores (Most Recent 20,000 Sessions) with Moving Average')
    ax.set_xlabel('Start')
    ax.set_ylabel('Engagement Score')
    ax.tick_params(rotation=45)
    ax.legend()

        st.pyplot(fig)

def show_time_series_satisfaction_scores():
        
        query = """
        SELECT srdf."MSISDN/Number", srdf."Satisfaction Score", xd."Start"
        FROM satisfaction_result_df srdf
        LEFT JOIN xdr_data xd ON srdf."MSISDN/Number" = xd."MSISDN/Number"
    """
    df_combined = pd.read_sql(query, engine)

        df_filtered = df_combined.dropna(subset=['Satisfaction Score', 'Start'])

        df_sorted = df_filtered.sort_values(by='Start', ascending=True)

        df_recent = df_sorted.tail(20000)

        df_recent['Start'] = pd.to_datetime(df_recent['Start'])

        df_recent_sorted = df_recent.sort_values(by='Start')

        window_size = 1000      df_recent_sorted['Satisfaction Score MA'] = df_recent_sorted['Satisfaction Score'].rolling(window=window_size).mean()

        fig, ax = plt.subplots(figsize=(12, 6))

        sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Satisfaction Score', ax=ax, label='Satisfaction Score')
    sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Satisfaction Score MA', ax=ax, label='Satisfaction Score MA', color='red')

        ax.set_title('Time Series Analysis of Satisfaction Scores (Most Recent 20,000 Sessions) with Moving Average')
    ax.set_xlabel('Start')
    ax.set_ylabel('Satisfaction Score')
    ax.tick_params(rotation=45)
    ax.legend()

        st.pyplot(fig)

if __name__ == '__main__':
    show()
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy import create_engine
from urllib.parse import quote

username = 'postgres'
password = 'nati@postgres'
hostname = 'localhost'
port = '5432'
database_name = 'TellCo'

escaped_password = quote(password, safe='')

engine = create_engine(f'postgresql://{username}:{escaped_password}@{hostname}:{port}/{database_name}')

def show():
    st.header("User Overview Analysis Page")

        analysis_option = st.selectbox(
        'Select an analysis:',
        ['Top 5 Handset Manufacturers', 'Distribution of Total DL', 'Distribution of Total UL', 'Total UL + DL vs. Application', 'Correlation Analysis']
    )

        if analysis_option == 'Top 5 Handset Manufacturers':
        show_top_manufacturers()
    elif analysis_option == 'Distribution of Total DL':
        show_distribution_total_dl()
    elif analysis_option == 'Distribution of Total UL':
        show_distribution_total_ul()
    elif analysis_option == 'Total UL + DL vs. Application':
        show_total_vs_application()
    elif analysis_option == 'Correlation Analysis':
        show_correlation_analysis()

def show_top_manufacturers():
    st.subheader("Top 5 Handset Manufacturers")
    query_top_manufacturers = """
        SELECT "Handset Manufacturer", COUNT(*) AS "Count"
        FROM xdr_data
        GROUP BY "Handset Manufacturer"
        ORDER BY "Count" DESC
        LIMIT 5
    """
    df_top_manufacturers = pd.read_sql(query_top_manufacturers, engine)
    st.table(df_top_manufacturers)

def show_distribution_total_dl():
    st.subheader("Distribution of Total DL (Bar Plot)")
    query_total_dl = """
        SELECT "MSISDN/Number", "Total DL (Bytes)"
        FROM xdr_data
    """
    df_total_dl = pd.read_sql(query_total_dl, engine)
    fig, ax = plt.subplots()
    sns.histplot(data=df_total_dl, x='Total DL (Bytes)', bins=30, kde=True)
    ax.set_xlabel("Total DL (Bytes)")
    ax.set_ylabel("Count")
    ax.set_title("Distribution of Total DL")
    st.pyplot(fig)

def show_distribution_total_ul():
    st.subheader("Distribution of Total UL (Bar Plot)")
    query_total_ul = """
        SELECT "MSISDN/Number", "Total UL (Bytes)"
        FROM xdr_data
    """
    df_total_ul = pd.read_sql(query_total_ul, engine)
    fig, ax = plt.subplots()
    sns.histplot(data=df_total_ul, x='Total UL (Bytes)', bins=30, kde=True)
    ax.set_xlabel("Total UL (Bytes)")
    ax.set_ylabel("Count")
    ax.set_title("Distribution of Total UL")
    st.pyplot(fig)

def show_total_vs_application():
    st.subheader("Total UL + DL vs. Application")
    query_total_vs_application = """
        SELECT "MSISDN/Number", "Social Media DL (Bytes)", "Google DL (Bytes)", "Email DL (Bytes)", "Youtube DL (Bytes)",
               "Netflix DL (Bytes)", "Gaming DL (Bytes)", "Other DL (Bytes)",
               "Social Media UL (Bytes)", "Google UL (Bytes)", "Email UL (Bytes)", "Youtube UL (Bytes)",
               "Netflix UL (Bytes)", "Gaming UL (Bytes)", "Other UL (Bytes)"
        FROM xdr_data
    """
    df = pd.read_sql(query_total_vs_application, engine)

        application_columns = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']

        total_data = {}
    for app in application_columns:
        total_data[app] = df[app + ' DL (Bytes)'].sum() + df[app + ' UL (Bytes)'].sum()

        total_data_df = pd.DataFrame.from_dict(total_data, orient='index', columns=['Total Data Volume'])
    total_data_df.index.name = 'Applications'

    fig, ax = plt.subplots()
    sns.barplot(x=total_data_df.index, y='Total Data Volume', data=total_data_df)
    ax.set_xlabel("Applications")
    ax.set_ylabel("Total Data Volume")
    ax.set_title("Total Upload + Download Data Volume vs. Application")
    ax.tick_params(rotation=45)
    st.pyplot(fig)

def show_correlation_analysis():
    st.subheader("Correlation Analysis")

        query_correlation = """
        SELECT "Social Media DL (Bytes)", "Social Media UL (Bytes)",
               "Youtube DL (Bytes)", "Youtube UL (Bytes)",
               "Netflix DL (Bytes)", "Netflix UL (Bytes)",
               "Google DL (Bytes)", "Google UL (Bytes)",
               "Email DL (Bytes)", "Email UL (Bytes)",
               "Gaming DL (Bytes)", "Gaming UL (Bytes)",
               "Other DL (Bytes)", "Other UL (Bytes)"
        FROM xdr_data
    """
    df = pd.read_sql(query_correlation, engine)

        total_data_volume = pd.DataFrame()

        total_data_volume['Social Media Total'] = df['Social Media DL (Bytes)'] + df['Social Media UL (Bytes)']
    total_data_volume['Youtube Total'] = df['Youtube DL (Bytes)'] + df['Youtube UL (Bytes)']
    total_data_volume['Netflix Total'] = df['Netflix DL (Bytes)'] + df['Netflix UL (Bytes)']
    total_data_volume['Google Total'] = df['Google DL (Bytes)'] + df['Google UL (Bytes)']
    total_data_volume['Email Total'] = df['Email DL (Bytes)'] + df['Email UL (Bytes)']
    total_data_volume['Gaming Total'] = df['Gaming DL (Bytes)'] + df['Gaming UL (Bytes)']
    total_data_volume['Other Total'] = df['Other DL (Bytes)'] + df['Other UL (Bytes)']

        correlation_matrix = total_data_volume.corr()

        color_scale = sns.light_palette("seagreen", as_cmap=True)

        st.table(correlation_matrix.style.background_gradient(cmap=color_scale, axis=None))
import streamlit as st
import pandas as pd
import os, sys
from sqlalchemy import create_engine
from urllib.parse import quote
import matplotlib.pyplot as plt
import seaborn as sns

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

username = 'postgres'
password = 'nati@postgres'
hostname = 'localhost'
port = '5432'
database_name = 'TellCo'

escaped_password = quote(password, safe='')

engine = create_engine(f'postgresql://{username}:{escaped_password}@{hostname}:{port}/{database_name}')


def show():
    st.header("User Satisfaction Page")

        main_option = st.selectbox(
        'Select an analysis:',
        ['Top 10 Users by Satisfaction Score', 'Satisfaction Score Changes for the Last 20,000 xDR Sessions']
    )

    if main_option == 'Top 10 Users by Satisfaction Score':
        top_10_users_by_satisfaction_score()

    elif main_option == 'Satisfaction Score Changes for the Last 20,000 xDR Sessions':
        show_time_series_satisfaction_scores()



def top_10_users_by_satisfaction_score():
    """
    This function displays the top 10 users based on their Satisfaction scores.
    """
        query = """
    SELECT *
    FROM satisfaction_result_df;
    """

        data = pd.read_sql(query, engine)

        data_sorted = data.sort_values(by='Satisfaction Score', ascending=False)

        top_10_users = data_sorted.head(10)
        st.subheader("Top 10 Users by Satisfaction Score")
    st.table(top_10_users[['MSISDN/Number', 'Satisfaction Score']])

def show_time_series_satisfaction_scores():
        
        query = """
        SELECT srdf."MSISDN/Number", srdf."Satisfaction Score", xd."Start"
        FROM satisfaction_result_df srdf
        LEFT JOIN xdr_data xd ON srdf."MSISDN/Number" = xd."MSISDN/Number"
    """
    df_combined = pd.read_sql(query, engine)

        df_filtered = df_combined.dropna(subset=['Satisfaction Score', 'Start'])

        df_sorted = df_filtered.sort_values(by='Start', ascending=True)

        df_recent = df_sorted.tail(20000)

        df_recent['Start'] = pd.to_datetime(df_recent['Start'])

        df_recent_sorted = df_recent.sort_values(by='Start')

        window_size = 1000      df_recent_sorted['Satisfaction Score MA'] = df_recent_sorted['Satisfaction Score'].rolling(window=window_size).mean()

        fig, ax = plt.subplots(figsize=(12, 6))

        sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Satisfaction Score', ax=ax, label='Satisfaction Score')
    sns.lineplot(data=df_recent_sorted, x=df_recent_sorted.index, y='Satisfaction Score MA', ax=ax, label='Satisfaction Score MA', color='red')

        ax.set_title('Time Series Analysis of Satisfaction Scores (Most Recent 20,000 Sessions) with Moving Average')
    ax.set_xlabel('Start')
    ax.set_ylabel('Satisfaction Score')
    ax.tick_params(rotation=45)
    ax.legend()

        st.pyplot(fig)
Repository Structure: '
' ├── notebook
│   └── note_book.ipynb
└── app.py
 '
' Commit History: 
{"insertions": [86, 4], "deletions": [0, 6], "lines": [86, 10], "committed_datetime": ["2023-12-13 19:50:58", "2023-12-14 08:50:54"], "commit_count": 2} 
 Content: 
import pandas as pd

from sqlalchemy import create_engine, MetaData, Table

import psycopg2

databse_name = 'telecome'

table_name = 'xdr_data'

con_param ={"host": "localhost","user": "postgres", "password":"1272", "port":"5432", "database":databse_name}

print(databse_name)

engin  = create_engine(f"postgresql://{con_param['user']}:{con_param['password']}@{con_param['host']}/{con_param['port']}/{con_param['database']}")



sql_query = 'SELECT * FROM xdr_data'



conn = psycopg2.connect(

    host="localhost",

    port=5432,

    database="telecome",

    user="postgres",

    password="1272"

)

cursor =conn.cursor()

cursor.execute(sql_query)
 

import pandas as pd

from sqlalchemy import create_engine

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from scipy.stats.mstats import winsorize 

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

import plotly.express as px

from sklearn.cluster import KMeans

from sklearn.metrics import euclidean_distances

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.compose import ColumnTransformer

from sklearn.pipeline import Pipeline

from sklearn.impute import SimpleImputer

from sklearn.preprocessing import OneHotEncoder
database_name = 'telecom'

table_name = 'xdr_data'



connection_params = {"host": "localhost", "user": "postgres", "password": "Musy19", "port": "5432", "database": database_name}



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")




sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query, con= engine)
csv_file_path = "C:\\Users\\ADMIN\\10Academy\\Week-1\\telecom"

df.to_csv(csv_file_path, index=False)
df.head()
df.info() 
df.describe()
df.shape
df.isnull()
df.columns = df.columns.str.replace(' ', '_')

df.info()
transposed_df = df.T

transposed_df
num_col = df.select_dtypes (include = ['number']).columns

cat_col = df.select_dtypes(include=['object']).columns



df_num = df[num_col]

df_cat = df[cat_col]



print("Numerical Columns:", num_col)

print("Categorical Columns:", cat_col)
df_cat.info()
top_handsets = df_cat['Handset_Type'].value_counts().head(10)

top_handsets
top_handsets_manufacturer = df_cat['Handset_Manufacturer'].value_counts().head(3).index

top_handsets_manufacturer
for manufacturer in top_handsets_manufacturer:

    
    manufacturer_data = df_cat[df_cat['Handset_Manufacturer'] == manufacturer]

    

    
    top_handsets_per_manufacturer = manufacturer_data['Handset_Type'].value_counts().head(5)



top_handsets_per_manufacturer  

sessions_per_user = df_num.groupby('MSISDN/Number')['Bearer_Id'].count().reset_index()

sessions_per_user.columns = ['MSISDN/Number', 'Number_of_xDR_Sessions']




print(sessions_per_user)

user_session_duration = df_num.groupby('MSISDN/Number')['Dur._(ms)'].sum().reset_index()

user_session_duration.columns = ['MSISDN/Number', 'Total_session_Duration_ms']



print(user_session_duration)
user_data_usage = df_num.groupby('MSISDN/Number')[['Total_DL_(Bytes)', 'Total_UL_(Bytes)']].sum().reset_index()



print(user_data_usage)

application_columns = [

    

    'Social_Media_DL_(Bytes)','Social_Media_UL_(Bytes)','Youtube_DL_(Bytes)', 'Youtube_UL_(Bytes)','Netflix_DL_(Bytes)', 'Netflix_UL_(Bytes)','Google_DL_(Bytes)', 'Google_UL_(Bytes)','Email_DL_(Bytes)', 'Email_UL_(Bytes)','Gaming_DL_(Bytes)', 'Gaming_UL_(Bytes)','Other_DL_(Bytes)', 'Other_UL_(Bytes)'

    

    ]




user_app_data = df.groupby('MSISDN/Number')[application_columns].sum().reset_index()




print(user_app_data)


missing_values = df.isnull()




missing_count = missing_values.sum()




columns_with_missing = missing_count[missing_count > 0]




print("Columns with Missing Values:")

print(columns_with_missing)

totalCells = np.product(df.shape)


totalMissing = missing_count.sum()




print("The dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")



if df_num.isnull().any().any():

    df_num = df_num.fillna(df_num.mean())







winsorized_data = winsorize(df_num.to_numpy(), limits=[0.01, 0.01], axis=0)




df_num = pd.DataFrame(winsorized_data,columns=df_num.columns)
df_num.describe()

plt.figure(figsize=(10,6))

plt.hist(df_num)

plt.title('xDR_DATA distribution')

plt.xlabel('Columns')

plt.ylabel('Frequency')

plt.show()


for column in df_num.columns:

    plt.figure(figsize=(10,6))

    plt.hist(df_num[column])

    plt.title(f'xDR_DATA distribution of {column}')

    plt.xlabel('Columns')

    plt.ylabel('Frequency')

    plt.show()


plt.figure(figsize=(10,6))

plt.boxplot(df_num)

plt.title('xDR_DATA distribution')

plt.xlabel('Columns')

plt.ylabel('Frequency')

plt.show()

for column in df_num.columns:

    plt.figure(figsize=(8,6))

    sns.boxplot(df_num[column]) 

    plt.title(f'Boxplot for xDR of {column}')

    plt.xlabel('columns')

    plt.show()
df_num['Total_DL_UL_Sum'] = df_num['Total_DL_(Bytes)'] + df_num['Total_UL_(Bytes)']

df_num['Total_DL_UL_Sum']
correlations = df_num[application_columns + ['Total_DL_UL_Sum']].corr()

correlations
for app_column in application_columns:

    plt.figure(figsize=(8, 6))

    sns.scatterplot(x='Total_DL_UL_Sum', y=app_column, data=df_num)

    plt.title(f'Scatter Plot: {app_column} vs Total_DL_UL_Sum')

    plt.xlabel('Total_DL_UL_Sum')

    plt.ylabel(app_column)

    plt.show()
import random



for app_column in application_columns:

    plt.figure(figsize=(8, 6))

    

    
    sampled_data = df_num.sample(n=50, random_state=42)  
    

    sns.scatterplot(x='Total_DL_UL_Sum', y=app_column, data=sampled_data)

    

    plt.title(f'Scatter Plot: {app_column} vs Total_DL_UL_Sum (Sampled 20 points)')

    plt.xlabel('Total_DL_UL_Sum')

    plt.ylabel(app_column)

    plt.show()

correlation_matrix = df_num[application_columns + ['Total_DL_UL_Sum']].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')

plt.show()

df_num['Total_Session_Duration'] = df_num.groupby('MSISDN/Number')['Dur._(ms)'].transform('sum')




df_num['Decile'] = pd.qcut(df_num['Total_Session_Duration'], q=10, labels=False)




total_data_per_decile = df_num.groupby('Decile')['Total_DL_UL_Sum'].sum().reset_index()



print(total_data_per_decile)



subset_df_num = df_num[application_columns]




correlation_matrix = subset_df_num.corr()




print(correlation_matrix)


scaler = StandardScaler()

df_num_scaled = scaler.fit_transform(df_num)




pca = PCA()

df_num_pca_array= pca.fit_transform(df_num_scaled)




explained_variance_ratio = pca.explained_variance_ratio_

cumulative_explained_variance = explained_variance_ratio.cumsum()




num_components = np.argmax(cumulative_explained_variance >= 0.95) + 1



pca = PCA(n_components = num_components)

df_num_pca_array = pca.fit_transform(df_num_scaled)



df_num_pca = pd.DataFrame(df_num_pca_array, columns=[f'PC_{i+1}' for i in range(df_num_pca_array.shape[1])])


plt.figure(figsize=(10, 6))

plt.plot(range(1, len(explained_variance_ratio) +1), cumulative_explained_variance, marker = 'o', linestyle= '--')

plt.title('Scree Plot for PCA')

plt.xlabel('Numbered Principal Components')

plt.ylabel('Cumulative Explained Variance')

plt.show()
sessions_frequency = df_num.groupby('MSISDN/Number')['Bearer_Id'].count()



sessions_frequency



session_duration = df_num.groupby('MSISDN/Number')['Dur._(ms)'].sum()

session_duration

total_traffic = df_num.groupby('MSISDN/Number')[['Total_UL_(Bytes)', 'Total_DL_(Bytes)']].sum().sum(axis=1)

total_traffic



engagement_metrics = pd.DataFrame({

    'Sessions_Frequency': sessions_frequency,

    'Session_Duration': session_duration,

    'Total_Traffic': total_traffic

})




top_10_sessions_frequency = engagement_metrics['Sessions_Frequency'].nlargest(10)

top_10_session_duration = engagement_metrics['Session_Duration'].nlargest(10)

top_10_total_traffic = engagement_metrics['Total_Traffic'].nlargest(10)




print("Top 10 Customers by Sessions Frequency:")

print(top_10_sessions_frequency)



print("\nTop 10 Customers by Session Duration:")

print(top_10_session_duration)



print("\nTop 10 Customers by Total Traffic:")

print(top_10_total_traffic)



df_num.reset_index(drop=True, inplace=True)




sessions_frequency = df_num.groupby('MSISDN/Number')['Bearer_Id'].count()

session_duration = df_num.groupby('MSISDN/Number')['Dur._(ms)'].sum()

total_traffic = df_num.groupby('MSISDN/Number')[['Total_UL_(Bytes)', 'Total_DL_(Bytes)']].sum().sum(axis=1)




sessions_frequency_series = pd.Series(sessions_frequency, name='Session_Frequency')

session_duration_series = pd.Series(session_duration, name='Session_Duration')

total_traffic_series = pd.Series(total_traffic, name='Total_Traffic')




engagement_metrics = pd.concat([sessions_frequency_series, session_duration_series, total_traffic_series], axis=1)




scaler = StandardScaler()

engagement_metrics_normalized = scaler.fit_transform(engagement_metrics)




kmeans = KMeans(n_clusters=3, random_state=42)


cluster_labels = kmeans.fit_predict(engagement_metrics_normalized)




df_clusters = pd.DataFrame({'MSISDN/Number': engagement_metrics.index, 'Cluster': cluster_labels})




df_num = pd.merge(df_num, df_clusters, on='MSISDN/Number')




print(df_num['Cluster'].value_counts())









df_num.reset_index(drop=True, inplace=True)




sessions_frequency = df_num.groupby('MSISDN/Number')['Bearer_Id'].count()

session_duration = df_num.groupby('MSISDN/Number')['Dur._(ms)'].sum()

total_traffic = df_num.groupby('MSISDN/Number')[['Total_UL_(Bytes)', 'Total_DL_(Bytes)']].sum().sum(axis=1)




sessions_frequency_series = pd.Series(sessions_frequency, name='Session_Frequency')

session_duration_series = pd.Series(session_duration, name='Session_Duration')

total_traffic_series = pd.Series(total_traffic, name='Total_Traffic')




engagement_metrics = pd.concat([sessions_frequency_series, session_duration_series, total_traffic_series], axis=1)




scaler = StandardScaler()

engagement_metrics_normalized = scaler.fit_transform(engagement_metrics)




kmeans = KMeans(n_clusters=3, random_state=42)


cluster_labels = kmeans.fit_predict(engagement_metrics_normalized)




df_clusters = pd.DataFrame({'MSISDN/Number': engagement_metrics.index, 'Cluster': cluster_labels})




df_clusters['Cluster'] = df_clusters['Cluster'].astype('category')

df_clusters['Cluster'] = df_clusters['Cluster'].cat.rename_categories({'0': 'Cluster 0', '1': 'Cluster 1', '2': 'Cluster 2'})




df_num = pd.merge(df_num, df_clusters, on='MSISDN/Number')




print(df_num['Cluster'].value_counts())




application_columns = [

    'Social_Media_DL_(Bytes)', 'Social_Media_UL_(Bytes)',

    'Youtube_DL_(Bytes)', 'Youtube_UL_(Bytes)',

    'Netflix_DL_(Bytes)', 'Netflix_UL_(Bytes)',

    'Google_DL_(Bytes)', 'Google_UL_(Bytes)',

    'Email_DL_(Bytes)', 'Email_UL_(Bytes)',

    'Gaming_DL_(Bytes)', 'Gaming_UL_(Bytes)',

    'Other_DL_(Bytes)', 'Other_UL_(Bytes)'

]




total_traffic_per_app_user = df_num.groupby('MSISDN/Number')[application_columns].sum().sum(axis=1).reset_index(name='Total_DL_UL_(Bytes)')




top_10_per_app = total_traffic_per_app_user.groupby('MSISDN/Number').apply(lambda x: x.nlargest(10, 'Total_DL_UL_(Bytes)')).reset_index(drop=True)




print(top_10_per_app)








engagement_metrics = df_num[['Session_Frequency', 'Session_Duration', 'Total_Traffic']]




scaler = StandardScaler()

engagement_metrics_normalized = scaler.fit_transform(engagement_metrics)




kmeans = KMeans(n_clusters=3, random_state=42)

df_num['Cluster'] = kmeans.fit_predict(engagement_metrics_normalized)




print(df_num['Cluster'].value_counts())

import matplotlib.pyplot as plt





df_num[application_columns] = df_num[application_columns].apply(pd.to_numeric, errors='coerce')




df_num['MSISDN/Number'] = pd.to_numeric(df_num['MSISDN/Number'], errors='coerce')




total_traffic_per_user_app = df_num.groupby(['MSISDN/Number'] + application_columns).sum()




top_3_apps_per_user = total_traffic_per_user_app.apply(lambda x: x.nlargest(3), axis=1)




plt.figure(figsize=(12, 6))

top_3_apps_per_user.columns = [f'Top {i+1}' for i in range(3)]

top_3_apps_per_user.sum().sort_values().plot(kind='barh', color='skyblue')

plt.title('Top 3 Most Used Applications')

plt.xlabel('Total Traffic (Bytes)')

plt.ylabel('Application')

plt.show()










df_num['TCP_Retrans_Total'] = df_num['TCP_DL_Retrans._Vol_(Bytes)'] + df_num['TCP_UL_Retrans._Vol_(Bytes)']




avg_tcp_retransmission_per_customer = df_num.groupby('MSISDN/Number')['TCP_Retrans_Total'].mean()




avg_tcp_retransmission_per_customer = avg_tcp_retransmission_per_customer.reset_index()




print(avg_tcp_retransmission_per_customer)










df_num['Avg_RTT_DL_(ms)'] = pd.to_numeric(df_num['Avg_RTT_DL_(ms)'], errors='coerce')

df_num['Avg_RTT_UL_(ms)'] = pd.to_numeric(df_num['Avg_RTT_UL_(ms)'], errors='coerce')




df_num['Avg_RTT'] = df_num[['Avg_RTT_DL_(ms)', 'Avg_RTT_UL_(ms)']].mean(axis=1)




avg_rtt_per_customer = df_num.groupby('MSISDN/Number')['Avg_RTT'].mean()




avg_rtt_per_customer = avg_rtt_per_customer.reset_index()




print(avg_rtt_per_customer)




df_num['Avg_Bearer_TP_DL_(kbps)'] = pd.to_numeric(df_num['Avg_Bearer_TP_DL_(kbps)'], errors='coerce')

df_num['Avg_Bearer_TP_UL_(kbps)'] = pd.to_numeric(df_num['Avg_Bearer_TP_UL_(kbps)'], errors='coerce')




df_num['Avg_Throughput'] = df_num[['Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']].mean(axis=1)




avg_throughput_per_customer = df_num.groupby('MSISDN/Number')['Avg_Throughput'].mean()




avg_throughput_per_customer = avg_throughput_per_customer.reset_index()




print(avg_throughput_per_customer)




df_num['TCP_DL_Retrans._Vol_(Bytes)'] = pd.to_numeric(df_num['TCP_DL_Retrans._Vol_(Bytes)'], errors='coerce')

df_num['TCP_UL_Retrans._Vol_(Bytes)'] = pd.to_numeric(df_num['TCP_UL_Retrans._Vol_(Bytes)'], errors='coerce')




top_tcp_values = df_num[['TCP_DL_Retrans._Vol_(Bytes)', 'TCP_UL_Retrans._Vol_(Bytes)']].stack().value_counts().head(10)

print("Top 10 TCP Values:")

print(top_tcp_values)




bottom_tcp_values = df_num[['TCP_DL_Retrans._Vol_(Bytes)', 'TCP_UL_Retrans._Vol_(Bytes)']].stack().value_counts().tail(10)

print("\nBottom 10 TCP Values:")

print(bottom_tcp_values)




most_frequent_tcp_values = df_num[['TCP_DL_Retrans._Vol_(Bytes)', 'TCP_UL_Retrans._Vol_(Bytes)']].stack().value_counts().idxmax()

print("\nMost Frequent TCP Value:")

print(most_frequent_tcp_values)




df_num['Avg_RTT_DL_(ms)'] = pd.to_numeric(df_num['Avg_RTT_DL_(ms)'], errors='coerce')

df_num['Avg_RTT_UL_(ms)'] = pd.to_numeric(df_num['Avg_RTT_UL_(ms)'], errors='coerce')




top_rtt_values = df_num[['Avg_RTT_DL_(ms)', 'Avg_RTT_UL_(ms)']].stack().value_counts().head(10)

print("Top 10 RTT Values:")

print(top_rtt_values)




bottom_rtt_values = df_num[['Avg_RTT_DL_(ms)', 'Avg_RTT_UL_(ms)']].stack().value_counts().tail(10)

print("\nBottom 10 RTT Values:")

print(bottom_rtt_values)




most_frequent_rtt_values = df_num[['Avg_RTT_DL_(ms)', 'Avg_RTT_UL_(ms)']].stack().value_counts().idxmax()

print("\nMost Frequent RTT Value:")

print(most_frequent_rtt_values)




df_num['Avg_Bearer_TP_DL_(kbps)'] = pd.to_numeric(df_num['Avg_Bearer_TP_DL_(kbps)'], errors='coerce')

df_num['Avg_Bearer_TP_UL_(kbps)'] = pd.to_numeric(df_num['Avg_Bearer_TP_UL_(kbps)'], errors='coerce')




top_throughput_values = df_num[['Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']].stack().value_counts().head(10)

print("Top 10 Throughput Values:")

print(top_throughput_values)




bottom_throughput_values = df_num[['Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']].stack().value_counts().tail(10)

print("\nBottom 10 Throughput Values:")

print(bottom_throughput_values)




most_frequent_throughput_values = df_num[['Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']].stack().value_counts().idxmax()

print("\nMost Frequent Throughput Value:")

print(most_frequent_throughput_values)




df_num['Avg_Bearer_TP_DL_(kbps)'] = pd.to_numeric(df_num['Avg_Bearer_TP_DL_(kbps)'], errors='coerce')

df_num['Avg_Bearer_TP_UL_(kbps)'] = pd.to_numeric(df_num['Avg_Bearer_TP_UL_(kbps)'], errors='coerce')




average_throughput_per_handset = df.groupby('Handset_Type')[['Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']].mean()




print("Distribution of Average Throughput per Handset Type:")

print(average_throughput_per_handset)


average_throughput_per_handset.plot(kind='bar', figsize=(12, 6))

plt.title('Distribution of Average Throughput per Handset Type')

plt.xlabel('Handset Type')

plt.ylabel('Average Throughput (kbps)')

plt.legend(["Downlink", "Uplink"])

plt.show()



df['TCP_DL_Retrans._Vol_(Bytes)'] = pd.to_numeric(df['TCP_DL_Retrans._Vol_(Bytes)'], errors='coerce')

df['TCP_UL_Retrans._Vol_(Bytes)'] = pd.to_numeric(df['TCP_UL_Retrans._Vol_(Bytes)'], errors='coerce')




average_tcp_retransmission_per_handset = df.groupby('Handset_Type')[['TCP_DL_Retrans._Vol_(Bytes)', 'TCP_UL_Retrans._Vol_(Bytes)']].mean()




average_tcp_retransmission_per_handset.plot(kind='bar', figsize=(12, 6))

plt.title('Average TCP Retransmission per Handset Type')

plt.xlabel('Handset Type')

plt.ylabel('Average TCP Retransmission Volume (Bytes)')

plt.legend(["Downlink", "Uplink"])

plt.show()




features = ['Avg_RTT_DL_(ms)', 'Avg_RTT_UL_(ms)', 'Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']




data_for_clustering = df[features].copy()




data_for_clustering.fillna(data_for_clustering.mean(), inplace=True)




scaler = StandardScaler()

scaled_data = scaler.fit_transform(data_for_clustering)




kmeans = KMeans(n_clusters=3, random_state=42)

df['Cluster'] = kmeans.fit_predict(scaled_data)




plt.figure(figsize=(10, 6))

for cluster in df['Cluster'].unique():

    cluster_data = df[df['Cluster'] == cluster]

    plt.scatter(cluster_data['Avg_RTT_DL_(ms)'], cluster_data['Avg_Bearer_TP_DL_(kbps)'], label=f'Cluster {cluster}')



plt.title('K-Means Clustering of Users Based on Experience Metrics')

plt.xlabel('Avg_RTT_DL_(ms)')

plt.ylabel('Avg_Bearer_TP_DL_(kbps)')

plt.legend()

plt.show()






less_engaged_cluster = df_num['Cluster'].value_counts().idxmin()  
less_engaged_centroid = kmeans.cluster_centers_[less_engaged_cluster]  



engagement_features = ['Avg_RTT_DL_(ms)', 'Avg_RTT_UL_(ms)', 'Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']




data_for_engagement = df_num[engagement_features].copy()




data_for_engagement.fillna(data_for_engagement.mean(), inplace=True)




engagement_scores = euclidean_distances(data_for_engagement, [less_engaged_centroid])




df_num['Engagement_Score'] = engagement_scores.flatten()




print(df_num[['MSISDN/Number', 'Cluster', 'Engagement_Score']])






worst_experience_cluster = df_num['Cluster'].value_counts().idxmax()  
worst_experience_centroid = kmeans.cluster_centers_[worst_experience_cluster]  



experience_features = ['Avg_RTT_DL_(ms)', 'Avg_RTT_UL_(ms)', 'Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)']




data_for_experience = df_num[experience_features].copy()




data_for_experience.fillna(data_for_experience.mean(), inplace=True)




experience_scores = euclidean_distances(data_for_experience, [worst_experience_centroid])




df_num['Experience_Score'] = experience_scores.flatten()




print(df_num[['MSISDN/Number', 'Cluster', 'Experience_Score']])


df_num['Satisfaction_Score'] = (df_num['Engagement_Score'] + df_num['Experience_Score']) / 2




top_satisfied_customers = df_num.sort_values(by='Satisfaction_Score', ascending=False).head(10)

print(top_satisfied_customers[['MSISDN/Number', 'Satisfaction_Score']])




features = ['Dur. (ms)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Handset Manufacturer', 'Handset Type']

target = 'Satisfaction_Score'  



X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)




numeric_features = ['Dur. (ms)', 'Dur. (s)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']

numeric_transformer = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='mean')),

    ('scaler', StandardScaler())

])



categorical_features = ['Handset Manufacturer', 'Handset Type']

categorical_transformer = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')),

    ('onehot', OneHotEncoder(handle_unknown='ignore'))

])




preprocessor = ColumnTransformer(

    transformers=[

        ('num', numeric_transformer, numeric_features),

        ('cat', categorical_transformer, categorical_features)

    ])




model = Pipeline(steps=[

    ('preprocessor', preprocessor),

    ('regressor', LinearRegression())

])




model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')








F_cluster = ['Engagement_Score', 'Experience_Score']




data_for_clustering = df[F_cluster].copy()




data_for_clustering.dropna(inplace=True)




scaler = StandardScaler()

scaled_data = scaler.fit_transform(data_for_clustering)




kmeans = KMeans(n_clusters=2, random_state=42)

df['Cluster_Engagement_Experience'] = kmeans.fit_predict(scaled_data)




plt.figure(figsize=(8, 5))

for cluster in df['Cluster_Engagement_Experience'].unique():

    cluster_data = df[df['Cluster_Engagement_Experience'] == cluster]

    plt.scatter(cluster_data['Engagement_Score'], cluster_data['Experience_Score'], label=f'Cluster {cluster}')



plt.title('K-Means Clustering of Engagement and Experience Scores')

plt.xlabel('Engagement Score')

plt.ylabel('Experience Score')

plt.legend()

plt.show()

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler







features = ['Your_Engagement_Column', 'Your_Experience_Column']




data_for_clustering = df[features].copy()




data_for_clustering.dropna(inplace=True)




scaler = StandardScaler()

scaled_data = scaler.fit_transform(data_for_clustering)




kmeans = KMeans(n_clusters=2, random_state=42)

df['Cluster_Engagement_Experience'] = kmeans.fit_predict(scaled_data)
import pandas as pd

from sqlalchemy import create_engine

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from scipy.stats.mstats import winsorize 

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

import plotly.express as px

from sklearn.cluster import KMeans

from sklearn.impute import SimpleImputer
database_name = 'telecom'

table_name = 'xdr_data'



connection_params = {"host": "localhost", "user": "postgres", "password": "Musy19", "port": "5432", "database": database_name}



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")




sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query, con= engine)
csv_file_path = "C:\\Users\\ADMIN\\10Academy\\Week-1\\telecom"

df.to_csv(csv_file_path, index=False)
df.columns = df.columns.str.replace(' ', '_')

df.info()

num_col = df.select_dtypes (include = ['number']).columns

cat_col = df.select_dtypes(include=['object']).columns



df_num = df[num_col]

df_cat = df[cat_col]
application_columns = [

    

    'Social_Media_DL_(Bytes)','Social_Media_UL_(Bytes)','Youtube_DL_(Bytes)', 'Youtube_UL_(Bytes)','Netflix_DL_(Bytes)', 'Netflix_UL_(Bytes)','Google_DL_(Bytes)', 'Google_UL_(Bytes)','Email_DL_(Bytes)', 'Email_UL_(Bytes)','Gaming_DL_(Bytes)', 'Gaming_UL_(Bytes)','Other_DL_(Bytes)', 'Other_UL_(Bytes)'

    

    ]



if df_num.isnull().any().any():

    df_num = df_num.fillna(df_num.mean())







winsorized_data = winsorize(df_num.to_numpy(), limits=[0.01, 0.01], axis=0)




df_num = pd.DataFrame(winsorized_data,columns=df_num.columns)
df_num.info()

scaler = StandardScaler()

df_scaled = scaler.fit_transform(df_num)




pca = PCA()

df_pca= pca.fit_transform(df_scaled)




explained_variance_ratio = pca.explained_variance_ratio_

cumulative_explained_variance = explained_variance_ratio.cumsum()




num_components = np.argmax(cumulative_explained_variance >= 0.95) + 1



pca = PCA(n_components = num_components)

df_pca = pca.fit_transform(df_scaled)

transposed_df = df_num.T

transposed_df

sessions_per_user = df_num.groupby('MSISDN/Number')['Bearer_Id'].count().reset_index()

sessions_per_user.columns = ['MSISDN/Number', 'Number_of_xDR_Sessions']




print(sessions_per_user)



sessions_per_user = df_num.groupby('MSISDN/Number')['Bearer_Id'].count().reset_index()

user_session_duration = df_num.groupby('MSISDN/Number')['Dur._(ms)'].sum().reset_index()



session_frequency = sessions_per_user / user_session_duration




print(session_frequency)


session_duration = df_num[['MSISDN/Number', 'Dur._(ms)']]



session_duration['Total_Duration_Sec'] = session_duration['Dur._(ms)'] / 1000



session_duration['Avg_Duration_Sec'] = session_duration.groupby('MSISDN/Number')['Total_Duration_Sec'].transform('mean')



session_duration.head(10)


user_traffic = df_num[['MSISDN/Number', 'Total_DL_(Bytes)', 'Total_UL_(Bytes)']]




user_traffic['Total_Traffic_Bytes'] = user_traffic['Total_DL_(Bytes)'] + user_traffic['Total_UL_(Bytes)']




user_traffic['Avg_Traffic_Bytes'] = user_traffic.groupby('MSISDN/Number')['Total_Traffic_Bytes'].transform('mean')



user_traffic.head(10)

user_traffic.locn['Total_Traffic_Bytes'] = user_traffic['Total_DL_(Bytes)'] + user_traffic['Total_UL_(Bytes)']

user_traffic['Total_Traffic_Bytes'] 

imputer = SimpleImputer(strategy='mean')

user_traffic_imputed = imputer.fit_transform(user_traffic[['Total_Traffic_Bytes', 'Avg_Traffic_Bytes']])




scaler = StandardScaler()

normalized_data = scaler.fit_transform(user_traffic_imputed)




kmeans = KMeans(n_clusters=3, random_state=42)

df_num['Cluster'] = kmeans.fit_predict(normalized_data)






plt.figure(figsize=(10, 6))

plt.scatter(df_num['Total_Traffic_Bytes'], df_num['Avg_Traffic_Bytes'], c=df_num['Cluster'], cmap='viridis')

plt.title('K-Means Clustering of User Engagement')

plt.xlabel('Total Traffic (Bytes)')

plt.ylabel('Average Traffic (Bytes)')

plt.show()
import pandas as pd

import numpy as np

import psycopg2

import matplotlib.pyplot as plt

import seaborn as sns

from psycopg2 import sql

from sqlalchemy import create_engine

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

connection_params = {

  "host": "localhost",

  "user": "postgres",

  "password": "post33
  "port": "5432"

}

db_name = 'telecom'

connection_params["database"] = db_name

engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

db_conn = engine.connect()

df = pd.read_sql("select * from \"xdr_data\"", db_conn);

df
df.isna().sum()

percent_missing = df.isna().sum() * 100 / len(df)

missing_percent = pd.DataFrame({'column_name': df.columns,'percent_missing': percent_missing})

missing_percent.sort_values('percent_missing', inplace=True)

missing_percent

columns_to_remove = missing_percent[missing_percent['percent_missing'] >= 30].index.tolist()

columns_to_remove

columns_to_remove = [col for col in columns_to_remove if col not in ['TCP UL Retrans. Vol (Bytes)',

    'TCP DL Retrans. Vol (Bytes)']]


clean_df = df.drop(columns_to_remove,axis=1)

clean_df.isna().sum()
clean_df['TCP DL Retrans. Vol (Bytes)'] = clean_df['TCP DL Retrans. Vol (Bytes)'].fillna(method='bfill')

clean_df['TCP UL Retrans. Vol (Bytes)'] = clean_df['TCP UL Retrans. Vol (Bytes)'].fillna(method='bfill')

clean_df['Avg RTT DL (ms)'] = clean_df['Avg RTT DL (ms)'].fillna(method='ffill')

clean_df['Avg RTT UL (ms)'] = clean_df['Avg RTT UL (ms)'].fillna(method='ffill')

clean_df.isna().sum()

clean_df['Handset Type'] = clean_df['Handset Type'].fillna('undefined')

clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].fillna('undefined')

clean_df.isna().sum()

clean_df.dropna(inplace=True)

clean_df = clean_df.drop_duplicates()

clean_df.rename(columns={'Dur. (ms)': 'Dur (s)', 'Dur. (ms).1': 'Dur (ms)'}, inplace=True)

clean_df.drop(['Dur (s)'], axis=1, inplace=True)

clean_df.isna().sum()
clean_df.dtypes

clean_df['Bearer Id'] = clean_df['Bearer Id'].astype("int64")

clean_df['MSISDN/Number'] = clean_df['MSISDN/Number'].astype("int64")

clean_df

clean_df['Last Location Name'] = clean_df['Last Location Name'].astype("string")

clean_df['Handset Type'] = clean_df['Handset Type'].astype("string")

clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].astype("string")

clean_df.dtypes
clean_df.hist(bins=60,figsize=(20,10))
clean_df.columns
clean_df['Handset Type'].value_counts()
pd.DataFrame(clean_df['Handset Type'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
clean_df['Handset Manufacturer'].value_counts()
pd.DataFrame(clean_df['Handset Manufacturer'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
samsung_manu = clean_df[clean_df['Handset Manufacturer'] == 'Samsung']

pd.DataFrame(samsung_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
apple_manu = clean_df[clean_df['Handset Manufacturer'] == 'Apple']

pd.DataFrame(apple_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
huawei_manu = clean_df[clean_df['Handset Manufacturer'] == 'Huawei']

pd.DataFrame(huawei_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
clean_df.columns

def fix_outliers(df: pd.DataFrame):

    
    for col in df.select_dtypes('float64').columns.tolist():

        Q1 = df[col].quantile(0.25)

        Q3 = df[col].quantile(0.75)

        IQR = Q3 - Q1

        lower = Q1 - (IQR * 1.5)

        upper = Q3 + (IQR * 1.5)



        df[col] = np.where(df[col] > upper, upper, df[col])

        df[col] = np.where(df[col] < lower, lower, df[col])

    return df



def hist(df:pd.DataFrame, column:str, color:str='orange')->None:

    sns.displot(data=df, x=column, color=color, kde=True, height=6, aspect=2)

    plt.title(f'Distribution of {column}', size=20, fontweight='bold')

    plt.show()

def scatter(df: pd.DataFrame, x_col: str, y_col: str) -> None:

    plt.figure(figsize=(12, 7))

    sns.scatterplot(data = df, x=x_col, y=y_col)

    plt.title(f'{x_col} Vs. {y_col}\n', size=20)

    plt.xticks(fontsize=14)

    plt.yticks( fontsize=14)

    plt.show()
clean_df = fix_outliers(clean_df)




clean_df['Dur (ms)'].plot(kind='box',figsize=(20,10))



hist(clean_df, 'Dur (ms)', 'blue')
clean_df['Data_Volume_Gaming (Bytes)'] = clean_df['Gaming DL (Bytes)'] +  clean_df['Gaming UL (Bytes)']

clean_df['Data_Volume_Social (Bytes)'] = clean_df['Social Media DL (Bytes)'] +  clean_df['Social Media UL (Bytes)']

clean_df['Data_Volume_Google (Bytes)'] = clean_df['Google DL (Bytes)'] +  clean_df['Google UL (Bytes)']

clean_df['Data_Volume_Email (Bytes)'] = clean_df['Email DL (Bytes)'] +  clean_df['Email UL (Bytes)']

clean_df['Data_Volume_Youtube (Bytes)'] = clean_df['Youtube DL (Bytes)'] +  clean_df['Youtube UL (Bytes)']

clean_df['Data_Volume_Netflix (Bytes)'] = clean_df['Netflix DL (Bytes)'] +  clean_df['Netflix UL (Bytes)']                                                               

clean_df['Data_Volume_Other (Bytes)'] = clean_df['Other DL (Bytes)'] +  clean_df['Other UL (Bytes)']

clean_df['Data_Volume_Total (Bytes)'] = clean_df['Total DL (Bytes)'] +  clean_df['Total UL (Bytes)']

agg = clean_df.groupby('MSISDN/Number')[['Data_Volume_Gaming (Bytes)','Data_Volume_Social (Bytes)','Data_Volume_Google (Bytes)'\

                                 ,'Data_Volume_Email (Bytes)','Data_Volume_Youtube (Bytes)','Data_Volume_Netflix (Bytes)'\

                                 ,'Data_Volume_Other (Bytes)','Data_Volume_Total (Bytes)','Dur (ms)']].aggregate('sum')

agg_df = pd.DataFrame(agg)

agg_df['count'] = clean_df['MSISDN/Number'].value_counts()

agg_df
hist(clean_df, 'Data_Volume_Gaming (Bytes)')
hist(clean_df, 'Data_Volume_Social (Bytes)')
hist(clean_df, 'Data_Volume_Google (Bytes)')
hist(clean_df, 'Data_Volume_Email (Bytes)')
hist(clean_df, 'Data_Volume_Youtube (Bytes)')
hist(clean_df, 'Data_Volume_Netflix (Bytes)')
hist(clean_df, 'Data_Volume_Other (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Gaming (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Social (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Google (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Email (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Netflix (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Other (Bytes)', 'Data_Volume_Total (Bytes)')
def heatmap(df:pd.DataFrame, title:str, cmap='Reds')->None:

    plt.figure(figsize=(13, 7))

    sns.heatmap(df, annot=True, cmap=cmap, vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=True )

    plt.title(title, size=18, fontweight='bold')

    plt.show()
df_corr = clean_df[[

    'Data_Volume_Gaming (Bytes)', 'Data_Volume_Social (Bytes)', 'Data_Volume_Google (Bytes)',

    'Data_Volume_Email (Bytes)', 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Netflix (Bytes)',

    'Data_Volume_Other (Bytes)']

].corr()

df_corr
heatmap(df_corr, "Correlation of Data Volume of Applications")
df_float = clean_df.select_dtypes(include=[float])

df_int = clean_df.select_dtypes(include=[int])

df = clean_df

number_array = df_float.columns.to_list() + df_int.columns.to_list()

number_array

features = number_array


x = df.loc[:, features].values


x = StandardScaler().fit_transform(x)
number = 20

number_array = range(0,number)

pca = PCA(n_components=number)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents

             , columns = number_array)

principalDf
pca.explained_variance_ratio_.sum()
number = 30

number_array = range(0,number)

pca = PCA(n_components=number)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents

             , columns = number_array)

principalDf

pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
number = 40

number_array = range(0,number)

pca = PCA(n_components=number)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents

             , columns = number_array)

principalDf

pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()

number = 2

number_array = range(0,number)

pca = PCA(n_components=number)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents

             , columns = ["1","2"])

principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
df.columns
try:

    print('writing to the database')

    frame = clean_df.to_sql(

        "clean_df", con=engine, if_exists='replace')

    print('successful')

except Exception as e:

  print("Error writing to database: ", e)
import pandas as pd

import numpy as np

import psycopg2

import matplotlib.pyplot as plt

import seaborn as sns

from psycopg2 import sql

from sqlalchemy import create_engine

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

connection_params = {

  "host": "localhost",

  "user": "postgres",

  "password": "post33
  "port": "5432"

}

db_name = 'telecom'

connection_params["database"] = db_name

engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

db_conn = engine.connect()

df = pd.read_sql("select * from \"xdr_data\"", db_conn);

df
df.isna().sum()

percent_missing = df.isna().sum() * 100 / len(df)

missing_percent = pd.DataFrame({'column_name': df.columns,'percent_missing': percent_missing})

missing_percent.sort_values('percent_missing', inplace=True)

missing_percent

columns_to_remove = missing_percent[missing_percent['percent_missing'] >= 30].index.tolist()

columns_to_remove

clean_df = df.drop(columns_to_remove,axis=1)

clean_df.isna().sum()

clean_df['Avg RTT DL (ms)'] = clean_df['Avg RTT DL (ms)'].fillna(method='ffill')

clean_df['Avg RTT UL (ms)'] = clean_df['Avg RTT UL (ms)'].fillna(method='ffill')

clean_df.isna().sum()

clean_df['Handset Type'] = clean_df['Handset Type'].fillna('undefined')

clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].fillna('undefined')

clean_df.isna().sum()

clean_df.dropna(inplace=True)

clean_df = clean_df.drop_duplicates()

clean_df.drop(['Dur. (ms).1'], axis=1, inplace=True)

clean_df.isna().sum()
clean_df.dtypes


clean_df['Bearer Id'] = clean_df['Bearer Id'].astype("int64")

clean_df['MSISDN/Number'] = clean_df['MSISDN/Number'].astype("int64")

clean_df

clean_df['Last Location Name'] = clean_df['Last Location Name'].astype("string")

clean_df['Handset Type'] = clean_df['Handset Type'].astype("string")

clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].astype("string")

clean_df.dtypes
clean_df.hist(bins=60,figsize=(20,10))
clean_df.columns
clean_df['Handset Type'].value_counts()
pd.DataFrame(clean_df['Handset Type'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
clean_df['Handset Manufacturer'].value_counts()
pd.DataFrame(clean_df['Handset Manufacturer'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
samsung_manu = clean_df[clean_df['Handset Manufacturer'] == 'Samsung']

pd.DataFrame(samsung_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
apple_manu = clean_df[clean_df['Handset Manufacturer'] == 'Apple']

pd.DataFrame(apple_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
huawei_manu = clean_df[clean_df['Handset Manufacturer'] == 'Huawei']

pd.DataFrame(huawei_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
clean_df.columns

def fix_outliers(df: pd.DataFrame):

    
    for col in df.select_dtypes('float64').columns.tolist():

        Q1 = df[col].quantile(0.25)

        Q3 = df[col].quantile(0.75)

        IQR = Q3 - Q1

        lower = Q1 - (IQR * 1.5)

        upper = Q3 + (IQR * 1.5)



        df[col] = np.where(df[col] > upper, upper, df[col])

        df[col] = np.where(df[col] < lower, lower, df[col])

    return df



def hist(df:pd.DataFrame, column:str, color:str='orange')->None:

    sns.displot(data=df, x=column, color=color, kde=True, height=6, aspect=2)

    plt.title(f'Distribution of {column}', size=20, fontweight='bold')

    plt.show()

def scatter(df: pd.DataFrame, x_col: str, y_col: str) -> None:

    plt.figure(figsize=(12, 7))

    sns.scatterplot(data = df, x=x_col, y=y_col)

    plt.title(f'{x_col} Vs. {y_col}\n', size=20)

    plt.xticks(fontsize=14)

    plt.yticks( fontsize=14)

    plt.show()
clean_df = fix_outliers(clean_df)




clean_df['Dur. (ms)'].plot(kind='box',figsize=(20,10))



hist(clean_df, 'Dur. (ms)', 'blue')
clean_df['Data_Volume_Gaming (Bytes)'] = clean_df['Gaming DL (Bytes)'] +  clean_df['Gaming UL (Bytes)']

clean_df['Data_Volume_Social (Bytes)'] = clean_df['Social Media DL (Bytes)'] +  clean_df['Social Media UL (Bytes)']

clean_df['Data_Volume_Google (Bytes)'] = clean_df['Google DL (Bytes)'] +  clean_df['Google UL (Bytes)']

clean_df['Data_Volume_Email (Bytes)'] = clean_df['Email DL (Bytes)'] +  clean_df['Email UL (Bytes)']

clean_df['Data_Volume_Youtube (Bytes)'] = clean_df['Youtube DL (Bytes)'] +  clean_df['Youtube UL (Bytes)']

clean_df['Data_Volume_Netflix (Bytes)'] = clean_df['Netflix DL (Bytes)'] +  clean_df['Netflix UL (Bytes)']                                                               

clean_df['Data_Volume_Other (Bytes)'] = clean_df['Other DL (Bytes)'] +  clean_df['Other UL (Bytes)']

clean_df['Data_Volume_Total (Bytes)'] = clean_df['Total DL (Bytes)'] +  clean_df['Total UL (Bytes)']

agg = clean_df.groupby('MSISDN/Number')[['Data_Volume_Gaming (Bytes)','Data_Volume_Social (Bytes)','Data_Volume_Google (Bytes)'\

                                 ,'Data_Volume_Email (Bytes)','Data_Volume_Youtube (Bytes)','Data_Volume_Netflix (Bytes)'\

                                 ,'Data_Volume_Other (Bytes)','Data_Volume_Total (Bytes)','Dur. (ms)']].aggregate('sum')

agg_df = pd.DataFrame(agg)

agg_df['count'] = clean_df['MSISDN/Number'].value_counts()

agg_df
hist(clean_df, 'Data_Volume_Gaming (Bytes)')
hist(clean_df, 'Data_Volume_Social (Bytes)')
hist(clean_df, 'Data_Volume_Google (Bytes)')
hist(clean_df, 'Data_Volume_Email (Bytes)')
hist(clean_df, 'Data_Volume_Youtube (Bytes)')
hist(clean_df, 'Data_Volume_Netflix (Bytes)')
hist(clean_df, 'Data_Volume_Other (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Gaming (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Social (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Google (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Email (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Netflix (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Other (Bytes)', 'Data_Volume_Total (Bytes)')
def heatmap(df:pd.DataFrame, title:str, cmap='Reds')->None:

    plt.figure(figsize=(13, 7))

    sns.heatmap(df, annot=True, cmap=cmap, vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=True )

    plt.title(title, size=18, fontweight='bold')

    plt.show()
df_corr = clean_df[[

    'Data_Volume_Gaming (Bytes)', 'Data_Volume_Social (Bytes)', 'Data_Volume_Google (Bytes)',

    'Data_Volume_Email (Bytes)', 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Netflix (Bytes)',

    'Data_Volume_Other (Bytes)']

].corr()

df_corr
heatmap(df_corr, "Correlation of Data Volume of Applications")
df_float = clean_df.select_dtypes(include=[float])

df_int = clean_df.select_dtypes(include=[int])

df = clean_df

number_array = df_float.columns.to_list() + df_int.columns.to_list()

number_array

features = number_array


x = df.loc[:, features].values


x = StandardScaler().fit_transform(x)
pca.explained_variance_ratio_.sum()
number = 20

number_array = range(0,number)

pca = PCA(n_components=number)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents

             , columns = number_array)

principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
number = 40

number_array = range(0,number)

pca = PCA(n_components=number)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents

             , columns = number_array)

principalDf

pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()

number = 2

number_array = range(0,number)

pca = PCA(n_components=number)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents

             , columns = ["1","2"])

principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
import pandas as pd

import numpy as np

import psycopg2

import matplotlib.pyplot as plt

import seaborn as sns

import pickle

from psycopg2 import sql

from sqlalchemy import create_engine



from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler, normalize

import plotly.io as pio

import plotly.express as px

import plotly.graph_objects as go

from plotly.subplots import make_subplots
connection_params = {

  "host": "localhost",

  "user": "postgres",

  "password": "post33
  "port": "5432"

}

db_name = 'telecom'

connection_params["database"] = db_name

engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

db_conn = engine.connect()

df = pd.read_sql("select * from \"clean_df\"", db_conn);

df.info()
user_experience_df = df[[

    'MSISDN/Number',

    'Avg RTT DL (ms)',

    'Avg RTT UL (ms)',

    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

    'TCP DL Retrans. Vol (Bytes)',

    'TCP UL Retrans. Vol (Bytes)',

    'Handset Type']]

user_experience_df


user_experience_df['Avg RTT Total (ms)'] = user_experience_df['Avg RTT DL (ms)'] + user_experience_df['Avg RTT UL (ms)']

user_experience_df['Avg Bearer TP Total (kbps)'] = user_experience_df['Avg Bearer TP DL (kbps)'] + user_experience_df['Avg Bearer TP UL (kbps)']

user_experience_df['TCP Retrans. Vol Total (Bytes)'] = user_experience_df['TCP DL Retrans. Vol (Bytes)'] + user_experience_df['TCP UL Retrans. Vol (Bytes)']

user_experience_df_agg = user_experience_df.groupby(

    'MSISDN/Number').agg({

        'Avg RTT Total (ms)': 'sum',

        'Avg Bearer TP Total (kbps)': 'sum',

        'TCP Retrans. Vol Total (Bytes)': 'sum',

        'Handset Type': [lambda x: x.mode()[0]]})

user_experience_df_agg.head()
user_experience_df_agg_new = pd.DataFrame(columns=[

    "Avg RTT Total (ms)",

    "Avg Bearer TP Total (kbps)",

    "TCP Retrans. Vol Total (Bytes)",

    "Handset Type"])



user_experience_df_agg_new["Avg RTT Total (ms)"] = user_experience_df_agg["Avg RTT Total (ms)"]['sum']

user_experience_df_agg_new["Avg Bearer TP Total (kbps)"] = user_experience_df_agg["Avg Bearer TP Total (kbps)"]['sum']

user_experience_df_agg_new["TCP Retrans. Vol Total (Bytes)"] = user_experience_df_agg["TCP Retrans. Vol Total (Bytes)"]['sum']

user_experience_df_agg_new["Handset Type"] = user_experience_df_agg["Handset Type"]['<lambda>']

user_experience_df_agg_new.head()
telco_tcp = user_experience_df_agg_new.sort_values('TCP Retrans. Vol Total (Bytes)', ascending=False)

top_10 = telco_tcp.head(10)['TCP Retrans. Vol Total (Bytes)']

bottom_10 = telco_tcp.tail(10)['TCP Retrans. Vol Total (Bytes)']

most_10 = user_experience_df_agg_new['TCP Retrans. Vol Total (Bytes)'].value_counts().head(10)
def plotly_multi_hist(sr, rows, cols, title_text, subplot_titles):

    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)

    for i in range(rows):

        for j in range(cols):

            x = ["-> " + str(i) for i in sr[i+j].index]

            fig.add_trace(go.Bar(x=x, y=sr[i+j].values ), row=i+1, col=j+1)

    fig.update_layout(showlegend=False, title_text=title_text)

    fig.show()
pio.renderers.default = "notebook"

plotly_multi_hist([top_10, bottom_10, most_10], 1, 3,

    "TCP values", ['Top 10', 'Bottom 10', 'Most 10'])
telco_rtt = user_experience_df_agg_new.sort_values('Avg RTT Total (ms)', ascending=False)

top_10 = telco_rtt.head(10)['Avg RTT Total (ms)']

bottom_10 = telco_rtt.tail(10)['Avg RTT Total (ms)']

most_10 = user_experience_df_agg_new['Avg RTT Total (ms)'].value_counts().head(10)
plotly_multi_hist([top_10, bottom_10, most_10], 1, 3,

    "RTT values", ['Top 10', 'Bottom 10', 'Most 10'])
telco_tp = user_experience_df_agg_new.sort_values('Avg Bearer TP Total (kbps)', ascending=False)

top_10 = telco_tp.head(10)['Avg Bearer TP Total (kbps)']

bottom_10 = telco_tp.tail(10)['Avg Bearer TP Total (kbps)']

most_10 = user_experience_df_agg_new['Avg Bearer TP Total (kbps)'].value_counts().head(10)
plotly_multi_hist([top_10, bottom_10, most_10], 1, 3,

    "TP values", ['Top 10', 'Bottom 10', 'Most 10'])
handset_type_df = user_experience_df_agg_new.groupby('Handset Type').mean()

handset_type_df.head()
handset_tp_df = handset_type_df.sort_values(

    'Avg Bearer TP Total (kbps)', ascending=False)

handset_tp_df.head()
handset_tcp_df = handset_tp_df.sort_values(

    'TCP Retrans. Vol Total (Bytes)', ascending=False)

handset_tcp_df.head()
user_experience_df_agg_new.boxplot()
def replace_outliers_with_fences(df,columns):

    for col in columns:

        Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)

        IQR = Q3 - Q1

        cut_off = IQR * 1.5

        lower, upper = Q1 - cut_off, Q3 + cut_off



        df[col] = np.where(df[col] > upper, upper, df[col])

        df[col] = np.where(df[col] < lower, lower, df[col])

    return df
user_experience_df_agg_new = user_experience_df_agg_new[["Avg RTT Total (ms)",

    "Avg Bearer TP Total (kbps)",

    "TCP Retrans. Vol Total (Bytes)"]]

replace_outliers_with_fences(user_experience_df_agg_new,["Avg RTT Total (ms)",

    "Avg Bearer TP Total (kbps)",

    "TCP Retrans. Vol Total (Bytes)"])
user_experience_df_agg_new.boxplot()
scaler_instance = StandardScaler()

scaled_data = scaler_instance.fit_transform(user_experience_df_agg_new)

scaled_data
normalized_data = normalize(scaled_data)

normalized_data
kmeans = KMeans(n_clusters=3, random_state=1).fit(normalized_data)

kmeans.labels_
user_experience_df_agg_new.insert(0, 'cluster', kmeans.labels_)

user_experience_df_agg_new
user_experience_df_agg_new['cluster'].value_counts()
fig = px.scatter(user_experience_df_agg_new, x='TCP Retrans. Vol Total (Bytes)', y='Avg Bearer TP Total (kbps)',

                 color='cluster', size='Avg RTT Total (ms)')

fig.show()
cluster0 = user_experience_df_agg_new[user_experience_df_agg_new["cluster"]==0]

cluster0[["Avg RTT Total (ms)",

    "Avg Bearer TP Total (kbps)",

    "TCP Retrans. Vol Total (Bytes)"]].describe()

cluster1 = user_experience_df_agg_new[user_experience_df_agg_new["cluster"]==1]

cluster1[["Avg RTT Total (ms)",

    "Avg Bearer TP Total (kbps)",

    "TCP Retrans. Vol Total (Bytes)"]].describe()

cluster2 = user_experience_df_agg_new[user_experience_df_agg_new["cluster"]==2]

cluster2[["Avg RTT Total (ms)",

    "Avg Bearer TP Total (kbps)",

    "TCP Retrans. Vol Total (Bytes)"]].describe()


try:

    print('writing to the database')

    frame = user_experience_df_agg_new.to_sql(

        "user_experience", con=engine, if_exists='replace')

    print('successful')

except Exception as e:

  print("Error writing to database: ", e)

with open("../models/user_experience.pkl", "wb") as f:

    pickle.dump(kmeans, f)
import pandas as pd

import numpy as np

import psycopg2

import matplotlib.pyplot as plt

import pickle

import seaborn as sns

from psycopg2 import sql

from sqlalchemy import create_engine



from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler, normalize

import plotly.io as pio

import plotly.express as px

import plotly.graph_objects as go

from plotly.subplots import make_subplots

connection_params = {

  "host": "localhost",

  "user": "postgres",

  "password": "post33
  "port": "5432"

}

db_name = 'telecom'

connection_params["database"] = db_name

engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

db_conn = engine.connect()

df = pd.read_sql("select * from \"clean_df\"", db_conn);

df.info()
user_engagement_df = df[['MSISDN/Number', 'Bearer Id', 'Dur (ms)', 'Data_Volume_Total (Bytes)']]

user_engagement_df
user_engagement_df = user_engagement_df.groupby(

    'MSISDN/Number').agg({'Bearer Id': 'count', 'Dur (ms)': 'sum', 'Data_Volume_Total (Bytes)': 'sum'})

user_engagement_df = user_engagement_df.rename(

    columns={'Bearer Id': 'xDR Sessions'})

user_engagement_df.head()

user_engagement_df.nlargest(10, 'xDR Sessions')
user_engagement_df.nlargest(10, 'Dur (ms)')
user_engagement_df.nlargest(10, 'Data_Volume_Total (Bytes)')
user_engagement_df.boxplot();
def replace_outliers_with_fences(df,columns):

    for col in columns:

        Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)

        IQR = Q3 - Q1

        cut_off = IQR * 1.5

        lower, upper = Q1 - cut_off, Q3 + cut_off



        df[col] = np.where(df[col] > upper, upper, df[col])

        df[col] = np.where(df[col] < lower, lower, df[col])

    return df
user_engagement_df = replace_outliers_with_fences(user_engagement_df,['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)'])
user_engagement_df.boxplot()
scaler_instance = StandardScaler()

scaled_data = scaler_instance.fit_transform(user_engagement_df)

scaled_data
normalized_data = normalize(scaled_data)

normalized_data
kmeans = KMeans(n_clusters=3, random_state=1).fit(normalized_data)

kmeans.labels_
user_engagement_df.insert(0, 'cluster', kmeans.labels_)

user_engagement_df
user_engagement_df['cluster'].value_counts()
pio.renderers.default = "notebook"

fig = px.scatter(user_engagement_df, x='Data_Volume_Total (Bytes)', y='Dur (ms)',

                 color='cluster', size='xDR Sessions')

fig.show()
sns.pairplot(

    user_engagement_df[['cluster','xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']],

     hue = 'cluster', diag_kind = 'kde',

             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},

             height=3

);
cluster0 = user_engagement_df[user_engagement_df["cluster"]==0]

cluster0[['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']].describe()
cluster1 = user_engagement_df[user_engagement_df["cluster"]==1]

cluster1[['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']].describe()
cluster2 = user_engagement_df[user_engagement_df["cluster"]==2]

cluster2[['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']].describe()
user_app_engagement_df = df[['MSISDN/Number', 'Data_Volume_Gaming (Bytes)', 'Data_Volume_Social (Bytes)',

    'Data_Volume_Google (Bytes)', 'Data_Volume_Email (Bytes)', 'Data_Volume_Youtube (Bytes)',

    'Data_Volume_Netflix (Bytes)', 'Data_Volume_Other (Bytes)']]
user_app_engagement_df = user_app_engagement_df.groupby(

    'MSISDN/Number').sum()

user_app_engagement_df.head()
gaming = user_app_engagement_df.nlargest(10, "Data_Volume_Gaming (Bytes)")['Data_Volume_Gaming (Bytes)']

social_media = user_app_engagement_df.nlargest(10, "Data_Volume_Social (Bytes)")['Data_Volume_Social (Bytes)']

google = user_app_engagement_df.nlargest(10, "Data_Volume_Google (Bytes)")['Data_Volume_Google (Bytes)']

email = user_app_engagement_df.nlargest(10, "Data_Volume_Email (Bytes)")['Data_Volume_Email (Bytes)']

youtube = user_app_engagement_df.nlargest(10, "Data_Volume_Youtube (Bytes)")['Data_Volume_Youtube (Bytes)']

social_media = user_app_engagement_df.nlargest(10, "Data_Volume_Netflix (Bytes)")['Data_Volume_Netflix (Bytes)']

other = user_app_engagement_df.nlargest(10, "Data_Volume_Other (Bytes)")['Data_Volume_Other (Bytes)']
from scipy.spatial.distance import cdist

def choose_kmeans(df: pd.DataFrame, num: int):

    
    
    distortions = []

    inertias = []

    K = range(1, num)

    for k in K:

        kmeans = KMeans(n_clusters=k, random_state=0).fit(df)

        distortions.append(sum(

            np.min(cdist(df, kmeans.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])

        inertias.append(kmeans.inertia_)



    return (distortions, inertias)
distortions, inertias =  choose_kmeans(normalized_data, 20)
fig = make_subplots(

    rows=1, cols=2, subplot_titles=("Distortion", "Inertia")

)

fig.add_trace(go.Scatter(x=np.array(range(1, 20)), y=distortions), row=1, col=1)

fig.add_trace(go.Scatter(x=np.array(range(1, 20)), y=inertias), row=1, col=2)

fig.update_layout(title_text="The Elbow Method", height=500)

fig.show()
kmeans = KMeans(n_clusters=6, random_state=6).fit(normalized_data)

kmeans.labels_
user_engagement_df["cluster"]= kmeans.labels_

user_engagement_df
fig = px.scatter(user_engagement_df, x='Data_Volume_Total (Bytes)', y='Dur (ms)',

                 color='cluster', size='xDR Sessions')

fig.show()

try:

    print('writing to the database')

    frame = user_engagement_df.to_sql(

        "user_engagement", con=engine, if_exists='replace')

    print('successful')

except Exception as e:

  print("Error writing to database: ", e)

with open("../models/user_engagement.pkl", "wb") as f:

    pickle.dump(kmeans, f)
import os
from abc import ABC, abstractmethod
from sqlalchemy.engine.base import Engine
from typing import Generic, Optional, TypeVar
from dotenv import load_dotenv

RawConnectionT = TypeVar("RawConnectionT")

class ConnectionBase(ABC, Generic[RawConnectionT]):
    """The abstract base class that all connections must inherit from

        This base class provides connection authors with a way to set up 
        database parameters like database name, database password, port, and also 
        instance of database engine.
    """
    def __init__(self, **kwargs):
        """ create a BaseConnection

        Parameters
        ----------
        **kwargs: dict
            dictionary of parameters of any length to pass tho the connection class

        Returns
        -------
        None

        """
        self._kwargs = kwargs if kwargs else {
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD'),
            'host': os.getenv('DB_HOST'),
            'port': os.getenv('DB_PORT'),
            'database': os.getenv('DB_NAME'),
        }
        self._raw_instance: Optional[Engine] = self._connect()

    @property
    def _instance(self) -> RawConnectionT:
        """Get an instance of the underlying connection, creating a new one if needed."""
        if self._raw_instance is None:
            self._raw_instance = self._connect()

        return self._raw_instance

    @abstractmethod
    def _connect(self) -> RawConnectionT:
        """Create an instance of an underlying connection object.

        This abstract method is the one method that we require subclasses of
        BaseConnection to provide an implementation for. It is called when first
        creating a connection and when reconnecting after a connection is reset.

        Returns
        -------
        RawConnectionT
            The underlying connection object.
        """
        raise NotImplementedError

    def __enter__(self):
        self._raw_instance = self._connect()
        return self._raw_instance

    def __exit__(self, exc_type, exc_value, traceback):
        if self._raw_instance is not None:
            self._raw_instance.dispose()
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

import logging
from sqlalchemy import create_engine
from connections.connection_base import ConnectionBase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PostgresConnection(ConnectionBase):
    """A class representing a connection to a PostgreSQL database.

    This class inherits from ConnectionBase, providing a standardized way to manage
    the connection to the PostgreSQL database.

    Parameters
    ----------
    **kwargs : dict
        Keyword arguments containing the database connection parameters.

    Attributes
    ----------
    _kwargs : dict
        Dictionary containing the database connection parameters.
    _raw_instance : sqlalchemy.engine.base.Engine
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def _connect(self):
        """Create an instance of the underlying SQLAlchemy Engine for PostgreSQL.

        Returns
        -------
        sqlalchemy.engine.base.Engine
            The SQLAlchemy Engine instance representing the PostgreSQL database connection.

        Raises
        ------
        Exception
            If there is an error connecting to the database.
        """
        try:
            engine = create_engine(
                f"postgresql://{self._kwargs['user']}:{self._kwargs['password']}@{self._kwargs['host']}:{self._kwargs['port']}/{self._kwargs['database']}"
            )

            engine.connect()

            return engine

        except Exception as e:
            logger.error(f"Error connecting to the database: {e}")
            raise
import os, sys



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

df = rd.read_data()
df.shape
df.head()
df.info()
df.isnull().sum()
df.duplicated().sum()
df.describe()
for col in df.columns:

    print(df[col].value_counts())
import os, sys

import pandas as pd

rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.write_to_db as wd

import scripts.data_cleaning as sd 

df = rd.read_data()
df = sd.drop_high_missing_columns(df, 0.7)

df.columns

def remove_missing_values():

    columns_to_check = [

    'Start', 

    'Start ms', 

    'End', 

    'End ms', 

    'Dur. (ms)',

    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

    'Activity Duration DL (ms)',                        

    'Activity Duration UL (ms)',                        

    'Dur. (ms).1',

    'Total UL (Bytes)',                                 

    'Total DL (Bytes)'

    ]



    return sd.remove_rows_with_missing_values(df, columns_to_check)



cleaned_df = remove_missing_values()
df.shape
cleaned_df.isnull().sum()

def impute_columns():

    columns_to_impute = [

        "Avg RTT DL (ms)",                             

        "Avg RTT UL (ms)",                              

        "TCP DL Retrans. Vol (Bytes)",                  

        "TCP UL Retrans. Vol (Bytes)",                  

        "DL TP < 50 Kbps (%)",                            

        "50 Kbps < DL TP < 250 Kbps (%)",              

        "250 Kbps < DL TP < 1 Mbps (%)",                  

        "DL TP > 1 Mbps (%)",                             

        "UL TP < 10 Kbps (%)",                           

        "10 Kbps < UL TP < 50 Kbps (%)",                 

        "50 Kbps < UL TP < 300 Kbps (%)",                 

        "UL TP > 300 Kbps (%)",                           

        "HTTP DL (Bytes)",                              

        "HTTP UL (Bytes)",                             

        "Nb of sec with 125000B < Vol DL",             

        "Nb of sec with 1250B < Vol UL < 6250B",        

        "Nb of sec with 31250B < Vol DL < 125000B",                   

        "Nb of sec with 6250B < Vol DL < 31250B",           

        "Nb of sec with Vol DL < 6250B",                  

        "Nb of sec with Vol UL < 1250B"                 

    ]



    return sd.impute_numeric_missing(cleaned_df, columns_to_impute)





imputed_df = impute_columns()
imputed_df.isnull().sum()
imputed_df.shape
def replace_with_mode():

    columns_to_replace = [

        'Bearer Id',

        'IMSI',

        'MSISDN/Number',

        'IMEI',        

        'Last Location Name',

        'Handset Manufacturer',

        'Handset Type' 

    ]



    return sd.replace_column_with_mode(imputed_df, columns_to_replace)



cleaned_df = replace_with_mode()
cleaned_df.isnull().sum()
cleaned_df.shape
def handle_outliers():

    columns = [

        "Avg RTT DL (ms)",                             

        "Avg RTT UL (ms)",                              

        "TCP DL Retrans. Vol (Bytes)",                  

        "TCP UL Retrans. Vol (Bytes)",                  

        "DL TP < 50 Kbps (%)",                            

        "50 Kbps < DL TP < 250 Kbps (%)",              

        "250 Kbps < DL TP < 1 Mbps (%)",                  

        "DL TP > 1 Mbps (%)",                             

        "UL TP < 10 Kbps (%)",                           

        "10 Kbps < UL TP < 50 Kbps (%)",                 

        "50 Kbps < UL TP < 300 Kbps (%)",                 

        "UL TP > 300 Kbps (%)",                           

        "HTTP DL (Bytes)",                              

        "HTTP UL (Bytes)",                             

        "Nb of sec with 125000B < Vol DL",             

        "Nb of sec with 1250B < Vol UL < 6250B",        

        "Nb of sec with 31250B < Vol DL < 125000B",                   

        "Nb of sec with 6250B < Vol DL < 31250B",          

        "Nb of sec with Vol DL < 6250B",                  

        "Nb of sec with Vol UL < 1250B"                 

    ]



    return sd.handle_outliers(cleaned_df, columns)



processed_df = handle_outliers()

processed_df.head()
processed_df.shape
wd.write_data(processed_df, 'processed_data')
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

def drop_high_missing_columns(
    df: pd.DataFrame, 
    threshold=0.8
    ) -> pd.DataFrame:
    """Drop columns with missing values exceeding a specified threshold.

    Parameters
    ----------
    - df: DataFrame
        The dataframe of raw data set.
    - threshold: float, default 0.8
        The threshold for the percentage of missing values in a column.
        
    Returns
    -------
    - Dataframe object
        where columns specified in the columns_to_drop list, 
        which have missing values, dropped.
        
    """
    missing_percentage = df.isnull().mean()
    columns_to_drop = missing_percentage[missing_percentage > threshold].index
    df_cleaned = df.drop(columns=columns_to_drop)
    return df_cleaned

def impute_numeric_missing(
    df:pd.DataFrame, 
    columns_list:list[str],
    strategy:str='mean'
    ) -> pd.DataFrame:
    """Impute missing values for numerical columns.

    Parameters
    ----------
    - df: DataFrame
    - columns_list
        list of columnst to Impute the missing values
    - strategy: str, default 'mean'
        Imputation strategy, options: 'mean', 'zero'.

    Returns
    -------
    - Dataframe object
        with imputed missing columns from columns_list
    """
    numeric_columns = df[columns_list].select_dtypes(include='number').columns
    
    if strategy == 'zero':
        df_imputed = df.copy()  
        df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(0)
    else:
        imputation_values = df[numeric_columns].mean() if strategy == 'mean' else df[numeric_columns].median()
        df_imputed = df.copy() 
        df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(imputation_values)

    return df_imputed

def remove_rows_with_missing_values(
    df: pd.DataFrame, 
    columns_to_check: list[str]
    ):
    """
    Remove rows from a DataFrame where any of the specified columns have missing values.

    Parameters
    ----------
    df : pandas.DataFrame
        The input DataFrame.
    columns_to_check : list of str
        A list of column names to check for missing values. Rows will be dropped
        if any of these columns have missing values.

    Returns
    -------
    pandas.DataFrame
        A new DataFrame with rows removed where any of the specified columns have missing values.

    """
    cleaned_df = df.dropna(subset=columns_to_check, how='any')
    return cleaned_df

import pandas as pd

def replace_column_with_mode(
    df: pd.DataFrame, 
    column_names: list[str]
    ) -> pd.DataFrame:
    """Replace missing values in a column with its mode.

    Parameters
    ----------
    - df: DataFrame
    - column_list: list[str]
        Name of the list of columns to replace missing values.

    Returns
    -------
    - DataFrame object
        with missing values in the specified column replaced by its mode.
    """
    df_mode = df.copy()  
    for column_name in column_names:
        mode_value = df_mode[column_name].mode().iloc[0]
        df_mode[column_name] = df_mode[column_name].fillna(mode_value)

    return df_mode

def handle_outliers(
    df: pd.DataFrame, 
    columns: list[str], 
    method:str='mean'
    ) -> pd.DataFrame:
    """Handle outliers in specified columns using a specified method.

    Parameters
    ----------
    - df: DataFrame
    - columns: list
        List of columns to handle outliers.
    - method: str, default 'clip'
        Outlier handling method, options: 'clip', 'remove', 'mean'

    Returns
    -------
    - Dataframe object
        where outliers are handled
    """
    if method == 'clip':
        for col in columns:
            df[col] = np.clip(df[col], df[col].quantile(0.05), df[col].quantile(0.95))
    elif method == 'remove':
        for col in columns:
            q1 = df[col].quantile(0.25)
            q3 = df[col].quantile(0.75)
            iqr = q3 - q1
            df = df[(df[col] >= q1 - 1.5 * iqr) & (df[col] <= q3 + 1.5 * iqr)]
    elif method == 'mean':
        for col in columns:
            mean_val = df[col].mean()
            df[col] = np.where(
                (df[col] < df[col].quantile(0.05)) | (df[col] > df[col].quantile(0.95)),
                mean_val,
                df[col]
            )
    return df
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def read_data(table_name='xdr_data'):
    """Read data from a PostgreSQL database table.

    Parameters
    ----------
    table_name : str, optional
        The name of the table from which to fetch data. Default is 'xdr_data'.

    Returns
    -------
    pandas.DataFrame
        A DataFrame containing the data retrieved from the specified table.

    Raises
    ------
    Exception
        If there is an error during the data retrieval process.

    Notes
    -----
    This function uses the `PostgresConnection` class to establish a connection
    to the PostgreSQL database. Ensure that the necessary environment variables
    (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
    this function.
    """
    try:
        with PostgresConnection() as postgres_conn:
            df = pd.read_sql_query(f"SELECT * FROM {table_name};", postgres_conn)
            logger.info('Data fetched succesfully')
            return df

    except Exception as e:
        logger.error(f"Error in the main block: {e}")
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def write_data(
        df: pd.DataFrame,
        table_name : str
        ) -> None:
        """Write DataFrame to a PostgreSQL database table.

        Parameters
        ----------
        df : pandas.DataFrame
            The DataFrame containing the data to be written.
        table_name : str
            The name of the table where the data will be written.

        Returns
        -------
        None

        Raises
        ------
        Exception
            If there is an error during the data writing process.

        Notes
        -----
        This function uses the `PostgresConnection` class to establish a connection
        to the PostgreSQL database. Ensure that the necessary environment variables
        (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
        this function.
        """
        try:
            conn = PostgresConnection()

            df.to_sql(name=table_name, con=conn._instance, index=False, if_exists='replace')

            logger.info('Data written successfully')
        
        except Exception as e:
            logger.error(f"Error in the main block: {e}")
import os, sys

import pandas as pd

from pandasql import sqldf



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
df.shape
pysqldf = lambda q: sqldf(q, globals())
df.columns
query = ''' 

    SELECT DISTINCT 

        "Handset Type", 

        COUNT(*) as UsageCount

    FROM df

    GROUP BY "Handset Type" 

    ORDER BY UsageCount DESC

    limit 10

'''



result_df = pysqldf(query)

result_df

query = ''' 

    SELECT DISTINCT 

        "Handset Manufacturer", 

        COUNT(*) as "Number of Users"

    FROM df

    GROUP BY "Handset Manufacturer" 

    ORDER BY "Number of Users" DESC

    limit 3

'''



result_df = pysqldf(query)

result_df
query = ''' 

    SELECT "Handset Manufacturer", AVG("Dur. (ms)") AS "Avg Session Duration (ms)"

    FROM df

    WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')

    GROUP BY "Handset Manufacturer";



'''

result_df = pysqldf(query)

result_df

query = ''' 

   SELECT

    "Handset Manufacturer",

    SUM("Total UL (Bytes)" + "Total DL (Bytes)") AS "Total Data Volume (Bytes)"

    FROM df

    WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')

    GROUP BY "Handset Manufacturer";

'''

result_df = sqldf(query)

result_df
query = ''' 

    WITH RankedHandsets AS (

    SELECT

        "Handset Manufacturer",

        "Handset Type",

        RANK() OVER (PARTITION BY "Handset Manufacturer" ORDER BY COUNT(*) DESC) AS "Rank"

    FROM df

    WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')

    GROUP BY "Handset Manufacturer", "Handset Type"

    )

    SELECT

        "Handset Manufacturer",

        "Handset Type",

        "Rank"

    FROM RankedHandsets

    WHERE "Rank" <= 5;

'''



result_df = sqldf(query)

result_df
query = ''' 

    SELECT "MSISDN/Number" AS UserIdentifer,

    COUNT(*) AS NumberOfXDRSessions

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY NumberOfXDRSessions DESc;

'''



result_df = pysqldf(query)

result_df
query = '''

    SELECT COUNT(*) AS num_users

    FROM (

        SELECT COUNT(*) AS NumberOfXDRSessions

        FROM df

        GROUP BY "MSISDN/Number"

        HAVING COUNT(*) >= 10

    ) AS subquery

'''



result_df = pysqldf(query)

result_df
query = ''' 

    SELECT

        "MSISDN/Number" AS UserIdentifier,

        SUM("Dur. (ms)") / 1000 AS TotalSessionDurationInSeconds

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalSessionDurationInSeconds DESC;



'''



result_df = pysqldf(query)

result_df
query = ''' 

    SELECT

        CAST(SUBSTRING(CAST("Start" AS TEXT), INSTR(CAST("Start" AS TEXT), ' ') + 1, INSTR(CAST("Start" AS TEXT), ':') - INSTR(CAST("Start" AS TEXT), ' ') - 1) AS INTEGER) AS HourOfDay,

        COUNT(*) AS NumberOfSessions

    FROM df

    WHERE "Start" IS NOT NULL

    GROUP BY HourOfDay

    ORDER BY NumberOfSessions DESC;



'''



result_df = pysqldf(query) 

result_df
query = ''' 

    SELECT

        "MSISDN/Number" AS User,

        COUNT(*) AS SessionCount,

        SUM("Dur. (ms)") AS TotalSessionDuration

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalSessionDuration DESC;



'''



result_df = pysqldf(query)

result_df
result_df = util.get_total_download_for_each_app(df, "Total DL (Bytes)", "Total UL (Bytes)")
query = '''SELECT COUNT(*) AS num_users

    FROM result_df

    WHERE Total >= 100000000;

    '''



pysqldf(query)

util.get_total_download_for_each_app(df, "Social Media DL (Bytes)", "Social Media UL (Bytes)")
util.get_total_download_for_each_app(df, "YouTube DL (Bytes)", "YouTube UL (Bytes)")
util.get_total_download_for_each_app(df, "Netflix DL (Bytes)", "Netflix UL (Bytes)")
util.get_total_download_for_each_app(df, "Google DL (Bytes)", "Google UL (Bytes)")
util.get_total_download_for_each_app(df, "Email DL (Bytes)", "Email UL (Bytes)")
util.get_total_download_for_each_app(df, "Gaming DL (Bytes)", "Gaming UL (Bytes)")
util.get_total_download_for_each_app(df, "Other DL", "Other UL")
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
quantitative_columns = [

        "Avg RTT DL (ms)",                             

        "Avg RTT UL (ms)",   

        "Avg Bearer TP DL (kbps)",

        "Avg Bearer TP UL (kbps)",                          

        "TCP DL Retrans. Vol (Bytes)",                  

        "TCP UL Retrans. Vol (Bytes)",                  

        "DL TP < 50 Kbps (%)",                            

        "50 Kbps < DL TP < 250 Kbps (%)",              

        "250 Kbps < DL TP < 1 Mbps (%)",                  

        "DL TP > 1 Mbps (%)",                             

        "UL TP < 10 Kbps (%)",                           

        "10 Kbps < UL TP < 50 Kbps (%)",                 

        "50 Kbps < UL TP < 300 Kbps (%)",                 

        "UL TP > 300 Kbps (%)",

        "Activity Duration DL (ms)",

        "Activity Duration UL (ms)",                           

        "HTTP DL (Bytes)",                              

        "HTTP UL (Bytes)", 

        "Dur. (ms).1",                            

        "Nb of sec with 125000B < Vol DL",  

        "Nb of sec with 1250B < Vol UL < 6250B",        

        "Nb of sec with 31250B < Vol DL < 125000B",                   

        "Nb of sec with 6250B < Vol DL < 31250B",       

        "Nb of sec with Vol DL < 6250B",                  

        "Nb of sec with Vol UL < 1250B", 

        "Social Media DL (Bytes)",

        "Social Media UL (Bytes)",

        "Youtube DL (Bytes)",

        "Youtube UL (Bytes)",

        "Netflix DL (Bytes)",

        "Netflix UL (Bytes)",

        "Google DL (Bytes)",

        "Google UL (Bytes)",

        "Email DL (Bytes)",

        "Email UL (Bytes)",

        "Gaming DL (Bytes)",

        "Gaming UL (Bytes)",

        "Other DL (Bytes)",

        "Other UL (Bytes)",

        "Total DL (Bytes)",

        "Total UL (Bytes)"

]

df = dc.handle_outliers(df, quantitative_columns)
df.shape
descriptive_stats = df[quantitative_columns].describe()

descriptive_stats
plt.hist(df['Avg RTT UL (ms)'], bins=20, color='blue', alpha=0.7)

plt.xlabel('Average RTT UL (ms)')

plt.ylabel('Frequency')

plt.title('Histogram of Avg RTT DL (ms)')

plt.show()










sns.boxplot(x=df['Avg Bearer TP DL (kbps)'])

plt.xlabel('Average Bearer TP DL (kbps)')

plt.title('Boxplot of Avg Bearer TP DL (kbps)')

plt.show()

plt.figure(figsize=(8, 5))

df['Handset Type'].value_counts().head(10).plot(kind='bar', color='green')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.title('Distribution of Handset Types')

plt.show()

correlation_matrix = df[

    [

        "Social Media DL (Bytes)",

        "Social Media UL (Bytes)",

        "Youtube DL (Bytes)",

        "Youtube UL (Bytes)",

        "Netflix DL (Bytes)",

        "Netflix UL (Bytes)",

        "Google DL (Bytes)",

        "Google UL (Bytes)",

        "Email DL (Bytes)",

        "Email UL (Bytes)",

        "Gaming DL (Bytes)",

        "Gaming UL (Bytes)",

        "Other DL (Bytes)",

        "Other UL (Bytes)",

        "Total DL (Bytes)",

        "Total UL (Bytes)"

]].corr()

correlation_matrix


sns.scatterplot(x='Netflix DL (Bytes)', y='Total DL (Bytes)', data=df)

plt.title('Youtube download vs Total Download')

plt.show()
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = ''' 

        SELECT

            "MSISDN/Number" AS MSISDN,

            COUNT(DISTINCT "Bearer Id") AS SessionFrequency

        FROM df

        GROUP BY "MSISDN/Number"

        ORDER BY SessionFrequency DESC

        LIMIT 10

'''



result_df = pysqldf(query)

result_df
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration

    FROM df

    GROUP BY "MSISDN/Number" 

    ORDER BY SessionDuration DESC

    LIMIT 10;

    ''' 



result_df = pysqldf(query)

result_df

query = ''' 

    SELECT 

        "MSISDN/Number",

        SUM("Total DL (Bytes)") AS TotalDownload,

        SUM("Total UL (Bytes)") AS TotalUpload,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalTraffic DESC

    LIMIT 10;

'''



pysqldf(query)
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration,

        COUNT(DISTINCT "Bearer Id") AS SessionFrequency,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ''' 



aggregated_df = pysqldf(query)

aggregated_df.tail()
scaler = MinMaxScaler()

columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]

customer_id = aggregated_df['MSISDN/Number']

transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)

df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
processed_df = dc.handle_outliers(normalized_data, columns_to_normalize)
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(processed_df['SessionDuration'], processed_df['SessionFrequency'], processed_df['TotalTraffic'], c='blue', marker='o')



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
selected_columns = columns_to_normalize

X = processed_df[selected_columns]

k = 3



kmeans = KMeans(n_clusters=3, random_state=0) 

kmeans.fit(X)



processed_df['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green'}



for cluster_label, color in cluster_colors.items():

    cluster_data = processed_df[processed_df['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['SessionDuration'], 

        cluster_data['SessionFrequency'], 

        cluster_data['TotalTraffic'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.write_to_db as wd

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = ''' 

        SELECT

            "MSISDN/Number" AS MSISDN,

            COUNT(DISTINCT "Bearer Id") AS SessionFrequency

        FROM df

        GROUP BY "MSISDN/Number"

        ORDER BY SessionFrequency DESC

        LIMIT 10

'''



result_df = pysqldf(query)

result_df
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration

    FROM df

    GROUP BY "MSISDN/Number" 

    ORDER BY SessionDuration DESC

    LIMIT 10;

    ''' 



result_df = pysqldf(query)

result_df

query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration

    FROM df

    GROUP BY "MSISDN/Number" 

    ORDER BY SessionDuration DESC;

    ''' 



result_df = pysqldf(query)

result_df['SessionDuration'].median()
query = ''' 

    SELECT 

        "MSISDN/Number",

        SUM("Total DL (Bytes)") AS TotalDownload,

        SUM("Total UL (Bytes)") AS TotalUpload,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalTraffic DESC

    LIMIT 10;

'''



pysqldf(query)
query = ''' 

    SELECT 

        "MSISDN/Number",

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalTraffic DESC

'''



pysqldf(query)['TotalTraffic'].median()
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration,

        COUNT(DISTINCT "Bearer Id") AS SessionFrequency,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ''' 



aggregated_df = pysqldf(query)

aggregated_df.tail()
aggregated_df = dc.handle_outliers(aggregated_df, ["SessionDuration","SessionFrequency","TotalTraffic"])
scaler = MinMaxScaler()

columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]

customer_id = aggregated_df['MSISDN/Number']

transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)

df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')cases = 707,443



Total deaths = 3,891

ax.set_zlabel('TotalTraffic')



plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 3



kmeans = KMeans(n_clusters=3, random_state=0, n_init=25) 

kmeans.fit(X)



df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green'}



for cluster_label, color in cluster_colors.items():

    cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['SessionDuration'], 

        cluster_data['SessionFrequency'], 

        cluster_data['TotalTraffic'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
grouped_df = (df_normalized

                .groupby('Cluster')[["SessionDuration", "SessionFrequency", "TotalTraffic"]]

                .agg(['min', 'max', 'mean', 'sum']))



grouped_df

wcss = []

X = df_normalized.iloc[:, [1, 3]]



for i in range(1, 11):

    kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)

    kmeans.fit(X)

    wcss.append(kmeans.inertia_)



plt.plot(range(1, 11), wcss)

plt.xlabel('Number of clusters')

plt.ylabel('WCSS')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 7



kmeans = KMeans(n_clusters=k, random_state=0, n_init=25) 

kmeans.fit(X)



df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green', 3:'black', 4:'yellow', 5:'brown', 6:'purple'}



for cluster_label, color in cluster_colors.items():

    cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['SessionDuration'], 

        cluster_data['SessionFrequency'], 

        cluster_data['TotalTraffic'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
wd.write_data(df_normalized, 'user_engagement')
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.write_to_db as wd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
def helper(field1 : str, field2 : str) -> pd.DataFrame:

    query = f''' 

        SELECT

            "MSISDN/Number" AS CustomerID,

            AVG("{field1}") AS AvgDL,

            AVG("{field2}") AS AvgUL,

            (

                AVG("{field1}") + 

                AVG("{field2}")

            ) / 2 AS Avg

        FROM df

        GROUP BY "MSISDN/Number";

    '''

    return pysqldf(query)
avg_retransmission = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")

avg_retransmission
avg_rtt = helper("Avg RTT DL (ms)", "Avg RTT UL (ms)")

avg_rtt
query = ''' 

    SELECT

        "MSISDN/Number" AS User_MSISDN,

        "Handset Type" AS HandsetType,

        COUNT(*) AS HandsetTypeCount

    FROM df

    GROUP BY "MSISDN/Number", "Handset Type";



'''



count_handset = pysqldf(query) 

count_handset

avg_throughput = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")

avg_throughput
def helper(field, order):

    query = f''' 

        SELECT 

            "{field}",

            COUNT("{field}") AS "Frequency"

        FROM df

        GROUP BY "{field}"

        ORDER BY "Frequency" {order} 

        LIMIT 10;

    '''



    return pysqldf(query)
top_tcp = helper("TCP DL Retrans. Vol (Bytes)", "DESC")

top_tcp
bottom_tcp = helper("TCP DL Retrans. Vol (Bytes)", "ASC")

bottom_tcp
top_rtt = helper("Avg RTT DL (ms)", "DESC")

top_rtt
bottom_rtt = helper("Avg RTT DL (ms)", "ASC")

bottom_rtt
top_throughput = helper("Avg Bearer TP DL (kbps)", "Desc")

top_throughput
bottom_throughput = helper("Avg Bearer TP DL (kbps)", "ASC")

bottom_throughput
def helper(field1, field2):

    query = f'''

        SELECT

            "Handset Type" AS HandsetType,

            AVG("{field1}" + "{field2}") / 2 AS Average

        FROM df

        GROUP BY "Handset Type"

        ORDER BY Average DESC;

    '''

    return pysqldf(query)
avg_throughput_per_handset = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")

avg_throughput_per_handset
average_tcp_per_handset = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")

average_tcp_per_handset
query = ''' 

    SELECT 

        "MSISDN/Number" AS CustomerID,

        (AVG("TCP DL Retrans. Vol (Bytes)") + AVG("TCP UL Retrans. Vol (Bytes)")) / 2 AS AvgTCP,

        (AVG("Avg RTT DL (ms)") + AVG("Avg RTT UL (ms)")) / 2 AS AvgRTT,

        (AVG("Avg Bearer TP DL (kbps)") + AVG("Avg Bearer TP UL (kbps)")) / 2 AS AvgThroughput

    FROM df

    GROUP BY CustomerID

'''



agg_df = pysqldf(query)

agg_df.head()
agg_df = dc.handle_outliers(agg_df, ["AvgTCP", "AvgRTT","AvgThroughput"])
scaler = MinMaxScaler()

columns_to_normalize = ["AvgTCP", "AvgRTT","AvgThroughput"]



customer_id = agg_df['CustomerID']

transformed_data = scaler.fit_transform(agg_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)



df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(df_normalized["AvgTCP"], df_normalized["AvgRTT"], df_normalized["AvgThroughput"], c='blue', marker='o')



ax.set_xlabel("AvgTCP")

ax.set_ylabel("AvgRTT")

ax.set_zlabel("AvgThroughput")



plt.title('3D Scatter Plot of AvgTCP, AvgRTT, and AvgThroughput')

plt.show()
wcss = []

X = df_normalized.iloc[:, [1, 3]]



for i in range(1, 11):

    kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)

    kmeans.fit(X)

    wcss.append(kmeans.inertia_)



plt.plot(range(1, 11), wcss)

plt.xlabel('Number of clusters')

plt.ylabel('WCSS')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 3



kmeans = KMeans(n_clusters=k, random_state=0, n_init=15) 

kmeans.fit(X)



df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green'}





for cluster_label, color in cluster_colors.items():

    cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['AvgTCP'], 

        cluster_data['AvgRTT'], 

        cluster_data['AvgThroughput'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('AvgTCP')

ax.set_ylabel('AvgRTT')

ax.set_zlabel('AvgThroughput')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
grouped_df = (df_normalized

                .groupby('Cluster')[["AvgTCP", "AvgRTT", "AvgThroughput"]]

                .agg(['min', 'max', 'mean', 'sum']))



grouped_df
wd.write_data(df_normalized, 'user_experience')
import os, sys

import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.linear_model import LinearRegression

from sklearn.cluster import KMeans

from sklearn.model_selection import train_test_split

from sklearn.metrics import euclidean_distances

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.write_to_db as wd

import scripts.utils as util
experience_df = rd.read_data(table_name='user_experience')

engagement_df = rd.read_data(table_name='user_engagement')
pysqldf = lambda q: sqldf(q, globals())
experience_df.head()
engagement_df.head()
merged_df = engagement_df.merge(experience_df, left_on='MSISDN/Number', right_on="CustomerID", how='inner')

merged_df.head()
experience_columns = ["AvgTCP","AvgRTT","AvgThroughput"]

engagement_columns = ["SessionDuration","SessionFrequency","TotalTraffic"]
k = 3

X_eng = merged_df[engagement_columns]

X_exp = merged_df[experience_columns]
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)

cluster_labels = kmeans.fit_predict(X_eng)

merged_df['cluster'] = cluster_labels
query = ''' 

    SELECT 

        cluster,

        AVG(("SessionDuration"+"SessionFrequency"+"TotalTraffic") / 3) AS AVG 

    FROM merged_df

    GROUP BY cluster;

'''



grouped = pysqldf(query)

less_engaged_cluster = grouped['AVG'].idxmin()

less_engaged_cluster
less_engaged_cluster_center = kmeans.cluster_centers_[less_engaged_cluster]



distances = euclidean_distances(X_eng, [less_engaged_cluster_center]).flatten()

merged_df['EngagementScore'] =  distances

merged_df.head()
query = ''' 

    SELECT 

        cluster,

        AVG(("AvgTCP" + "AvgRTT" + "AvgThroughput") / 3) AS AVG 

    FROM merged_df

    GROUP BY cluster;

'''



grouped = pysqldf(query)

worst_experience_cluster = grouped['AVG'].idxmin()

worst_experience_cluster
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)

cluster_labels = kmeans.fit_predict(X_exp)



worst_experience_cluster_cluster_center = kmeans.cluster_centers_[worst_experience_cluster]



distances = euclidean_distances(X_eng, [worst_experience_cluster_cluster_center]).flatten()

merged_df['ExperienceScore'] =  distances

merged_df.head()
merged_df['SatisfactionScore'] = (merged_df['EngagementScore'] + merged_df['ExperienceScore']) / 2

merged_df.head()
query = ''' 

    SELECT 

        "MSISDN/Number",

        SatisfactionScore

    FROM merged_df

    ORDER BY SatisfactionScore DESC

    LIMIT 10

'''



pysqldf(query)
merged_df['SatisfactionScore'].describe()

bins = np.arange(0, 1.4142, 0.35355)

labels = ["Low Satisfaction", "Medium Satisfaction", "High Satisfaction"]



merged_df['SatisfactionGroup'] = pd.cut(merged_df['SatisfactionScore'], bins=bins, labels=labels, include_lowest=True)



group_counts = merged_df['SatisfactionGroup'].value_counts()



plt.pie(group_counts, labels=group_counts.index, autopct='%1.1f%%', startangle=90)

plt.title('Satisfaction Score Distribution')



plt.show()
bins
features = engagement_columns + experience_columns

X = merged_df[features]

y = merged_df.SatisfactionScore



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression() 

model.fit(X_train, y_train)



y_pred  = model.predict(X_test)

selected_columns = ["EngagementScore", "SatisfactionScore"]

X = merged_df[selected_columns]

k = 2



kmeans = KMeans(n_clusters=k, random_state=0, n_init=15) 

kmeans.fit(X)



merged_df['cluster'] = kmeans.labels_
cluster_colors = {0: 'red', 1: 'blue'}

for cluster_label, color in cluster_colors.items():

    cluster_df = merged_df[merged_df["cluster"] == cluster_label]

    plt.scatter(cluster_df["EngagementScore"], cluster_df["ExperienceScore"], c=color)





plt.xlabel("EngagementScore")

plt.ylabel("ExperienceScore")

plt.title("'2D Scatter Plot'of EngagementScore vs ExperienceScore")



plt.show()

query = ''' 

    SELECT 

        cluster,

        AVG("SatisfactionScore") As "Average Satisfaction"

    FROM merged_df 

    GROUP BY cluster;

'''

avg_satsifaction = pysqldf(query)

avg_satsifaction
query = ''' 

    SELECT 

        cluster,

        AVG("ExperienceScore") AS "Average Experience" 

    FROM merged_df 

    GROUP BY cluster 

'''



avg_experience = pysqldf(query)

avg_experience
df = pd.DataFrame({

    "UserID": merged_df["CustomerID"],

    "SatisfactionScore": merged_df["SatisfactionScore"],

    "ExperienceScore": merged_df["ExperienceScore"],

    "EngagementScore": merged_df["EngagementScore"]

})



wd.write_data(df, "satisfaction_score")
Repository Structure: '
' ├── .flake8
├── notebooks
│   ├── parse_slack_data.ipynb
│   └── README.md
├── .pre-commit-config.yaml
├── Teleco_Analytics.ipynb
├── src
│   ├── loader.py
│   ├── __init__.py
│   ├── config.py
│   └── utils.py
├── .gitignore
├── .github
│   └── workflows
├── Makefile
├── tests
│   └── __init__.py
├── README.md
├── .vscode
├── view_tree.py
├── script
│   └── dbconn.py
├── app.py
├── docs
│   └── dbschema.md
├── style_guide.md
├── setup.cfg
├── pyproject.toml
└── requirements.txt
 '
' Commit History: 
{"insertions": [69, 60, 8, 294, 15, 319, 39, 179, 387, 1479], "deletions": [40, 508, 17, 7, 91, 17, 41, 33, 14, 55], "lines": [109, 568, 25, 301, 106, 336, 80, 212, 401, 1534], "committed_datetime": ["2023-12-13 10:41:20", "2023-12-13 15:35:06", "2023-12-13 15:39:52", "2023-12-13 17:58:10", "2023-12-13 17:58:45", "2023-12-14 11:31:30", "2023-12-14 12:17:03", "2023-12-14 15:13:43", "2023-12-16 11:04:21", "2023-12-16 12:47:08"], "commit_count": 10} 
 Content: 
import sys

import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt
sys.path.append('script')

from script import dbconn

pgconn = dbconn.db_connection_psycopg()

raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')


handset_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')




sorted_handsets = handset_counts.sort_values(by='Count', ascending=False)




top_10_handsets = sorted_handsets.head(10)




print(top_10_handsets)

manufacturer_counts = raw_df['Handset Manufacturer'].value_counts().reset_index()

manufacturer_counts.columns = ['Handset Manufacturer', 'Count']




sorted_manufacturers = manufacturer_counts.sort_values(by='Count', ascending=False)




top_3_manufacturers = sorted_manufacturers.head(3)




print(top_3_manufacturers)


manufacturer_type_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')




sorted_manufacturer_types = manufacturer_type_counts.groupby('Handset Manufacturer').apply(lambda x: x.nlargest(5, 'Count')).reset_index(drop=True)




top_3_manufacturers = sorted_manufacturer_types['Handset Manufacturer'].unique()[:3]



for manufacturer in top_3_manufacturers:

    print(f"Top 5 handsets for {manufacturer}:")

    manufacturer_handsets = sorted_manufacturer_types[sorted_manufacturer_types['Handset Manufacturer'] == manufacturer]

    print(manufacturer_handsets)

    print()

sessions_per_user = raw_df.groupby('MSISDN/Number')['Bearer Id'].count()

print(sessions_per_user)

session_duration_per_user = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()




print(session_duration_per_user)

total_data_per_user = raw_df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum()




print(total_data_per_user)

applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']




total_data_per_user_app = raw_df.groupby('MSISDN/Number')[[col + ' DL (Bytes)' for col in applications] + [col + ' UL (Bytes)' for col in applications]].sum()




print(total_data_per_user_app)

raw_df.isna().sum()

def percent_missing(df):

    
    totalCells = np.product(df.shape)



    
    missingCount = df.isnull().sum()



    
    totalMissing = missingCount.sum()



    
    percentageMissing = (totalMissing / totalCells) * 100



    print("The dataset contains", round(percentageMissing, 2), "%", "missing values.")



percent_missing(raw_df)

numeric_columns = raw_df.select_dtypes(include=[np.number]).columns

raw_df[numeric_columns] = raw_df[numeric_columns].fillna(raw_df[numeric_columns].mean())




non_numeric_columns = raw_df.select_dtypes(exclude=[np.number]).columns

raw_df[non_numeric_columns] = raw_df[non_numeric_columns].fillna(raw_df[non_numeric_columns].mode().iloc[0])




for col in numeric_columns:

    z_scores = (raw_df[col] - raw_df[col].mean()) / raw_df[col].std()

    outliers = (z_scores > 3) | (z_scores < -3)

    raw_df.loc[outliers, col] = raw_df[col].mean()




missing_values_after_treatment = raw_df.isnull().sum()

print("Missing Values After Treatment:\n", missing_values_after_treatment)
raw_df.isna().sum()

def fix_missing_ffill(df, col):

    df[col] = df[col].fillna(method='ffill')

    return df[col]



raw_df['Start'] = fix_missing_ffill(raw_df, 'Start')

raw_df['End'] = fix_missing_ffill(raw_df, 'End')

raw_df['Last Location Name'] = fix_missing_ffill(raw_df, 'Last Location Name')



missing_values = raw_df.isna().sum()

print(missing_values)

raw_df.describe()



quantitative_variables = []




for column in raw_df.columns:   

    if raw_df[column].dtype in [int, float]:

        quantitative_variables.append(column)




for column_name in quantitative_variables:

    column_data = raw_df[column_name]

    data_range = column_data.max() - column_data.min()

    print("Range of", column_name, ":", data_range)


clean_Data = raw_df.dropna()

column_names = clean_Data.columns



for column_name in column_names:

    column_data = clean_Data[column_name]    

    plt.hist(column_data, bins=10)

    plt.xlabel(column_name)

    plt.ylabel('Frequency')

    plt.title('Histogram of ' + column_name)

    plt.show()





user_total_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()




user_deciles = pd.qcut(user_total_duration, q=10, labels=False, duplicates='drop')




data_per_decile = raw_df.groupby(user_deciles)[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index()


columns = [

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

    'Other DL (Bytes)', 'Other UL (Bytes)'

]




subset_df = raw_df[columns]




correlation_matrix = subset_df.corr()




print(correlation_matrix)




aggregated_data = raw_df.groupby('MSISDN/Number').agg({

    'Bearer Id': 'nunique',  
    'Dur. (ms)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    'Total DL (Bytes)': 'sum'  
}).reset_index()




aggregated_data.rename(columns={

    'Bearer Id': 'Session Frequency',

    'Dur. (ms)': 'Session Duration',

    'Total UL (Bytes)': 'Total Upload Traffic',

    'Total DL (Bytes)': 'Total Download Traffic'

}, inplace=True)




top_10_frequency = aggregated_data.nlargest(10, 'Session Frequency')

top_10_duration = aggregated_data.nlargest(10, 'Session Duration')

top_10_upload_traffic = aggregated_data.nlargest(10, 'Total Upload Traffic')

top_10_download_traffic = aggregated_data.nlargest(10, 'Total Download Traffic')




print("Top 10 customers by Session Frequency:")

print(top_10_frequency)



print("\nTop 10 customers by Session Duration:")

print(top_10_duration)



print("\nTop 10 customers by Total Upload Traffic:")

print(top_10_upload_traffic)



print("\nTop 10 customers by Total Download Traffic:")

print(top_10_download_traffic)


from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans



aggregated_data = raw_df.groupby('MSISDN/Number').agg({

    'Bearer Id': 'nunique',  
    'Dur. (ms)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    'Total DL (Bytes)': 'sum'  
}).reset_index()




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  



k = 3

kmeans = KMeans(n_clusters=k, random_state=42)

kmeans.fit(normalized_data)




aggregated_data['Cluster'] = kmeans.labels_




top_customers_per_cluster = []

for i in range(k):

    cluster_customers = aggregated_data[aggregated_data['Cluster'] == i].nlargest(10, 'Bearer Id')

    top_customers_per_cluster.append(cluster_customers)




for i, cluster_customers in enumerate(top_customers_per_cluster):

    print(f"\nTop 10 customers in Cluster {i+1}:")

    print(cluster_customers)





aggregated_data = raw_df.groupby('MSISDN/Number').agg({

    'Bearer Id': 'nunique',  
    'Dur. (ms)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    'Total DL (Bytes)': 'sum'  
}).reset_index()




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  



k = 3

kmeans = KMeans(n_clusters=k, random_state=42)

kmeans.fit(normalized_data)




aggregated_data['Cluster'] = kmeans.labels_




cluster_metrics = aggregated_data.groupby('Cluster').agg({

    'Bearer Id': ['min', 'max', 'mean', 'sum'],  
    'Dur. (ms)': ['min', 'max', 'mean', 'sum'],  
    'Total UL (Bytes)': ['min', 'max', 'mean', 'sum'],  
    'Total DL (Bytes)': ['min', 'max', 'mean', 'sum']  
})




print("Non-normalized metrics for each cluster:")

print(cluster_metrics)




app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 

               'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 

               'Gaming DL (Bytes)', 'Other DL (Bytes)']

app_traffic = raw_df[app_columns].copy()




app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']




app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')




app_traffic = app_traffic.groupby(['MSISDN/Number', 'Application'])['Total Traffic'].sum().reset_index()




top_users_per_app = []

unique_apps = app_traffic['Application'].unique()



for app in unique_apps:

    top_users = app_traffic[app_traffic['Application'] == app].nlargest(10, 'Total Traffic')

    top_users_per_app.append(top_users)




for i, app in enumerate(unique_apps):

    print(f"\nTop 10 most engaged users for Application '{app}':")

    print(top_users_per_app[i])




app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

app_traffic = raw_df[app_columns].copy()




app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']




app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')




app_traffic = app_traffic.groupby('Application')['Total Traffic'].sum().reset_index()




app_traffic = app_traffic.sort_values('Total Traffic', ascending=False)




top_3_apps = app_traffic.head(3)




plt.figure(figsize=(8, 6))

plt.bar(top_3_apps['Application'], top_3_apps['Total Traffic'])

plt.xlabel('Application')

plt.ylabel('Total Traffic')

plt.title('Top 3 Most Used Applications')

plt.show()


average_tcp_retransmission = raw_df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean()




print("Average TCP Retransmission per Customer:")

print(average_tcp_retransmission)

average_rtt = raw_df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean()




print("Average RTT per Customer:")

print(average_rtt)

handset_type = raw_df.groupby('MSISDN/Number')['Handset Type'].first()




print("Aggregate handset type per Customer:")

print(handset_type)

average_throughput = raw_df.groupby('MSISDN/Number')['Avg Bearer TP DL (kbps)'].mean()




print("Average Throughput per Customer:")

print(average_throughput)

top_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nlargest(10)




bottom_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)




most_frequent_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)




print("Top 10 TCP Values:")

print(top_tcp_values)

print("\nBottom 10 TCP Values:")

print(bottom_tcp_values)

print("\nMost Frequent TCP Values:")

print(most_frequent_tcp_values)

top_rtt_values = raw_df['Avg RTT DL (ms)'].nlargest(10)




bottom_rtt_values = raw_df['Avg RTT DL (ms)'].nsmallest(10)




most_frequent_rtt_values = raw_df['Avg RTT DL (ms)'].value_counts().head(10)




print("Top 10 RTT Values:")

print(top_rtt_values)

print("\nBottom 10 RTT Values:")

print(bottom_rtt_values)

print("\nMost Frequent RTT Values:")

print(most_frequent_rtt_values)

top_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nlargest(10)




bottom_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nsmallest(10)




most_frequent_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].value_counts().head(10)




print("Top 10 Throughput Values:")

print(top_throughput_values)

print("\nBottom 10 Throughput Values:")

print(bottom_throughput_values)

print("\nMost Frequent Throughput Values:")

print(most_frequent_throughput_values)

avg_throughput_distribution = raw_df.groupby('Handset Type')['Avg Bearer TP DL (kbps)'].mean()




plt.figure(figsize=(12, 6))

avg_throughput_distribution.plot(kind='bar')

plt.xlabel('Handset Type')

plt.ylabel('Average Throughput (kbps)')

plt.title('Distribution of Average Throughput per Handset Type')

plt.xticks(rotation=90)

plt.show()

avg_tcp_retransmission = raw_df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean()




plt.figure(figsize=(12, 6))

avg_tcp_retransmission.plot(kind='bar')

plt.xlabel('Handset Type')

plt.ylabel('Average TCP Retransmission')

plt.title('Average TCP Retransmission View per Handset Type')

plt.xticks(rotation=90)

plt.show()
from sklearn.cluster import KMeans




data = raw_df[['Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'TCP DL Retrans. Vol (Bytes)']]




kmeans = KMeans(n_clusters=3, random_state=42)

kmeans.fit(data)




raw_df['Cluster'] = kmeans.labels_




cluster_descriptions = raw_df.groupby('Cluster').agg({

    'Avg RTT DL (ms)': 'mean',

    'Avg Bearer TP DL (kbps)': 'mean',

    'TCP DL Retrans. Vol (Bytes)': 'mean'

})




print("Cluster Descriptions:")

print(cluster_descriptions)




from sklearn.cluster import KMeans

from sklearn.preprocessing import MinMaxScaler



engagement_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

engagement_data = raw_df[engagement_columns]




kmeans = KMeans(n_clusters=3, random_state=42).fit(MinMaxScaler().fit_transform(engagement_data))

raw_df['Cluster'] = kmeans.predict(MinMaxScaler().fit_transform(engagement_data))




raw_df['Engagement Score'] = kmeans.transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)




top_10_least_engaged_users = raw_df.nsmallest(10, 'Engagement Score')

print("Top 10 least engaged users:")

print(top_10_least_engaged_users[['MSISDN/Number', 'Cluster', 'Engagement Score']])



engagement_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

engagement_data = raw_df[engagement_columns]




kmeans = KMeans(n_clusters=3, random_state=42).fit(MinMaxScaler().fit_transform(engagement_data))

raw_df['Cluster'] = kmeans.predict(MinMaxScaler().fit_transform(engagement_data))




worst_experience_cluster = np.argmin(kmeans.inertia_)

worst_experience_cluster_centroid = kmeans.cluster_centers_[worst_experience_cluster]




raw_df['Experience Score'] = kmeans.transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)




top_10_worst_experience_users = raw_df.nsmallest(10, 'Experience Score')

print("Top 10 users with the worst experience:")

print(top_10_worst_experience_users[['MSISDN/Number', 'Cluster', 'Experience Score']])




engagement_data = raw_df[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']]

engagement_scores = KMeans(n_clusters=3, random_state=42).fit_transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)




experience_scores = KMeans(n_clusters=3, random_state=42).fit_transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)




satisfaction_scores = (engagement_scores + experience_scores) / 2




raw_df['Satisfaction Score'] = satisfaction_scores




top_10_satisfied_customers = raw_df.nlargest(10, 'Satisfaction Score')

print("Top 10 satisfied customers:")

print(top_10_satisfied_customers[['MSISDN/Number', 'Satisfaction Score']])



from sklearn.ensemble import RandomForestRegressor

from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error, r2_score




features = raw_df[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']]

target = raw_df['Satisfaction Score']




X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)




rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

rf_model.fit(X_train, y_train)




y_pred = rf_model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)




print("Mean Squared Error (MSE):", mse)

print("R-squared (R2) Score:", r2)



from sklearn.cluster import KMeans

from sklearn.preprocessing import MinMaxScaler




engagement_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

engagement_data = raw_df[engagement_columns]




scaler = MinMaxScaler().fit(engagement_data)

scaled_engagement_data = scaler.transform(engagement_data)




kmeans = KMeans(n_clusters=2, random_state=42).fit(scaled_engagement_data)




cluster_labels = kmeans.labels_




raw_df['Engagement Cluster'] = cluster_labels




experience_scores = kmeans.transform(scaled_engagement_data).min(axis=1)




raw_df['Experience Score'] = experience_scores




print("Engagement Clusters:")

print(raw_df['Engagement Cluster'].value_counts())

print("\nExperience Scores:")

print(raw_df['Experience Score'])

cluster_agg = raw_df.groupby('Engagement Cluster').agg({'Satisfaction Score': 'mean', 'Experience Score': 'mean'})

print(cluster_agg)
data_to_export = raw_df[['MSISDN/Number', 'Engagement Score', 'Experience Score', 'Satisfaction Score']]

def export_table_to_postgres(pgconn, df, table_name):

    
    cursor = pgconn.cursor()



    
    create_table_query = """

    CREATE TABLE {} (

        user_id VARCHAR(255),

        engagement_score FLOAT,

        experience_score FLOAT,

        satisfaction_score FLOAT

    );

    """.format(table_name)

    cursor.execute(create_table_query)

    pgconn.commit()



    
    for _, row in df.iterrows():

        insert_query = """

        INSERT INTO {} (user_id, engagement_score, experience_score, satisfaction_score)

        VALUES (%s, %s, %s, %s);

        """.format(table_name)

        cursor.execute(insert_query, tuple(row))

        pgconn.commit()



    print("Data exported successfully to PostgreSQL table:", table_name)
 

 

Repository Structure: '
' ├── readme.md
├── notebooks
│   ├── starter.ipynb
│   ├── sql_to_data_frame.ipynb
│   └── experience_analytics.ipynb
├── .gitignore
├── .env
├── .vscode
└── docs
    └── Field Descriptions.xlsx
 '
' Commit History: 
{"insertions": [887], "deletions": [1964], "lines": [2851], "committed_datetime": ["2023-12-17 04:49:55"], "commit_count": 1} 
 Content: 
import matplotlib.pyplot as plt

import seaborn as sns

from scipy.stats import zscore
from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import pandas as pd

from sqlalchemy import create_engine

user = 'postgres'

password = '368545'

host = 'localhost'

port = '5432'

database = 'telecom'

connection_str = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}'

engine = create_engine(connection_str)

sql_query = 'SELECT * FROM xdr_data'

df = pd.read_sql(sql_query, con= engine)

df
df['Dur. (ms)']

df['Dur. (ms).1']
df['Handset Type'].value_counts().nlargest(10)
df['Handset Manufacturer'].value_counts().head(3)
top_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index

top_handsets_by_manufacturer = pd.DataFrame(columns=['Manufacturer', 'Handset Type', 'Count'])

for manufacturer in top_manufacturers:

    top_handsets = df[df['Handset Manufacturer'] == manufacturer]['Handset Type'].value_counts().head(5)

    top_handsets_df = pd.DataFrame({'Manufacturer': [manufacturer] * len(top_handsets.index),

                                     'Handset Type': top_handsets.index,

                                     'Count': top_handsets.values})

    top_handsets_by_manufacturer = pd.concat([top_handsets_by_manufacturer, top_handsets_df])

top_handsets_by_manufacturer
application_columns = [

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

    'Other DL (Bytes)', 'Other UL (Bytes)'

]




application_columns.append('Bearer Id')




filtered_df = df[['MSISDN/Number'] + application_columns]

filtered_df




user_behavior_aggregated = filtered_df.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',                    
    'Social Media DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

}).reset_index()




print("Aggregated User Behavior:")

user_behavior_aggregated
user_behavior_aggregated.describe()

numerical_columns = user_behavior_aggregated.select_dtypes(include='number').columns

non_numerical_columns = user_behavior_aggregated.select_dtypes(exclude='number').columns




user_behavior_aggregated[numerical_columns] = user_behavior_aggregated[numerical_columns].fillna(user_behavior_aggregated[numerical_columns].median())








z_scores = zscore(df.select_dtypes(include='number'))

abs_z_scores = abs(z_scores)

filtered_entries = (abs_z_scores < 3).all(axis=1)




df.loc[~filtered_entries, numerical_columns] = df[numerical_columns].mean()




summary_statistics = user_behavior_aggregated




correlation_matrix = user_behavior_aggregated[numerical_columns].corr()

plt.figure(figsize=(12, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Heatmap')

plt.show()




print("Summary Statistics:")

variable_info = pd.DataFrame(columns=['Variable', 'Data Type'])




variable_info['Variable'] = summary_statistics.columns

variable_info['Data Type'] = summary_statistics.dtypes.values




print("Relevant Variables and Data Types:")

print(variable_info)
basic_metrics = summary_statistics.describe().transpose()[['mean', '50%', 'std', 'min', 'max']]

basic_metrics['IQR'] = summary_statistics.quantile(0.75) - summary_statistics.quantile(0.25)




print("Basic Metrics for Each Variable:")

print(basic_metrics)
quantitative_variables = df.select_dtypes(include='number')




dispersion_parameters = pd.DataFrame(index=quantitative_variables.columns)

dispersion_parameters['Mean'] = quantitative_variables.mean()

dispersion_parameters['Median'] = quantitative_variables.median()

dispersion_parameters['Standard Deviation'] = quantitative_variables.std()

dispersion_parameters['Variance'] = quantitative_variables.var()

dispersion_parameters['Range'] = quantitative_variables.max() - quantitative_variables.min()




print("Dispersion Parameters for Each Quantitative Variable:")

print(dispersion_parameters)
numerical_variables = summary_statistics.select_dtypes(include='number')




numerical_variables.hist(bins=20, figsize=(15, 10))

plt.suptitle('Histograms for Numerical Variables', y=1.02)

plt.show()
plt.figure(figsize=(15, 10))

sns.boxplot(data=numerical_variables)

plt.title('Boxplots for Numerical Variables')

plt.show()
categorical_variables = summary_statistics.select_dtypes(include='number')




for column in categorical_variables.columns:

    plt.figure(figsize=(2, 1))

    categorical_variables[column].value_counts().plot(kind='bar')

    plt.title(f'Bar Chart for {column}')

    plt.xlabel(column)

    plt.ylabel('Frequency')

    plt.show()
applications_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']



total_data_columns = ['Total DL (Bytes)', 'Total UL (Bytes)']

user['Total DL (Bytes)'] = summary_statistics['Social Media DL (Bytes)'] + summary_statistics['Youtube DL (Bytes)'] + summary_statistics['Netflix DL (Bytes)'] + summary_statistics['Google DL (Bytes)'] + summary_statistics['Email DL (Bytes)'] + summary_statistics['Gaming DL (Bytes)'] + summary_statistics['Other DL (Bytes)']

summary_statistics['Total UL (Bytes)'] = summary_statistics['Social Media UL (Bytes)'] + summary_statistics['Youtube UL (Bytes)'] + summary_statistics['Netflix UL (Bytes)'] + summary_statistics['Google UL (Bytes)'] + summary_statistics['Email UL (Bytes)'] + summary_statistics['Gaming UL (Bytes)'] + summary_statistics['Other UL (Bytes)']




selected_columns = applications_columns + total_data_columns

selected_data = summary_statistics[selected_columns]




correlation_matrix = selected_data.corr()




plt.figure(figsize=(12, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Heatmap: Applications vs. Total Data')

plt.show()
summary_statistics

total_duration_per_msisdn = df.groupby('MSISDN/Number')[['Dur. (ms)', 'Dur. (ms).1']].sum().sum(axis=1).reset_index()

total_duration_per_msisdn.columns = ['MSISDN/Number', 'Total Duration (ms)']




summary_statistics = pd.merge(summary_statistics, total_duration_per_msisdn, on='MSISDN/Number', how='left')











summary_statistics['Total Data (DL+UL)'] = summary_statistics['Total DL (Bytes)'] + summary_statistics['Total UL (Bytes)']




summary_statistics['Decile Class'] = pd.qcut(summary_statistics['Total Duration (ms)'], q=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], labels=False, duplicates='drop')




total_data_per_decile = summary_statistics.groupby('Decile Class')['Total Data (DL+UL)'].sum().reset_index()




print("Total Data (DL+UL) per Decile Class:")

print(total_data_per_decile)
selected_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                     'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                     'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                     'Google DL (Bytes)', 'Google UL (Bytes)',

                     'Email DL (Bytes)', 'Email UL (Bytes)',

                     'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                     'Other DL (Bytes)', 'Other UL (Bytes)']




selected_data = summary_statistics[selected_columns]




correlation_matrix = selected_data.corr()



plt.figure(figsize=(12, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Heatmap: Data Usage by Application')

plt.show()
selected_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                     'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                     'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                     'Google DL (Bytes)', 'Google UL (Bytes)',

                     'Email DL (Bytes)', 'Email UL (Bytes)',

                     'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                     'Other DL (Bytes)', 'Other UL (Bytes)']




selected_data = df[selected_columns]




numerical_columns = selected_data.select_dtypes(include=['float64']).columns

categorical_columns = selected_data.select_dtypes(include=['object']).columns




selected_data[numerical_columns] = selected_data[numerical_columns].fillna(selected_data[numerical_columns].median())




selected_data[categorical_columns] = selected_data[categorical_columns].fillna(selected_data[categorical_columns].mode().iloc[0])




selected_data_transposed = selected_data.T




standardized_data_transposed = (selected_data_transposed - selected_data_transposed.mean()) / selected_data_transposed.std()




pca = PCA()

principal_components = pca.fit_transform(standardized_data_transposed)




pc_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(len(selected_columns))])




explained_variance_ratio = pca.explained_variance_ratio_

print("Explained Variance Ratio:")

print(explained_variance_ratio)
sessions_frequency = df.groupby('MSISDN/Number')['Dur. (ms).1'].count().reset_index()

sessions_frequency.columns = ['MSISDN/Number', 'Sessions Frequency']
duration_of_session = df.groupby('MSISDN/Number')['Dur. (ms).1'].sum().reset_index()

duration_of_session.columns = ['MSISDN/Number', 'Total Session Duration (ms)']
total_traffic = df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index()

total_traffic['Total Session Traffic (Bytes)'] = total_traffic['Total DL (Bytes)'] + total_traffic['Total UL (Bytes)']

total_traffic = total_traffic[['MSISDN/Number', 'Total Session Traffic (Bytes)']]
user_engagement = pd.merge(sessions_frequency, duration_of_session, on='MSISDN/Number')

user_engagement = pd.merge(user_engagement, total_traffic, on='MSISDN/Number')
user_engagement

user_engagement = pd.merge(sessions_frequency, duration_of_session, on='MSISDN/Number')

user_engagement = pd.merge(user_engagement, total_traffic, on='MSISDN/Number')




top_10_duration = user_engagement.nlargest(10, 'Total Session Duration (ms)')

top_10_frequency = user_engagement.nlargest(10, 'Sessions Frequency')

top_10_traffic = user_engagement.nlargest(10, 'Total Session Traffic (Bytes)')
top_10_traffic

engagement_metrics = user_engagement[['Sessions Frequency', 'Total Session Duration (ms)', 'Total Session Traffic (Bytes)']]




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics)




kmeans = KMeans(n_clusters=3, random_state=42)

user_engagement['Cluster'] = kmeans.fit_predict(normalized_metrics)




cluster_metrics = user_engagement.groupby('Cluster').agg({

    'Sessions Frequency': ['min', 'max', 'mean', 'sum'],

    'Total Session Duration (ms)': ['min', 'max', 'mean', 'sum'],

    'Total Session Traffic (Bytes)': ['min', 'max', 'mean', 'sum']

}).reset_index()

cluster_metrics
sns.pairplot(user_engagement, hue='Cluster', vars=['Sessions Frequency', 'Total Session Duration (ms)', 'Total Session Traffic (Bytes)'])

plt.show()




plt.figure(figsize=(15, 8))

sns.boxplot(x='Cluster', y='Sessions Frequency', data=user_engagement)

plt.title('Sessions Frequency')

plt.show()



plt.figure(figsize=(15, 8))

sns.boxplot(x='Cluster', y='Total Session Duration (ms)', data=user_engagement)

plt.title('Total Session Duration (ms)')

plt.show()



plt.figure(figsize=(15, 8))

sns.boxplot(x='Cluster', y='Total Session Traffic (Bytes)', data=user_engagement)

plt.title('Total Session Traffic (Bytes)')

plt.show()
application_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']


user_application_traffic = df.groupby('MSISDN/Number')[application_columns].sum().reset_index()




top_10_users_per_application = {}

for column in application_columns:

    top_10_users = user_application_traffic.nlargest(10, column)

    top_10_users_per_application[column] = top_10_users
print(user_application_traffic)
user_application_traffic.columns
for column in user_application_traffic:

    plt.figure(figsize=(10, 6))

    plt.bar(user_application_traffic['MSISDN/Number'], user_application_traffic[column])

    plt.title(f'Top 10 Users for {column}')

    plt.xlabel('MSISDN/Number')

    plt.ylabel(f'Total Traffic ({column})')

    plt.show()
inertia = []

for k in range(1, 11):

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_metrics)

    inertia.append(kmeans.inertia_)




plt.plot(range(1, 11), inertia, marker='o')

plt.title('Elbow Method for Optimal k')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.show() 

import missingno as msno

import pandas as pd

import logging

import sys

import os



sys.path.append(os.path.abspath(os.path.join("../script")))

from utils import percent_missing,format_float, find_agg, missing_values_table,convert_bytes_to_megabytes,fix_missing_ffill,fix_missing_bfill
import pandas as pd

from sqlalchemy import create_engine



database_name = 'week1'

table_name= 'xdr_data'



connection_params = { "host": "localhost", "user": "postgres", "password": "pgadmin",

                    "port": "5432", "database": database_name}



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")




sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query, con= engine)


df.info()

i = 0

for key, item in df.isnull().sum().items():

    if item==0:

        i+=1

        print(key)

print('the dataset contain {} columns with no missing values'.format(i))

columns = []

counts=[]

i=0

for key, item in df.isnull().sum().items():

    if item != 0:

        columns.append(key)

        counts.append(item)

        i+=1

print('the dataset has {} columns with missing values'.format(i))

pd.DataFrame({'column name':columns,'counts':counts})



msno.bar(df)
msno.matrix(df)

msno.heatmap(df)
msno.dendrogram(df)
totalCells, missingCount, totalMissing = percent_missing(df)

print("The Telcom dataset contains", round(

    ((totalMissing/totalCells) * 100), 2), "%", "missing values.")
mis_val_table_ren_columns = missing_values_table(df)

mis_val_table_ren_columns

import pandas as pd

import numpy as np

import logging



class MissingInformation:

    def __init__(self,df:pd.DataFrame):

        self.df = df

        logging.basicConfig(filename='../logfile.log', filemode='a',

                            encoding='utf-8', level=logging.DEBUG)

        

    def missing_values_table(self,df:pd.DataFrame)->pd.DataFrame:

        
        mis_val = df.isnull().sum()



        
        mis_val_percent = 100 * df.isnull().sum() / len(df)



        
        mis_val_dtype = df.dtypes



        
        mis_val_table = pd.concat(

            [mis_val, mis_val_percent, mis_val_dtype], axis=1)



        
        mis_val_table_ren_columns = mis_val_table.rename(

            columns={0: 'Missing Values', 1: '% of Total Values', 2: 'Dtype'})



        
        mis_val_table_ren_columns = mis_val_table_ren_columns[

            mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(

            '% of Total Values', ascending=False).round(1)



        
     

        logging.info("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"

                         "There are " + str(mis_val_table_ren_columns.shape[0]) +

                         " columns that have missing values.")



        
        return mis_val_table_ren_columns



    def percent_missing(self,df:pd.DataFrame):



        
        totalCells = np.product(df.shape)



        
        missingCount = df.isnull().sum()



        
        totalMissing = missingCount.sum()



        return totalCells, missingCount, totalMissing
import logging

import pandas as pd

import numpy as np



import logging

import re

class DataFrameInformation:

    

    def __init__(self,data:pd.DataFrame):

        self.data = data

        logging.basicConfig(filename='../logfile.log', filemode='a',

                            encoding='utf-8', level=logging.DEBUG)

        

    
    def get_skewness(self,data:pd.DataFrame):

        skewness = data.skew(axis=0, skipna=True)

        df_skewness = pd.DataFrame(skewness)

        df_skewness = df_skewness.rename(

            columns={0: 'skewness'})

        

        return df_skewness



    
    def get_skewness_missing_count(self,data:pd.DataFrame):

        df_skewness = self.get_skewness(data)

        minfo = MissingInformation(data)

        

        mis_val_table_ren_columns = minfo.missing_values_table(data)

        df1 = pd.concat([df_skewness, mis_val_table_ren_columns], axis=1)

        df1['Dtype'] = df1['Dtype'].fillna('float64')

        df1['% of Total Values'] = df1['% of Total Values'].fillna(0.0)

        df1['Missing Values'] = df1['Missing Values'].fillna(0)

        df1 = df1.sort_values(by='Missing Values', ascending=False)

        return df1



    def get_column_with_string(self,df: pd.DataFrame, text):

        return [col for col in df.columns if re.findall(text, col) != []]



    def get_dataframe_information(self,df: pd.DataFrame):

        columns = []

        counts = []

        i = 0



        for key, item in df.isnull().sum().items():

            if item != 0:

                columns.append(key)

                counts.append(item)

                i += 1

        logging.info(

            'the dataset contain {} columns with missing values'.format(i))

        return pd.DataFrame({'column name': columns, 'counts': counts})

import numpy as np

numeric_columns = df.select_dtypes(include=np.number)

skewness = numeric_columns.skew(axis=0, skipna=True)




df_skewness = pd.DataFrame({'skewness': skewness})

df_skewness

df_skewness.plot(kind='bar')
df1 = pd.concat([df_skewness, mis_val_table_ren_columns], axis=1)

df1['Dtype'] = df1['Dtype'].fillna('float64')

df1['% of Total Values'] = df1['% of Total Values'].fillna(0.0)

df1['Missing Values'] = df1['Missing Values'].fillna(0)

df1.sort_values(by='Missing Values', ascending=False)
df['Nb of sec with Vol UL < 1250B'].plot(kind='kde')











from scipy.stats.mstats import winsorize

class CleanData:

    def __init__(self,df:pd.DataFrame):

        self.df = df

        logging.basicConfig(filename='../logfile.log', filemode='a',

                            encoding='utf-8', level=logging.DEBUG)

    

    def convert_dtype(self, df: pd.DataFrame, columns, dtype):

        for col in columns:

            df[col] = df[col].astype(dtype=dtype)

        return df

    

    def format_float(self,value):

        return f'{value:,.2f}'



    def convert_bytes_to_megabytes(self, df:pd.DataFrame, columns):



        megabyte = 1*10e+5

        for col in columns:

            df[col] = df[col] / megabyte

        return df

    

    def convert_ms_to_sec(self, df:pd.DataFrame, columns): 

        s = 10e+3

        for col in columns:

            df[col] = df[col] / s

        return df 

    

    def fix_missing_ffill(self, df: pd.DataFrame,col):

        df[col] = df[col].fillna(method='ffill')

        return df[col]

  

    def fix_missing_bfill(self, df: pd.DataFrame, col):

        df[col] = df[col].fillna(method='bfill')

        return df[col]

    

    def drop_column(self, df: pd.DataFrame, columns) -> pd.DataFrame:

        for col in columns:

            df = df.drop([col], axis=1)

        return df



    def drop_missing_count_greaterthan_20p(self,data:pd.DataFrame):

        data_info = DataFrameInformation(data)

        df = data_info.get_skewness_missing_count(data)

        not_fill = df[(df['% of Total Values'] >= 20.0)].index.tolist()

        df_clean = self.drop_column(data, not_fill)

         

        return df_clean

    

    def fill_mode(self, df: pd.DataFrame, columns) -> pd.DataFrame:

        for col in columns:

            df[col] = df[col].fillna(df[col].mode()[0])

        return df



    def fix_outlier(self,df:pd.DataFrame, columns):

        for column in columns:

            df[column] = np.where(df[column] > df[column].quantile(0.95), df[column].median(), df[column])

            

        return df



    def handle_outliers(self, df: pd.DataFrame,lower,upper):

       

        selected_columns = df.select_dtypes(include='float64').columns

        for col in selected_columns:

            df[col] = winsorize(df[col], (lower, upper))

        return df
df_clean = df.copy()

data_cleaner = CleanData(df_clean)

df_clean['End'] = fix_missing_ffill(df_clean, 'End')

df_clean['Start'] = fix_missing_ffill(df_clean, 'Start')



df_clean['Handset Manufacturer'] = df_clean['Handset Manufacturer'].fillna(

    df_clean['Handset Manufacturer'].mode()[0])

df_clean['Handset Type'] = df_clean['Handset Type'].fillna(

    df_clean['Handset Type'].mode()[0])

df_clean['Last Location Name'] = df_clean['Last Location Name'].fillna(

    df_clean['Last Location Name'].mode()[0])



drop_column = mis_val_table_ren_columns[mis_val_table_ren_columns['% of Total Values']>15].index

print(drop_column.tolist())
df_clean = df.drop(drop_column.tolist(),axis=1)

df_clean.head()
df_clean['Nb of sec with Vol DL < 6250B'] = fix_missing_bfill(

    df_clean, 'Nb of sec with Vol DL < 6250B')

missing_percentage = (df.isnull().sum() / len(df)) * 100




fill_mode = missing_percentage[(missing_percentage < 20.0) & (missing_percentage >= 0.4)].index.tolist()

not_fill_mode = ['IMEI', 'IMSI', 'MSISDN/Number']



fill_mode_columns = [x for x in fill_mode if x not in not_fill_mode]




columns_to_fill = [col for col in fill_mode_columns if col in df_clean.columns]




df_clean[columns_to_fill] = df_clean[columns_to_fill].fillna(df_clean[columns_to_fill].mode().iloc[0])








print(df_clean[columns_to_fill].isnull().sum())  def cap_outliers(series):

    q1 = series.quantile(0.25)

    q3 = series.quantile(0.75)

    iqr = q3 - q1

    lower_bound = q1 - 1.5 * iqr

    upper_bound = q3 + 1.5 * iqr

    return series.clip(lower=lower_bound, upper=upper_bound)
missing_info = MissingInformation(df_clean)

mis_val_table_after_clean = missing_info.missing_values_table(df_clean)

mis_val_table_after_clean
df_clean.dropna(inplace=True)

mis_val_table_after_clean = missing_info.missing_values_table(df_clean)

mis_val_table_after_clean
df_clean.info()
df_clean

df_clean.to_csv('../Data/clean_data.csv',index=False)
df_clean = pd.read_csv('../Data/clean_data.csv')

df_clean.shape
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt 

import seaborn as sns 
clean_data = pd.read_csv('../Data/clean_data.csv')

clean_data.head()

clean_data.columns
handset_count = clean_data['Handset Type'].value_counts()

print(len(handset_count), "users")

print("Number of posts per user")

handset_count[:10].plot(kind='bar', color=['teal', 'green', 'blue','purple','pink'])
handset_manufacturer = clean_data['Handset Manufacturer'].value_counts()

print(len(handset_manufacturer), "users")

print("Number of posts per user")

handset_manufacturer[:3].plot(

    kind='bar', color=['teal', 'green', 'blue'])











manufacturer_counts = clean_data['Handset Manufacturer'].value_counts()



top_3_manufacturers = manufacturer_counts.head(3).index



filtered_df = clean_data[clean_data['Handset Manufacturer'].isin(top_3_manufacturers)]



handset_counts = filtered_df.groupby(['Handset Manufacturer','Handset Type']).size()



top_5_handsets = handset_counts.groupby('Handset Manufacturer').nlargest(5)





top_5_handsets.unstack().plot(kind='bar', figsize=(10,6))

plt.title('top 5 handsets per manufacturer')

plt.xlabel('manufacturer')

plt.ylabel('freqency')

plt.xticks(rotation=0)

plt.legend(title = 'handset model')

plt.show()
handset_man= clean_data[clean_data['Handset Manufacturer'].isin(['Apple','Sumsung','Huawei'])]

handset_man.groupby('Handset Manufacturer')['Handset Type'].value_counts()[:10].plot.bar(

    figsize=(12, 10), fontsize=15)

aggregated_data = clean_data.groupby('MSISDN/Number').agg({

    'Dur. (ms)': 'count',  
    'Dur. (ms)': 'sum',  
    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum',

    'Social Media DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

}).reset_index()

aggregated_data.head()  
summary_stats = clean_data.describe()

summary_stats

dispersion_params = clean_data[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)', 

                          'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

                          'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].std()

dispersion_params

clean_data[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)', 

      'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

      'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].hist(figsize=(10, 10))

plt.tight_layout()

plt.show()

app_vs_total_data = clean_data[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)', 

                          'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

                          'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].corr()

app_vs_total_data

clean_data['Total_Session_duration'] = clean_data['Dur. (ms)'].groupby(clean_data['MSISDN/Number']).transform('sum')

clean_data['Decile_Class'] = pd.qcut(clean_data['Total_Session_duration'], q=5, labels=False)




total_data_per_decile = clean_data.groupby('Decile_Class')[['Total UL (Bytes)', 'Total DL (Bytes)']].sum()

total_data_per_decile

correlation_matrix = clean_data[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 

                           'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 

                           'Other DL (Bytes)']].corr()

correlation_matrix
import sklearn

from sklearn.decomposition import PCA






pca_data = clean_data[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 

                 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 

                 'Other DL (Bytes)']]




pca_data_standardized = (pca_data - pca_data.mean()) / pca_data.std()




pca = PCA(n_components=2)

pca.fit(pca_data_standardized)




components = pca.components_

explained_variance = pca.explained_variance_ratio_



print("Principal Components:")

print(components)

print("Explained Variance Ratio:")

print(explained_variance)
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans
df = pd.read_csv('../Data/clean_data.csv')

session_frequency = df['MSISDN/Number'].value_counts()

session_frequency

average_session_duration = df.groupby('MSISDN/Number')['Dur. (ms)'].mean()

average_session_duration

df['Total_Traffic'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']

total_traffic_per_user = df.groupby('MSISDN/Number')['Total_Traffic'].sum()

total_traffic_per_user


engagement_metrics = pd.DataFrame({

    'Session_Frequency': session_frequency,

    'Average_Session_Duration': average_session_duration,

    'Total_Traffic': total_traffic_per_user

}).reset_index()




engagement_metrics.head()

engagement_metrics.columns = ['Session_Frequency', 'Average_Session_Duration', 'Total_Traffic']
top_10_session_frequency = engagement_metrics.nlargest(10, 'Session_Frequency')

top_10_session_frequency
top_10_avg_duration = engagement_metrics.nlargest(10, 'Average_Session_Duration')

top_10_avg_duration
top_10_total_traffic = engagement_metrics.nlargest(10, 'Total_Traffic')

top_10_total_traffic

scaler = MinMaxScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics.drop('MSISDN/Number', axis=1))

normalized_metrics

kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)

engagement_metrics['Cluster'] = kmeans.fit_predict(normalized_metrics)


cluster_stats = engagement_metrics.groupby('Cluster').agg({

    'Session_Frequency': ['min', 'max', 'mean', 'sum'],

    'Average_Session_Duration': ['min', 'max', 'mean', 'sum'],

    'Total_Traffic': ['min', 'max', 'mean', 'sum']

}).reset_index()

cluster_stats

app_traffic = df.groupby('MSISDN/Number')[['Social Media DL (Bytes)', 'Google DL (Bytes)', 

                                             'Email DL (Bytes)', 'Youtube DL (Bytes)',

                                             'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 

                                             'Other DL (Bytes)']].sum()

app_traffic

app_traffic['Total_App_Traffic'] = app_traffic.sum(axis=1)
top_10_social_media = app_traffic.nlargest(10, 'Social Media DL (Bytes)')

top_10_social_media
top_10_google = app_traffic.nlargest(10, 'Google DL (Bytes)')

top_10_google
top_10_youtube = app_traffic.nlargest(10, 'Youtube DL (Bytes)')

top_10_youtube 

total_app_traffic = app_traffic.drop('Total_App_Traffic', axis=1).sum()




top_3_apps = total_app_traffic.nlargest(3)

top_3_apps.plot(kind='bar', xlabel='Applications', ylabel='Total Traffic', title='Top 3 Most Used Applications')

plt.show()

inertia_values = []

for k in range(1, 11):

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_metrics)

    inertia_values.append(kmeans.inertia_)




plt.plot(range(1, 11), inertia_values, marker='o')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.title('Elbow Method for Optimal k')

plt.show()
import sys

import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt
sys.path.append('script')

from script import dbconn

pgconn = dbconn.db_connection_psycopg()

raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')
raw_df.info
raw_df.describe
raw_df.columns

top_10_handsets = raw_df['Handset Type'].value_counts().head(10)

print(top_10_handsets)

top_3_manufacturers = raw_df['Handset Manufacturer'].value_counts().head(3)

print(top_3_manufacturers)

top_3_manufacturers = raw_df['Handset Manufacturer'].value_counts().head(3).index



for manufacturer in top_3_manufacturers:

    top_5_handsets = raw_df.loc[raw_df['Handset Manufacturer'] == manufacturer, 'Handset Type'].value_counts().head(5)

    print(f"Top 5 handsets for {manufacturer}:")

    print(top_5_handsets)

    print()

user_sessions = raw_df.groupby('MSISDN/Number')['Bearer Id'].count().reset_index()

user_sessions.columns = ['MSISDN/Number', 'Number of xDR Sessions']

print(user_sessions)

user_session_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum().reset_index()

user_session_duration.columns = ['MSISDN/Number', 'Session Duration']

print(user_session_duration)

user_data = raw_df.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum',

    'Total UL (Bytes)': 'sum'

}).reset_index()

user_data.columns = ['MSISDN/Number', 'Total DL Data', 'Total UL Data']

print(user_data)

user_session_data = raw_df.groupby('MSISDN/Number').agg({

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()

user_session_data['Total Data Volume'] = user_session_data['Total UL (Bytes)'] + user_session_data['Total DL (Bytes)']

user_session_data = user_session_data[['MSISDN/Number', 'Total Data Volume']]

print(user_session_data)
raw_df.info()
raw_df.isna().sum()
raw_df.describe



def percent_missing(df):

    
    totalCells = np.product(df.shape)



    
    missingCount = df.isnull().sum()



    
    totalMissing = missingCount.sum()



    
    percentageMissing = (totalMissing / totalCells) * 100



    print("The dataset contains", round(percentageMissing, 2), "%", "missing values.")



percent_missing(raw_df)








raw_df.fillna(raw_df.mean(), inplace=True)




num_columns = raw_df.select_dtypes(include=[np.number]).columns



for col in num_columns:

    z_scores = (raw_df[col] - raw_df[col].mean()) / raw_df[col].std()

    outliers = (z_scores > 3) | (z_scores < -3)

    raw_df[col][outliers] = raw_df[col].mean()




missing_values_after_treatment = raw_df.isnull().sum()

print("Missing Values After Treatment:\n", missing_values_after_treatment)

missing_percent = (raw_df.isnull().sum() / len(raw_df)) * 100




columns_to_drop = missing_percent[missing_percent > 30].index

df_clean = raw_df.drop(columns_to_drop, axis=1)




print("Shape of cleaned DataFrame:", df_clean.shape)
missing_values = raw_df.isna().sum()

print(missing_values)

def fix_missing_ffill(df, col):

    df[col] = df[col].fillna(method='ffill')

    return df[col]



raw_df['Start'] = fix_missing_ffill(raw_df, 'Start')

raw_df['End'] = fix_missing_ffill(raw_df, 'End')

raw_df['Last Location Name'] = fix_missing_ffill(raw_df, 'Last Location Name')



missing_values = raw_df.isna().sum()

print(missing_values)



metrics = raw_df.describe()

mean = metrics.loc['mean']

median = metrics.loc['50%']

mode = raw_df.mode().iloc[0]

minimum = metrics.loc['min']

maximum = metrics.loc['max']

std_deviation = metrics.loc['std']




print("Mean:\n", mean)

print("\nMedian:\n", median)

print("\nMode:\n", mode)

print("\nMinimum:\n", minimum)

print("\nMaximum:\n", maximum)

print("\nStandard Deviation:\n", std_deviation)





quantitative_vars = raw_df.select_dtypes(include=[np.number])




dispersion_parameters = quantitative_vars.agg(['mean', 'median', 'std', 'min', 'max', 'var'])




print("Dispersion Parameters:\n", dispersion_parameters)




variables = raw_df.columns




for variable in variables:

    if raw_df[variable].dtype == 'int64' or raw_df[variable].dtype == 'float64':

        
        plt.figure(figsize=(8, 6))

        sns.histplot(data=raw_df, x=variable, kde=True)

        plt.title(f'Distribution of {variable}')

        plt.xlabel(variable)

        plt.ylabel('Frequency')

        plt.show()

    else:

        
        plt.figure(figsize=(8, 6))

        sns.countplot(data=raw_df, x=variable)

        plt.title(f'Count of {variable}')

        plt.xlabel(variable)

        plt.ylabel('Count')

        plt.xticks(rotation=90)

        plt.show()

variables = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

             'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

             'Other DL (Bytes)']




subset_df = raw_df[variables]




correlation_matrix = subset_df.corr()




print("Correlation Matrix:")

print(correlation_matrix)




variables = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

             'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

             'Other DL (Bytes)']




subset_df = raw_df[variables]




scaler = StandardScaler()

scaled_data = scaler.fit_transform(subset_df)




pca = PCA(n_components=2)

principal_components = pca.fit_transform(scaled_data)




pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])




explained_variance_ratio = pca.explained_variance_ratio_




print("Interpretation of PCA results:")

print("- The first principal component (PC1) explains", round(explained_variance_ratio[0] * 100, 2), "% of the variance in the data.")

print("- The second principal component (PC2) explains", round(explained_variance_ratio[1] * 100, 2), "% of the variance in the data.")

print("- PC1 captures the most significant patterns and trends in the data, such as overall data usage level.")

print("- PC2 captures additional variation that is orthogonal to PC1 and represents specific usage patterns or differences between the applications.")




engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                      'Other DL (Bytes)']




grouped_df = raw_df.groupby('MSISDN/Number')[engagement_metrics].sum()




grouped_df['Total Engagement'] = grouped_df.sum(axis=1)




for metric in engagement_metrics:

    top_10_customers = grouped_df.nlargest(10, metric)

    print(f"Top 10 customers for {metric}:")

    print(top_10_customers)

    print()


engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                      'Other DL (Bytes)']




engagement_data = raw_df[engagement_metrics]




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(engagement_data)




kmeans = KMeans(n_clusters=3)

kmeans.fit(normalized_data)




raw_df['Engagement Cluster'] = kmeans.labels_




print(raw_df['Engagement Cluster'].value_counts())




cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].mean()

print(cluster_stats)

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans






engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                      'Other DL (Bytes)']




engagement_data = raw_df[engagement_metrics]




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(engagement_data)




kmeans = KMeans(n_clusters=3, random_state=42)

kmeans.fit(normalized_data)




raw_df['Engagement Cluster'] = kmeans.labels_




cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])




cluster_stats.plot(kind='bar', figsize=(12, 6))

plt.xlabel('Engagement Cluster')

plt.ylabel('Metrics')

plt.title('Metrics for Each Engagement Cluster')

plt.legend(['Min', 'Max', 'Mean', 'Sum'])

plt.show()




print(cluster_stats)

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans







engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                      'Other DL (Bytes)']




engagement_data = raw_df[engagement_metrics]




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(engagement_data)




kmeans = KMeans(n_clusters=3, random_state=42)

kmeans.fit(normalized_data)




raw_df['Engagement Cluster'] = kmeans.labels_




cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])




cluster_stats.plot(kind='bar', figsize=(12, 6))

plt.xlabel('Engagement Cluster')

plt.ylabel('Metrics')

plt.title('Metrics for Each Engagement Cluster')

plt.legend(['Min', 'Max', 'Mean', 'Sum'])

plt.show()




print(cluster_stats)




engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                      'Other DL (Bytes)']




engagement_data = raw_df[engagement_metrics]




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(engagement_data)




kmeans = KMeans(n_clusters=3, random_state=42)

kmeans.fit(normalized_data)




raw_df['Engagement Cluster'] = kmeans.labels_




cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])




cluster_stats.plot(kind='bar', figsize=(12, 6))

plt.xlabel('Engagement Cluster')

plt.ylabel('Metrics')

plt.title('Metrics for Each Engagement Cluster')

plt.legend(['Min', 'Max', 'Mean', 'Sum'])

plt.show()




print(cluster_stats)






application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                       'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                       'Other DL (Bytes)']




user_traffic = raw_df.groupby('MSISDN/Number')[application_columns].sum()




top_10_users_per_app = pd.DataFrame()

for column in application_columns:

    top_10_users = user_traffic.nlargest(10, column)

    top_10_users_per_app[column] = top_10_users.index




print("Top 10 most engaged users per application:")

print(top_10_users_per_app)






application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                       'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                       'Other DL (Bytes)']




total_traffic = raw_df[application_columns].sum()




top_3_applications = total_traffic.nlargest(3)




plt.bar(top_3_applications.index, top_3_applications.values)

plt.xlabel('Application')

plt.ylabel('Total Traffic')

plt.title('Top 3 Most Used Applications')

plt.show()






engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                      'Other DL (Bytes)']




engagement_data = raw_df[engagement_metrics]




scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(engagement_data)




k_values = range(1, 11)  
inertia_values = []  


for k in k_values:

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_data)

    inertia_values.append(kmeans.inertia_)




plt.plot(k_values, inertia_values, marker='o')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.title('Elbow Curve')

plt.show()

raw_df.isna().sum()

customer_avg_retransmission = raw_df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean()




print(customer_avg_retransmission)

customer_avg_rtt = raw_df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean()




print(customer_avg_rtt)

customer_handset_type = raw_df.groupby('MSISDN/Number')['Handset Type'].first()




print(customer_handset_type)

customer_avg_throughput = raw_df.groupby('MSISDN/Number')['Avg Bearer TP DL (kbps)'].mean()




print(customer_avg_throughput)


top_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nlargest(10)

print("Top 10 TCP Values:")

print(top_tcp_values)




bottom_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)

print("\nBottom 10 TCP Values:")

print(bottom_tcp_values)




most_frequent_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)

print("\nMost Frequent TCP Values:")

print(most_frequent_tcp_values)


top_rtt_values = raw_df['Avg RTT DL (ms)'].nlargest(10)

print("Top 10 RTT Values:")

print(top_rtt_values)




bottom_rtt_values = raw_df['Avg RTT DL (ms)'].nsmallest(10)

print("\nBottom 10 RTT Values:")

print(bottom_rtt_values)




most_frequent_rtt_values = raw_df['Avg RTT DL (ms)'].value_counts().head(10)

print("\nMost Frequent RTT Values:")

print(most_frequent_rtt_values)


top_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nlargest(10)

print("Top 10 Throughput Values:")

print(top_throughput_values)




bottom_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nsmallest(10)

print("\nBottom 10 Throughput Values:")

print(bottom_throughput_values)




most_frequent_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].value_counts().head(10)

print("\nMost Frequent Throughput Values:")

print(most_frequent_throughput_values)




grouped_df = raw_df.groupby('Handset Type')['Avg Bearer TP DL (kbps)'].mean()




print("Distribution of Average Throughput per Handset Type:")

print(grouped_df)


grouped_df = raw_df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean()




print("Average TCP Retransmission View per Handset Type:")

print(grouped_df)




selected_columns = ['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                    'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']




metrics_df = raw_df[selected_columns]




scaler = StandardScaler()

scaled_metrics = scaler.fit_transform(metrics_df)




k = 3




kmeans = KMeans(n_clusters=k, random_state=42)

kmeans.fit(scaled_metrics)




cluster_labels = kmeans.labels_




raw_df['Cluster'] = cluster_labels




cluster_descriptions = {

    0: "Cluster with high RTT, high average bearer throughput, and high TCP retransmission",

    1: "Cluster with moderate RTT, average bearer throughput, and TCP retransmission",

    2: "Cluster with low RTT, low average bearer throughput, and low TCP retransmission"

}




print("Cluster Counts:")

print(raw_df['Cluster'].value_counts())




print("\nCluster Descriptions:")

for cluster, description in cluster_descriptions.items():

    print(f"Cluster {cluster}: {description}")






engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',

                      'Other DL (Bytes)']




engagement_scores = []

for _, row in raw_df.iterrows():

    user_data = row[engagement_metrics].values

    euclidean_distance = np.linalg.norm(user_data)

    engagement_scores.append(euclidean_distance)




raw_df['Engagement Score'] = engagement_scores




print(raw_df[['MSISDN/Number', 'Engagement Score']])




experience_metrics = ['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                      'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']




experience_scores = []

for _, row in raw_df.iterrows():

    user_data = row[experience_metrics].values

    euclidean_distance = np.linalg.norm(user_data)

    experience_scores.append(euclidean_distance)




raw_df['Experience Score'] = experience_scores




print(raw_df[['MSISDN/Number', 'Experience Score']])






engagement_scores = raw_df['Engagement Score']

experience_scores = raw_df['Experience Score']




satisfaction_scores = (engagement_scores + experience_scores) / 2




raw_df['Satisfaction Score'] = satisfaction_scores




sorted_df = raw_df.sort_values(by='Satisfaction Score', ascending=False)




top_10_satisfied_customers = sorted_df.head(10)




print(top_10_satisfied_customers[['MSISDN/Number', 'Satisfaction Score']])



from sklearn.metrics import  r2_score




features = ['Engagement Score', 'Experience Score']  
target = 'Satisfaction Score'




X_train, X_test, y_train, y_test = train_test_split(raw_df[features], raw_df[target], test_size=0.2, random_state=42)




rf_model = RandomForestRegressor()




rf_model.fit(X_train, y_train)




y_pred = rf_model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)




print('Mean Squared Error:', mse)

print('R^2 Score:', r2)




engagement_scores = raw_df['Engagement Score']

experience_scores = raw_df['Experience Score']




data = pd.DataFrame({'Engagement Score': engagement_scores, 'Experience Score': experience_scores})




scaler = StandardScaler()

scaled_data = scaler.fit_transform(data)




k = 2




kmeans = KMeans(n_clusters=k, random_state=42)

kmeans.fit(scaled_data)




cluster_labels = kmeans.labels_




data['Cluster'] = cluster_labels




print(data)






engagement_scores = raw_df['Engagement Score']

experience_scores = raw_df['Experience Score']




data = pd.DataFrame({'Engagement Score': engagement_scores, 'Experience Score': experience_scores})




scaler = StandardScaler()

scaled_data = scaler.fit_transform(data)




k = 2




kmeans = KMeans(n_clusters=k, random_state=42)

kmeans.fit(scaled_data)




cluster_labels = kmeans.labels_




raw_df['Cluster'] = cluster_labels




cluster_stats = raw_df.groupby('Cluster').agg({'Satisfaction Score': 'mean', 'Experience Score': 'mean'})




print(cluster_stats)






def db_connection_psycopg():

    pgconn = psycopg2.connect(dbname="telecom",

                              user="postgres",

                              password="Newsnow@1991",

                              host="localhost",

                              port="5432")

    return pgconn




conn = db_connection_psycopg()




cursor = conn.cursor()




drop_table_query = sql.SQL("DROP TABLE IF EXISTS {}").format(sql.Identifier('Satisfaction_score'))

cursor.execute(drop_table_query)




create_table_query = sql.SQL("""

    CREATE TABLE Satisfaction_score (

        user_id BIGINT,

        engagement_score FLOAT,

        experience_score FLOAT,

        satisfaction_score FLOAT

    )

""")

cursor.execute(create_table_query)




for _, row in raw_df.iterrows():

    
    user_id = row['MSISDN/Number']

    engagement_score = row['Engagement Score']

    experience_score = row['Experience Score']

    satisfaction_score = row['Satisfaction Score']

    

    
    insert_query = sql.SQL("""

        INSERT INTO Satisfaction_score (user_id, engagement_score, experience_score, satisfaction_score)

        VALUES (%s, %s, %s, %s)

    """)

    cursor.execute(insert_query, (user_id, engagement_score, experience_score, satisfaction_score))




conn.commit()

cursor.close()

conn.close()
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import os

import sys

import warnings

warnings.filterwarnings('ignore')



sys.path.append('../helpers/')



from data import Database

from utils import Helpers
db = Database(host="localhost", database="telecom", user="postgres", password="heisenberg")

df = db.read_table_to_dataframe('xdr_data')

df.head()
df.info()
df.describe()
def percent_missing(df):

    totalCells = np.product(df.shape)

    missingCount = df.isnull().sum()



    
    totalMissing = missingCount.sum()

    print("The dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")
percent_missing(df)
df.isna().sum()
percent_missing(df['Nb of sec with 6250B < Vol UL < 37500B'])
df['Handset Manufacturer'].unique()
df['Handset Type'].describe()
top_10_handsets = df['Handset Type'].value_counts().head(10)

print(top_10_handsets)
top_3_manufacturers = df['Handset Manufacturer'].value_counts().head(3)

print(top_3_manufacturers)
top_3_manufacturers_list = top_3_manufacturers.index.tolist()

top_5_handsets_per_manufacturer = df[df['Handset Manufacturer'].isin(top_3_manufacturers_list)]




top_5_handsets_per_manufacturer = top_5_handsets_per_manufacturer.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')




top_5_handsets_per_manufacturer = top_5_handsets_per_manufacturer.sort_values(by=['Handset Manufacturer', 'Count'], ascending=[True, False])

top_5_handsets_per_manufacturer = top_5_handsets_per_manufacturer.groupby('Handset Manufacturer').head(5)

top_5_handsets_per_manufacturer
user_data = df.groupby('MSISDN/Number')

user_data.head()



num_xdr = user_data.size().rename('Number of XDR Sessions')

session_duration = user_data['Dur. (ms)'].sum() / 1000

total_up_dl_per_user = user_data[['Total DL (Bytes)', 'Total UL (Bytes)']].sum()



app_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)',

               'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

               'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

               'Other DL (Bytes)', 'Other UL (Bytes)']



for data in app_columns:

    df[data] /= (1024 * 1024)

total_data = user_data[app_columns].sum()



user_behavior = pd.concat([num_xdr, session_duration, total_up_dl_per_user, total_data], axis=1) 
user_behavior
user_behavior.describe()



top_10_xdr = user_behavior['Number of XDR Sessions'].nlargest(10)

top_10_xdr



top_10_duration = user_behavior['Dur. (ms)'].nlargest(10)

top_10_duration_seconds = top_10_duration / 1000

top_10_duration_seconds



top_10_download = user_behavior['Total DL (Bytes)'].nlargest(10)

top_10_download /= (1024 * 1024)

top_10_download
missing_values = df.isnull().sum()

missing_values
data_types = df.dtypes

type_counts = data_types.value_counts()

type_counts



for column in df.columns:

    if df[column].dtype == 'float64' and df[column].isnull().any():

        mean_value = df[column].mean()

        df[column].fillna(mean_value, inplace=True)

df.isnull().sum()
description = df.describe(include='all')

description
numeric_columns = df.select_dtypes(include=['float64', 'int64'])




mean_values_all = numeric_columns.mean()

median_values = numeric_columns.median()

std_deviation = numeric_columns.std()



median_values
numeric_columns = df.select_dtypes(include=['float64', 'int64'])

dispersion_parameters = numeric_columns.agg(['std', 'var'])

dispersion_parameters
numeric_columns = df.select_dtypes(include=['float64', 'int64'])

numeric_columns.hist(bins=20, figsize=(12, 10))

plt.tight_layout()

plt.show()
plt.figure(figsize=(8,6))

plt.hist(df['Total UL (Bytes)'], bins= 20, color='skyblue')

plt.show()
plt.figure(figsize=(8,6))

plt.hist(df['Avg Bearer TP DL (kbps)'], bins= 20, color='skyblue')

plt.show()
plt.figure(figsize=(8,6))

plt.hist(df['Total UL (Bytes)'], bins= 20, color='skyblue')

plt.show()
plt.figure(figsize=(8,6))

plt.hist(df['Start ms'], bins= 20, color='skyblue')

plt.show()




threshold = 30



cols_with_outliers = []



for col in df.columns:

    if df[col].dtype in ['int64', 'float64']:

        mean = df[col].mean()

        std = df[col].std()

        outliers = df[(df[col] - mean).abs() > threshold * std]

        

        

        if len(outliers) > 0:

            cols_with_outliers.append(col)
print('Potential columns with outliers: ', cols_with_outliers)

print(len(cols_with_outliers))



import matplotlib.pyplot as plt



plt.figure(figsize=(8,6))



plt.hist(df['Activity Duration UL (ms)'], bins = 20, color='skyblue', edgecolor='black')

plt.xlabel('Activity Duration UL (ms)')

plt.ylabel('Number')

plt.show()
df['Activity Duration UL (ms)'].describe()




Q1 = df['Activity Duration UL (ms)'].quantile(0.25)

Q3 = df['Activity Duration UL (ms)'].quantile(0.75)



IQR = Q3 - Q1

upper_ts = Q3 + 1.5 * IQR



outliers_activity_duration = df[df['Activity Duration UL (ms)'] > upper_ts]

outliers_activity_duration
outliers_activity_duration['Activity Duration UL (ms)'].describe()
plt.figure(figsize=(8, 6))

plt.hist(outliers_activity_duration['Activity Duration UL (ms)'], bins=30, color='skyblue', edgecolor='black')

plt.xlabel('Activity Duration UL (ms)')

plt.ylabel('Frequency')

plt.title('Distribution of Activity Duration UL (ms) - Outliers Subset')

plt.grid(True)

plt.show()
import pandas as pd
import psycopg2
from sqlalchemy import create_engine


def connect_to_database(connection_params):
    try:
        connection = psycopg2.connect(**connection_params)
        return connection
    except psycopg2.Error as e:
        print(f"Error: Unable to connect to the database. {e}")
        return None


class Database:
    def __init__(self, host, database, user, password):
        self.connection_params = {
            'dbname': 'telecom',
            'user': 'postgres',
            'password': 'heisenberg',
            'host': 'localhost',
            'port': '5432'
        }
        self.conn = connect_to_database(self.connection_params)

    def read_table_to_dataframe(self, table_name):
        if self.conn:
            query = f"SELECT * FROM {table_name};"
            df = pd.read_sql_query(query, self.conn)
            return df
        else:
            print("Error: No connection detected!")
            return None

    def write_dataframe_to_table(self, df, table_name, if_exists='replace'):
        engine = create_engine(
            f"postgresql://{self.connection_params['user']}:{self.connection_params['password']}@"
            f"{self.connection_params['host']}:{self.connection_params['port']}/{self.connection_params['dbname']}"
        )
        df.to_sql(table_name, engine, index=False, if_exists=if_exists)
        print(f"Dataframe successfully written to the '{table_name}' table.")

    def update_table_by_appending(self, df, table_name):
        self.write_dataframe_to_table(df, table_name, if_exists='append')

    def delete_table(self, table_name):
        if self.conn:
            cursor = self.conn.cursor()
            cursor.execute(f"DROP TABLE IF EXISTS {table_name};")
            self.conn.commit()
            cursor.close()
            print(f"Table '{table_name}' successfully deleted.")
        else:
            print("Error: No connection detected.")
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import os

import sys

import warnings

warnings.filterwarnings('ignore')



sys.path.append('../helpers/')



from data import Database

from utils import Helpers
db = Database(host="localhost", database="telecom", user="postgres", password="heisenberg")

df = db.read_table_to_dataframe('xdr_data')

df.head()



for column in df.columns:

    if df[column].dtype == 'float64' and df[column].isnull().any():

        mean_value = df[column].mean()

        df[column].fillna(mean_value, inplace=True)

df.isnull().sum()

correlation_matrix_UL = df[['Google UL (Bytes)', 'Social Media UL (Bytes)', 'Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)']].corr()



correlation_matrix_DL = df[['Google DL (Bytes)', 'Social Media DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)']].corr()








plt.figure(figsize=(8, 6))

sns.heatmap(correlation_matrix_UL, annot=True, cmap=sns.cubehelix_palette(as_cmap=True),)

plt.title('Correlation Matrix of Applications and Total DL+UL')

plt.show()
plt.figure(figsize=(8, 6))

sns.heatmap(correlation_matrix_DL, annot=True, cmap=sns.cubehelix_palette(as_cmap=True),)

plt.title('Correlation Matrix of Applications and Total DL+UL')

plt.show()
import pandas as pd

import matplotlib.pyplot as plt




upload_columns = ['Google UL (Bytes)', 'Social Media UL (Bytes)', 'Email UL (Bytes)',

                  'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)']




def bytes_to_megabytes(value):

    try:

        return float(value) / (1024 * 1024)

    except (ValueError, TypeError):

        return pd.NA  



total_upload_data = df[upload_columns].applymap(bytes_to_megabytes).sum()




plt.figure(figsize=(8, 6))

total_upload_data.sort_values(ascending=False).plot(kind='bar', color='skyblue')

plt.xlabel('Applications')

plt.ylabel('Total Upload Data (MB)')

plt.title('Total Upload Data for Each Application (in MB)')

plt.xticks(rotation=45)

plt.show()

download_columns = ['Google DL (Bytes)', 'Social Media DL (Bytes)', 'Email DL (Bytes)',

                  'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)']




def bytes_to_megabytes(value):

    try:

        return float(value) / (1024 * 1024)

    except (ValueError, TypeError):

        return pd.NA  



total_download_data = df[download_columns].applymap(bytes_to_megabytes).sum()






plt.figure(figsize=(8, 6))

total_download_data.sort_values(ascending=False).plot(kind='bar', color='skyblue')

plt.xlabel('Applications')

plt.ylabel('Total Upload Data (MB)')

plt.title('Total Upload Data for Each Application (in MB)')

plt.xticks(rotation=45)

plt.show()


df['Total_Session_Duration'] = df['Dur. (ms)'] + df['Dur. (ms).1']




df['Decile_Rank'] = pd.qcut(df['Total_Session_Duration'], q=10, labels=False)




df['Total_Data_DL_UL'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']




data_per_decile = df.groupby('Decile_Rank')['Total_Data_DL_UL'].sum()




 

data_per_decile

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA




numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

numerical_columns = numerical_columns.drop('MSISDN/Number', errors='ignore')  
numerical_columns = numerical_columns.drop('Bearer Id', errors='ignore')  


numerical_data = df[numerical_columns]

numerical_data = numerical_data.dropna()



scaler = StandardScaler()

numerical_data = scaler.fit_transform(numerical_data)



pca = PCA(n_components=2)

pca_result = pca.fit_transform(numerical_data)

pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

final_df = pd.concat([df, pca_df], axis=1)





final_df

final_df.info()
df.info()
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt


class Helpers:
    @staticmethod
    def normalize(df, columns):
        scaler = StandardScaler()
        df_normalized = df.copy()
        df_normalized[columns] = scaler.fit_transform(df_normalized[columns])
        return df_normalized

    @staticmethod
    def reduce(df):
        numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
        numerical_columns = numerical_columns.drop('MSISDN/Number', axis=1)
        numerical_data = df[numerical_columns]
        numerical_data = numerical_data.dropna()

        scaler = StandardScaler()
        numerical_data = scaler.fit_transform(numerical_data)

        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(numerical_data)
        pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])
        final_df = pd.concat([df, pca_df], axis=1)
        return final_df

    @staticmethod
    def perform_kmeans(agg_data: pd.DataFrame, columns: list, n_clusters: int):
        cluster_data = agg_data[columns]

                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        agg_data['Cluster'] = kmeans.fit_predict(cluster_data)

        return agg_data[['MSISDN/Number', 'Cluster']]

    @staticmethod
    def plot_clusters(data, x_column, y_column, cluster_column):
        plt.figure(figsize=(8, 6))
        clusters = data[cluster_column].unique()
        
        for cluster in clusters:
            cluster_data = data[data[cluster_column] == cluster]
            plt.scatter(cluster_data[x_column], cluster_data[y_column], label=f'Cluster {cluster}')
        
        plt.xlabel(x_column)
        plt.ylabel(y_column)
        plt.title(f'Clusters based on {x_column} vs {y_column}')
        plt.legend()
        plt.show()
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import os

import sys

import warnings

warnings.filterwarnings('ignore')



sys.path.append('../helpers/')



from data import Database

from utils import Helpers
db = Database(host="localhost", database="telecom", user="postgres", password="heisenberg")

df = db.read_table_to_dataframe('xdr_data')

df.head()



for column in df.columns:

    if df[column].dtype == 'float64' and df[column].isnull().any():

        mean_value = df[column].mean()

        df[column].fillna(mean_value, inplace=True)

        

df.isnull().sum()

no_null_df = df

no_null_df.isnull().sum()









task_4_1_columns = [

    'MSISDN/Number',

    'Avg RTT DL (ms)',

    'Avg RTT UL (ms)',

    'Handset Type',

    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

    'TCP DL Retrans. Vol (Bytes)',

    'TCP UL Retrans. Vol (Bytes)'

]






task4_1_data = no_null_df[task_4_1_columns]

task4_1_data.isnull().sum()





tcp_retransmission = (

    task4_1_data.groupby('MSISDN/Number')

    .agg({'TCP DL Retrans. Vol (Bytes)': 'mean', 'TCP UL Retrans. Vol (Bytes)': 'mean'})

    .reset_index()

)




tcp_retransmission.columns = ['MSISDN/Number', 'Avg_TCP_DL_Retrans (MB)', 'Avg_TCP_UL_Retrans (MB)']

tcp_retransmission /= (1024 * 1024)

tcp_retransmission.head()



rtt_data = no_null_df[task_4_1_columns]



rtt_data = (

    task4_1_data.groupby('MSISDN/Number')

    .agg({'Avg RTT DL (ms)': 'mean', 'Avg RTT UL (ms)': 'mean'})

    .reset_index()

)



rtt_data.columns = ['MSISDN/Number', 'Avg_RTT_DL (ms)', 'Avg_RTT_UL (ms)']

rtt_data.head()
most_freq = task4_1_data['Handset Type'].mode()[0]



task4_1_data['Handset Type'].fillna(most_freq, inplace=True)




handset = task4_1_data.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode()[0]).reset_index()

handset.columns = ['MSISDN/Number', 'Most Used Handset Type']

handset.head()

import matplotlib.pyplot as plt




handset_counts = handset['Most Used Handset Type'].value_counts().nlargest(10)



plt.figure(figsize=(10, 6))

handset_counts.sort_values().plot(kind='barh', color='skyblue')  
plt.title('Most Used Handset Types')

plt.xlabel('Frequency')

plt.ylabel('Handset Types')

plt.tight_layout()

plt.show()

handset['Most Used Handset Type'].value_counts()

throughput_df = task4_1_data[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']]

throughput_df['Avg Throughput (kbps)'] = (throughput_df['Avg Bearer TP DL (kbps)'] + throughput_df['Avg Bearer TP UL (kbps)']) / 2


throughput_df



top_10_tcp = tcp_retransmission.nlargest(10, 'Avg_TCP_DL_Retrans (MB)')

top_10_tcp



bottom_10_tcp = tcp_retransmission.nsmallest(10, 'Avg_TCP_DL_Retrans (MB)')

bottom_10_tcp
top_10_rtt = rtt_data.nsmallest(10, 'Avg_RTT_DL (ms)')

top_10_rtt

bottom_10_rtt = rtt_data.nlargest(10, 'Avg_RTT_DL (ms)')

bottom_10_rtt
top_10_throughput = throughput_df.nlargest(10, 'Avg Throughput (kbps)')

top_10_throughput
bottom_10_throughput = throughput_df.nsmallest(10, 'Avg Throughput (kbps)')

bottom_10_throughput
avg_tp_per_handset = task4_1_data.groupby('Handset Type')[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean()

avg_tp_per_handset




avg_tp_per_handset.nlargest(10, 'Avg Bearer TP DL (kbps)')

avg_tp_per_handset.nsmallest(10, 'Avg Bearer TP DL (kbps)')
avg_tcp_ret_p_handset = task4_1_data.groupby('Handset Type')[['TCP DL Retrans. Vol (Bytes)']].mean()

avg_tcp_ret_p_handset /= (1024 * 1024)

avg_tcp_ret_p_handset.nlargest(10, 'TCP DL Retrans. Vol (Bytes)')
numeric_columns = [

    'Avg RTT DL (ms)',

    'Avg RTT UL (ms)',

    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

    'TCP DL Retrans. Vol (Bytes)',

    'TCP UL Retrans. Vol (Bytes)'

]



numeric_data = no_null_df[['MSISDN/Number'] + numeric_columns]




agg_data = numeric_data.groupby('MSISDN/Number').agg('mean').reset_index()




agg_data.head()
columns_for_kmeans = [

    'Avg RTT DL (ms)',

    'Avg RTT UL (ms)',

    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

    'TCP DL Retrans. Vol (Bytes)',

    'TCP UL Retrans. Vol (Bytes)'

]



result_clusters = Helpers.perform_kmeans(agg_data=agg_data, columns=columns_for_kmeans, n_clusters=3)

result_clusters.head(10)

result_clusters['Cluster'].unique()
x_column = 'Avg RTT DL (ms)'  

y_column = 'Avg Bearer TP DL (kbps)'  

cluster_column = 'Cluster'  



Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
x_column = 'TCP DL Retrans. Vol (Bytes)'  

y_column = 'Avg Bearer TP DL (kbps)'  

cluster_column = 'Cluster'  



Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
x_column = 'TCP DL Retrans. Vol (Bytes)'  

y_column = 'Avg RTT DL (ms)'  

cluster_column = 'Cluster'  



Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
x_column = 'TCP DL Retrans. Vol (Bytes)'  

y_column = 'Avg Bearer TP UL (kbps)'  

cluster_column = 'Cluster'  



Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
from scipy.spatial import distance



centroids = result_clusters.groupby('Cluster').mean()




for idx, user_data in agg_data.iterrows():

    user_point = user_data[columns_for_kmeans]  
    

    
    distances = [distance.euclidean(user_point, centroid) for _, centroid in centroids.iterrows()]

    

    
    less_engaged_centroid = distances.index(min(distances))

    worst_experience_centroid = distances.index(max(distances))

    

    
    agg_data.at[idx, 'Engagement_Score'] = min(distances)

    agg_data.at[idx, 'Experience_Score'] = max(distances)

agg_data.info()
agg_data['Satisfaction_Score'] = (agg_data['Engagement_Score'] + agg_data['Experience_Score']) / 2

top_10_satisfied_customers = agg_data.nsmallest(10, 'Satisfaction_Score')[['MSISDN/Number', 'Satisfaction_Score']]

top_10_satisfied_customers

agg_data['Satisfaction_Score'] = (agg_data['Engagement_Score'] + agg_data['Experience_Score']) / 2

bottom_10_satisfied_customers = agg_data.nlargest(10, 'Satisfaction_Score')[['MSISDN/Number', 'Satisfaction_Score']]

bottom_10_satisfied_customers
add_data['Satisfaction Score] = 
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score




X = agg_data.drop(['Satisfaction_Score', 'MSISDN/Number'], axis=1) 

y = agg_data['Satisfaction_Score'] 



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



model = LinearRegression()

model.fit(X_train, y_train)



y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)

rmse = np.sqrt(mse)

r2 = r2_score(y_test, y_pred)



print(f"Mean Squared Error: {mse}")

print(f"Root Mean Squared Error: {rmse}")

print(f"R-squared Score: {r2}")
plt.figure(figsize=(8, 6))

sns.scatterplot(x=y_test, y=y_pred)

plt.plot(y_test, y_test, color='red')

plt.title('Actual vs. Predicted Values')

plt.xlabel('Actual Values')

plt.ylabel('Predicted Values')

plt.show()
from sklearn.cluster import KMeans

import pandas as pd



scores = agg_data[['Engagement_Score', 'Experience_Score']]



kmeans = KMeans(n_clusters=2, random_state=42)

kmeans.fit(scores)



agg_data['Score_Cluster'] = kmeans.labels_



plt.figure(figsize=(8, 6))

plt.scatter(agg_data['Engagement_Score'], agg_data['Experience_Score'], c=agg_data['Score_Cluster'], cmap='viridis', edgecolors='k')

plt.title('K-Means Clustering of Engagement & Experience Scores (k=2)')

plt.xlabel('Engagement Score')

plt.ylabel('Experience Score')

plt.colorbar(label='Cluster')

plt.show()

cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=['Engagement_Score', 'Experience_Score'])

cluster_centers
import psycopg2




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




conn = psycopg2.connect(**db_params)




cursor = conn.cursor()




cursor.execute("SELECT * FROM xdr_data;")




row = cursor.fetchone()




if row:

    print(row)

else:

    print("No results to fetch")




cursor.close()

conn.close()


column_names = [desc[0] for desc in cursor.description]

print(column_names)




print(row)


bearer_id = row[0]

start_time = row[1]





print(f"Bearer ID: {bearer_id}")

print(f"Start Time: {start_time}")


import pandas as pd

import psycopg2

import psycopg2



db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




conn = psycopg2.connect(**db_params)




cursor = conn.cursor()




cursor.execute("SELECT * FROM xdr_data;")




rows = cursor.fetchall()




column_names = [desc[0] for desc in cursor.description]




df = pd.DataFrame(rows, columns=column_names)




print(df)




cursor.close()

conn.close()

print(df.head)

df_cleaned = df.dropna()
df_cleaned
import seaborn as sns

import matplotlib.pyplot as plt




numeric_df = df.select_dtypes(include=['float64', 'int64'])




correlation_matrix = numeric_df.corr()

plt.figure(figsize=(15, 12))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Matrix')

plt.show()

selected_columns = ['Dur. (ms)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)']

numeric_df = df[selected_columns]

numeric_df
import numpy as np

import pandas as pd

import psycopg2
import matplotlib.pyplot as plt

import seaborn as sns
import pandas as pd

from sqlalchemy import create_engine




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)

print(mydata.shape)
print(mydata.info())
sns.heatmap(mydata.isnull(), cbar=False, cmap='viridis')

plt.show()
missing_percentage = mydata.isnull().mean() * 100

print(missing_percentage)
top_10_summary = mydata.head(10).describe(include='all')

print(top_10_summary)

activity_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                    'Other DL (Bytes)', 'Other UL (Bytes)']



activity_data = mydata[activity_columns]


plt.figure(figsize=(14, 8))

activity_data.sum().plot(kind='bar')

plt.title('Distribution of Activity Types')

plt.ylabel('Total Bytes')

plt.show()

session_frequency = mydata['MSISDN/Number'].value_counts()



plt.figure(figsize=(12, 6))

session_frequency.hist(bins=30, edgecolor='black')

plt.title('Distribution of User Session Frequency')

plt.xlabel('Number of Sessions')

plt.ylabel('Frequency')

plt.show()

plt.figure(figsize=(10, 6))




sns.histplot(mydata['Dur. (ms)'], bins=20, kde=True, color='skyblue')




plt.xlabel('Session Duration (ms)')

plt.ylabel('Frequency')

plt.title('Distribution of Session Duration in Top 10 Rows')




plt.show()


columns_to_input = ['Start', 'End', 'Dur. (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']



for column in columns_to_input:

    
    if pd.api.types.is_numeric_dtype(mydata[column]):

        
        mydata[column].fillna(mydata[column].mean(), inplace=True)

    else:

        
        mydata[column].fillna(method='ffill', inplace=True)

columns_to_input 

total_users = mydata['MSISDN/Number'].nunique()

print("Total Number of Unique Users:", total_users)

top_10_handsets = mydata['Handset Type'].value_counts().head(10)




print("Top 10 Handsets Used by Customers:")

print(top_10_handsets)


top_10_handsets = mydata['Handset Type'].value_counts().head(10).index




mydata_top_10 = mydata[mydata['Handset Type'].isin(top_10_handsets)]



plt.figure(figsize=(14, 6))

sns.countplot(x='Handset Type', data=mydata_top_10, order=top_10_handsets, palette='viridis')




plt.xticks(rotation=45, ha='right')



for index, value in enumerate(mydata_top_10['Handset Type'].value_counts()):

    plt.text(index, value + 0.1, f"{value}", ha='center', va='bottom')  


plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.tight_layout()

plt.show()




average_duration = mydata['Dur. (ms)'].mean()

print("Average Duration of User Sessions (in ms):", average_duration)

top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3).index




mydata_top_3_manufacturers = mydata[mydata['Handset Manufacturer'].isin(top_3_manufacturers)]




top_5_handsets_per_manufacturer = (

    mydata_top_3_manufacturers.groupby('Handset Manufacturer')['Handset Type']

    .value_counts()

    .groupby(level=0, group_keys=False)

    .nlargest(5)

    .reset_index(name='Count')

)




print("Top 5 Handsets per Top 3 Handset Manufacturers:")

print(top_5_handsets_per_manufacturer)


top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3)




print("Top 3 Handset Manufacturers:")

print(top_3_manufacturers)


data = {

    'Handset Manufacturer': ['Apple', 'Samsung', 'Huawei'],

    'Count': [59565, 40839, 34423]

}



df = pd.DataFrame(data)




plt.figure(figsize=(8, 8))

plt.pie(df['Count'], labels=df['Handset Manufacturer'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral', 'lightgreen'])

plt.title('Distribution of Top 3 Handset Manufacturers')

plt.show()


plt.figure(figsize=(16, 8))

sns.barplot(x='Handset Type', y='Count', hue='Handset Manufacturer', data=top_5_handsets_per_manufacturer, palette='viridis')

plt.xticks(rotation=45, ha='right')

plt.title('Top 5 Handsets per Top 3 Handset Manufacturers')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.tight_layout()

plt.legend(title='Handset Manufacturer')

plt.show()

data = {

    'Handset Type': ['Apple iPhone 6S (A1688)', 'Apple iPhone 6 (A1586)', 'Apple iPhone 7 (A1778)', 'Apple iPhone Se (A1723)', 'Apple iPhone 8 (A1905)'],

    'Count': [9419, 9023, 6326, 5187, 4993]

}



df = pd.DataFrame(data)




manufacturers = ['Apple', 'Samsung', 'Huawei']



plt.figure(figsize=(20, 5))



for i, manufacturer in enumerate(manufacturers, 1):

    plt.subplot(1, 3, i)

    

    
    manufacturer_data = df[df['Handset Type'].str.contains(manufacturer)]

    

    
    plt.pie(manufacturer_data['Count'], labels=manufacturer_data['Handset Type'], autopct='%1.1f%%', startangle=140)

    

    
    plt.title(f'Distribution of Users for {manufacturer}')




plt.tight_layout()




plt.show()


user_xdr_sessions = mydata.groupby('MSISDN/Number')['Bearer Id'].count().reset_index(name='Number_of_xDR_sessions')




print("Aggregated Information - Number of xDR Sessions per User:")

print(user_xdr_sessions)


mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['End'] = pd.to_datetime(mydata['End'])

mydata['Session_Duration'] = (mydata['End'] - mydata['Start']).dt.total_seconds()

user_session_duration = mydata.groupby('MSISDN/Number')['Session_Duration'].sum().reset_index(name='Total_Session_Duration')

print("Aggregated Session Duration per User:")

print(user_session_duration)

user_data_volume = mydata.groupby('MSISDN/Number').agg(

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Download and Upload Data per User:")

print(user_data_volume)


user_app_data_volume = mydata.groupby('MSISDN/Number').agg(

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Data Volume per User for Each Application:")

print(user_app_data_volume)


user_aggregated_data = mydata.groupby('MSISDN/Number').agg(

    Number_of_xDR_sessions=('Bearer Id', 'count'),

    Session_Duration=('Session_Duration', 'sum'),

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum'),

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()


plt.figure(figsize=(15, 10))

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Number_of_xDR_sessions'], label='Number of xDR Sessions')

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Session_Duration'], label='Session Duration', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_DL_data'], label='Total DL Data', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_UL_data'], label='Total UL Data', alpha=0.7)

plt.legend()

plt.xlabel('MSISDN/Number')

plt.ylabel('Aggregated Values')

plt.title('Aggregated Information per User')

plt.xticks(rotation=45, ha='right')

plt.tight_layout()

plt.show()

summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.index, y=top_handsets.values)

plt.xticks(rotation=45, ha='right')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.show()

plt.figure(figsize=(10, 6))

sns.histplot(mydata['Session_Duration'], bins=30, kde=True)

plt.title('Distribution of Session Durations')

plt.xlabel('Session Duration (seconds)')

plt.ylabel('Frequency')

plt.show()

plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()

mixed_data_columns = mydata.select_dtypes(include='object').columns

print(f'Columns with mixed data types: {mixed_data_columns}')


plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()


summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.values, y=top_handsets.index, orient='h')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.show()

basic_metrics = mydata.describe()


dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]



plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.show()

import matplotlib.pyplot as plt

import seaborn as sns





application = 'Social Media'




subset_data = mydata[[f'{application} DL (Bytes)', f'{application} UL (Bytes)', 'Total DL (Bytes)', 'Total UL (Bytes)']]




subset_data['Total DL+UL Data'] = subset_data['Total DL (Bytes)'] + subset_data['Total UL (Bytes)']




plt.figure(figsize=(10, 6))

sns.scatterplot(x=f'{application} DL (Bytes)', y='Total DL+UL Data', data=subset_data)

plt.title(f'Relationship between {application} and Total DL+UL Data')

plt.xlabel(f'{application} DL (Bytes)')

plt.ylabel('Total DL+UL Data')

plt.show()


customer_engagement = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',

    'Dur. (ms)': 'sum',

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()




customer_engagement.columns = ['MSISDN', 'Session Count', 'Total Duration', 'Total UL (Bytes)', 'Total DL (Bytes)']




customer_engagement['Total Traffic (Bytes)'] = customer_engagement['Total UL (Bytes)'] + customer_engagement['Total DL (Bytes)']




top_10_session_count = customer_engagement.sort_values(by='Session Count', ascending=False).head(10)

top_10_duration = customer_engagement.sort_values(by='Total Duration', ascending=False).head(10)

top_10_total_traffic = customer_engagement.sort_values(by='Total Traffic (Bytes)', ascending=False).head(10)




print("Top 10 Customers by Session Count:")

print(top_10_session_count[['MSISDN', 'Session Count']])

print("\nTop 10 Customers by Total Duration:")

print(top_10_duration[['MSISDN', 'Total Duration']])

print("\nTop 10 Customers by Total Traffic:")

print(top_10_total_traffic[['MSISDN', 'Total Traffic (Bytes)']])

import matplotlib.pyplot as plt




def plot_top_10(data, metric, title):

    plt.figure(figsize=(10, 6))

    plt.bar(data['MSISDN'], data[metric], color='skyblue')

    plt.xlabel('MSISDN')

    plt.ylabel(metric)

    plt.title(title)

    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()

    plt.show()




plot_top_10(top_10_session_count, 'Session Count', 'Top 10 Customers by Session Count')




plot_top_10(top_10_duration, 'Total Duration', 'Top 10 Customers by Total Duration')




plot_top_10(top_10_total_traffic, 'Total Traffic (Bytes)', 'Top 10 Customers by Total Traffic')

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sqlalchemy import create_engine

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

from sklearn.impute import SimpleImputer




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)




imputer = SimpleImputer(strategy='mean')

imputed_data = imputer.fit_transform(mydata.select_dtypes(include=['float64']))




scaler = StandardScaler()

scaled_data = scaler.fit_transform(imputed_data)




pca = PCA()

principal_components = pca.fit_transform(scaled_data)




pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])




pca_interpretation = [

    "Principal components capture the variance in the data.",

    "Each principal component is a linear combination of the original features.",

    "The first few principal components explain a significant portion of the total variance.",

    "PCA is useful for dimensionality reduction and identifying patterns in high-dimensional data."

]




from pptx import Presentation

from pptx.util import Inches

import os




presentation = Presentation()




def add_slide(title, content):

    slide = presentation.slides.add_slide(presentation.slide_layouts[1])  
    title_box = slide.shapes.title

    content_box = slide.placeholders[1]

    title_box.text = title

    content_box.text = content




variable_description = mydata.dtypes.reset_index()

variable_description.columns = ['Variable', 'Data Type']

add_slide("Variable Description", variable_description.to_string(index=False))




basic_metrics = mydata.describe()

add_slide("Basic Metrics", basic_metrics.to_string())




dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]

add_slide("Non-Graphical Univariate Analysis", dispersion_parameters.to_string())




image_path = 'univariate_analysis_plot.png'

plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.savefig(image_path)

plt.close()

presentation.slides.add_slide(presentation.slide_layouts[5])  
presentation.slides[-1].shapes.add_picture(image_path, Inches(1), Inches(1), width=Inches(8))

os.remove(image_path)  



application_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                     'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                     'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                     'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']



total_data_cols = ['Total DL (Bytes)', 'Total UL (Bytes)']

bivariate_analysis = mydata[application_cols + total_data_cols].corr()

add_slide("Bivariate Analysis", bivariate_analysis.to_string())




mydata['Total_Session_Duration'] = mydata['Dur. (ms).1'] / 1000  
mydata['Decile_Class'] = pd.qcut(mydata['Total_Session_Duration'], q=5, labels=False, duplicates='drop')

total_data_per_decile = mydata.groupby('Decile_Class')[total_data_cols].sum()

add_slide("Variable Transformations", total_data_per_decile.to_string())





correlation_matrix = mydata[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                              'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                              'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                              'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']].corr()

add_slide("Correlation Analysis", correlation_matrix.to_string())




add_slide("PCA Interpretation", "\n".join(pca_interpretation))




presentation.save("telecom_analysis_results.pptx")

import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt







engagement_metrics = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',            
    'Dur. (ms).1': 'sum',            
    'Total UL (Bytes)': 'sum',       
    'Total DL (Bytes)': 'sum'        
}).reset_index()




engagement_metrics.columns = ['MSISDN', 'Session Frequency', 'Total Session Duration', 'Total UL Traffic', 'Total DL Traffic']




top_10_session_frequency = engagement_metrics.sort_values(by='Session Frequency', ascending=False).head(10)

top_10_duration = engagement_metrics.sort_values(by='Total Session Duration', ascending=False).head(10)

top_10_ul_traffic = engagement_metrics.sort_values(by='Total UL Traffic', ascending=False).head(10)

top_10_dl_traffic = engagement_metrics.sort_values(by='Total DL Traffic', ascending=False).head(10)



print("Top 10 Customers by Session Frequency:")

print(top_10_session_frequency)



print("\nTop 10 Customers by Total Session Duration:")

print(top_10_duration)



print("\nTop 10 Customers by Total UL Traffic:")

print(top_10_ul_traffic)



print("\nTop 10 Customers by Total DL Traffic:")

print(top_10_dl_traffic)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics.iloc[:, 1:])




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_metrics['Cluster'] = kmeans.fit_predict(normalized_metrics)




cluster_stats = engagement_metrics.groupby('Cluster').agg({

    'Session Frequency': ['min', 'max', 'mean', 'sum'],

    'Total Session Duration': ['min', 'max', 'mean', 'sum'],

    'Total UL Traffic': ['min', 'max', 'mean', 'sum'],

    'Total DL Traffic': ['min', 'max', 'mean', 'sum']

}).reset_index()



print("\nCluster Statistics:")

print(cluster_stats)




app_traffic = mydata.groupby('MSISDN/Number').agg({

    'Social Media DL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

}).reset_index()




top_10_social_media = app_traffic.sort_values(by='Social Media DL (Bytes) + Social Media UL (Bytes)', ascending=False).head(10)

top_10_youtube = app_traffic.sort_values(by='Youtube DL (Bytes) + Youtube UL (Bytes)', ascending=False).head(10)

top_10_gaming = app_traffic.sort_values(by='Gaming DL (Bytes) + Gaming UL (Bytes)', ascending=False).head(10)



print("\nTop 10 Most Engaged Users (Social Media):")

print(top_10_social_media)



print("\nTop 10 Most Engaged Users (YouTube):")

print(top_10_youtube)



print("\nTop 10 Most Engaged Users (Gaming):")

print(top_10_gaming)




top_3_apps = app_traffic.sum().nlargest(3)



plt.figure(figsize=(10, 6))

top_3_apps.plot(kind='bar', color='skyblue')

plt.title('Top 3 Most Used Applications')

plt.xlabel('Application')

plt.ylabel('Total Traffic (Bytes)')

plt.show()








inertia_values = []

possible_k_values = range(1, 11)



for k in possible_k_values:

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_metrics)

    inertia_values.append(kmeans.inertia_)




plt.figure(figsize=(10, 6))

plt.plot(possible_k_values, inertia_values, marker='o')

plt.title('Elbow Method for Optimal k')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.show()

import pandas as pd

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.impute import SimpleImputer

from kneed import KneeLocator




identifier_column = 'MSISDN/Number'




print(mydata.columns)




engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

engagement_metrics.columns = ['Session Frequency', 'Total Session Duration', 'Total Traffic']




imputer = SimpleImputer(strategy='mean')

engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)




print("Length of mydata:", len(mydata))

print("Length of normalized_metrics:", len(normalized_metrics))




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_clusters = kmeans.fit_predict(normalized_metrics)




print("Length of engagement_clusters:", len(engagement_clusters))




if len(mydata) == len(engagement_clusters):

    mydata['Engagement Cluster'] = engagement_clusters

else:

    print("Length mismatch! Unable to assign 'Engagement Cluster'.")




print("Missing values in mydata:")

print(mydata.isnull().sum())

engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

print("Length of engagement_metrics:", len(engagement_metrics))



engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)

print("Length of engagement_metrics_imputed:", len(engagement_metrics_imputed))



normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)

print("Length of normalized_metrics:", len(normalized_metrics))

print("Duplicate rows based on identifier column:")

print(mydata[mydata.duplicated(subset=identifier_column, keep=False)])
import pandas as pd





application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                        'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                        'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                        'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']




total_data_per_user = mydata.groupby('MSISDN/Number')[application_columns].sum()




total_data_per_user['Total Data Usage'] = total_data_per_user.sum(axis=1)




top_engaged_users = total_data_per_user.nlargest(10, 'Total Data Usage')




print(top_engaged_users)

import matplotlib.pyplot as plt





engagement_metric = 'Total Data Usage'




plt.figure(figsize=(10, 6))

top_engaged_users[engagement_metric].plot(kind='bar', color='skyblue')

plt.title('Top 10 Engaged Users per Application')

plt.xlabel('Bearer Id')

plt.ylabel(engagement_metric)

plt.show()


mydata.hist(figsize=(20, 30))

plt.show()

sns.boxplot(data=mydata)

plt.show()
import pandas as pd

from sqlalchemy import create_engine

from analysis_modules.user_overview import UserOverviewAnalysis

from analysis_modules.user_engagement import UserEngagementAnalysis

from analysis_modules.user_experience import UserExperienceAnalysis

from analysis_modules.user_satisfaction import UserSatisfactionAnalysis




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)
import numpy as np

import pandas as pd

import psycopg2
import matplotlib.pyplot as plt

import seaborn as sns
from sqlalchemy import create_engine




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)
mydata.head(5)

mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['End'] = pd.to_datetime(mydata['End'])

mydata['session_duration'] = (mydata['End'] - mydata['Start']).dt.total_seconds()

session_frequency = mydata.groupby('MSISDN/Number')['Bearer Id'].count()

average_session_duration = mydata.groupby('MSISDN/Number')['session_duration'].mean()

total_data_usage = mydata.groupby('MSISDN/Number')['Total UL (Bytes)'].sum() + mydata.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()

engagement_score = 0.4 * session_frequency + 0.6 * average_session_duration + 0.0 * total_data_usage

engagement_mydata = pd.DataFrame({

    'MSISDN/Number': engagement_score.index,

    'session_frequency': session_frequency.values,

    'average_session_duration': average_session_duration.values,

    'total_data_usage': total_data_usage.values,

    'engagement_score': engagement_score.values

})

print(engagement_mydata)

plt.scatter(engagement_mydata['session_frequency'], engagement_mydata['engagement_score'])

plt.xlabel('Session Frequency')

plt.ylabel('Engagement Score')

plt.title('Session Frequency vs. Engagement Score')

plt.show()

top_10_session_frequency = engagement_df.nlargest(10, 'session_frequency')

top_10_average_session_duration = engagement_df.nlargest(10, 'average_session_duration')

top_10_total_data_usage = engagement_df.nlargest(10, 'total_data_usage')

top_10_engagement_score = engagement_df.nlargest(10, 'engagement_score')

print("Top 10 Customers by Session Frequency:")

print(top_10_session_frequency)



print("\nTop 10 Customers by Average Session Duration:")

print(top_10_average_session_duration)



print("\nTop 10 Customers by Total Data Usage:")

print(top_10_total_data_usage)



print("\nTop 10 Customers by Engagement Score:")

print(top_10_engagement_score)
import matplotlib.pyplot as plt




fig, axs = plt.subplots(2, 2, figsize=(12, 10))

fig.suptitle('Distribution of Engagement Metrics Among Top 10 Customers', fontsize=16)




axs[0, 0].hist(top_10_session_frequency['session_frequency'], bins=10, color='skyblue', edgecolor='black')

axs[0, 0].set_title('Session Frequency')




axs[0, 1].hist(top_10_average_session_duration['average_session_duration'], bins=10, color='salmon', edgecolor='black')

axs[0, 1].set_title('Average Session Duration')




axs[1, 0].hist(top_10_total_data_usage['total_data_usage'], bins=10, color='lightgreen', edgecolor='black')

axs[1, 0].set_title('Total Data Usage')




axs[1, 1].hist(top_10_engagement_score['engagement_score'], bins=10, color='orange', edgecolor='black')

axs[1, 1].set_title('Engagement Score')



plt.show()


fig, axs = plt.subplots(2, 2, figsize=(12, 10))

fig.suptitle('Top 10 Customers by Engagement Metric', fontsize=16)




axs[0, 0].bar(top_10_session_frequency['MSISDN/Number'], top_10_session_frequency['session_frequency'], color='skyblue')

axs[0, 0].set_title('Top 10 Customers by Session Frequency')




axs[0, 1].bar(top_10_average_session_duration['MSISDN/Number'], top_10_average_session_duration['average_session_duration'], color='salmon')

axs[0, 1].set_title('Top 10 Customers by Average Session Duration')




axs[1, 0].bar(top_10_total_data_usage['MSISDN/Number'], top_10_total_data_usage['total_data_usage'], color='lightgreen')

axs[1, 0].set_title('Top 10 Customers by Total Data Usage')




axs[1, 1].bar(top_10_engagement_score['MSISDN/Number'], top_10_engagement_score['engagement_score'], color='orange')

axs[1, 1].set_title('Top 10 Customers by Engagement Score')



plt.show()


total_session_frequency = top_10_session_frequency['session_frequency'].sum()

total_average_session_duration = top_10_average_session_duration['average_session_duration'].sum()

total_total_data_usage = top_10_total_data_usage['total_data_usage'].sum()

total_engagement_score = top_10_engagement_score['engagement_score'].sum()




data = [total_session_frequency, total_average_session_duration, total_total_data_usage, total_engagement_score]




labels = ['Session Frequency', 'Average Session Duration', 'Total Data Usage', 'Engagement Score']




colors = ['skyblue', 'salmon', 'lightgreen', 'orange']




plt.figure(figsize=(8, 8))

plt.pie(data, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)

plt.title('Percentage Distribution of Engagement Metrics Among Top 10 Customers')

plt.show()

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans




numeric_columns = ['session_frequency', 'average_session_duration', 'total_data_usage', 'engagement_score']

engagement_metrics_subset = engagement_mydata[numeric_columns]




scaler = StandardScaler()

engagement_metrics_normalized = scaler.fit_transform(engagement_metrics_subset)




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_mydata['cluster'] = kmeans.fit_predict(engagement_metrics_normalized)




print(engagement_mydata[['MSISDN/Number', 'cluster']])

cluster_means = engagement_mydata.groupby('cluster').mean()

print(cluster_means)
import matplotlib.pyplot as plt

import numpy as np




labels = ['Session Frequency', 'Session Duration', 'Total Data Usage', 'Engagement Score']




cluster_0_values = [2.83, 102470, 1.58e9, 61483]

cluster_1_values = [1.14, 190785, 5.47e8, 114471]

cluster_2_values = [1.15, 68680, 5.41e8, 41208]




num_metrics = len(labels)




angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()




values = cluster_0_values + cluster_0_values[:1]

angles += angles[:1]




fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

ax.fill(angles, values, color='r', alpha=0.25, label='Cluster 0')




values = cluster_1_values + cluster_1_values[:1]

ax.fill(angles, values, color='b', alpha=0.25, label='Cluster 1')




values = cluster_2_values + cluster_2_values[:1]

ax.fill(angles, values, color='g', alpha=0.25, label='Cluster 2')




ax.set_xticks(angles[:-1])

ax.set_xticklabels(labels)




ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))




plt.show()

numeric_columns = ['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)', 'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)', '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)', 'HTTP DL (Bytes)', 'HTTP UL (Bytes)', 'Activity Duration DL (ms)', 'Activity Duration UL (ms)', 'Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)', 'Total UL (Bytes)', 'Total DL (Bytes)', 'session_duration']
import pandas as pd

from sklearn.cluster import KMeans

from sklearn.impute import SimpleImputer




numeric_columns = ['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                   'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',

                   'Total UL (Bytes)', 'Total DL (Bytes)', 'session_duration']




mydata_copy = mydata.copy()




if mydata_copy[numeric_columns].isnull().any().any():

    
    imputer = SimpleImputer(strategy='constant', fill_value=0.5)

    

    
    clustered_data_imputed = imputer.fit_transform(mydata_copy[numeric_columns])

else:

    
    clustered_data_imputed = mydata_copy[numeric_columns]




k = 3  




kmeans = KMeans(n_clusters=k, random_state=42)




kmeans.fit(clustered_data_imputed)




mydata_copy['cluster'] = kmeans.labels_




cluster_metrics = mydata_copy.groupby('cluster')[numeric_columns].agg(['min', 'max', 'mean', 'sum']).reset_index()




print(cluster_metrics)

import seaborn as sns

import matplotlib.pyplot as plt




sns.set(style="whitegrid")




sns.pairplot(mydata, hue="cluster", palette="dark", diag_kind="kde")

plt.show()




plt.figure(figsize=(16, 10))

for i, column in enumerate(numeric_columns, 1):

    plt.subplot(2, 2, i)

    sns.boxplot(x="cluster", y=column, data=mydata, palette="Set3")

    plt.title(f"{column} distribution by cluster")

plt.tight_layout()

plt.show()
import numpy as np

import pandas as pd

import psycopg2

import pandas as pd

from sqlalchemy import create_engine

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)

mydata = mydata.dropna()
mydata['Start'] = pd.to_datetime(mydata['Start'], errors='coerce')

mydata['End'] = pd.to_datetime(mydata['End'], errors='coerce')

numeric_columns = mydata.select_dtypes(include='number').columns

mydata[numeric_columns] = mydata[numeric_columns].fillna(mydata[numeric_columns].mean())
mydata.columns

plt.figure(figsize=(12, 8))

sns.histplot(mydata['Avg Bearer TP DL (kbps)'], bins=30, kde=True, color='blue')

plt.title('Distribution of Avg Bearer TP DL (kbps)')

plt.xlabel('Avg Bearer TP DL (kbps)')

plt.ylabel('Frequency')

plt.show()

numeric_columns = mydata.select_dtypes(include=['float64', 'int64']).columns

correlation_matrix = mydata[numeric_columns].corr()




plt.figure(figsize=(15, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Matrix')

plt.show()



plt.figure(figsize=(12, 8))

sns.boxplot(x='Handset Type', y='Avg RTT DL (ms)', data=mydata)

plt.title('Avg RTT DL (ms) by Handset Type')

plt.xlabel('Handset Type')

plt.ylabel('Avg RTT DL (ms)')

plt.show()

plt.figure(figsize=(12, 8))

sns.scatterplot(x='Total UL (Bytes)', y='Total DL (Bytes)', data=mydata, hue='Handset Manufacturer')

plt.title('User Engagement: Total UL vs Total DL')

plt.xlabel('Total UL (Bytes)')

plt.ylabel('Total DL (Bytes)')

plt.legend()

plt.show()


features = mydata[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']]

target = mydata['cluster']  


scaler = StandardScaler()

features_scaled = scaler.fit_transform(features)




X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)




model = LinearRegression()

model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')
import numpy as np

import pandas as pd

import psycopg2
import matplotlib.pyplot as plt

import seaborn as sns
from sqlalchemy import create_engine




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)
print(mydata.info())

print(mydata.shape)

mydata = mydata.dropna()
mydata['Start'] = pd.to_datetime(mydata['Start'], errors='coerce')

mydata['End'] = pd.to_datetime(mydata['End'], errors='coerce')

numeric_columns = mydata.select_dtypes(include='number').columns

mydata[numeric_columns] = mydata[numeric_columns].fillna(mydata[numeric_columns].mean())

print(mydata.head(5))
print(mydata.describe())

user_device_mapping = mydata.groupby(['IMSI', 'MSISDN/Number'])['IMEI'].unique()




print(user_device_mapping)
import matplotlib.pyplot as plt




plt.hist(mydata['Dur. (ms)'])

plt.xlabel('Session Duration (ms)')

plt.ylabel('Frequency')

plt.show()

top_users = mydata.nlargest(10, 'Total UL (Bytes)')

print(top_users)


app_columns = ['Social Media DL (Bytes)', 'Gaming UL (Bytes)']

app_usage = mydata[app_columns].sum()

print(app_usage)

network_tech_distribution = mydata['Bearer Id'].value_counts()

print(network_tech_distribution)

location_analysis = mydata.groupby('Last Location Name').agg({'Dur. (ms)': 'mean', 'Total DL (Bytes)': 'sum'})

print(location_analysis)

mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['Hour'] = mydata['Start'].dt.hour

time_analysis = mydata.groupby('Hour').agg({'Dur. (ms)': 'mean', 'Total DL (Bytes)': 'sum'})

print(time_analysis)

top_handsets = mydata['IMEI'].value_counts().nlargest(10)




print(top_handsets)

mydata['IMEI'] = mydata['IMEI'].astype(str)




mydata['Manufacturer'] = mydata['IMEI'].str[:8]




top_manufacturers = mydata['Manufacturer'].value_counts().nlargest(3)




print(top_manufacturers)
mydata['IMEI'] = mydata['IMEI'].astype(str)




mydata['Manufacturer'] = mydata['IMEI'].str[:8]




top_manufacturers = mydata['Manufacturer'].value_counts().nlargest(3).index




for manufacturer in top_manufacturers:

    
    manufacturer_data = mydata[mydata['Manufacturer'] == manufacturer]



    
    top_handsets = manufacturer_data['IMEI'].value_counts().nlargest(5)



    
    print(f"\nTop 5 handsets for {manufacturer}:")

    print(top_handsets)


sessions_per_user = mydata['MSISDN/Number'].value_counts()




print(sessions_per_user)
import matplotlib.pyplot as plt




plt.hist(sessions_per_user, bins=20, edgecolor='black')

plt.xlabel('Number of Sessions')

plt.ylabel('Number of Users')

plt.title('Distribution of xDR Sessions per User')

plt.show()

session_duration_per_user = mydata.groupby('MSISDN/Number')['Dur. (ms)'].sum()




print(session_duration_per_user)

plt.hist(session_duration_per_user, bins=20, edgecolor='black')

plt.xlabel('Total Session Duration (ms)')

plt.ylabel('Number of Users')

plt.title('Distribution of Session Durations per User')

plt.show()

total_data_per_user = mydata.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum',

    'Total UL (Bytes)': 'sum'

})




print(total_data_per_user)
import matplotlib.pyplot as plt




plt.scatter(total_data_per_user['Total DL (Bytes)'], total_data_per_user['Total UL (Bytes)'])

plt.xlabel('Total Download (Bytes)')

plt.ylabel('Total Upload (Bytes)')

plt.title('Total Download vs Total Upload per User')

plt.show()



mydata['Total_Social_Media_Usage'] = mydata['Social Media DL (Bytes)'] + mydata['Social Media UL (Bytes)']

mydata['Total_Google_Usage'] = mydata['Google DL (Bytes)'] + mydata['Google UL (Bytes)']





aggregated_data = mydata.groupby('MSISDN/Number').agg({

    'Total_Social_Media_Usage': 'sum',

    'Total_Google_Usage': 'sum',

    
})




print(aggregated_data)

applications_columns = [

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)'

]




for app_column in applications_columns:

    app_name = app_column.split(' ')[0]  
    total_column = f'Total_{app_name}_Usage'



    
    mydata[total_column] = mydata[app_column]




aggregated_data = mydata.groupby(['MSISDN/Number', 'Start']).agg({

    'Total_Social_Media_Usage': 'sum',

    'Total_Google_Usage': 'sum',

    'Total_Email_Usage': 'sum',

    'Total_Youtube_Usage': 'sum',

    'Total_Netflix_Usage': 'sum',

    'Total_Gaming_Usage': 'sum',

})




print(aggregated_data)

sessions_per_user = mydata['MSISDN/Number'].value_counts()

sessions_per_user.describe()
mydata['Session_Duration'] = mydata['Dur. (ms)'] / 1000  
session_duration_stats = mydata['Session_Duration'].describe()
session_duration_stats
applications_columns = ['Total_Social_Media_Usage', 'Total_Google_Usage', 'Total_Email_Usage', 'Total_Youtube_Usage', 'Total_Netflix_Usage', 'Total_Gaming_Usage']

total_data_per_app = mydata[applications_columns].sum()
total_data_per_app 
avg_data_per_app = mydata.groupby('MSISDN/Number')[applications_columns].mean()
avg_data_per_app
avg_rtt_dl = mydata['Avg RTT DL (ms)'].mean()

avg_rtt_ul = mydata['Avg RTT UL (ms)'].mean()
avg_rtt_dl
avg_rtt_ul
tcp_retransmission_dl = mydata['TCP DL Retrans. Vol (Bytes)'].sum()

tcp_retransmission_ul = mydata['TCP UL Retrans. Vol (Bytes)'].sum()
tcp_retransmission_dl
tcp_retransmission_ul
top_handsets = mydata['Handset Type'].value_counts().head(10)
top_handsets
top_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3)
top_manufacturers
busiest_hours = mydata['Hour'].value_counts().sort_index()
busiest_hours 
import matplotlib.pyplot as plt

import seaborn as sns




sns.set(style="whitegrid")



plt.figure(figsize=(12, 6))

sns.histplot(mydata['Session_Duration'], bins=30, kde=True, color='skyblue')

plt.title('Distribution of Session Duration')

plt.xlabel('Session Duration (seconds)')

plt.ylabel('Frequency')

plt.show()

applications_columns = ['Total_Social_Media_Usage', 'Total_Google_Usage', 'Total_Email_Usage', 'Total_Youtube_Usage', 'Total_Netflix_Usage', 'Total_Gaming_Usage']

plt.figure(figsize=(12, 8))

sns.boxplot(data=mydata[applications_columns])

plt.title('Box Plot of Total Data Volume per Application')

plt.xlabel('Application')

plt.ylabel('Total Data Volume (Bytes)')

plt.xticks(rotation=45, ha='right')

plt.show()


busiest_hours = mydata['Hour'].value_counts().sort_index()

plt.figure(figsize=(12, 6))

sns.lineplot(x=busiest_hours.index, y=busiest_hours.values, marker='o', color='orange')

plt.title('Busiest Hours of Network Usage')

plt.xlabel('Hour of the Day')

plt.ylabel('Number of Sessions')

plt.show()
import matplotlib.pyplot as plt

import seaborn as sns




numeric_variables = mydata.select_dtypes(include='number')




for col in numeric_variables.columns:

    plt.figure(figsize=(8, 5))

    sns.histplot(mydata[col], bins=20, kde=True, color='skyblue')

    plt.title(f'Histogram of {col}')

    plt.xlabel(col)

    plt.ylabel('Frequency')

    plt.show()

for col in numeric_variables.columns:

    plt.figure(figsize=(8, 5))

    sns.boxplot(x=mydata[col], color='lightcoral')

    plt.title(f'Box Plot of {col}')

    plt.xlabel(col)

    plt.show()

import matplotlib.pyplot as plt

import seaborn as sns




applications_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)']




for app_col in applications_columns:

    plt.figure(figsize=(8, 5))

    sns.scatterplot(x=mydata[app_col], y=mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)'], alpha=0.7)

    plt.title(f'Scatter Plot: {app_col} vs Total Data Usage')

    plt.xlabel(app_col)

    plt.ylabel('Total DL+UL Data Usage (Bytes)')

    plt.show()

correlation_matrix = mydata[applications_columns + ['Total DL (Bytes)', 'Total UL (Bytes)']].corr()




print("Correlation Matrix:")

print(correlation_matrix)

import statsmodels.api as sm




X = mydata['Social Media DL (Bytes)'] + mydata['Social Media UL (Bytes)']

X = sm.add_constant(X)

y = mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)']



model = sm.OLS(y, X).fit()




print(model.summary())

import pandas as pd

import numpy as np








mydata['Total_Duration'] = mydata['Dur. (ms)']




mydata['Duration_Decile'] = pd.qcut(mydata['Total_Duration'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False, precision=0, duplicates='drop')




mydata['Total_Data'] = mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)']




data_per_decile = mydata.groupby('Duration_Decile')['Total_Data'].sum().reset_index()




print(data_per_decile)








selected_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                    'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']




app_data = mydata[selected_columns]




correlation_matrix = app_data.corr()




print(correlation_matrix)

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

selected_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                    'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']


app_data = mydata[selected_columns]

scaler = StandardScaler()

app_data_standardized = scaler.fit_transform(app_data)

pca = PCA()

pca_result = pca.fit_transform(app_data_standardized)

explained_variance_ratio = pca.explained_variance_ratio_




print("Explained Variance Ratio:", explained_variance_ratio)

top_components = pca_result[:, :2]




print("Top 2 Principal Components:")

print(top_components)

top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3).index




mydata_top_3_manufacturers = mydata[mydata['Handset Manufacturer'].isin(top_3_manufacturers)]




top_5_handsets_per_manufacturer = (

    mydata_top_3_manufacturers.groupby('Handset Manufacturer')['Handset Type']

    .value_counts()

    .groupby(level=0, group_keys=False)

    .nlargest(5)

    .reset_index(name='Count')

)




print("Top 5 Handsets per Top 3 Handset Manufacturers:")

print(top_5_handsets_per_manufacturer)


top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3)




print("Top 3 Handset Manufacturers:")

print(top_3_manufacturers)


data = {

    'Handset Manufacturer': ['Apple', 'Samsung', 'Huawei'],

    'Count': [59565, 40839, 34423]

}



df = pd.DataFrame(data)




plt.figure(figsize=(8, 8))

plt.pie(df['Count'], labels=df['Handset Manufacturer'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral', 'lightgreen'])

plt.title('Distribution of Top 3 Handset Manufacturers')

plt.show()


plt.figure(figsize=(16, 8))

sns.barplot(x='Handset Type', y='Count', hue='Handset Manufacturer', data=top_5_handsets_per_manufacturer, palette='viridis')

plt.xticks(rotation=45, ha='right')

plt.title('Top 5 Handsets per Top 3 Handset Manufacturers')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.tight_layout()

plt.legend(title='Handset Manufacturer')

plt.show()

data = {

    'Handset Type': ['Apple iPhone 6S (A1688)', 'Apple iPhone 6 (A1586)', 'Apple iPhone 7 (A1778)', 'Apple iPhone Se (A1723)', 'Apple iPhone 8 (A1905)'],

    'Count': [9419, 9023, 6326, 5187, 4993]

}



df = pd.DataFrame(data)




manufacturers = ['Apple', 'Samsung', 'Huawei']



plt.figure(figsize=(20, 5))



for i, manufacturer in enumerate(manufacturers, 1):

    plt.subplot(1, 3, i)

    

    
    manufacturer_data = df[df['Handset Type'].str.contains(manufacturer)]

    

    
    plt.pie(manufacturer_data['Count'], labels=manufacturer_data['Handset Type'], autopct='%1.1f%%', startangle=140)

    

    
    plt.title(f'Distribution of Users for {manufacturer}')




plt.tight_layout()




plt.show()


user_xdr_sessions = mydata.groupby('MSISDN/Number')['Bearer Id'].count().reset_index(name='Number_of_xDR_sessions')




print("Aggregated Information - Number of xDR Sessions per User:")

print(user_xdr_sessions)


mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['End'] = pd.to_datetime(mydata['End'])

mydata['Session_Duration'] = (mydata['End'] - mydata['Start']).dt.total_seconds()

user_session_duration = mydata.groupby('MSISDN/Number')['Session_Duration'].sum().reset_index(name='Total_Session_Duration')

print("Aggregated Session Duration per User:")

print(user_session_duration)

user_data_volume = mydata.groupby('MSISDN/Number').agg(

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Download and Upload Data per User:")

print(user_data_volume)


user_app_data_volume = mydata.groupby('MSISDN/Number').agg(

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Data Volume per User for Each Application:")

print(user_app_data_volume)


user_aggregated_data = mydata.groupby('MSISDN/Number').agg(

    Number_of_xDR_sessions=('Bearer Id', 'count'),

    Session_Duration=('Session_Duration', 'sum'),

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum'),

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()


plt.figure(figsize=(15, 10))

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Number_of_xDR_sessions'], label='Number of xDR Sessions')

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Session_Duration'], label='Session Duration', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_DL_data'], label='Total DL Data', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_UL_data'], label='Total UL Data', alpha=0.7)

plt.legend()

plt.xlabel('MSISDN/Number')

plt.ylabel('Aggregated Values')

plt.title('Aggregated Information per User')

plt.xticks(rotation=45, ha='right')

plt.tight_layout()

plt.show()

summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.index, y=top_handsets.values)

plt.xticks(rotation=45, ha='right')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.show()

plt.figure(figsize=(10, 6))

sns.histplot(mydata['Session_Duration'], bins=30, kde=True)

plt.title('Distribution of Session Durations')

plt.xlabel('Session Duration (seconds)')

plt.ylabel('Frequency')

plt.show()

plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()

mixed_data_columns = mydata.select_dtypes(include='object').columns

print(f'Columns with mixed data types: {mixed_data_columns}')


plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()


summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.values, y=top_handsets.index, orient='h')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.show()

basic_metrics = mydata.describe()


dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]



plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.show()

import matplotlib.pyplot as plt

import seaborn as sns





application = 'Social Media'




subset_data = mydata[[f'{application} DL (Bytes)', f'{application} UL (Bytes)', 'Total DL (Bytes)', 'Total UL (Bytes)']]




subset_data['Total DL+UL Data'] = subset_data['Total DL (Bytes)'] + subset_data['Total UL (Bytes)']




plt.figure(figsize=(10, 6))

sns.scatterplot(x=f'{application} DL (Bytes)', y='Total DL+UL Data', data=subset_data)

plt.title(f'Relationship between {application} and Total DL+UL Data')

plt.xlabel(f'{application} DL (Bytes)')

plt.ylabel('Total DL+UL Data')

plt.show()


customer_engagement = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',

    'Dur. (ms)': 'sum',

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()




customer_engagement.columns = ['MSISDN', 'Session Count', 'Total Duration', 'Total UL (Bytes)', 'Total DL (Bytes)']




customer_engagement['Total Traffic (Bytes)'] = customer_engagement['Total UL (Bytes)'] + customer_engagement['Total DL (Bytes)']




top_10_session_count = customer_engagement.sort_values(by='Session Count', ascending=False).head(10)

top_10_duration = customer_engagement.sort_values(by='Total Duration', ascending=False).head(10)

top_10_total_traffic = customer_engagement.sort_values(by='Total Traffic (Bytes)', ascending=False).head(10)




print("Top 10 Customers by Session Count:")

print(top_10_session_count[['MSISDN', 'Session Count']])

print("\nTop 10 Customers by Total Duration:")

print(top_10_duration[['MSISDN', 'Total Duration']])

print("\nTop 10 Customers by Total Traffic:")

print(top_10_total_traffic[['MSISDN', 'Total Traffic (Bytes)']])

import matplotlib.pyplot as plt




def plot_top_10(data, metric, title):

    plt.figure(figsize=(10, 6))

    plt.bar(data['MSISDN'], data[metric], color='skyblue')

    plt.xlabel('MSISDN')

    plt.ylabel(metric)

    plt.title(title)

    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()

    plt.show()




plot_top_10(top_10_session_count, 'Session Count', 'Top 10 Customers by Session Count')




plot_top_10(top_10_duration, 'Total Duration', 'Top 10 Customers by Total Duration')




plot_top_10(top_10_total_traffic, 'Total Traffic (Bytes)', 'Top 10 Customers by Total Traffic')

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sqlalchemy import create_engine

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

from sklearn.impute import SimpleImputer




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)




imputer = SimpleImputer(strategy='mean')

imputed_data = imputer.fit_transform(mydata.select_dtypes(include=['float64']))




scaler = StandardScaler()

scaled_data = scaler.fit_transform(imputed_data)




pca = PCA()

principal_components = pca.fit_transform(scaled_data)




pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])




pca_interpretation = [

    "Principal components capture the variance in the data.",

    "Each principal component is a linear combination of the original features.",

    "The first few principal components explain a significant portion of the total variance.",

    "PCA is useful for dimensionality reduction and identifying patterns in high-dimensional data."

]




from pptx import Presentation

from pptx.util import Inches

import os




presentation = Presentation()




def add_slide(title, content):

    slide = presentation.slides.add_slide(presentation.slide_layouts[1])  
    title_box = slide.shapes.title

    content_box = slide.placeholders[1]

    title_box.text = title

    content_box.text = content




variable_description = mydata.dtypes.reset_index()

variable_description.columns = ['Variable', 'Data Type']

add_slide("Variable Description", variable_description.to_string(index=False))




basic_metrics = mydata.describe()

add_slide("Basic Metrics", basic_metrics.to_string())




dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]

add_slide("Non-Graphical Univariate Analysis", dispersion_parameters.to_string())




image_path = 'univariate_analysis_plot.png'

plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.savefig(image_path)

plt.close()

presentation.slides.add_slide(presentation.slide_layouts[5])  
presentation.slides[-1].shapes.add_picture(image_path, Inches(1), Inches(1), width=Inches(8))

os.remove(image_path)  



application_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                     'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                     'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                     'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']



total_data_cols = ['Total DL (Bytes)', 'Total UL (Bytes)']

bivariate_analysis = mydata[application_cols + total_data_cols].corr()

add_slide("Bivariate Analysis", bivariate_analysis.to_string())




mydata['Total_Session_Duration'] = mydata['Dur. (ms).1'] / 1000  
mydata['Decile_Class'] = pd.qcut(mydata['Total_Session_Duration'], q=5, labels=False, duplicates='drop')

total_data_per_decile = mydata.groupby('Decile_Class')[total_data_cols].sum()

add_slide("Variable Transformations", total_data_per_decile.to_string())





correlation_matrix = mydata[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                              'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                              'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                              'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']].corr()

add_slide("Correlation Analysis", correlation_matrix.to_string())




add_slide("PCA Interpretation", "\n".join(pca_interpretation))




presentation.save("telecom_analysis_results.pptx")

import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt







engagement_metrics = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',            
    'Dur. (ms).1': 'sum',            
    'Total UL (Bytes)': 'sum',       
    'Total DL (Bytes)': 'sum'        
}).reset_index()




engagement_metrics.columns = ['MSISDN', 'Session Frequency', 'Total Session Duration', 'Total UL Traffic', 'Total DL Traffic']




top_10_session_frequency = engagement_metrics.sort_values(by='Session Frequency', ascending=False).head(10)

top_10_duration = engagement_metrics.sort_values(by='Total Session Duration', ascending=False).head(10)

top_10_ul_traffic = engagement_metrics.sort_values(by='Total UL Traffic', ascending=False).head(10)

top_10_dl_traffic = engagement_metrics.sort_values(by='Total DL Traffic', ascending=False).head(10)



print("Top 10 Customers by Session Frequency:")

print(top_10_session_frequency)



print("\nTop 10 Customers by Total Session Duration:")

print(top_10_duration)



print("\nTop 10 Customers by Total UL Traffic:")

print(top_10_ul_traffic)



print("\nTop 10 Customers by Total DL Traffic:")

print(top_10_dl_traffic)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics.iloc[:, 1:])




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_metrics['Cluster'] = kmeans.fit_predict(normalized_metrics)




cluster_stats = engagement_metrics.groupby('Cluster').agg({

    'Session Frequency': ['min', 'max', 'mean', 'sum'],

    'Total Session Duration': ['min', 'max', 'mean', 'sum'],

    'Total UL Traffic': ['min', 'max', 'mean', 'sum'],

    'Total DL Traffic': ['min', 'max', 'mean', 'sum']

}).reset_index()



print("\nCluster Statistics:")

print(cluster_stats)




app_traffic = mydata.groupby('MSISDN/Number').agg({

    'Social Media DL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

}).reset_index()




top_10_social_media = app_traffic.sort_values(by='Social Media DL (Bytes) + Social Media UL (Bytes)', ascending=False).head(10)

top_10_youtube = app_traffic.sort_values(by='Youtube DL (Bytes) + Youtube UL (Bytes)', ascending=False).head(10)

top_10_gaming = app_traffic.sort_values(by='Gaming DL (Bytes) + Gaming UL (Bytes)', ascending=False).head(10)



print("\nTop 10 Most Engaged Users (Social Media):")

print(top_10_social_media)



print("\nTop 10 Most Engaged Users (YouTube):")

print(top_10_youtube)



print("\nTop 10 Most Engaged Users (Gaming):")

print(top_10_gaming)




top_3_apps = app_traffic.sum().nlargest(3)



plt.figure(figsize=(10, 6))

top_3_apps.plot(kind='bar', color='skyblue')

plt.title('Top 3 Most Used Applications')

plt.xlabel('Application')

plt.ylabel('Total Traffic (Bytes)')

plt.show()








inertia_values = []

possible_k_values = range(1, 11)



for k in possible_k_values:

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_metrics)

    inertia_values.append(kmeans.inertia_)




plt.figure(figsize=(10, 6))

plt.plot(possible_k_values, inertia_values, marker='o')

plt.title('Elbow Method for Optimal k')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.show()

import pandas as pd

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.impute import SimpleImputer

from kneed import KneeLocator




identifier_column = 'MSISDN/Number'




print(mydata.columns)




engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

engagement_metrics.columns = ['Session Frequency', 'Total Session Duration', 'Total Traffic']




imputer = SimpleImputer(strategy='mean')

engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)




print("Length of mydata:", len(mydata))

print("Length of normalized_metrics:", len(normalized_metrics))




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_clusters = kmeans.fit_predict(normalized_metrics)




print("Length of engagement_clusters:", len(engagement_clusters))




if len(mydata) == len(engagement_clusters):

    mydata['Engagement Cluster'] = engagement_clusters

else:

    print("Length mismatch! Unable to assign 'Engagement Cluster'.")




print("Missing values in mydata:")

print(mydata.isnull().sum())

engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

print("Length of engagement_metrics:", len(engagement_metrics))



engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)

print("Length of engagement_metrics_imputed:", len(engagement_metrics_imputed))



normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)

print("Length of normalized_metrics:", len(normalized_metrics))

print("Duplicate rows based on identifier column:")

print(mydata[mydata.duplicated(subset=identifier_column, keep=False)])
import pandas as pd





application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                        'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                        'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                        'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']




total_data_per_user = mydata.groupby('MSISDN/Number')[application_columns].sum()




total_data_per_user['Total Data Usage'] = total_data_per_user.sum(axis=1)




top_engaged_users = total_data_per_user.nlargest(10, 'Total Data Usage')




print(top_engaged_users)

import matplotlib.pyplot as plt





engagement_metric = 'Total Data Usage'




plt.figure(figsize=(10, 6))

top_engaged_users[engagement_metric].plot(kind='bar', color='skyblue')

plt.title('Top 10 Engaged Users per Application')

plt.xlabel('Bearer Id')

plt.ylabel(engagement_metric)

plt.show()


mydata.hist(figsize=(20, 30))

plt.show()

sns.boxplot(data=mydata)

plt.show()

import streamlit as st
import numpy as np
import pandas as pd
from sklearn import datasets
from sqlalchemy import create_engine

db_params = {
    'dbname': 'week1',
    'user': 'postgres',
    'password': 'habte',
    'host': 'localhost',
    'port': '5432'
}

engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')

sql_query = "SELECT * FROM xdr_data;"

mydata = pd.read_sql_query(sql_query, engine)

st.title('Welcome to Streamlit')

st.write("""
         
  Select the best one here
         
""")
my_dataset = st.sidebar.selectbox("Select Dataset", ("Iris", "Breast Cancer", "Wine Dataset", "Custom", "PostgreSQL Data"))

if my_dataset == "Custom":
    uploaded_file = st.sidebar.file_uploader("Upload a CSV file", type=["csv"])
    if uploaded_file is not None:
        data = pd.read_csv(uploaded_file)
        x = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values

        st.write("Custom Dataset:")
        st.write("First 5 rows of the dataset:")
        st.write(data.head())

    else:
        st.warning("Please upload a CSV file.")
        st.stop()

elif my_dataset == "PostgreSQL Data":
    st.write("PostgreSQL Dataset:")
    st.write("First 5 rows of the dataset:")
    st.write(mydata.head())

else:
    def get_dataset(my_dataset):
        if my_dataset == "Iris":
            data = datasets.load_iris()
        elif my_dataset == "Breast Cancer":
            data = datasets.load_breast_cancer()
        elif my_dataset == "Wine Dataset":
            data = datasets.load_wine()
        else:
            st.warning("Please select a valid dataset.")
            st.stop()

        x = data.data
        y = data.target
        return x, y

    x, y = get_dataset(my_dataset)

    st.write("Shape of dataset", x.shape)
    st.write("Number of classes", len(np.unique(y)))
import numpy as np

import pandas as pd

import psycopg2

import pandas as pd

from sqlalchemy import create_engine

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)

mydata = mydata.dropna()
mydata['Start'] = pd.to_datetime(mydata['Start'], errors='coerce')

mydata['End'] = pd.to_datetime(mydata['End'], errors='coerce')

numeric_columns = mydata.select_dtypes(include='number').columns

mydata[numeric_columns] = mydata[numeric_columns].fillna(mydata[numeric_columns].mean())
mydata.columns

plt.figure(figsize=(12, 8))

sns.histplot(mydata['Avg Bearer TP DL (kbps)'], bins=30, kde=True, color='blue')

plt.title('Distribution of Avg Bearer TP DL (kbps)')

plt.xlabel('Avg Bearer TP DL (kbps)')

plt.ylabel('Frequency')

plt.show()

numeric_columns = mydata.select_dtypes(include=['float64', 'int64']).columns

correlation_matrix = mydata[numeric_columns].corr()




plt.figure(figsize=(15, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Matrix')

plt.show()



plt.figure(figsize=(12, 8))

sns.boxplot(x='Handset Type', y='Avg RTT DL (ms)', data=mydata)

plt.title('Avg RTT DL (ms) by Handset Type')

plt.xlabel('Handset Type')

plt.ylabel('Avg RTT DL (ms)')

plt.show()

plt.figure(figsize=(12, 8))

sns.scatterplot(x='Total UL (Bytes)', y='Total DL (Bytes)', data=mydata, hue='Handset Manufacturer')

plt.title('User Engagement: Total UL vs Total DL')

plt.xlabel('Total UL (Bytes)')

plt.ylabel('Total DL (Bytes)')

plt.legend()

plt.show()


features = mydata[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']]

target = mydata['cluster']  


scaler = StandardScaler()

features_scaled = scaler.fit_transform(features)




X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)




model = LinearRegression()

model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')
import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from sqlalchemy import create_engine



class UserOverviewAnalysis:

    def __init__(self, mydata):

        self.mydata = mydata



    def clean_and_preprocess(self):

        
        cleaned_data = self.mydata.dropna()

        cleaned_data['Start'] = pd.to_datetime(cleaned_data['Start'], errors='coerce')

        cleaned_data['End'] = pd.to_datetime(cleaned_data['End'], errors='coerce')

        numeric_columns = cleaned_data.select_dtypes(include='number').columns

        cleaned_data.loc[:, numeric_columns] = cleaned_data.loc[:, numeric_columns].fillna(cleaned_data[numeric_columns].mean())

        return cleaned_data



    def visualize_results(self):

        sns.set_theme(style="whitegrid")

        plt.figure(figsize=(10, 6))

        
        
        plt.show()



    def aggregate_user_behaviour(self):

        
        user_device_mapping = self.mydata.groupby(['IMSI', 'MSISDN/Number'])['IMEI'].unique()

        aggregated_data = self.mydata.groupby('Bearer Id').sum()  

        return aggregated_data



    def top_10_handsets(self):

        top_10_handsets = self.mydata['IMEI'].value_counts().nlargest(10)

        return top_10_handsets



    def top_3_manufacturers(self):

        cleaned_data = self.clean_and_preprocess()  
        cleaned_data['IMEI'] = cleaned_data['IMEI'].astype(str)

        cleaned_data['Manufacturer'] = cleaned_data['IMEI'].str[:8]

        top_3_manufacturers = cleaned_data['Manufacturer'].value_counts().nlargest(3)

        return top_3_manufacturers



    def top_5_handsets_per_manufacturer(self):

        cleaned_data = self.clean_and_preprocess()  
        cleaned_data['IMEI'] = cleaned_data['IMEI'].astype(str)

        cleaned_data['Manufacturer'] = cleaned_data['IMEI'].str[:8]

        top_manufacturers = cleaned_data['Manufacturer'].value_counts().nlargest(3).index



        combined_top_handsets = pd.DataFrame()



        for manufacturer in top_manufacturers:

            manufacturer_data = cleaned_data[cleaned_data['Manufacturer'] == manufacturer]

            top_handsets = manufacturer_data['IMEI'].value_counts().nlargest(5).reset_index()

            top_handsets.columns = ['IMEI', f'Top 5 Handsets for {manufacturer}']

            combined_top_handsets = pd.concat([combined_top_handsets, top_handsets], axis=1)



        return combined_top_handsets



    def data_analysis(self):

        applications = ['Social Media DL (Bytes)', 'Gaming UL (Bytes)']

        plt.figure(figsize=(12, 8))

        for app in applications:

            sns.histplot(self.mydata[self.mydata[app] > 0][app], label=app, kde=True)



        plt.title('Distribution of Session Durations for Each Application')

        plt.xlabel('Session Duration')

        plt.ylabel('Frequency')

        plt.legend()

        plt.show()




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}



engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')



sql_query = "SELECT * FROM xdr_data;"

mydata = pd.read_sql_query(sql_query, engine)



user_analysis = UserOverviewAnalysis(mydata)

cleaned_data = user_analysis.clean_and_preprocess()

aggregated_data = user_analysis.aggregate_user_behaviour()

top_10_handsets = user_analysis.top_10_handsets()

top_3_manufacturers = user_analysis.top_3_manufacturers()

top_5_handsets_per_manufacturer = user_analysis.top_5_handsets_per_manufacturer()

user_analysis.visualize_results()

user_analysis.data_analysis()
import pandas as pd
from sqlalchemy import create_engine
import matplotlib.pyplot as plt
import seaborn as sns

class UserEngagementAnalysis:
    def __init__(self, db_params):
        self.db_params = db_params
        self.mydata = self.load_data_from_database()

    def load_data_from_database(self):
        engine = create_engine(f'postgresql+psycopg2://{self.db_params["user"]}:{self.db_params["password"]}@{self.db_params["host"]}:{self.db_params["port"]}/{self.db_params["dbname"]}')
        sql_query = "SELECT * FROM xdr_data;"
        mydata = pd.read_sql_query(sql_query, engine)
        return mydata

    def perform_analysis(self):
        """
        Perform user engagement analysis by calling various analysis functions.
        """
        self.aggregate_per_user_session_duration()
        self.aggregate_per_user_data_usage()
        self.aggregate_per_user_social_media_usage()

    def aggregate_per_user_session_duration(self):
        """
        Aggregate per user the total session duration.
        """
        result = self.mydata.groupby('MSISDN/Number')['Dur. (ms)'].sum().reset_index()
        print("Per user total session duration:")
        print(result)

    def aggregate_per_user_data_usage(self):
        """
        Aggregate per user the total download (DL) and upload (UL) data.
        """
        data_columns = ['Total DL (Bytes)', 'Total UL (Bytes)']
        result = self.mydata.groupby('MSISDN/Number')[data_columns].sum().reset_index()
        print("Per user total download and upload data:")
        print(result)
                self.plot_data_usage_distribution(result, 'Data Usage Distribution')

    def aggregate_per_user_social_media_usage(self):
        """
        Aggregate per user the total data volume for Social Media.
        """
        social_media_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)']
        result = self.mydata.groupby('MSISDN/Number')[social_media_columns].sum().reset_index()
        print("Per user total social media usage:")
        print(result)

    def plot_data_usage_distribution(self, data, title):
        """
        Plot the distribution of data usage.
        """
        plt.figure(figsize=(10, 6))
        sns.histplot(data['Total DL (Bytes)'] + data['Total UL (Bytes)'], bins=30, kde=True, color='skyblue')
        plt.title(title)
        plt.xlabel('Total Data Usage (Bytes)')
        plt.ylabel('Frequency')
        plt.show()

db_params = {
    'dbname': 'week1',
    'user': 'postgres',
    'password': 'habte',
    'host': 'localhost',
    'port': '5432'
}
import pandas as pd
import seaborn as sns
import numpy as np
import psycopg2
import matplotlib.pyplot as plt
from sqlalchemy import create_engine

class UserOverviewAnalysis:
    def __init__(self, mydata):
        self.mydata = mydata

    def clean_and_preprocess(self):
                cleaned_data = self.mydata.dropna()
        cleaned_data['Start'] = pd.to_datetime(cleaned_data['Start'], errors='coerce')
        cleaned_data['End'] = pd.to_datetime(cleaned_data['End'], errors='coerce')
        numeric_columns = cleaned_data.select_dtypes(include='number').columns
        cleaned_data.loc[:, numeric_columns] = cleaned_data.loc[:, numeric_columns].fillna(cleaned_data[numeric_columns].mean())
        return cleaned_data

                            
    def aggregate_user_behaviour(self):
                        aggregated_data = self.mydata.drop(columns=['datetime_column']).groupby('Bearer Id').sum()
        return aggregated_data
    
    def sessions_per_user(self):
        sessions_user = mydata['MSISDN/Number'].value_counts()
        return sessions_user
    
    def total_data_per_user(self):
        total_per_user= mydata.groupby('MSISDN/Number').agg({
    'Total DL (Bytes)': 'sum',
    'Total UL (Bytes)': 'sum'
})
        return total_per_user
    
    def top_handsets_by(self):
        top_by= self.mydata['Handset Type'].value_counts().head(10)
        return top_by
    
    def user_device_mapping(self):
                user_mapping = mydata.groupby(['IMSI', 'MSISDN/Number'])['IMEI'].unique()
        return user_mapping
    def top_10_user(self):
                top_users = mydata.nlargest(10, 'Total UL (Bytes)')
        return top_users
    
    def application_columns(self):
                app_columns = ['Social Media DL (Bytes)', 'Gaming UL (Bytes)']
        app_usage = mydata[app_columns].sum()
        return app_usage
    
    def network_tech_distribution(self):
                network_distribution = mydata['Bearer Id'].value_counts()
        return network_distribution

    def top_10_handsets(self):
        top_10_handsets = self.mydata['Handset Type'].value_counts().head(10)
        return top_10_handsets

    def top_3_manufacturers(self):
       top_manufacturer=self.mydata['Handset Manufacturer'].value_counts().head(3)
       return top_manufacturer

    def top_5_handsets_per_manufacturer(self):
        cleaned_data = self.clean_and_preprocess()  
        cleaned_data.loc[:, 'Start'] = pd.to_datetime(cleaned_data['Start'], errors='coerce')
        cleaned_data.loc[:, 'End'] = pd.to_datetime(cleaned_data['End'], errors='coerce')
        cleaned_data.loc[:, 'IMEI'] = cleaned_data['IMEI'].astype(str)
        cleaned_data.loc[:, 'Manufacturer'] = cleaned_data['IMEI'].str[:8]
        top_manufacturers = cleaned_data.loc[:, 'Manufacturer'].value_counts().nlargest(3).index


        combined_top_handsets = pd.DataFrame()

        for manufacturer in top_manufacturers:
            manufacturer_data = cleaned_data[cleaned_data['Manufacturer'] == manufacturer]
            top_handsets = manufacturer_data['IMEI'].value_counts().nlargest(5).reset_index()
            top_handsets.columns = ['IMEI', f'Top 5 Handsets for {manufacturer}']
            combined_top_handsets = pd.concat([combined_top_handsets, top_handsets], axis=1)

        return combined_top_handsets

                                                                                                           
db_params = {
    'dbname': 'week1',
    'user': 'postgres',
    'password': 'habte',
    'host': 'localhost',
    'port': '5432'
}

engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')

sql_query = "SELECT * FROM xdr_data;"
mydata = pd.read_sql_query(sql_query, engine)


user_analysis = UserOverviewAnalysis(mydata)
import streamlit as st
import pandas as pd
from sqlalchemy import create_engine
from analysis_modules.user_overview import UserOverviewAnalysis
from analysis_modules.user_engagement import UserEngagementAnalysis
from analysis_modules.user_experience import UserExperienceAnalysis
from analysis_modules.user_satisfaction import UserSatisfactionAnalysis

db_params = {
    'dbname': 'week1',
    'user': 'postgres',
    'password': 'habte',
    'host': 'localhost',
    'port': '5432'
}

engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')

sql_query = "SELECT * FROM xdr_data;"

mydata = pd.read_sql_query(sql_query, engine)

selected_analysis = st.sidebar.selectbox("Select Analysis", ["User Overview", "User Engagement", "User Experience", "User Satisfaction"])

if selected_analysis == "User Overview":
    analysis = UserOverviewAnalysis(mydata)
elif selected_analysis == "User Engagement":
    analysis = UserEngagementAnalysis(mydata)
elif selected_analysis == "User Experience":
    analysis = UserExperienceAnalysis(mydata)
elif selected_analysis == "User Satisfaction":
    analysis = UserSatisfactionAnalysis(mydata)

st.title(f"{selected_analysis} Analysis")
result = analysis.perform_analysis()
st.write(result)
import numpy as np

import pandas as pd

import psycopg2
import matplotlib.pyplot as plt

import seaborn as sns
from sqlalchemy import create_engine




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)

print(mydata.info())

print(mydata.shape)

mydata = mydata.dropna()
mydata['Start'] = pd.to_datetime(mydata['Start'], errors='coerce')

mydata['End'] = pd.to_datetime(mydata['End'], errors='coerce')

numeric_columns = mydata.select_dtypes(include='number').columns

mydata[numeric_columns] = mydata[numeric_columns].fillna(mydata[numeric_columns].mean())

print(mydata.head(5))
print(mydata.describe())

user_device_mapping = mydata.groupby(['IMSI', 'MSISDN/Number'])['IMEI'].unique()




print(user_device_mapping)
import matplotlib.pyplot as plt




plt.hist(mydata['Dur. (ms)'])

plt.xlabel('Session Duration (ms)')

plt.ylabel('Frequency')

plt.show()

top_users = mydata.nlargest(10, 'Total UL (Bytes)')

print(top_users)


app_columns = ['Social Media DL (Bytes)', 'Gaming UL (Bytes)']

app_usage = mydata[app_columns].sum()

print(app_usage)

network_tech_distribution = mydata['Bearer Id'].value_counts()

print(network_tech_distribution)

location_analysis = mydata.groupby('Last Location Name').agg({'Dur. (ms)': 'mean', 'Total DL (Bytes)': 'sum'})

print(location_analysis)

mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['Hour'] = mydata['Start'].dt.hour

time_analysis = mydata.groupby('Hour').agg({'Dur. (ms)': 'mean', 'Total DL (Bytes)': 'sum'})

print(time_analysis)

top_handsets = mydata['IMEI'].value_counts().nlargest(10)




print(top_handsets)

mydata['IMEI'] = mydata['IMEI'].astype(str)




mydata['Manufacturer'] = mydata['IMEI'].str[:8]




top_manufacturers = mydata['Manufacturer'].value_counts().nlargest(3)




print(top_manufacturers)
mydata['IMEI'] = mydata['IMEI'].astype(str)


mydata['Manufacturer'] = mydata['IMEI'].str[:8]


top_manufacturers = mydata['Manufacturer'].value_counts().nlargest(3).index


for manufacturer in top_manufacturers:

    
    manufacturer_data = mydata[mydata['Manufacturer'] == manufacturer]

    
    top_handsets = manufacturer_data['IMEI'].value_counts().nlargest(5)



    
    print(f"\nTop 5 handsets for {manufacturer}:")

    print(top_handsets)


sessions_per_user = mydata['MSISDN/Number'].value_counts()




print(sessions_per_user)
import matplotlib.pyplot as plt




plt.hist(sessions_per_user, bins=20, edgecolor='black')

plt.xlabel('Number of Sessions')

plt.ylabel('Number of Users')

plt.title('Distribution of xDR Sessions per User')

plt.show()

session_duration_per_user = mydata.groupby('MSISDN/Number')['Dur. (ms)'].sum()




print(session_duration_per_user)

plt.hist(session_duration_per_user, bins=20, edgecolor='black')

plt.xlabel('Total Session Duration (ms)')

plt.ylabel('Number of Users')

plt.title('Distribution of Session Durations per User')

plt.show()

total_data_per_user = mydata.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum',

    'Total UL (Bytes)': 'sum'

})




print(total_data_per_user)
import matplotlib.pyplot as plt




plt.scatter(total_data_per_user['Total DL (Bytes)'], total_data_per_user['Total UL (Bytes)'])

plt.xlabel('Total Download (Bytes)')

plt.ylabel('Total Upload (Bytes)')

plt.title('Total Download vs Total Upload per User')

plt.show()



mydata['Total_Social_Media_Usage'] = mydata['Social Media DL (Bytes)'] + mydata['Social Media UL (Bytes)']

mydata['Total_Google_Usage'] = mydata['Google DL (Bytes)'] + mydata['Google UL (Bytes)']





aggregated_data = mydata.groupby('MSISDN/Number').agg({

    'Total_Social_Media_Usage': 'sum',

    'Total_Google_Usage': 'sum',

    
})




print(aggregated_data)

applications_columns = [

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)'

]




for app_column in applications_columns:

    app_name = app_column.split(' ')[0]  
    total_column = f'Total_{app_name}_Usage'



    
    mydata[total_column] = mydata[app_column]




aggregated_data = mydata.groupby(['MSISDN/Number', 'Start']).agg({

    'Total_Social_Media_Usage': 'sum',

    'Total_Google_Usage': 'sum',

    'Total_Email_Usage': 'sum',

    'Total_Youtube_Usage': 'sum',

    'Total_Netflix_Usage': 'sum',

    'Total_Gaming_Usage': 'sum',

})




print(aggregated_data)

sessions_per_user = mydata['MSISDN/Number'].value_counts()

sessions_per_user.describe()
mydata['Session_Duration'] = mydata['Dur. (ms)'] / 1000  
session_duration_stats = mydata['Session_Duration'].describe()
session_duration_stats
applications_columns = ['Total_Social_Media_Usage', 'Total_Google_Usage', 'Total_Email_Usage', 'Total_Youtube_Usage', 'Total_Netflix_Usage', 'Total_Gaming_Usage']

total_data_per_app = mydata[applications_columns].sum()
total_data_per_app 
avg_data_per_app = mydata.groupby('MSISDN/Number')[applications_columns].mean()
avg_data_per_app
avg_rtt_dl = mydata['Avg RTT DL (ms)'].mean()

avg_rtt_ul = mydata['Avg RTT UL (ms)'].mean()
avg_rtt_dl
avg_rtt_ul
tcp_retransmission_dl = mydata['TCP DL Retrans. Vol (Bytes)'].sum()

tcp_retransmission_ul = mydata['TCP UL Retrans. Vol (Bytes)'].sum()
tcp_retransmission_dl
tcp_retransmission_ul
top_handsets = mydata['Handset Type'].value_counts().head(10)
top_handsets
top_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3)
top_manufacturers
busiest_hours = mydata['Hour'].value_counts().sort_index()
busiest_hours 
import matplotlib.pyplot as plt

import seaborn as sns




sns.set(style="whitegrid")



plt.figure(figsize=(12, 6))

sns.histplot(mydata['Session_Duration'], bins=30, kde=True, color='skyblue')

plt.title('Distribution of Session Duration')

plt.xlabel('Session Duration (seconds)')

plt.ylabel('Frequency')

plt.show()

applications_columns = ['Total_Social_Media_Usage', 'Total_Google_Usage', 'Total_Email_Usage', 'Total_Youtube_Usage', 'Total_Netflix_Usage', 'Total_Gaming_Usage']

plt.figure(figsize=(12, 8))

sns.boxplot(data=mydata[applications_columns])

plt.title('Box Plot of Total Data Volume per Application')

plt.xlabel('Application')

plt.ylabel('Total Data Volume (Bytes)')

plt.xticks(rotation=45, ha='right')

plt.show()


busiest_hours = mydata['Hour'].value_counts().sort_index()

plt.figure(figsize=(12, 6))

sns.lineplot(x=busiest_hours.index, y=busiest_hours.values, marker='o', color='orange')

plt.title('Busiest Hours of Network Usage')

plt.xlabel('Hour of the Day')

plt.ylabel('Number of Sessions')

plt.show()
import matplotlib.pyplot as plt

import seaborn as sns




numeric_variables = mydata.select_dtypes(include='number')


for col in numeric_variables.columns:

    plt.figure(figsize=(8, 5))

    sns.histplot(mydata[col], bins=20, kde=True, color='skyblue')

    plt.title(f'Histogram of {col}')

    plt.xlabel(col)

    plt.ylabel('Frequency')

    plt.show()

for col in numeric_variables.columns:

    plt.figure(figsize=(8, 5))

    sns.boxplot(x=mydata[col], color='lightcoral')

    plt.title(f'Box Plot of {col}')

    plt.xlabel(col)

    plt.show()

import matplotlib.pyplot as plt

import seaborn as sns




applications_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)']




for app_col in applications_columns:

    plt.figure(figsize=(8, 5))

    sns.scatterplot(x=mydata[app_col], y=mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)'], alpha=0.7)

    plt.title(f'Scatter Plot: {app_col} vs Total Data Usage')

    plt.xlabel(app_col)

    plt.ylabel('Total DL+UL Data Usage (Bytes)')

    plt.show()

correlation_matrix = mydata[applications_columns + ['Total DL (Bytes)', 'Total UL (Bytes)']].corr()




print("Correlation Matrix:")

print(correlation_matrix)

import statsmodels.api as sm




X = mydata['Social Media DL (Bytes)'] + mydata['Social Media UL (Bytes)']

X = sm.add_constant(X)

y = mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)']



model = sm.OLS(y, X).fit()




print(model.summary())

import pandas as pd

import numpy as np








mydata['Total_Duration'] = mydata['Dur. (ms)']




mydata['Duration_Decile'] = pd.qcut(mydata['Total_Duration'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False, precision=0, duplicates='drop')




mydata['Total_Data'] = mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)']




data_per_decile = mydata.groupby('Duration_Decile')['Total_Data'].sum().reset_index()




print(data_per_decile)








selected_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                    'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']




app_data = mydata[selected_columns]




correlation_matrix = app_data.corr()




print(correlation_matrix)

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

selected_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                    'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']


app_data = mydata[selected_columns]

scaler = StandardScaler()

app_data_standardized = scaler.fit_transform(app_data)

pca = PCA()

pca_result = pca.fit_transform(app_data_standardized)

explained_variance_ratio = pca.explained_variance_ratio_




print("Explained Variance Ratio:", explained_variance_ratio)

top_components = pca_result[:, :2]




print("Top 2 Principal Components:")

print(top_components)

top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3).index




mydata_top_3_manufacturers = mydata[mydata['Handset Manufacturer'].isin(top_3_manufacturers)]




top_5_handsets_per_manufacturer = (

    mydata_top_3_manufacturers.groupby('Handset Manufacturer')['Handset Type']

    .value_counts()

    .groupby(level=0, group_keys=False)

    .nlargest(5)

    .reset_index(name='Count')

)




print("Top 5 Handsets per Top 3 Handset Manufacturers:")

print(top_5_handsets_per_manufacturer)


top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3)




print("Top 3 Handset Manufacturers:")

print(top_3_manufacturers)


data = {

    'Handset Manufacturer': ['Apple', 'Samsung', 'Huawei'],

    'Count': [59565, 40839, 34423]

}



df = pd.DataFrame(data)




plt.figure(figsize=(8, 8))

plt.pie(df['Count'], labels=df['Handset Manufacturer'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral', 'lightgreen'])

plt.title('Distribution of Top 3 Handset Manufacturers')

plt.show()


plt.figure(figsize=(16, 8))

sns.barplot(x='Handset Type', y='Count', hue='Handset Manufacturer', data=top_5_handsets_per_manufacturer, palette='viridis')

plt.xticks(rotation=45, ha='right')

plt.title('Top 5 Handsets per Top 3 Handset Manufacturers')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.tight_layout()

plt.legend(title='Handset Manufacturer')

plt.show()

data = {

    'Handset Type': ['Apple iPhone 6S (A1688)', 'Apple iPhone 6 (A1586)', 'Apple iPhone 7 (A1778)', 'Apple iPhone Se (A1723)', 'Apple iPhone 8 (A1905)'],

    'Count': [9419, 9023, 6326, 5187, 4993]

}



df = pd.DataFrame(data)




manufacturers = ['Apple', 'Samsung', 'Huawei']



plt.figure(figsize=(20, 5))



for i, manufacturer in enumerate(manufacturers, 1):

    plt.subplot(1, 3, i)

    

    
    manufacturer_data = df[df['Handset Type'].str.contains(manufacturer)]

    

    
    plt.pie(manufacturer_data['Count'], labels=manufacturer_data['Handset Type'], autopct='%1.1f%%', startangle=140)

    

    
    plt.title(f'Distribution of Users for {manufacturer}')




plt.tight_layout()




plt.show()


user_xdr_sessions = mydata.groupby('MSISDN/Number')['Bearer Id'].count().reset_index(name='Number_of_xDR_sessions')




print("Aggregated Information - Number of xDR Sessions per User:")

print(user_xdr_sessions)


mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['End'] = pd.to_datetime(mydata['End'])

mydata['Session_Duration'] = (mydata['End'] - mydata['Start']).dt.total_seconds()

user_session_duration = mydata.groupby('MSISDN/Number')['Session_Duration'].sum().reset_index(name='Total_Session_Duration')

print("Aggregated Session Duration per User:")

print(user_session_duration)

user_data_volume = mydata.groupby('MSISDN/Number').agg(

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Download and Upload Data per User:")

print(user_data_volume)


user_app_data_volume = mydata.groupby('MSISDN/Number').agg(

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Data Volume per User for Each Application:")

print(user_app_data_volume)


user_aggregated_data = mydata.groupby('MSISDN/Number').agg(

    Number_of_xDR_sessions=('Bearer Id', 'count'),

    Session_Duration=('Session_Duration', 'sum'),

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum'),

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()


plt.figure(figsize=(15, 10))

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Number_of_xDR_sessions'], label='Number of xDR Sessions')

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Session_Duration'], label='Session Duration', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_DL_data'], label='Total DL Data', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_UL_data'], label='Total UL Data', alpha=0.7)

plt.legend()

plt.xlabel('MSISDN/Number')

plt.ylabel('Aggregated Values')

plt.title('Aggregated Information per User')

plt.xticks(rotation=45, ha='right')

plt.tight_layout()

plt.show()

summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.index, y=top_handsets.values)

plt.xticks(rotation=45, ha='right')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.show()

plt.figure(figsize=(10, 6))

sns.histplot(mydata['Session_Duration'], bins=30, kde=True)

plt.title('Distribution of Session Durations')

plt.xlabel('Session Duration (seconds)')

plt.ylabel('Frequency')

plt.show()

plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()

mixed_data_columns = mydata.select_dtypes(include='object').columns

print(f'Columns with mixed data types: {mixed_data_columns}')


plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()


summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.values, y=top_handsets.index, orient='h')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.show()

basic_metrics = mydata.describe()


dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]



plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.show()

import matplotlib.pyplot as plt

import seaborn as sns





application = 'Social Media'




subset_data = mydata[[f'{application} DL (Bytes)', f'{application} UL (Bytes)', 'Total DL (Bytes)', 'Total UL (Bytes)']]




subset_data['Total DL+UL Data'] = subset_data['Total DL (Bytes)'] + subset_data['Total UL (Bytes)']




plt.figure(figsize=(10, 6))

sns.scatterplot(x=f'{application} DL (Bytes)', y='Total DL+UL Data', data=subset_data)

plt.title(f'Relationship between {application} and Total DL+UL Data')

plt.xlabel(f'{application} DL (Bytes)')

plt.ylabel('Total DL+UL Data')

plt.show()


customer_engagement = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',

    'Dur. (ms)': 'sum',

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()




customer_engagement.columns = ['MSISDN', 'Session Count', 'Total Duration', 'Total UL (Bytes)', 'Total DL (Bytes)']




customer_engagement['Total Traffic (Bytes)'] = customer_engagement['Total UL (Bytes)'] + customer_engagement['Total DL (Bytes)']




top_10_session_count = customer_engagement.sort_values(by='Session Count', ascending=False).head(10)

top_10_duration = customer_engagement.sort_values(by='Total Duration', ascending=False).head(10)

top_10_total_traffic = customer_engagement.sort_values(by='Total Traffic (Bytes)', ascending=False).head(10)




print("Top 10 Customers by Session Count:")

print(top_10_session_count[['MSISDN', 'Session Count']])

print("\nTop 10 Customers by Total Duration:")

print(top_10_duration[['MSISDN', 'Total Duration']])

print("\nTop 10 Customers by Total Traffic:")

print(top_10_total_traffic[['MSISDN', 'Total Traffic (Bytes)']])

import matplotlib.pyplot as plt




def plot_top_10(data, metric, title):

    plt.figure(figsize=(10, 6))

    plt.bar(data['MSISDN'], data[metric], color='skyblue')

    plt.xlabel('MSISDN')

    plt.ylabel(metric)

    plt.title(title)

    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()

    plt.show()




plot_top_10(top_10_session_count, 'Session Count', 'Top 10 Customers by Session Count')




plot_top_10(top_10_duration, 'Total Duration', 'Top 10 Customers by Total Duration')




plot_top_10(top_10_total_traffic, 'Total Traffic (Bytes)', 'Top 10 Customers by Total Traffic')

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sqlalchemy import create_engine

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

from sklearn.impute import SimpleImputer




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)




imputer = SimpleImputer(strategy='mean')

imputed_data = imputer.fit_transform(mydata.select_dtypes(include=['float64']))




scaler = StandardScaler()

scaled_data = scaler.fit_transform(imputed_data)




pca = PCA()

principal_components = pca.fit_transform(scaled_data)




pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])




pca_interpretation = [

    "Principal components capture the variance in the data.",

    "Each principal component is a linear combination of the original features.",

    "The first few principal components explain a significant portion of the total variance.",

    "PCA is useful for dimensionality reduction and identifying patterns in high-dimensional data."

]




from pptx import Presentation

from pptx.util import Inches

import os




presentation = Presentation()




def add_slide(title, content):

    slide = presentation.slides.add_slide(presentation.slide_layouts[1])  
    title_box = slide.shapes.title

    content_box = slide.placeholders[1]

    title_box.text = title

    content_box.text = content




variable_description = mydata.dtypes.reset_index()

variable_description.columns = ['Variable', 'Data Type']

add_slide("Variable Description", variable_description.to_string(index=False))




basic_metrics = mydata.describe()

add_slide("Basic Metrics", basic_metrics.to_string())




dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]

add_slide("Non-Graphical Univariate Analysis", dispersion_parameters.to_string())




image_path = 'univariate_analysis_plot.png'

plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.savefig(image_path)

plt.close()

presentation.slides.add_slide(presentation.slide_layouts[5])  
presentation.slides[-1].shapes.add_picture(image_path, Inches(1), Inches(1), width=Inches(8))

os.remove(image_path)  



application_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                     'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                     'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                     'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']



total_data_cols = ['Total DL (Bytes)', 'Total UL (Bytes)']

bivariate_analysis = mydata[application_cols + total_data_cols].corr()

add_slide("Bivariate Analysis", bivariate_analysis.to_string())




mydata['Total_Session_Duration'] = mydata['Dur. (ms).1'] / 1000  
mydata['Decile_Class'] = pd.qcut(mydata['Total_Session_Duration'], q=5, labels=False, duplicates='drop')

total_data_per_decile = mydata.groupby('Decile_Class')[total_data_cols].sum()

add_slide("Variable Transformations", total_data_per_decile.to_string())





correlation_matrix = mydata[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                              'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                              'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                              'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']].corr()

add_slide("Correlation Analysis", correlation_matrix.to_string())




add_slide("PCA Interpretation", "\n".join(pca_interpretation))




presentation.save("telecom_analysis_results.pptx")

import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt







engagement_metrics = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',            
    'Dur. (ms).1': 'sum',            
    'Total UL (Bytes)': 'sum',       
    'Total DL (Bytes)': 'sum'        
}).reset_index()




engagement_metrics.columns = ['MSISDN', 'Session Frequency', 'Total Session Duration', 'Total UL Traffic', 'Total DL Traffic']




top_10_session_frequency = engagement_metrics.sort_values(by='Session Frequency', ascending=False).head(10)

top_10_duration = engagement_metrics.sort_values(by='Total Session Duration', ascending=False).head(10)

top_10_ul_traffic = engagement_metrics.sort_values(by='Total UL Traffic', ascending=False).head(10)

top_10_dl_traffic = engagement_metrics.sort_values(by='Total DL Traffic', ascending=False).head(10)



print("Top 10 Customers by Session Frequency:")

print(top_10_session_frequency)



print("\nTop 10 Customers by Total Session Duration:")

print(top_10_duration)



print("\nTop 10 Customers by Total UL Traffic:")

print(top_10_ul_traffic)



print("\nTop 10 Customers by Total DL Traffic:")

print(top_10_dl_traffic)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics.iloc[:, 1:])




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_metrics['Cluster'] = kmeans.fit_predict(normalized_metrics)




cluster_stats = engagement_metrics.groupby('Cluster').agg({

    'Session Frequency': ['min', 'max', 'mean', 'sum'],

    'Total Session Duration': ['min', 'max', 'mean', 'sum'],

    'Total UL Traffic': ['min', 'max', 'mean', 'sum'],

    'Total DL Traffic': ['min', 'max', 'mean', 'sum']

}).reset_index()



print("\nCluster Statistics:")

print(cluster_stats)




app_traffic = mydata.groupby('MSISDN/Number').agg({

    'Social Media DL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

}).reset_index()




top_10_social_media = app_traffic.sort_values(by='Social Media DL (Bytes) + Social Media UL (Bytes)', ascending=False).head(10)

top_10_youtube = app_traffic.sort_values(by='Youtube DL (Bytes) + Youtube UL (Bytes)', ascending=False).head(10)

top_10_gaming = app_traffic.sort_values(by='Gaming DL (Bytes) + Gaming UL (Bytes)', ascending=False).head(10)



print("\nTop 10 Most Engaged Users (Social Media):")

print(top_10_social_media)



print("\nTop 10 Most Engaged Users (YouTube):")

print(top_10_youtube)



print("\nTop 10 Most Engaged Users (Gaming):")

print(top_10_gaming)




top_3_apps = app_traffic.sum().nlargest(3)



plt.figure(figsize=(10, 6))

top_3_apps.plot(kind='bar', color='skyblue')

plt.title('Top 3 Most Used Applications')

plt.xlabel('Application')

plt.ylabel('Total Traffic (Bytes)')

plt.show()








inertia_values = []

possible_k_values = range(1, 11)



for k in possible_k_values:

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_metrics)

    inertia_values.append(kmeans.inertia_)




plt.figure(figsize=(10, 6))

plt.plot(possible_k_values, inertia_values, marker='o')

plt.title('Elbow Method for Optimal k')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.show()

import pandas as pd

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.impute import SimpleImputer

from kneed import KneeLocator




identifier_column = 'MSISDN/Number'




print(mydata.columns)




engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

engagement_metrics.columns = ['Session Frequency', 'Total Session Duration', 'Total Traffic']




imputer = SimpleImputer(strategy='mean')

engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)




print("Length of mydata:", len(mydata))

print("Length of normalized_metrics:", len(normalized_metrics))




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_clusters = kmeans.fit_predict(normalized_metrics)




print("Length of engagement_clusters:", len(engagement_clusters))




if len(mydata) == len(engagement_clusters):

    mydata['Engagement Cluster'] = engagement_clusters

else:

    print("Length mismatch! Unable to assign 'Engagement Cluster'.")




print("Missing values in mydata:")

print(mydata.isnull().sum())

engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

print("Length of engagement_metrics:", len(engagement_metrics))



engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)

print("Length of engagement_metrics_imputed:", len(engagement_metrics_imputed))



normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)

print("Length of normalized_metrics:", len(normalized_metrics))

print("Duplicate rows based on identifier column:")

print(mydata[mydata.duplicated(subset=identifier_column, keep=False)])
import pandas as pd





application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                        'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                        'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                        'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']




total_data_per_user = mydata.groupby('MSISDN/Number')[application_columns].sum()




total_data_per_user['Total Data Usage'] = total_data_per_user.sum(axis=1)




top_engaged_users = total_data_per_user.nlargest(10, 'Total Data Usage')




print(top_engaged_users)

import matplotlib.pyplot as plt





engagement_metric = 'Total Data Usage'




plt.figure(figsize=(10, 6))

top_engaged_users[engagement_metric].plot(kind='bar', color='skyblue')

plt.title('Top 10 Engaged Users per Application')

plt.xlabel('Bearer Id')

plt.ylabel(engagement_metric)

plt.show()


mydata.hist(figsize=(20, 30))

plt.show()

sns.boxplot(data=mydata)

plt.show()
import numpy as np

import pandas as pd

import psycopg2
import matplotlib.pyplot as plt

import seaborn as sns
from sqlalchemy import create_engine




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)

print(mydata.info())

print(mydata.shape)

mydata = mydata.dropna()
mydata['Start'] = pd.to_datetime(mydata['Start'], errors='coerce')

mydata['End'] = pd.to_datetime(mydata['End'], errors='coerce')

numeric_columns = mydata.select_dtypes(include='number').columns

mydata[numeric_columns] = mydata[numeric_columns].fillna(mydata[numeric_columns].mean())

print(mydata.head(5))
print(mydata.describe())

user_device_mapping = mydata.groupby(['IMSI', 'MSISDN/Number'])['IMEI'].unique()




print(user_device_mapping)
import matplotlib.pyplot as plt




plt.hist(mydata['Dur. (ms)'])

plt.xlabel('Session Duration (ms)')

plt.ylabel('Frequency')

plt.show()

top_users = mydata.nlargest(10, 'Total UL (Bytes)')

print(top_users)

all_users_sorted = mydata.sort_values(by='Total UL (Bytes)', ascending=False)

print(all_users_sorted)


app_columns = ['Social Media DL (Bytes)', 'Gaming UL (Bytes)']

app_usage = mydata[app_columns].sum()

print(app_usage)

network_tech_distribution = mydata['Bearer Id'].value_counts()

print(network_tech_distribution)

location_analysis = mydata.groupby('Last Location Name').agg({'Dur. (ms)': 'mean', 'Total DL (Bytes)': 'sum'})

print(location_analysis)

mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['Hour'] = mydata['Start'].dt.hour

time_analysis = mydata.groupby('Hour').agg({'Dur. (ms)': 'mean', 'Total DL (Bytes)': 'sum'})

print(time_analysis)

top_handsets = mydata['IMEI'].value_counts().nlargest(10)




print(top_handsets)

mydata['IMEI'] = mydata['IMEI'].astype(str)




mydata['Manufacturer'] = mydata['IMEI'].str[:8]




top_manufacturers = mydata['Manufacturer'].value_counts().nlargest(3)




print(top_manufacturers)
mydata['IMEI'] = mydata['IMEI'].astype(str)


mydata['Manufacturer'] = mydata['IMEI'].str[:8]


top_manufacturers = mydata['Manufacturer'].value_counts().nlargest(3).index


for manufacturer in top_manufacturers:

    
    manufacturer_data = mydata[mydata['Manufacturer'] == manufacturer]

    
    top_handsets = manufacturer_data['IMEI'].value_counts().nlargest(5)



    
    print(f"\nTop 5 handsets for {manufacturer}:")

    print(top_handsets)


sessions_per_user = mydata['MSISDN/Number'].value_counts()




print(sessions_per_user)
import matplotlib.pyplot as plt




plt.hist(sessions_per_user, bins=20, edgecolor='black')

plt.xlabel('Number of Sessions')

plt.ylabel('Number of Users')

plt.title('Distribution of xDR Sessions per User')

plt.show()

session_duration_per_user = mydata.groupby('MSISDN/Number')['Dur. (ms)'].sum()




print(session_duration_per_user)

plt.hist(session_duration_per_user, bins=20, edgecolor='black')

plt.xlabel('Total Session Duration (ms)')

plt.ylabel('Number of Users')

plt.title('Distribution of Session Durations per User')

plt.show()

total_data_per_user = mydata.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum',

    'Total UL (Bytes)': 'sum'

})




print(total_data_per_user)
import matplotlib.pyplot as plt




plt.scatter(total_data_per_user['Total DL (Bytes)'], total_data_per_user['Total UL (Bytes)'])

plt.xlabel('Total Download (Bytes)')

plt.ylabel('Total Upload (Bytes)')

plt.title('Total Download vs Total Upload per User')

plt.show()



mydata['Total_Social_Media_Usage'] = mydata['Social Media DL (Bytes)'] + mydata['Social Media UL (Bytes)']

mydata['Total_Google_Usage'] = mydata['Google DL (Bytes)'] + mydata['Google UL (Bytes)']





aggregated_data = mydata.groupby('MSISDN/Number').agg({

    'Total_Social_Media_Usage': 'sum',

    'Total_Google_Usage': 'sum',

    
})




print(aggregated_data)

applications_columns = [

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)'

]




for app_column in applications_columns:

    app_name = app_column.split(' ')[0]  
    total_column = f'Total_{app_name}_Usage'



    
    mydata[total_column] = mydata[app_column]




aggregated_data = mydata.groupby(['MSISDN/Number', 'Start']).agg({

    'Total_Social_Media_Usage': 'sum',

    'Total_Google_Usage': 'sum',

    'Total_Email_Usage': 'sum',

    'Total_Youtube_Usage': 'sum',

    'Total_Netflix_Usage': 'sum',

    'Total_Gaming_Usage': 'sum',

})




print(aggregated_data)

sessions_per_user = mydata['MSISDN/Number'].value_counts()

sessions_per_user.describe()
mydata['Session_Duration'] = mydata['Dur. (ms)'] / 1000  
session_duration_stats = mydata['Session_Duration'].describe()
session_duration_stats
applications_columns = ['Total_Social_Media_Usage', 'Total_Google_Usage', 'Total_Email_Usage', 'Total_Youtube_Usage', 'Total_Netflix_Usage', 'Total_Gaming_Usage']

total_data_per_app = mydata[applications_columns].sum()
total_data_per_app 
avg_data_per_app = mydata.groupby('MSISDN/Number')[applications_columns].mean()
avg_data_per_app
avg_rtt_dl = mydata['Avg RTT DL (ms)'].mean()

avg_rtt_ul = mydata['Avg RTT UL (ms)'].mean()
avg_rtt_dl
avg_rtt_ul
tcp_retransmission_dl = mydata['TCP DL Retrans. Vol (Bytes)'].sum()

tcp_retransmission_ul = mydata['TCP UL Retrans. Vol (Bytes)'].sum()
tcp_retransmission_dl
tcp_retransmission_ul
top_handsets = mydata['Handset Type'].value_counts().head(10)
top_handsets
top_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3)
top_manufacturers
busiest_hours = mydata['Hour'].value_counts().sort_index()
busiest_hours 
import matplotlib.pyplot as plt

import seaborn as sns




sns.set(style="whitegrid")



plt.figure(figsize=(12, 6))

sns.histplot(mydata['Session_Duration'], bins=30, kde=True, color='skyblue')

plt.title('Distribution of Session Duration')

plt.xlabel('Session Duration (seconds)')

plt.ylabel('Frequency')

plt.show()

applications_columns = ['Total_Social_Media_Usage', 'Total_Google_Usage', 'Total_Email_Usage', 'Total_Youtube_Usage', 'Total_Netflix_Usage', 'Total_Gaming_Usage']

plt.figure(figsize=(12, 8))

sns.boxplot(data=mydata[applications_columns])

plt.title('Box Plot of Total Data Volume per Application')

plt.xlabel('Application')

plt.ylabel('Total Data Volume (Bytes)')

plt.xticks(rotation=45, ha='right')

plt.show()


busiest_hours = mydata['Hour'].value_counts().sort_index()

plt.figure(figsize=(12, 6))

sns.lineplot(x=busiest_hours.index, y=busiest_hours.values, marker='o', color='orange')

plt.title('Busiest Hours of Network Usage')

plt.xlabel('Hour of the Day')

plt.ylabel('Number of Sessions')

plt.show()
import matplotlib.pyplot as plt

import seaborn as sns




numeric_variables = mydata.select_dtypes(include='number')


for col in numeric_variables.columns:

    plt.figure(figsize=(8, 5))

    sns.histplot(mydata[col], bins=20, kde=True, color='skyblue')

    plt.title(f'Histogram of {col}')

    plt.xlabel(col)

    plt.ylabel('Frequency')

    plt.show()

for col in numeric_variables.columns:

    plt.figure(figsize=(8, 5))

    sns.boxplot(x=mydata[col], color='lightcoral')

    plt.title(f'Box Plot of {col}')

    plt.xlabel(col)

    plt.show()

import matplotlib.pyplot as plt

import seaborn as sns




applications_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)']




for app_col in applications_columns:

    plt.figure(figsize=(8, 5))

    sns.scatterplot(x=mydata[app_col], y=mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)'], alpha=0.7)

    plt.title(f'Scatter Plot: {app_col} vs Total Data Usage')

    plt.xlabel(app_col)

    plt.ylabel('Total DL+UL Data Usage (Bytes)')

    plt.show()

correlation_matrix = mydata[applications_columns + ['Total DL (Bytes)', 'Total UL (Bytes)']].corr()




print("Correlation Matrix:")

print(correlation_matrix)

import statsmodels.api as sm




X = mydata['Social Media DL (Bytes)'] + mydata['Social Media UL (Bytes)']

X = sm.add_constant(X)

y = mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)']



model = sm.OLS(y, X).fit()




print(model.summary())

import pandas as pd

import numpy as np








mydata['Total_Duration'] = mydata['Dur. (ms)']




mydata['Duration_Decile'] = pd.qcut(mydata['Total_Duration'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False, precision=0, duplicates='drop')




mydata['Total_Data'] = mydata['Total DL (Bytes)'] + mydata['Total UL (Bytes)']




data_per_decile = mydata.groupby('Duration_Decile')['Total_Data'].sum().reset_index()




print(data_per_decile)








selected_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                    'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']




app_data = mydata[selected_columns]




correlation_matrix = app_data.corr()




print(correlation_matrix)

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

selected_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                    'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']


app_data = mydata[selected_columns]

scaler = StandardScaler()

app_data_standardized = scaler.fit_transform(app_data)

pca = PCA()

pca_result = pca.fit_transform(app_data_standardized)

explained_variance_ratio = pca.explained_variance_ratio_




print("Explained Variance Ratio:", explained_variance_ratio)

top_components = pca_result[:, :2]




print("Top 2 Principal Components:")

print(top_components)

top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3).index




mydata_top_3_manufacturers = mydata[mydata['Handset Manufacturer'].isin(top_3_manufacturers)]




top_5_handsets_per_manufacturer = (

    mydata_top_3_manufacturers.groupby('Handset Manufacturer')['Handset Type']

    .value_counts()

    .groupby(level=0, group_keys=False)

    .nlargest(5)

    .reset_index(name='Count')

)




print("Top 5 Handsets per Top 3 Handset Manufacturers:")

print(top_5_handsets_per_manufacturer)


top_3_manufacturers = mydata['Handset Manufacturer'].value_counts().head(3)




print("Top 3 Handset Manufacturers:")

print(top_3_manufacturers)


data = {

    'Handset Manufacturer': ['Apple', 'Samsung', 'Huawei'],

    'Count': [59565, 40839, 34423]

}



df = pd.DataFrame(data)




plt.figure(figsize=(8, 8))

plt.pie(df['Count'], labels=df['Handset Manufacturer'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral', 'lightgreen'])

plt.title('Distribution of Top 3 Handset Manufacturers')

plt.show()


plt.figure(figsize=(16, 8))

sns.barplot(x='Handset Type', y='Count', hue='Handset Manufacturer', data=top_5_handsets_per_manufacturer, palette='viridis')

plt.xticks(rotation=45, ha='right')

plt.title('Top 5 Handsets per Top 3 Handset Manufacturers')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.tight_layout()

plt.legend(title='Handset Manufacturer')

plt.show()

data = {

    'Handset Type': ['Apple iPhone 6S (A1688)', 'Apple iPhone 6 (A1586)', 'Apple iPhone 7 (A1778)', 'Apple iPhone Se (A1723)', 'Apple iPhone 8 (A1905)'],

    'Count': [9419, 9023, 6326, 5187, 4993]

}



df = pd.DataFrame(data)




manufacturers = ['Apple', 'Samsung', 'Huawei']



plt.figure(figsize=(20, 5))



for i, manufacturer in enumerate(manufacturers, 1):

    plt.subplot(1, 3, i)

    

    
    manufacturer_data = df[df['Handset Type'].str.contains(manufacturer)]

    

    
    plt.pie(manufacturer_data['Count'], labels=manufacturer_data['Handset Type'], autopct='%1.1f%%', startangle=140)

    

    
    plt.title(f'Distribution of Users for {manufacturer}')




plt.tight_layout()




plt.show()


user_xdr_sessions = mydata.groupby('MSISDN/Number')['Bearer Id'].count().reset_index(name='Number_of_xDR_sessions')




print("Aggregated Information - Number of xDR Sessions per User:")

print(user_xdr_sessions)


mydata['Start'] = pd.to_datetime(mydata['Start'])

mydata['End'] = pd.to_datetime(mydata['End'])

mydata['Session_Duration'] = (mydata['End'] - mydata['Start']).dt.total_seconds()

user_session_duration = mydata.groupby('MSISDN/Number')['Session_Duration'].sum().reset_index(name='Total_Session_Duration')

print("Aggregated Session Duration per User:")

print(user_session_duration)

user_data_volume = mydata.groupby('MSISDN/Number').agg(

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Download and Upload Data per User:")

print(user_data_volume)


user_app_data_volume = mydata.groupby('MSISDN/Number').agg(

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()




print("Aggregated Data Volume per User for Each Application:")

print(user_app_data_volume)


user_aggregated_data = mydata.groupby('MSISDN/Number').agg(

    Number_of_xDR_sessions=('Bearer Id', 'count'),

    Session_Duration=('Session_Duration', 'sum'),

    Total_DL_data=('Total DL (Bytes)', 'sum'),

    Total_UL_data=('Total UL (Bytes)', 'sum'),

    Social_Media_DL=('Social Media DL (Bytes)', 'sum'),

    Social_Media_UL=('Social Media UL (Bytes)', 'sum'),

    Google_DL=('Google DL (Bytes)', 'sum'),

    Google_UL=('Google UL (Bytes)', 'sum'),

    Email_DL=('Email DL (Bytes)', 'sum'),

    Email_UL=('Email UL (Bytes)', 'sum'),

    Youtube_DL=('Youtube DL (Bytes)', 'sum'),

    Youtube_UL=('Youtube UL (Bytes)', 'sum'),

    Netflix_DL=('Netflix DL (Bytes)', 'sum'),

    Netflix_UL=('Netflix UL (Bytes)', 'sum'),

    Gaming_DL=('Gaming DL (Bytes)', 'sum'),

    Gaming_UL=('Gaming UL (Bytes)', 'sum'),

    Other_DL=('Other DL (Bytes)', 'sum'),

    Other_UL=('Other UL (Bytes)', 'sum')

).reset_index()


plt.figure(figsize=(15, 10))

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Number_of_xDR_sessions'], label='Number of xDR Sessions')

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Session_Duration'], label='Session Duration', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_DL_data'], label='Total DL Data', alpha=0.7)

plt.bar(user_aggregated_data['MSISDN/Number'], user_aggregated_data['Total_UL_data'], label='Total UL Data', alpha=0.7)

plt.legend()

plt.xlabel('MSISDN/Number')

plt.ylabel('Aggregated Values')

plt.title('Aggregated Information per User')

plt.xticks(rotation=45, ha='right')

plt.tight_layout()

plt.show()

summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.index, y=top_handsets.values)

plt.xticks(rotation=45, ha='right')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.show()

plt.figure(figsize=(10, 6))

sns.histplot(mydata['Session_Duration'], bins=30, kde=True)

plt.title('Distribution of Session Durations')

plt.xlabel('Session Duration (seconds)')

plt.ylabel('Frequency')

plt.show()

plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()

mixed_data_columns = mydata.select_dtypes(include='object').columns

print(f'Columns with mixed data types: {mixed_data_columns}')


plt.figure(figsize=(10, 6))

sns.scatterplot(x='Total DL (Bytes)', y='Total UL (Bytes)', data=mydata)

plt.title('Scatter Plot of Total DL vs Total UL Data')

plt.xlabel('Total DL Data (Bytes)')

plt.ylabel('Total UL Data (Bytes)')

plt.show()


summary_statistics = mydata.describe()




top_handsets = mydata['Handset Type'].value_counts().head(10)

plt.figure(figsize=(12, 6))

sns.barplot(x=top_handsets.values, y=top_handsets.index, orient='h')

plt.title('Distribution of Users by Handset Type (Top 10)')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.show()

basic_metrics = mydata.describe()


dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]



plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.show()

import matplotlib.pyplot as plt

import seaborn as sns





application = 'Social Media'




subset_data = mydata[[f'{application} DL (Bytes)', f'{application} UL (Bytes)', 'Total DL (Bytes)', 'Total UL (Bytes)']]




subset_data['Total DL+UL Data'] = subset_data['Total DL (Bytes)'] + subset_data['Total UL (Bytes)']




plt.figure(figsize=(10, 6))

sns.scatterplot(x=f'{application} DL (Bytes)', y='Total DL+UL Data', data=subset_data)

plt.title(f'Relationship between {application} and Total DL+UL Data')

plt.xlabel(f'{application} DL (Bytes)')

plt.ylabel('Total DL+UL Data')

plt.show()


customer_engagement = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',

    'Dur. (ms)': 'sum',

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()




customer_engagement.columns = ['MSISDN', 'Session Count', 'Total Duration', 'Total UL (Bytes)', 'Total DL (Bytes)']




customer_engagement['Total Traffic (Bytes)'] = customer_engagement['Total UL (Bytes)'] + customer_engagement['Total DL (Bytes)']




top_10_session_count = customer_engagement.sort_values(by='Session Count', ascending=False).head(10)

top_10_duration = customer_engagement.sort_values(by='Total Duration', ascending=False).head(10)

top_10_total_traffic = customer_engagement.sort_values(by='Total Traffic (Bytes)', ascending=False).head(10)




print("Top 10 Customers by Session Count:")

print(top_10_session_count[['MSISDN', 'Session Count']])

print("\nTop 10 Customers by Total Duration:")

print(top_10_duration[['MSISDN', 'Total Duration']])

print("\nTop 10 Customers by Total Traffic:")

print(top_10_total_traffic[['MSISDN', 'Total Traffic (Bytes)']])

import matplotlib.pyplot as plt




def plot_top_10(data, metric, title):

    plt.figure(figsize=(10, 6))

    plt.bar(data['MSISDN'], data[metric], color='skyblue')

    plt.xlabel('MSISDN')

    plt.ylabel(metric)

    plt.title(title)

    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()

    plt.show()




plot_top_10(top_10_session_count, 'Session Count', 'Top 10 Customers by Session Count')




plot_top_10(top_10_duration, 'Total Duration', 'Top 10 Customers by Total Duration')




plot_top_10(top_10_total_traffic, 'Total Traffic (Bytes)', 'Top 10 Customers by Total Traffic')

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sqlalchemy import create_engine

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA

from sklearn.impute import SimpleImputer




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)




imputer = SimpleImputer(strategy='mean')

imputed_data = imputer.fit_transform(mydata.select_dtypes(include=['float64']))




scaler = StandardScaler()

scaled_data = scaler.fit_transform(imputed_data)




pca = PCA()

principal_components = pca.fit_transform(scaled_data)




pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])




pca_interpretation = [

    "Principal components capture the variance in the data.",

    "Each principal component is a linear combination of the original features.",

    "The first few principal components explain a significant portion of the total variance.",

    "PCA is useful for dimensionality reduction and identifying patterns in high-dimensional data."

]




from pptx import Presentation

from pptx.util import Inches

import os




presentation = Presentation()




def add_slide(title, content):

    slide = presentation.slides.add_slide(presentation.slide_layouts[1])  
    title_box = slide.shapes.title

    content_box = slide.placeholders[1]

    title_box.text = title

    content_box.text = content




variable_description = mydata.dtypes.reset_index()

variable_description.columns = ['Variable', 'Data Type']

add_slide("Variable Description", variable_description.to_string(index=False))




basic_metrics = mydata.describe()

add_slide("Basic Metrics", basic_metrics.to_string())




dispersion_parameters = mydata.describe().loc[['std', 'min', '25%', '50%', '75%', 'max']]

add_slide("Non-Graphical Univariate Analysis", dispersion_parameters.to_string())




image_path = 'univariate_analysis_plot.png'

plt.figure(figsize=(15, 10))

sns.boxplot(data=mydata.select_dtypes(include=['float64']))

plt.title('Boxplot for Quantitative Variables')

plt.savefig(image_path)

plt.close()

presentation.slides.add_slide(presentation.slide_layouts[5])  
presentation.slides[-1].shapes.add_picture(image_path, Inches(1), Inches(1), width=Inches(8))

os.remove(image_path)  



application_cols = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                     'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                     'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                     'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']



total_data_cols = ['Total DL (Bytes)', 'Total UL (Bytes)']

bivariate_analysis = mydata[application_cols + total_data_cols].corr()

add_slide("Bivariate Analysis", bivariate_analysis.to_string())




mydata['Total_Session_Duration'] = mydata['Dur. (ms).1'] / 1000  
mydata['Decile_Class'] = pd.qcut(mydata['Total_Session_Duration'], q=5, labels=False, duplicates='drop')

total_data_per_decile = mydata.groupby('Decile_Class')[total_data_cols].sum()

add_slide("Variable Transformations", total_data_per_decile.to_string())





correlation_matrix = mydata[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                              'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                              'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                              'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']].corr()

add_slide("Correlation Analysis", correlation_matrix.to_string())




add_slide("PCA Interpretation", "\n".join(pca_interpretation))




presentation.save("telecom_analysis_results.pptx")

import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt







engagement_metrics = mydata.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',            
    'Dur. (ms).1': 'sum',            
    'Total UL (Bytes)': 'sum',       
    'Total DL (Bytes)': 'sum'        
}).reset_index()




engagement_metrics.columns = ['MSISDN', 'Session Frequency', 'Total Session Duration', 'Total UL Traffic', 'Total DL Traffic']




top_10_session_frequency = engagement_metrics.sort_values(by='Session Frequency', ascending=False).head(10)

top_10_duration = engagement_metrics.sort_values(by='Total Session Duration', ascending=False).head(10)

top_10_ul_traffic = engagement_metrics.sort_values(by='Total UL Traffic', ascending=False).head(10)

top_10_dl_traffic = engagement_metrics.sort_values(by='Total DL Traffic', ascending=False).head(10)



print("Top 10 Customers by Session Frequency:")

print(top_10_session_frequency)



print("\nTop 10 Customers by Total Session Duration:")

print(top_10_duration)



print("\nTop 10 Customers by Total UL Traffic:")

print(top_10_ul_traffic)



print("\nTop 10 Customers by Total DL Traffic:")

print(top_10_dl_traffic)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics.iloc[:, 1:])




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_metrics['Cluster'] = kmeans.fit_predict(normalized_metrics)




cluster_stats = engagement_metrics.groupby('Cluster').agg({

    'Session Frequency': ['min', 'max', 'mean', 'sum'],

    'Total Session Duration': ['min', 'max', 'mean', 'sum'],

    'Total UL Traffic': ['min', 'max', 'mean', 'sum'],

    'Total DL Traffic': ['min', 'max', 'mean', 'sum']

}).reset_index()



print("\nCluster Statistics:")

print(cluster_stats)




app_traffic = mydata.groupby('MSISDN/Number').agg({

    'Social Media DL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

}).reset_index()




top_10_social_media = app_traffic.sort_values(by='Social Media DL (Bytes) + Social Media UL (Bytes)', ascending=False).head(10)

top_10_youtube = app_traffic.sort_values(by='Youtube DL (Bytes) + Youtube UL (Bytes)', ascending=False).head(10)

top_10_gaming = app_traffic.sort_values(by='Gaming DL (Bytes) + Gaming UL (Bytes)', ascending=False).head(10)



print("\nTop 10 Most Engaged Users (Social Media):")

print(top_10_social_media)



print("\nTop 10 Most Engaged Users (YouTube):")

print(top_10_youtube)



print("\nTop 10 Most Engaged Users (Gaming):")

print(top_10_gaming)




top_3_apps = app_traffic.sum().nlargest(3)



plt.figure(figsize=(10, 6))

top_3_apps.plot(kind='bar', color='skyblue')

plt.title('Top 3 Most Used Applications')

plt.xlabel('Application')

plt.ylabel('Total Traffic (Bytes)')

plt.show()








inertia_values = []

possible_k_values = range(1, 11)



for k in possible_k_values:

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_metrics)

    inertia_values.append(kmeans.inertia_)




plt.figure(figsize=(10, 6))

plt.plot(possible_k_values, inertia_values, marker='o')

plt.title('Elbow Method for Optimal k')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.show()

import pandas as pd

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.impute import SimpleImputer

from kneed import KneeLocator




identifier_column = 'MSISDN/Number'




print(mydata.columns)




engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

engagement_metrics.columns = ['Session Frequency', 'Total Session Duration', 'Total Traffic']




imputer = SimpleImputer(strategy='mean')

engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)




scaler = StandardScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)




print("Length of mydata:", len(mydata))

print("Length of normalized_metrics:", len(normalized_metrics))




kmeans = KMeans(n_clusters=3, random_state=42)

engagement_clusters = kmeans.fit_predict(normalized_metrics)




print("Length of engagement_clusters:", len(engagement_clusters))




if len(mydata) == len(engagement_clusters):

    mydata['Engagement Cluster'] = engagement_clusters

else:

    print("Length mismatch! Unable to assign 'Engagement Cluster'.")




print("Missing values in mydata:")

print(mydata.isnull().sum())

engagement_metrics = mydata.groupby(identifier_column)['Bearer Id'].agg(['count', 'sum', 'mean'])

print("Length of engagement_metrics:", len(engagement_metrics))



engagement_metrics_imputed = imputer.fit_transform(engagement_metrics)

print("Length of engagement_metrics_imputed:", len(engagement_metrics_imputed))



normalized_metrics = scaler.fit_transform(engagement_metrics_imputed)

print("Length of normalized_metrics:", len(normalized_metrics))

print("Duplicate rows based on identifier column:")

print(mydata[mydata.duplicated(subset=identifier_column, keep=False)])
import pandas as pd





application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',

                        'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',

                        'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',

                        'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']




total_data_per_user = mydata.groupby('MSISDN/Number')[application_columns].sum()




total_data_per_user['Total Data Usage'] = total_data_per_user.sum(axis=1)




top_engaged_users = total_data_per_user.nlargest(10, 'Total Data Usage')




print(top_engaged_users)

import matplotlib.pyplot as plt





engagement_metric = 'Total Data Usage'




plt.figure(figsize=(10, 6))

top_engaged_users[engagement_metric].plot(kind='bar', color='skyblue')

plt.title('Top 10 Engaged Users per Application')

plt.xlabel('Bearer Id')

plt.ylabel(engagement_metric)

plt.show()


mydata.hist(figsize=(20, 30))

plt.show()

sns.boxplot(data=mydata)

plt.show()
import numpy as np

import pandas as pd

import psycopg2

import pandas as pd

from sqlalchemy import create_engine

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler




db_params = {

    'dbname': 'week1',

    'user': 'postgres',

    'password': 'habte',

    'host': 'localhost',

    'port': '5432'

}




engine = create_engine(f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}')




sql_query = "SELECT * FROM xdr_data;"




mydata = pd.read_sql_query(sql_query, engine)

mydata = mydata.dropna()
mydata['Start'] = pd.to_datetime(mydata['Start'], errors='coerce')

mydata['End'] = pd.to_datetime(mydata['End'], errors='coerce')

numeric_columns = mydata.select_dtypes(include='number').columns

mydata[numeric_columns] = mydata[numeric_columns].fillna(mydata[numeric_columns].mean())
mydata.columns

plt.figure(figsize=(12, 8))

sns.histplot(mydata['Avg Bearer TP DL (kbps)'], bins=30, kde=True, color='blue')

plt.title('Distribution of Avg Bearer TP DL (kbps)')

plt.xlabel('Avg Bearer TP DL (kbps)')

plt.ylabel('Frequency')

plt.show()

numeric_columns = mydata.select_dtypes(include=['float64', 'int64']).columns

correlation_matrix = mydata[numeric_columns].corr()




plt.figure(figsize=(15, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Matrix')

plt.show()



plt.figure(figsize=(12, 8))

sns.boxplot(x='Handset Type', y='Avg RTT DL (ms)', data=mydata)

plt.title('Avg RTT DL (ms) by Handset Type')

plt.xlabel('Handset Type')

plt.ylabel('Avg RTT DL (ms)')

plt.show()

plt.figure(figsize=(12, 8))

sns.scatterplot(x='Total UL (Bytes)', y='Total DL (Bytes)', data=mydata, hue='Handset Manufacturer')

plt.title('User Engagement: Total UL vs Total DL')

plt.xlabel('Total UL (Bytes)')

plt.ylabel('Total DL (Bytes)')

plt.legend()

plt.show()

print(mydata.columns)


features = mydata[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']]

target = mydata['cluster']  


scaler = StandardScaler()

features_scaled = scaler.fit_transform(features)




X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)




model = LinearRegression()

model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')

import matplotlib.pyplot as plt




plt.scatter(y_test, y_pred, color='blue', alpha=0.5)

plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Perfect Prediction')




plt.title('Actual vs. Predicted Values - Linear Regression')

plt.xlabel('Actual Values')

plt.ylabel('Predicted Values')

plt.legend()

plt.grid(True)




plt.show()

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler




cluster_features = mydata[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']]




scaler = StandardScaler()

cluster_features_scaled = scaler.fit_transform(cluster_features)




kmeans = KMeans(n_clusters=3, random_state=42)  
mydata['cluster'] = kmeans.fit_predict(cluster_features_scaled)




print(mydata['cluster'].value_counts())

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error, r2_score




cluster_features = mydata[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']]




scaler = StandardScaler()

cluster_features_scaled = scaler.fit_transform(cluster_features)




kmeans = KMeans(n_clusters=3, random_state=42)  
mydata['cluster'] = kmeans.fit_predict(cluster_features_scaled)




features = mydata[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']]

target = mydata['cluster']




scaler = StandardScaler()

features_scaled = scaler.fit_transform(features)




X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)







print(mydata[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'cluster']].head())
import pandas as pd
import seaborn as sns
import numpy as np
import psycopg2
import matplotlib.pyplot as plt
from sqlalchemy import create_engine
from sklearn.cluster import KMeans

class UserExperienceAnalysis:
    def __init__(self, db_params):
        self.db_params = db_params
        self.mydata = self.load_data_from_database()

    def load_data_from_database(self):
                engine = create_engine(f'postgresql+psycopg2://{self.db_params["user"]}:{self.db_params["password"]}@{self.db_params["host"]}:{self.db_params["port"]}/{self.db_params["dbname"]}')

                sql_query = "SELECT * FROM xdr_data;"

                mydata = pd.read_sql_query(sql_query, engine)
        return mydata
    def clean_and_preprocess(self):
                cleaned_data = self.mydata.dropna()
        cleaned_data['Start'] = pd.to_datetime(cleaned_data['Start'], errors='coerce')
        cleaned_data['End'] = pd.to_datetime(cleaned_data['End'], errors='coerce')
        numeric_columns = cleaned_data.select_dtypes(include='number').columns
        cleaned_data.loc[:, numeric_columns] = cleaned_data.loc[:, numeric_columns].fillna(cleaned_data[numeric_columns].mean())
        return cleaned_data

    def perform_user_experience_analysis(self):
        user_experience_metrics = pd.DataFrame({'MSISDN/Number': self.mydata['MSISDN/Number'].unique()})
        user_experience_metrics['AvgTCPRetransmission'] = self.aggregate_average_tcp_retransmission()['TCP DL Retrans. Vol (Bytes)']
        user_experience_metrics['AvgRTT'] = self.aggregate_average_rtt()['Avg RTT DL (ms)']
        user_experience_metrics['HandsetType'] = self.get_handset_type()['Handset Type']
        user_experience_metrics['SimpleTCPThroughput'] = self.calculate_simple_tcp_throughput()['SimpleTCPThroughput']

        return user_experience_metrics

    def aggregate_average_tcp_retransmission(self):
        return self.mydata.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()

    def aggregate_average_rtt(self):
        return self.mydata.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean().reset_index()

    def get_handset_type(self):
        return self.mydata.groupby('MSISDN/Number')['Handset Type'].first().reset_index()

    def calculate_simple_tcp_throughput(self):
                throughput = self.mydata['Avg Bearer TP DL (kbps)'] / (self.mydata['TCP DL Retrans. Vol (Bytes)'] + 1)
        return pd.DataFrame({'MSISDN/Number': self.mydata['MSISDN/Number'], 'SimpleTCPThroughput': throughput})

db_params = {
    'dbname': 'week1',
    'user': 'postgres',
    'password': 'habte',
    'host': 'localhost',
    'port': '5432'
}
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sqlalchemy import create_engine

class UserSatisfactionAnalysis:
    engagement_columns = [
        'Dur. (ms)',
        'TCP DL Retrans. Vol (Bytes)',
        'DL TP < 50 Kbps (%)',
        '50 Kbps < DL TP < 250 Kbps (%)',
        '250 Kbps < DL TP < 1 Mbps (%)',
        'DL TP > 1 Mbps (%)',
        'Activity Duration DL (ms)',
        'Activity Duration UL (ms)',
        'Social Media DL (Bytes)',
        'Google DL (Bytes)',
        'Email DL (Bytes)',
        'Youtube DL (Bytes)',
        'Netflix DL (Bytes)',
        'Gaming DL (Bytes)',
        'Other DL (Bytes)',
        'Total UL (Bytes)',
        'Total DL (Bytes)'
    ]

    def __init__(self, db_params):
        self.db_params = db_params
        self.mydata = self.load_data_from_database()

    def load_data_from_database(self):
        """
        Load data from the PostgreSQL database into a Pandas DataFrame.
        """
        engine = create_engine(f'postgresql+psycopg2://{self.db_params["user"]}:{self.db_params["password"]}@{self.db_params["host"]}:{self.db_params["port"]}/{self.db_params["dbname"]}')
        sql_query = "SELECT * FROM xdr_data;"
        mydata = pd.read_sql_query(sql_query, engine)
        return mydata

    def perform_user_satisfaction_analysis(self):
        """
        Perform user satisfaction analysis by calculating engagement and experience scores,
        deriving a satisfaction score, identifying the top 10 satisfied customers, and clustering users.
        """
                engagement_score, experience_score = self.calculate_scores()

                satisfaction_score = (engagement_score + experience_score) / 2

                self.mydata['SatisfactionScore'] = satisfaction_score

                top_satisfied_customers = self.mydata.nlargest(10, 'SatisfactionScore')

                kmeans_clusters = self.run_kmeans()

                cluster_aggregates = self.aggregate_scores_by_cluster(kmeans_clusters)

        return top_satisfied_customers, cluster_aggregates

    def calculate_scores(self):
        """
        Calculate engagement and experience scores based on relevant columns in the dataset.
        """
        
        engagement_score = self.mydata[self.engagement_columns].mean(axis=1)

                experience_score = 0

        return engagement_score, experience_score

    def run_kmeans(self):
        """
        Run k-means clustering on engagement and experience scores.
        """
                imputer = SimpleImputer(strategy='mean')
        kmeans_data_imputed = imputer.fit_transform(self.mydata[self.engagement_columns])

                kmeans_model = KMeans(n_clusters=2, random_state=42)
        kmeans_clusters = kmeans_model.fit_predict(kmeans_data_imputed)

        return kmeans_clusters

    def aggregate_scores_by_cluster(self, kmeans_clusters):
        """
        Aggregate average satisfaction scores per cluster.
        """
                cluster_data = self.mydata.copy()
        cluster_data['Cluster'] = kmeans_clusters
        cluster_aggregates = cluster_data.groupby('Cluster').agg({
            'SatisfactionScore': 'mean'
        }).reset_index()

        return cluster_aggregates

db_params = {
    'dbname': 'week1',
    'user': 'postgres',
    'password': 'habte',
    'host': 'localhost',
    'port': '5432'
}
Repository Structure: '
' ├── streamlit dashboard
│   ├── dashboard.py
│   ├── .gitignore
│   └── requirements.txt
├── Telecom EDA
│   └── telecom.ipynb
├── data_importation_script
│   ├── requirement.txt
│   └── telecom.py
└── README.md
 '
' Commit History: 
{"insertions": [14], "deletions": [12], "lines": [26], "committed_datetime": ["2023-12-17 01:51:23"], "commit_count": 1} 
 Content: 
import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
 st.title('Telecom Dashboard')
df = pd.read_csv('telecom.csv')
st.subheader('First 5 rows of the data')
st.dataframe(df.head())
st.subheader("Dataset Information")
info_data = {
    "Number of Rows": [df.shape[0]],
    "Number of Columns": [df.shape[1]],
    "Number of Missing Values": [df.isnull().sum().sum()]
}
st.table(pd.DataFrame(info_data))
st.subheader("Column Data Types")
data_types = df.dtypes.reset_index()
data_types.columns = ['Column', 'Data Type']
st.table(data_types)
st.sidebar.header("Statistical Summary")
stat_summary = df.describe().T
stat_summary = stat_summary.round(2)
st.sidebar.table(stat_summary)
def fill_na(df):
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            col_mean = df[col].mean()
            df[col].fillna(value=col_mean, inplace=True)
    return df
df = fill_na(df)
st.subheader("Dataset after Filling Missing Values with Mean")
st.dataframe(df.head())


st.subheader('Bivariet analysis')
df['all_tot'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']
df.head()
x_column = st.selectbox("Select X-axis Column", df.columns)
y_column = st.selectbox("Select Y-axis Column", df.columns)
fig, ax = plt.subplots(figsize=(7, 5))
sns.scatterplot(data=df, x=x_column, y=y_column, hue=y_column, ax=ax)
plt.title(f"Scatter Plot: {y_column} vs {x_column}")
plt.xlabel(x_column)
plt.ylabel(y_column)
st.pyplot(fig)
df_sess = df[['MSISDN/Number','Dur. (ms)', 'all_tot']]
sess_group = pd.DataFrame(df_sess.groupby('MSISDN/Number')['Dur. (ms)'].sum())
sess_group['decile'] = pd.qcut(sess_group['Dur. (ms)'], q = [0.1, 0.2,0.3,0.4,0.5,1], labels=False)
print(sess_group)
st.subheader("Decile Cut Analysis Result")
df_merg = pd.merge(sess_group, df_sess[['all_tot', 'MSISDN/Number']], on='MSISDN/Number')
st.dataframe(df_merg.head())
df_merg_sum = pd.DataFrame(df_merg.groupby('decile')['all_tot'].sum())
st.subheader('grouping the dataframe per the decile cut')
st.dataframe(df_merg_sum)
selected_columns = st.multiselect("Select columns for correlation analysis", df.columns)
df_corr = df[selected_columns]
correl = df_corr.corr()
st.subheader("Correlation Matrix of Selected Columns")
st.dataframe(correl)
plt.figure(figsize=(10, 8))
sns.heatmap(correl, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
st.subheader("Correlation Heatmap")
st.pyplot() 

Repository Structure: '
' ├── Data
│   ├── telecom.sql
├── Dashboard
│   ├── dashboard.py
│   ├── secrets.toml
│   └── .ipynb_checkpoints
├── .ipynb_checkpoints
│   └── task_1.py-checkpoint.ipynb
├── notebook
│   ├── .ipynb_checkpoints
│   └── task_1.py.ipynb
└── task_1.py.ipynb
 '
' Commit History: 
{"insertions": [2465, 1781], "deletions": [703, 369], "lines": [3168, 2150], "committed_datetime": ["2023-12-14 17:09:45", "2023-12-14 23:59:45"], "commit_count": 2} 
 Content: 
import pandas as pd

from sqlalchemy import create_engine

import numpy as np

from IPython.display import Image

import seaborn as sns

import matplotlib.pyplot as plt
path = '../Data'
df = pd.read_csv(path+"/teledata.csv")

df.head(5)
df.columns.tolist()
len(df.columns)

df.shape
print (f"There are {df.shape[0]} rows and {df.shape[1]} columns")

df.isnull().sum().sum()


df.describe()
def percent_missing(df):

    
    totalCells = np.product(df.shape)

    
    missingCount = df.isnull().sum()

    

    totalMissing = missingCount.sum() 

    

    print("The tele database contains", round(((totalMissing/totalCells) * 100),2), "%", 'missing values')



percent_missing(df)
df.isna().sum()
percent_missing(df['Nb of sec with 37500B < Vol UL'])

percent_missing(df['Nb of sec with 6250B < Vol UL < 37500B'])

percent_missing(df['Nb of sec with 1250B < Vol UL < 6250B'])

percent_missing(df['Nb of sec with 125000B < Vol DL'])

percent_missing(df['Nb of sec with 31250B < Vol DL < 125000B'])

percent_missing(df['Nb of sec with 6250B < Vol DL < 31250B'])

percent_missing(df['TCP DL Retrans. Vol (Bytes)'])

percent_missing(df['TCP UL Retrans. Vol (Bytes)'])

percent_missing(df['HTTP DL (Bytes)'])

percent_missing(df['HTTP UL (Bytes)'])

percent_missing(df['Bearer Id'])







df_clean = df.drop(['Nb of sec with 37500B < Vol UL',

                    'Nb of sec with 6250B < Vol UL < 37500B','Nb of sec with 1250B < Vol UL < 6250B',

                    'Nb of sec with 125000B < Vol DL',

                    'Nb of sec with 31250B < Vol DL < 125000B',

                    'Nb of sec with 6250B < Vol DL < 31250B',

                   'TCP DL Retrans. Vol (Bytes)',

                    'TCP UL Retrans. Vol (Bytes)',

                    'HTTP DL (Bytes)',

                    'HTTP UL (Bytes)'], axis=1)

df_clean=df_clean.dropna(subset=['End'])

df_clean.shape

print (f"The shape of the dataframe after cleaning some data is {df_clean.shape}")
df_clean.isna().sum()
df_clean.sample(5)
df_clean['Start'] = pd.to_datetime(df['Start'])

df_clean['End'] = pd.to_datetime(df['End'])
df_clean.isna().sum()
df_clean.sample(5)
df_clean.head()
df_clean.skew(axis=0)
missing_columns = ["Avg RTT DL (ms)",

                   "Avg RTT UL (ms)",

                   "DL TP < 50 Kbps (%)",

                   "50 Kbps < DL TP < 250 Kbps (%)",

                   "250 Kbps < DL TP < 1 Mbps (%)",

                   "DL TP > 1 Mbps (%)",

                   "UL TP < 10 Kbps (%)",

                   "10 Kbps < UL TP < 50 Kbps (%)",

                   "50 Kbps < UL TP < 300 Kbps (%)",

                   "UL TP > 300 Kbps (%)",

                   "Nb of sec with Vol DL < 6250B",

                   "Nb of sec with Vol UL < 1250B"]
def fix_missing_mean(df, col):

    df[col] = df[col].fillna(df[col].mean())

    return df[col]



def fix_missing_median(df, col):

    df[col] = df[col].fillna(df[col].median())  

    return df[col]



def fix_missing_mode(df, col):

    df[col] = df[col].fillna(df_clean[col].mode()[0])

    return df[col]
for i in missing_columns:

    df_clean[i] = fix_missing_median(df_clean, i)

df_clean["Last Location Name"] = fix_missing_mode(df_clean,"Last Location Name")
df_clean.isna().sum()
df_clean['Bearer Id'].hist()
df_clean=df_clean.dropna(subset=['Bearer Id'])

df_clean=df_clean.dropna(subset=['MSISDN/Number'])

df_clean=df_clean.dropna(subset=['IMSI'])

df_clean.isna().sum()
df['Handset Type'].value_counts()[0:10]
df['Handset Manufacturer'].value_counts()[0:3]
df_top = df_clean.query('`Handset Manufacturer` in ["Apple", "Samsung", "Huawei"]')

len(df_top)
df_top_apple = df_clean.loc[df['Handset Manufacturer']=='Apple']

df_top_samsung = df_clean.loc[df['Handset Manufacturer']=='Samsung']

df_top_huawei = df_clean.loc[df['Handset Manufacturer']=='Huawei']


df_top['Handset Type'].value_counts()[0:5]

df_top_apple['Handset Type'].value_counts()[0:5]

df_top_samsung['Handset Type'].value_counts()[0:5]

df_top_huawei['Handset Type'].value_counts()[0:5]
Users = df_clean.groupby(['IMSI'])



xDR_users = df_clean['IMSI'].value_counts()

session_duration = Users['Dur. (ms)'].sum()

Total_session_UL = Users['Total UL (Bytes)'].sum()

Total_session_DL = Users['Total DL (Bytes)'].sum()

Total_Download_Social = Users['Social Media DL (Bytes)'].sum() + Users['Social Media UL (Bytes)'].sum()

Total_Download_Google = Users['Google DL (Bytes)'].sum() + Users['Google UL (Bytes)'].sum()

Total_Download_Email = Users['Email DL (Bytes)'].sum() + Users['Email UL (Bytes)'].sum()

Total_Download_Youtube = Users['Youtube DL (Bytes)'].sum() + Users['Youtube UL (Bytes)'].sum()

Total_Download_Netflix = Users['Netflix DL (Bytes)'].sum() + Users['Netflix UL (Bytes)'].sum()

Total_Download_Gaming = Users['Gaming DL (Bytes)'].sum() + Users['Gaming UL (Bytes)'].sum()

Total_Download_Other = Users['Other DL (Bytes)'].sum() + Users['Other DL (Bytes)'].sum()


new = pd.concat([xDR_users,

                 session_duration,

                 Total_session_UL,

                 Total_session_DL,

                 Total_Download_Social,

                 Total_Download_Google,

                 Total_Download_Email,

                 Total_Download_Youtube,

                 Total_Download_Netflix,

                 Total_Download_Gaming,

                 Total_Download_Other

                ], axis="columns", sort=True)



new = new.rename(columns={0: 'Social media total volume (Bytes)'})

new = new.rename(columns={1: 'Google total volume (Bytes)'})

new = new.rename(columns={2: 'Email total volume (Bytes)'})

new = new.rename(columns={3: 'Youtube total volume (Bytes)'})

new = new.rename(columns={4: 'Netflix total volume (Bytes)'})

new = new.rename(columns={5: 'Gaming total volume (Bytes)'})

new = new.rename(columns={'Other DL (Bytes)': 'Other source total volume (Bytes)'})

new = new.rename(columns={'IMSI': 'number of sessions'})

df_norm = (new-new.min())/(new.max()-new.min())

df_norm.head(10)
new1 = new.copy()


def fix_outlier(df, column):

    df[column] = np.where(df[column] > df[column].quantile(0.95), df[column].median(),df[column])

    

    return df[column]


for i in df_norm.columns.tolist():

    df_norm[i]=fix_outlier(df_norm,i)


for i in new.columns.tolist():

    new[i]=fix_outlier(new,i)

len(new)

fig, ax=plt.subplots(1,2, figsize=(20, 6))

sns.histplot(df_norm['number of sessions'], ax=ax[0])

ax[0].set_title("numbers of sessions")

sns.histplot(df_norm['Dur. (ms)'], ax=ax[1])

ax[1].set_title("Dur. (ms)")

plt.ylim(0, 2500)

fig, ax=plt.subplots(1,2, figsize=(20, 6))

sns.histplot(df_norm['Total UL (Bytes)'], ax=ax[0])

ax[0].set_title("Total UL (Bytes)")

sns.histplot(df_norm['Total DL (Bytes)'], ax=ax[1])

ax[1].set_title("Total DL (Bytes)")

fig, ax=plt.subplots(1,2, figsize=(20, 6))

sns.histplot(new['Netflix total volume (Bytes)'], ax=ax[0])

ax[0].set_title("Netflix total volume (Bytes)")

sns.histplot(new['Gaming total volume (Bytes)'], ax=ax[1])

ax[1].set_title("Gaming total volume (Bytes)")

plt.ylim(0, 2500)

fig, ax=plt.subplots(1, figsize=(9, 6))

sns.histplot(new['Other source total volume (Bytes)'])

ax.set_title("Other source total volume")

plt.ylim(0, 2500)

new['Total_DL_UL'] = new['Total DL (Bytes)'] + new['Total UL (Bytes)']
a4_dims = (20, 7)

fig, axs = plt.subplots(ncols=6,figsize=a4_dims)

sns.scatterplot(x="Social media total volume (Bytes)", y="Total_DL_UL",ax=axs[0], data=new)

sns.scatterplot(x="Gaming total volume (Bytes)", y="Total_DL_UL", ax=axs[1],data=new)

sns.scatterplot(x="Email total volume (Bytes)", y="Total_DL_UL",ax=axs[2], data=new)

sns.scatterplot(x="Youtube total volume (Bytes)", y="Total_DL_UL",ax=axs[3], data=new)

sns.scatterplot(x="Google total volume (Bytes)", y="Total_DL_UL",ax=axs[4], data=new)

sns.scatterplot(x="Other source total volume (Bytes)", y="Total_DL_UL",ax=axs[5], data=new)

plt.show()


new['decile'] = pd.qcut(new['Dur. (ms)'], 5, labels =False, )

new.head(10)

data_per_decile = new.groupby(['decile']).agg({'Total_DL_UL':['sum']})

data_per_decile.columns = ["_".join(x) for x in data_per_decile.columns.ravel()]



data_per_decile.head(10)

df_new=df_norm[['Social media total volume (Bytes)',

                'Google total volume (Bytes)',

                'Email total volume (Bytes)',

                'Youtube total volume (Bytes)',

                'Netflix total volume (Bytes)',

                'Gaming total volume (Bytes)',

                'Other source total volume (Bytes)']]





corrMatrix = df_new.corr()

f, ax = plt.subplots(figsize=(13, 9))

sns.heatmap(corrMatrix,annot=True,fmt='.3f',annot_kws={"size":12})

plt.show()

new.index.name = 'customer ID'

new.reset_index(inplace=True)



new.head()





new.sort_values(by='number of sessions', ascending=False)[['customer ID','number of sessions']]

new.sort_values(by='Dur. (ms)', ascending=False)[['customer ID','Dur. (ms)']].head(10)

new.sort_values(by='Total_DL_UL', ascending=False)[['customer ID','Total_DL_UL']].head(10)
from sklearn.cluster import KMeans

from sklearn.preprocessing import MinMaxScaler

df_model = new[['customer ID','Dur. (ms)', 'Total_DL_UL','number of sessions']]

df_scaled = df_model.copy()

df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']] = (df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']]-df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']].min())/(df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']].max()-df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']].min())

df_scaled.head(10)


kmeans = KMeans(n_clusters=3, random_state=0)

df_scaled['cluster'] = kmeans.fit_predict(df_scaled[['number of sessions', 'Dur. (ms)','Total_DL_UL']])



df_eng = df_scaled.copy()

df_scaled.head()

centroids = kmeans.cluster_centers_

cen_x = [i[0] for i in centroids] 

cen_y = [i[1] for i in centroids]


df_scaled['cen_x'] = df_scaled['cluster'].map({0:cen_x[0], 1:cen_x[1], 2:cen_x[2]})



df_scaled['cen_y'] = df_scaled['cluster'].map({0:cen_y[0], 1:cen_y[1], 2:cen_y[2]})

df_scaled.groupby(['cluster']).describe().transpose()




new.sort_values(by='Social media total volume (Bytes)', ascending=False)[['customer ID','Social media total volume (Bytes)']].head(10)

new.sort_values(by='Netflix total volume (Bytes)', ascending=False)[['customer ID','Netflix total volume (Bytes)']].head(10)



new.sort_values(by='Google total volume (Bytes)', ascending=False)[['customer ID','Google total volume (Bytes)']].head(10)

new.sort_values(by='Youtube total volume (Bytes)', ascending=False)[['customer ID','Youtube total volume (Bytes)']].head(10)

new.sort_values(by='Email total volume (Bytes)', 

                ascending=False)[['customer ID','Email total volume (Bytes)']].head(10)

new.sort_values(by='Gaming total volume (Bytes)', ascending=False)[['customer ID','Gaming total volume (Bytes)']].head(10)

new.sort_values(by='Other source total volume (Bytes)', ascending=False)[['customer ID','Other source total volume (Bytes)']].head(10)

labels = ['Social media', 'Google', 'Email','Youtube','Netflix','Gaming']

sizes = [new['Social media total volume (Bytes)'].sum(),

         new['Google total volume (Bytes)'].sum(),

         new['Email total volume (Bytes)'].sum(),

         new['Youtube total volume (Bytes)'].sum(),

         new['Netflix total volume (Bytes)'].sum(),

         new['Gaming total volume (Bytes)'].sum(),

        ]


fig1, ax1 = plt.subplots(figsize = (10, 10))

colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple']

ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, colors=colors)



ax1.axis('equal')

plt.title("Total traffics per application")

plt.show()


labels=['Gaming','Youtube','Netflix']



sizes.sort()

top3 = sizes[3:]

ys=top3

fig = plt.figure()

ax = fig.add_axes([0,0,1,1])

plt.title("Top 3 used application", color='brown')

ax.bar(labels,ys)

plt.show() 

import pandas as pd

import psycopg2

from sqlalchemy import create_engine, text

import numpy as np
db_params = {

    'host': '127.0.0.1',

    'user': 'root',

    'password': 'root',

    'port': '5439',

    'database': 'week_one'

}
engine = create_engine(f"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}")
table_name = 'xdr_data'
df = pd.read_sql_table(table_name, con=engine)

df.shape

df.info()

df.isnull().sum()

missing_value = df.isnull().sum().sum()

missing_value
np.product(df.shape)

missing_value_in_percent = missing_value / np.product(df.shape)  * 100

missing_value_in_percent

df.head()

numeric_columns = df.select_dtypes(include=['float', 'int'])

numeric_columns.skew(axis=0)
import pandas as pd

import psycopg2

from sqlalchemy import create_engine, text

import numpy as np

from dotenv import load_dotenv

import os

import matplotlib.pyplot as plt

import seaborn as sns

load_dotenv()
db_host = os.getenv("DB_HOST")

db_user = os.getenv("DB_USER")

db_password = os.getenv("DB_PASSWORD")

db_port = os.getenv("DB_PORT")

db_database = os.getenv("DB_NAME")
db_params = {

    'host': db_host,

    'user': db_user,

    'password': db_password,

    'port': db_port,

    'database': db_database

}
engine = create_engine(f"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}")

engine 
table_name = 'xdr_data'
df = pd.read_sql_table(table_name, con=engine)

df.shape

df.info()
df.describe()

df.isnull().sum()

missing_value = df.isnull().sum().sum()

missing_value
np.product(df.shape)

missing_value_in_percent = missing_value / np.product(df.shape)  * 100

missing_value_in_percent

df.head()
df["Handset Type"]

numeric_columns = df.select_dtypes(include=['float', 'int'])


numeric_columns = df.select_dtypes(include=['float', 'int']).columns
object_columns  = df.select_dtypes(include=["object"]).columns

for column_name in numeric_columns:

    column_skew = df[column_name].skew().round()



    if column_skew > 0:

        fill_value = df[column_name].median()

    elif column_skew < 0:

        fill_value = df[column_name].median()

    else:

        fill_value = df[column_name].mean()



    df[column_name].fillna(fill_value, inplace=True)


undefined_rows  = df[df['Handset Type'] == 'undefined']

handest_type_mode = df['Handset Type'].mode()[0]

df.loc[undefined_rows.index, 'Handset Type'] = handest_type_mode

data_range = df["Total DL (Bytes)"].max() - df["Total DL (Bytes)"].min()

q1 = df["Total DL (Bytes)"].quantile(0.25)

q3 = df["Total DL (Bytes)"].quantile(0.75)

variance = df["Total DL (Bytes)"].var()

std_deviation = df["Total DL (Bytes)"].std()

iqr = q3 - q1

print(f"{column_name} Range: {data_range}")

print(f"{column_name} IQR: {iqr}")

print(f"{column_name} Variance: {variance}")

print(f"{column_name} Standard Deviation: {std_deviation}")


plt.figure(figsize=(8, 5))

sns.boxplot(x=df["Total DL (Bytes)"], color='blue')

plt.title("Box Plot of Total Download (Bytes)")

plt.xlabel("Total Download (Bytes)")

plt.show()

handset_counts = df.groupby('Handset Type')['MSISDN/Number'].count()

handset_count_sorted = handset_counts.sort_values(ascending=False)

handset_count_sorted.head(10)

df["Handset Manufacturer"].value_counts().head(3)

top3_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index



df_top3 = df[df['Handset Manufacturer'].isin(top3_manufacturers)]





top5_handsets_per_manufacturer = (

    df_top3.groupby(['Handset Manufacturer', 'Handset Type'])

           ['MSISDN/Number']

           .count()

           .reset_index(name='Count')

           .sort_values(['Handset Manufacturer', 'Count'], ascending=[True, False])

           .groupby('Handset Manufacturer')

           .head(5)

)



top5_handsets_per_manufacturer


xdr_session_per_user = df.groupby(['MSISDN/Number'])['Bearer Id'].count()

xdr_session_per_user

session_duration = df.groupby(['MSISDN/Number'])['Dur. (ms)'].sum()

session_duration

total_data_per_user = df.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum', 

    'Total UL (Bytes)': 'sum'

})



total_data_per_user.columns = ['Total Download (DL)', 'Total Upload (UL)']

total_data_per_user
import pandas as pd

import psycopg2

from sqlalchemy import create_engine, text

import numpy as np

from dotenv import load_dotenv

import os

import matplotlib.pyplot as plt

import seaborn as sns

load_dotenv()
db_host = os.getenv("DB_HOST")

db_user = os.getenv("DB_USER")

db_password = os.getenv("DB_PASSWORD")

db_port = os.getenv("DB_PORT")

db_database = os.getenv("DB_NAME")
db_params = {

    'host': db_host,

    'user': db_user,

    'password': db_password,

    'port': db_port,

    'database': db_database

}
engine = create_engine(f"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}")
table_name = 'xdr_data'
df = pd.read_sql_table(table_name, con=engine)

df.shape

df.info()
df.describe()

df.isnull().sum()

missing_value = df.isnull().sum().sum()

missing_value
np.product(df.shape)

missing_value_in_percent = missing_value / np.product(df.shape)  * 100

missing_value_in_percent

df.head()
df["Handset Type"]

numeric_columns = df.select_dtypes(include=['float', 'int'])


numeric_columns = df.select_dtypes(include=['float', 'int']).columns
object_columns  = df.select_dtypes(include=["object"]).columns

for column_name in numeric_columns:

    column_skew = df[column_name].skew().round()



    if column_skew > 0:

        fill_value = df[column_name].median()

    elif column_skew < 0:

        fill_value = df[column_name].median()

    else:

        fill_value = df[column_name].mean()



    df[column_name].fillna(fill_value, inplace=True)


undefined_rows  = df[df['Handset Type'] == 'undefined']

handest_type_mode = df['Handset Type'].mode()[0]

df.loc[undefined_rows .index, 'Handset Type'] = handest_type_mode

data_range = df["Total DL (Bytes)"].max() - df["Total DL (Bytes)"].min()

q1 = df["Total DL (Bytes)"].quantile(0.25)

q3 = df["Total DL (Bytes)"].quantile(0.75)

variance = df["Total DL (Bytes)"].var()

std_deviation = df["Total DL (Bytes)"].std()

iqr = q3 - q1

print(f"{column_name} Range: {data_range}")

print(f"{column_name} IQR: {iqr}")

print(f"{column_name} Variance: {variance}")

print(f"{column_name} Standard Deviation: {std_deviation}")


plt.figure(figsize=(8, 5))

sns.boxplot(x=df["Total DL (Bytes)"], color='blue')

plt.title("Box Plot of Total Download (Bytes)")

plt.xlabel("Total Download (Bytes)")

plt.show()

handset_counts = df.groupby('Handset Type')['MSISDN/Number'].count()

handset_count_sorted = handset_counts.sort_values(ascending=False)

handset_count_sorted.head(10)

df["Handset Manufacturer"].value_counts().head(3)

top3_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index




df_top3 = df[df['Handset Manufacturer'].isin(top3_manufacturers)]





top5_handsets_per_manufacturer = (

    df_top3.groupby(['Handset Manufacturer', 'Handset Type'])

           ['MSISDN/Number']

           .count()

           .reset_index(name='Count')

           .sort_values(['Handset Manufacturer', 'Count'], ascending=[True, False])

           .groupby('Handset Manufacturer')

           .head(5)

)



top5_handsets_per_manufacturer


xdr_session_per_user = df.groupby(['MSISDN/Number'])['Bearer Id'].count()

xdr_session_per_user

session_duration = df.groupby(['MSISDN/Number'])['Dur. (ms)'].sum()

session_duration

total_data_per_user = df.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum', 

    'Total UL (Bytes)': 'sum'

})



total_data_per_user.columns = ['Total Download (DL)', 'Total Upload (UL)']

total_data_per_user
class UtilFunctions:
    def missing_value_in_percent(self, total_cells, missing_cells):
        return (total_cells / missing_cells) * 100
import psycopg2
from sqlalchemy import create_engine, text


class ConnectToDatabase:
    def __init__(self, db_params):
        try:
            self.engine = create_engine(
                f"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}"
            )
        except Exception as error:
            raise Exception("Cannot connect to the database. Please try again.")
    def get_engine(self):
        return self.engine
import os, sys

from dotenv import load_dotenv

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

pd.set_option('display.max_columns', 60)
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)

from src.loader import ConnectToDatabase


from src.utils import UtilFunctions

load_dotenv()

db_host = os.getenv("DB_HOST")

db_user = os.getenv("DB_USER")

db_password = os.getenv("DB_PASSWORD")

db_port = os.getenv("DB_PORT")

db_database = os.getenv("DB_NAME")

db_params = {

    'host': db_host,

    'user': db_user,

    'password': db_password,

    'port': db_port,

    'database': db_database

}
connect_to_database = ConnectToDatabase(db_params)

engine = connect_to_database.get_engine()

utils = UtilFunctions()
table_name = 'xdr_data'
df = pd.read_sql_table(table_name, con=engine)

df.shape

df.info()
df.isnull().sum()
df.describe()

df.head()




plt.hist(df["Email UL (Bytes)"], color = 'blue', edgecolor='black', bins=int(180/5))

plt.title("Email data volume (in Bytes) sent by the MS during this session")

plt.xlabel("Email UL in Bytes")

plt.ylabel("Frequency")

plt.show()


plt.hist(df["DL TP < 50 Kbps (%)"], color = 'green', edgecolor='orange', bins=int(180/5))

plt.title("Duration ratio when Bearer Downlink Throughput < ….")

plt.xlabel("DL TP < 50 Kbps")

plt.ylabel("Frequency")

plt.show()


plt.hist(df["Nb of sec with 1250B < Vol UL < 6250B"], color = 'blue', edgecolor="orange", bins=int(180/5))

plt.title("Nb of sec with 1250B < Vol UL < 6250B")

plt.xlabel("1250B < Vol UL < 6250B")

plt.ylabel("Frequency")

plt.show()

df.drop('Dur. (ms).1', axis=1, inplace=True)

df.head()










df.head()

numeric_columns = df.select_dtypes(include=['float', 'int'])


numeric_columns = df.select_dtypes(include=['float', 'int']).columns
object_columns  = df.select_dtypes(include=["object"]).columns

for column_name in numeric_columns:

    column_skew = df[column_name].skew().round()



    if column_skew > 0:

        fill_value = df[column_name].median()

    elif column_skew < 0:

        fill_value = df[column_name].median()

    else:

        fill_value = df[column_name].mean()



    df[column_name].fillna(fill_value, inplace=True)


undefined_rows  = df[df['Handset Type'] == 'undefined']

handest_type_mode = df['Handset Type'].mode()[0]

df.loc[undefined_rows.index, 'Handset Type'] = handest_type_mode

all_columns_for_user_analysis = ['MSISDN/Number', 'Handset Type', 'Handset Manufacturer',

    'Bearer Id', 'Dur. (ms)','Total DL (Bytes)','Total UL (Bytes)',

    'Social Media DL (Bytes)','Social Media UL (Bytes)', 'Google DL (Bytes)',

    'Google UL (Bytes)', 'Email DL (Bytes)','Email UL (Bytes)','Youtube DL (Bytes)',

    'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)','Gaming DL (Bytes)',

    'Gaming UL (Bytes)','Other UL (Bytes)','Total UL (Bytes)', 'Other DL (Bytes)']

df_user_analysis = df[all_columns_for_user_analysis]

all_columns_for_user_engagement_analysis = ['MSISDN/Number','Bearer Id','Dur. (ms)','Total DL (Bytes)',

    'Total UL (Bytes)','Social Media DL (Bytes)','Social Media UL (Bytes)',

    'Youtube DL (Bytes)','Youtube UL (Bytes)','Netflix DL (Bytes)','Netflix UL (Bytes)','Google DL (Bytes)','Google UL (Bytes)','Email DL (Bytes)',

    'Email UL (Bytes)','Gaming DL (Bytes)','Gaming UL (Bytes)','Other DL (Bytes)',

    'Other UL (Bytes)']

df_user_engagment_analysis = df[all_columns_for_user_engagement_analysis]

all_columns_needed_for_user_experience_analysis = [

    'MSISDN/Number',

    'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',

    'Avg RTT DL (ms)', 'Avg RTT UL (ms)',

    'Handset Type',

    'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

    'Total DL (Bytes)', 'Total UL (Bytes)',

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

    'Other DL (Bytes)', 'Other UL (Bytes)'

]

df_user_experience_analysis = df[all_columns_needed_for_user_experience_analysis]

file_path = '../data/user_analysis.csv'

df_user_analysis.to_csv(file_path, index=False)

file_path = '../data/user_engagement.csv'

df_user_engagment_analysis.to_csv(file_path, index=False)
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

df = pd.read_csv('../data/user_analysis.csv')

df.head()

handset_counts = df.groupby('Handset Type')['MSISDN/Number'].count()

handset_count_sorted = handset_counts.sort_values(ascending=False)

handset_count_sorted.head(10)
handset_count_sorted.head(10).plot(kind='bar', color='skyblue')

plt.title('Top 10 Handset Types')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.show()

top_3_handset_manufacturers = df["Handset Manufacturer"].value_counts().head(3)

top_3_handset_manufacturers
top_3_handset_manufacturers.plot(kind='bar', color='orange')

plt.title('Top 3 handsets manufacturers')

plt.xlabel('Handset Manufacturers')

plt.ylabel('Count')

plt.show()

top3_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index

df_top3 = df[df['Handset Manufacturer'].isin(top3_manufacturers)]

top5_handsets_per_manufacturer = (

    df_top3.groupby(['Handset Manufacturer', 'Handset Type'])

    ['MSISDN/Number']

    .count()

    .reset_index(name='Count')

    .sort_values(['Handset Manufacturer', 'Count'], ascending=[True, False])

    .groupby('Handset Manufacturer')

    .head(5)

)

top5_handsets_per_manufacturer

xdr_session_per_user = df.groupby(["MSISDN/Number"])["Bearer Id"].count()

xdr_session_per_user

session_duration = df.groupby(["MSISDN/Number"])["Dur. (ms)"].sum()

session_duration

total_data_per_user = df.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum', 

    'Total UL (Bytes)': 'sum'

})

total_data_per_user.columns = ['Total Download (DL)', 'Total Upload (UL)']

total_data_per_user





total_data_per_user_for_each_application = df.groupby('MSISDN/Number').agg({

    'Social Media DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

    

})

total_data_per_user_for_each_application


numeric_columns = df.select_dtypes(include=['float', 'int'])

mean_values = numeric_columns.mean()

median_values = numeric_columns.median()

summary_stats = pd.DataFrame({'Mean': mean_values, 'Median': median_values})

summary_stats

catagorical_columns = df.select_dtypes(include=['object'])

mode_values = catagorical_columns.mode().iloc[0]

mode_values





dispersion_data = pd.DataFrame(index=numeric_columns.columns)

dispersion_data["Range"] = numeric_columns.max() - numeric_columns.min()

dispersion_data["IQR"] = numeric_columns.quantile(0.75) - numeric_columns.quantile(0.25)

dispersion_data['Variance'] = numeric_columns.var()

dispersion_data['Std_Deviation'] = numeric_columns.std()

dispersion_data['Skewness'] = numeric_columns.skew()

dispersion_data


plt.hist(df['Youtube DL (Bytes)'], color="red", edgecolor="black")

plt.title("YouTube data volume (in Bytes) received by the MS during this session")

plt.xlabel("Youtube DL (Bytes)")

plt.ylabel("Frequency")

plt.show()
plt.boxplot(df['Youtube DL (Bytes)'], vert=False)

plt.title('YouTube data volume (in Bytes) received by the MS during this session')

plt.xlabel('Youtube DL (Bytes)')

plt.show()




selected_columns = ['Total DL (Bytes)', 'Total UL (Bytes)', 'Social Media DL (Bytes)', 'Social Media UL (Bytes)']


subset_df = df[selected_columns]




sns.pairplot(subset_df)

plt.show()



correlation_matrix = subset_df.corr()

correlation_matrix

sns.heatmap(correlation_matrix, cmap="RdBu")

plt.title('Correlation Heatmap')

plt.show()
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

df = pd.read_csv('../data/user_analysis.csv')

df.head()

session_frequency_per_customer = df.groupby('MSISDN/Number')['Bearer Id'].count().reset_index(name='Sessions Frequency')

top_10_session_frequency =  session_frequency_per_customer.sort_values(by='Sessions Frequency',ascending=False).head(10)

top_10_session_frequency
plt.scatter(top_10_session_frequency["MSISDN/Number"], top_10_session_frequency["Sessions Frequency"])

plt.xlabel("Customer ID")

plt.ylabel("Sessions Frequency")

plt.title("Top 10 Customers Based on Session Frequency")

plt.show()

session_duration_per_customer = df.groupby('MSISDN/Number')['Dur. (ms)'].sum().reset_index(name='Session Duration')

top_10_session_duration = session_duration_per_customer.sort_values(by="Session Duration", ascending=False).head(10)

top_10_session_duration
plt.scatter(top_10_session_duration["MSISDN/Number"], top_10_session_duration['Session Duration'])

plt.xlabel("Customer ID")

plt.ylabel("Sessions Duration")

plt.title("Top 10 Customers Based on Session Duration")

plt.show()

total_traffic_per_customer = df.groupby('MSISDN/Number').agg({

    'Total DL (Bytes)': 'sum',

    'Total UL (Bytes)': 'sum'

}).reset_index()

total_traffic_per_customer['Total Traffic (Bytes)'] = total_traffic_per_customer['Total DL (Bytes)'] 

+ total_traffic_per_customer['Total UL (Bytes)']

top_10_customer_total_traffic = total_traffic_per_customer.sort_values(by="Total UL (Bytes)", ascending=False).head(10)

top_10_customer_total_traffic
plt.scatter(top_10_customer_total_traffic['MSISDN/Number'], top_10_customer_total_traffic["Total Traffic (Bytes)"])

plt.xlabel("Custome ID")

plt.ylabel("Total Traffic (Bytes)")

plt.title("Top 10 Customers Based on total traffic")

plt.show()

df["Total Traffic (Bytes)"] = df["Total DL (Bytes)"] + df["Total UL (Bytes)"]


columns_to_normalize = ["Bearer Id", "Dur. (ms)", "Total Traffic (Bytes)"]

engagement_metrics = df[columns_to_normalize]

scaler = StandardScaler()

engagement_metrics_normalized = scaler.fit_transform(engagement_metrics)

df_normalized = pd.DataFrame(engagement_metrics_normalized, columns=columns_to_normalize)





kmeans = KMeans(n_clusters=3, random_state=42)

df_normalized['Cluster'] = kmeans.fit_predict(engagement_metrics_normalized)

df['Cluster']  = df_normalized['Cluster']

plt.scatter(df_normalized['Dur. (ms)'], df_normalized['Total Traffic (Bytes)'], c=df_normalized['Cluster'], cmap='rainbow')

plt.xlabel('Duration of the Session (ms)')

plt.ylabel('Total Traffic (Bytes)')

plt.title('K-Means Clustering of Customers based on Engagement Metrics')

plt.show()

clustered_data = df.groupby('Cluster')

cluster_metrics_for_duration = clustered_data["Bearer Id"].agg(['min', 'max', 'mean', 'sum'])

cluster_metrics_for_duration
cluster_metrics_for_duration.plot(kind='bar', title="Cluster for session")

plt.ylabel('Session')

plt.xlabel('Cluster')

plt.show()
cluster_metrics_for_session = clustered_data["Dur. (ms)"].agg(['min', 'max', 'mean', 'sum'])

cluster_metrics_for_duration
cluster_metrics_for_session.plot(kind='bar', title="Cluster for duration")

plt.ylabel('Duration')

plt.xlabel('Cluster')

plt.show()
cluster_metrics_for_traffic = clustered_data["Total Traffic (Bytes)"].agg(['min', 'max', 'mean', 'sum'])

cluster_metrics_for_traffic
cluster_metrics_for_session.plot(kind='bar', title="Cluster for Total Traffic (Bytes)")

plt.ylabel('Total Traffic')

plt.xlabel('Cluster')

plt.show()

application_columns = ['Social Media', 'Youtube', 'Netflix', 'Google', 'Email', 'Gaming', 'Other']

for app in application_columns:

    df[f'Total {app} Traffic (Bytes)'] = df[f'{app} DL (Bytes)'] + df[f'{app} UL (Bytes)']

user_total_traffic_per_app = df.groupby('MSISDN/Number')[[f'Total {app} Traffic (Bytes)' for app in application_columns]].sum()

df


last_7_columns = df.iloc[:, -7:]

column_sums = last_7_columns.sum()

sum_df = pd.DataFrame({'Column': column_sums.index, 'Sum': column_sums.values})

top_3_application = sum_df.sort_values(by='Sum', ascending=False).head(3)

top_3_application

%reload_ext autoreload

%autoreload 2

import missingno as msno 

import matplotlib.pyplot as plt 

import pandas as pd

from sqlalchemy import create_engine

import sys 

import os 

import numpy as np 

import seaborn as sns 

import sys, os

import sklearn

from sklearn.decomposition import PCA

import logging

from scipy.stats.mstats import winsorize





current_directory = os.getcwd()

parent_directory = os.path.abspath(os.path.join(current_directory, '..'))



if parent_directory not in sys.path:

    sys.path.insert(0, parent_directory)





from src.utils import percent_missing, format_float, find_agg, missing_values_table,convert_bytes_to_megabytes,fix_missing_ffill,fix_missing_bfill







database_name = 'tcom'

table_name= 'xdr_data'



connection_params = { "host": "localhost", "user": "postgres", "password": "1234",

                    "port": "5432", "database": database_name}



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")




sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query, con= engine)

df.head()
df.columns





df.info()
df.describe().T
df.shape
df.isnull().sum()

columns = []

counts=[]

i=0

for key, item in df.isnull().sum().items():

    if item != 0:

        columns.append(key)

        counts.append(item)

        i+=1

print('the dataset has {} columns with missing values'.format(i))

pd.DataFrame({'column name':columns,'counts':counts})
msno.bar(df)
msno.matrix(df)
msno.heatmap(df)
msno.dendrogram(df)
totalCells, missingCount, totalMissing = percent_missing(df)

print("The Telcom dataset contains", round(

    ((totalMissing/totalCells) * 100), 2), "%", "missing values.")
mis_val_table_ren_columns = missing_values_table(df)

mis_val_table_ren_columns
class MissingInformation:

    def __init__(self,df:pd.DataFrame):

        self.df = df

        logging.basicConfig(filename='../logfile.log', filemode='a',

                            encoding='utf-8', level=logging.DEBUG)

        

    def missing_values_table(self,df:pd.DataFrame)->pd.DataFrame:

        
        mis_val = df.isnull().sum()



        
        mis_val_percent = 100 * df.isnull().sum() / len(df)



        
        mis_val_dtype = df.dtypes



        
        mis_val_table = pd.concat(

            [mis_val, mis_val_percent, mis_val_dtype], axis=1)



        
        mis_val_table_ren_columns = mis_val_table.rename(

            columns={0: 'Missing Values', 1: '% of Total Values', 2: 'Dtype'})



        
        mis_val_table_ren_columns = mis_val_table_ren_columns[

            mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(

            '% of Total Values', ascending=False).round(1)



        
     

        logging.info("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"

                         "There are " + str(mis_val_table_ren_columns.shape[0]) +

                         " columns that have missing values.")



        
        return mis_val_table_ren_columns



    def percent_missing(self,df:pd.DataFrame):



        
        totalCells = np.product(df.shape)



        
        missingCount = df.isnull().sum()



        
        totalMissing = missingCount.sum()



        return totalCells, missingCount, totalMissing
class DataFrameInformation:

    

    def __init__(self,data:pd.DataFrame):

        self.data = data

        logging.basicConfig(filename='../logfile.log', filemode='a',

                            encoding='utf-8', level=logging.DEBUG)

        

    
    def get_skewness(self,data:pd.DataFrame):

        skewness = data.skew(axis=0, skipna=True)

        df_skewness = pd.DataFrame(skewness)

        df_skewness = df_skewness.rename(

            columns={0: 'skewness'})

        

        return df_skewness



    
    def get_skewness_missing_count(self,data:pd.DataFrame):

        df_skewness = self.get_skewness(data)

        minfo = MissingInformation(data)

        

        mis_val_table_ren_columns = minfo.missing_values_table(data)

        df1 = pd.concat([df_skewness, mis_val_table_ren_columns], axis=1)

        df1['Dtype'] = df1['Dtype'].fillna('float64')

        df1['% of Total Values'] = df1['% of Total Values'].fillna(0.0)

        df1['Missing Values'] = df1['Missing Values'].fillna(0)

        df1 = df1.sort_values(by='Missing Values', ascending=False)

        return df1



    def get_column_with_string(self,df: pd.DataFrame, text):

        return [col for col in df.columns if re.findall(text, col) != []]



    def get_dataframe_information(self,df: pd.DataFrame):

        columns = []

        counts = []

        i = 0



        for key, item in df.isnull().sum().items():

            if item != 0:

                columns.append(key)

                counts.append(item)

                i += 1

        logging.info(

            'the dataset contain {} columns with missing values'.format(i))

        return pd.DataFrame({'column name': columns, 'counts': counts})



numeric_columns = df.select_dtypes(include=np.number)

skewness = numeric_columns.skew(axis=0, skipna=True)




df_skewness = pd.DataFrame({'skewness': skewness})

df_skewness

df_skewness.plot(kind='bar')
d_f = pd.concat([df_skewness, mis_val_table_ren_columns], axis=1)

d_f['Dtype'] = d_f['Dtype'].fillna('float64')

d_f['% of Total Values'] = d_f['% of Total Values'].fillna(0.0)

d_f['Missing Values'] = d_f['Missing Values'].fillna(0)

d_f.sort_values(by='Missing Values', ascending=False)


df['Nb of sec with Vol UL < 1250B'].plot(kind='kde')

i = 0

for key, item in df.isnull().sum().items():

    if item==0:

        i+=1

        print(key)

print('the dataset contain {} columns with no missing values'.format(i))

columns = []

counts=[]

i=0

for key, item in df.isnull().sum().items():

    if item != 0:

        columns.append(key)

        counts.append(item)

        i+=1

print('the dataset contain {} columns with missing values'.format(i))

pd.DataFrame({'column name':columns,'counts':counts})
from scipy.stats.mstats import winsorize

class CleanData:

    def __init__(self,df:pd.DataFrame):

        self.df = df

        logging.basicConfig(filename='../logfile.log', filemode='a',

                            encoding='utf-8', level=logging.DEBUG)

    

    def convert_dtype(self, df: pd.DataFrame, columns, dtype):

        for col in columns:

            df[col] = df[col].astype(dtype=dtype)

        return df

    

    def format_float(self,value):

        return f'{value:,.2f}'



    def convert_bytes_to_megabytes(self, df:pd.DataFrame, columns):



        megabyte = 1*10e+5

        for col in columns:

            df[col] = df[col] / megabyte

        return df

    

    def convert_ms_to_sec(self, df:pd.DataFrame, columns): 

        s = 10e+3

        for col in columns:

            df[col] = df[col] / s

        return df 

    

    def fix_missing_ffill(self, df: pd.DataFrame,col):

        df[col] = df[col].fillna(method='ffill')

        return df[col]

  

    def fix_missing_bfill(self, df: pd.DataFrame, col):

        df[col] = df[col].fillna(method='bfill')

        return df[col]

    

    def drop_column(self, df: pd.DataFrame, columns) -> pd.DataFrame:

        for col in columns:

            df = df.drop([col], axis=1)

        return df



    def drop_missing_count_greaterthan_20p(self,data:pd.DataFrame):

        data_info = DataFrameInformation(data)

        df = data_info.get_skewness_missing_count(data)

        not_fill = df[(df['% of Total Values'] >= 20.0)].index.tolist()

        df_clean = self.drop_column(data, not_fill)

         

        return df_clean

    

    def fill_mode(self, df: pd.DataFrame, columns) -> pd.DataFrame:

        for col in columns:

            df[col] = df[col].fillna(df[col].mode()[0])

        return df



    def fix_outlier(self,df:pd.DataFrame, columns):

        for column in columns:

            df[column] = np.where(df[column] > df[column].quantile(0.95), df[column].median(), df[column])

            

        return df



    def handle_outliers(self, df: pd.DataFrame,lower,upper):

       

        selected_columns = df.select_dtypes(include='float64').columns

        for col in selected_columns:

            df[col] = winsorize(df[col], (lower, upper))

        return df
df_cleaned = df.copy()



data_cleaner = CleanData(df_cleaned)


df_cleaned['Handset Manufacturer'] = df_cleaned['Handset Manufacturer'].fillna(

    df_cleaned['Handset Manufacturer'].mode()[0])

df_cleaned['Handset Type'] = df_cleaned['Handset Type'].fillna(

    df_cleaned['Handset Type'].mode()[0])

df_cleaned['Last Location Name'] = df_cleaned['Last Location Name'].fillna(

    df_cleaned['Last Location Name'].mode()[0])

df_cleaned['End'] = fix_missing_ffill(df_cleaned, 'End')

df_cleaned['Start'] = fix_missing_ffill(df_cleaned, 'Start')







drop_column = mis_val_table_ren_columns[mis_val_table_ren_columns['% of Total Values']>15].index

print(drop_column.tolist())



df_cleaned= df.drop(drop_column.tolist(),axis=1)

df_cleaned.head()
df_cleaned['Nb of sec with Vol DL < 6250B'] = fix_missing_bfill(

    df_cleaned, 'Nb of sec with Vol DL < 6250B')





missing_percentage = (df.isnull().sum() / len(df)) * 100




fill_mode = missing_percentage[(missing_percentage < 20.0) & (missing_percentage >= 0.4)].index.tolist()

not_fill_mode = ['IMEI', 'IMSI', 'MSISDN/Number']



fill_mode_columns = [x for x in fill_mode if x not in not_fill_mode]




columns_to_fill = [col for col in fill_mode_columns if col in df_cleaned.columns]




df_cleaned[columns_to_fill] = df_cleaned[columns_to_fill].fillna(df_cleaned[columns_to_fill].mode().iloc[0])








print(df_cleaned[columns_to_fill].isnull().sum())  def cap_outliers(series):

    q1 = series.quantile(0.25)

    q3 = series.quantile(0.75)

    iqr = q3 - q1

    lower_bound = q1 - 1.5 * iqr

    upper_bound = q3 + 1.5 * iqr

    return series.clip(lower=lower_bound, upper=upper_bound)
missing_info = MissingInformation(df_cleaned)

mis_val_table_after_clean = missing_info.missing_values_table(df_cleaned)

mis_val_table_after_clean




df_cleaned.dropna(inplace=True)

mis_val_table_after_clean = missing_info.missing_values_table(df_cleaned)

mis_val_table_after_clean



df_cleaned.info()
df_cleaned.shape







handset_counts = df_cleaned['Handset Type'].value_counts()



sorted_handsets = handset_counts.sort_values(ascending=False)



top_10_handsets = sorted_handsets.head(10)








plt.figure(figsize=(10,6))

top_10_handsets.plot(kind='bar')

plt.title('Top 10 handsets used by customers')

plt.xlabel('handset type')

plt.ylabel('frequency')

plt.xticks(rotation=45)

plt.show()






manu_counts = df_cleaned['Handset Manufacturer'].value_counts()




top_3_manufacturers = manu_counts.head(3)




plt.figure(figsize=(8,6))

top_3_manufacturers.plot(kind='bar')

plt.title('Top 3 Handset Manufacturers')

plt.xlabel('Manufacturer')

plt.ylabel('Frequency')

plt.xticks(rotation=0)

plt.show()










manufacturer_counts = df_cleaned['Handset Manufacturer'].value_counts()



top_3_manufacturers = manufacturer_counts.head(3).index



filtered_df = df_cleaned[df_cleaned['Handset Manufacturer'].isin(top_3_manufacturers)]



handset_counts = filtered_df.groupby(['Handset Manufacturer','Handset Type']).size()



top_5_handsets = handset_counts.groupby('Handset Manufacturer').nlargest(5)





top_5_handsets.unstack().plot(kind='bar', figsize=(10,6))

plt.title('top 5 handsets per manufacturer')

plt.xlabel('manufacturer')

plt.ylabel('freqency')

plt.xticks(rotation=0)

plt.legend(title = 'handset model')

plt.show()
handset_man= df_cleaned[df_cleaned['Handset Manufacturer'].isin(['Apple','Sumsung','Huawei'])]

handset_man.groupby('Handset Manufacturer')['Handset Type'].value_counts()[:10].plot.bar(

    figsize=(12, 10), fontsize=15)




aggregated_data = df_cleaned.groupby('MSISDN/Number').agg({

    'Dur. (ms)': 'count',  
    'Dur. (ms)': 'sum',  
    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum',

    'Social Media DL (Bytes)': 'sum',

    'Social Media UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum',

    'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum',

    'Email UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum',

    'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum',

    'Netflix UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum',

    'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum',

    'Other UL (Bytes)': 'sum'

}).reset_index()

aggregated_data.head()  



dispersion_params = df_cleaned[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)', 

                          'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

                          'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].std()

dispersion_params
df_cleaned[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)', 

      'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

      'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].hist(figsize=(10, 10))

plt.tight_layout()

plt.show()

app_vs_total_data = df_cleaned[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)', 

                          'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

                          'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].corr()

app_vs_total_data


df_cleaned['Total_Session_duration'] = df_cleaned['Dur. (ms)'].groupby(df_cleaned['MSISDN/Number']).transform('sum')

df_cleaned['Decile_Class'] = pd.qcut(df_cleaned['Total_Session_duration'], q=5, labels=False)




total_data_per_decile = df_cleaned.groupby('Decile_Class')[['Total UL (Bytes)', 'Total DL (Bytes)']].sum()

total_data_per_decile

correlation_matrix = df_cleaned[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 

                           'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 

                           'Other DL (Bytes)']].corr()

correlation_matrix






pca_data = df_cleaned[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 

                 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 

                 'Other DL (Bytes)']]




pca_data_standardized = (pca_data - pca_data.mean()) / pca_data.std()




pca = PCA(n_components=2)

pca.fit(pca_data_standardized)




components = pca.components_

explained_variance = pca.explained_variance_ratio_



print("Principal Components:")

print(components)

print("Explained Variance Ratio:")

print(explained_variance)
import streamlit as st

st.title('User analytics in Telecommunication industry \n this is my dashboard to explain eda on telecommunication')

st.header('This is a header')
st.write("Welcome to my Streamlit Dashboard!")
%reload_ext autoreload

%autoreload 2

import missingno as msno 

import matplotlib.pyplot as plt 

import pandas as pd

from sqlalchemy import create_engine

import sys 

import os 

import numpy as np 

import seaborn as sns 

import sys, os

import sklearn

from sklearn.decomposition import PCA

import logging

from scipy.stats.mstats import winsorize

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans



df_cleaned = pd.read_csv('/home/hp/10academy/10telecom/useranalyst.csv')






aggregated_data = df_cleaned.groupby('MSISDN/Number').agg({

    'Dur. (ms)': 'count',  
    'Dur. (ms)': 'sum',  
    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum',

    

}).reset_index()

aggregated_data.head()  




import pandas as pd

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans








import pandas as pd

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans







customer_engagement = df_cleaned.groupby('MSISDN/Number').agg({

    'Dur. (ms)': ['count', 'sum'],  
    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum',

})




customer_engagement.columns = ['SessionCount', 'TotalDuration', 'Total UL (Bytes)', 'Total DL (Bytes)']




print(customer_engagement.head(10))




normalized_engagement = (customer_engagement - customer_engagement.min()) / (customer_engagement.max() - customer_engagement.min())




top_10_engaged_customers = {

    'Dur. (ms) Count': normalized_engagement.nlargest(10, 'SessionCount').index,

    'Dur. (ms) Sum': normalized_engagement.nlargest(10, 'TotalDuration').index,

    'Total UL (Bytes)': normalized_engagement.nlargest(10, 'Total UL (Bytes)').index,

    'Total DL (Bytes)': normalized_engagement.nlargest(10, 'Total DL (Bytes)').index

}

top_10_sessioncount_engaged_customers = normalized_engagement.nlargest(10, 'SessionCount').index




print(top_10_sessioncount_engaged_customers)


print("Top 10 Customers per Engagement Metric:")

print(top_10_engaged_customers)


















session_frequency = df_cleaned['MSISDN/Number'].value_counts()

session_frequency

average_session_duration = df_cleaned.groupby('MSISDN/Number')['Dur. (ms)'].mean()

average_session_duration

df_cleaned['Total_Traffic'] = df_cleaned['Total UL (Bytes)'] + df_cleaned['Total DL (Bytes)']

total_traffic_per_user = df_cleaned.groupby('MSISDN/Number')['Total_Traffic'].sum()

total_traffic_per_user


engagement_metrics = pd.DataFrame({

    'Session_Frequency': session_frequency,

    'Average_Session_Duration': average_session_duration,

    'Total_Traffic': total_traffic_per_user

}).reset_index()




engagement_metrics.head()



top_10_session_frequency = engagement_metrics.nlargest(10, 'Session_Frequency')

top_10_session_frequency
top_10_avg_duration = engagement_metrics.nlargest(10, 'Average_Session_Duration')

top_10_avg_duration
top_10_total_traffic = engagement_metrics.nlargest(10, 'Total_Traffic')

top_10_total_traffic

scaler = MinMaxScaler()

normalized_metrics = scaler.fit_transform(engagement_metrics.drop('MSISDN/Number', axis=1))

normalized_metrics

kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)

engagement_metrics['Cluster'] = kmeans.fit_predict(normalized_metrics)


cluster_stats = engagement_metrics.groupby('Cluster').agg({

    'Session_Frequency': ['min', 'max', 'mean', 'sum'],

    'Average_Session_Duration': ['min', 'max', 'mean', 'sum'],

    'Total_Traffic': ['min', 'max', 'mean', 'sum']

}).reset_index()

cluster_stats

app_traffic = df_cleaned.groupby('MSISDN/Number')[['Social Media DL (Bytes)', 'Google DL (Bytes)', 

                                             'Email DL (Bytes)', 'Youtube DL (Bytes)',

                                             'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 

                                             'Other DL (Bytes)']].sum()

app_traffic

app_traffic['Total_App_Traffic'] = app_traffic.sum(axis=1)

top_10_social_media = app_traffic.nlargest(10, 'Social Media DL (Bytes)')

top_10_social_media
top_10_google = app_traffic.nlargest(10, 'Google DL (Bytes)')

top_10_google
top_10_youtube = app_traffic.nlargest(10, 'Youtube DL (Bytes)')

top_10_youtube 

total_app_traffic = app_traffic.drop('Total_App_Traffic', axis=1).sum()




top_3_apps = total_app_traffic.nlargest(3)

top_3_apps.plot(kind='bar', xlabel='Applications', ylabel='Total Traffic', title='Top 3 Most Used Applications')

plt.show()

inertia_values = []

for k in range(1, 11):

    kmeans = KMeans(n_clusters=k, random_state=42)

    kmeans.fit(normalized_metrics)

    inertia_values.append(kmeans.inertia_)




plt.plot(range(1, 11), inertia_values, marker='o')

plt.xlabel('Number of Clusters (k)')

plt.ylabel('Inertia')

plt.title('Elbow Method for Optimal k')

plt.show()
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from streamlit_card import card
import psycopg2

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
from sklearn.metrics import euclidean_distances

from src.dbconn import connect_db
from preprocessing.handle_missing_val import fill_missing_with_mode, fill_missing_with_mean
from preprocessing.user_aggregation import aggregate_user
from preprocessing.user_analysis import plot_top_handsets, plot_top_manufacturers ,plot_top_handsets_per_manufacturer, plot_top_app_usage

st.set_page_config(
    page_title="Telecom Dashboard",
    page_icon=":bar_chart:",
    layout="wide"
)

df= pd.read_csv('my_db.csv')

df.head()

category_data = ['Start','End','Last Location Name', 'Handset Manufacturer','Handset Type']
category_data1 = df[['Start','End','Last Location Name', 'Handset Manufacturer','Handset Type']]
fill_missing_with_mode(category_data1, category_data)




df_fill = df.drop(['Start','End','Last Location Name', 'Handset Manufacturer','Handset Type'], axis=1)

df_fill1 = df_fill.drop(['Nb of sec with 125000B < Vol DL', 'Nb of sec with 1250B < Vol UL < 6250B','Nb of sec with 31250B < Vol DL < 125000B','Nb of sec with 37500B < Vol UL','Nb of sec with 6250B < Vol DL < 31250B','Nb of sec with 6250B < Vol UL < 37500B','HTTP DL (Bytes)','HTTP UL (Bytes)'], axis=1) 

df_fill_numeric = fill_missing_with_mean(df_fill1)

df = pd.concat([df_fill_numeric, category_data1], axis=1, join="inner")



applications_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',
                         'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',
                         'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)',
                         'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']

total_data_per_application = pd.DataFrame({
    'Application': applications_columns,
    'Total_Data_Volume': df[applications_columns].sum()
})

most_used_application = total_data_per_application.loc[total_data_per_application['Total_Data_Volume'].idxmax()]

app = most_used_application['Application']
gb = (((most_used_application['Total_Data_Volume'])/1024)/1024)/1024


st.title(":bar_chart: Telecom Dashboard")
st.markdown("
total_DL = int(df["Total DL (Bytes)"].sum())
total_UL = int(df["Total UL (Bytes)"].sum())
left_column, middle_column, right_column = st.columns(3)
with left_column:
    st.subheader("Total Download")
    st.subheader(f"{round(((total_DL/1024)/1024)/1024,2)} GB")
with middle_column:
    st.subheader("Total Upload")
    st.subheader(f"GB {round(((total_UL/1024)/1024)/1024,2)}")
with right_column:
    st.subheader(f"Application more used: {app}")
    st.subheader(f"GB {round(gb,2)}")

st.markdown("---")

def style_axes(ax):
    ax.set_facecolor('none')      ax.grid(False)      return ax







top_manufacturer_handsets = df.groupby('Handset Manufacturer')['Handset Type'].value_counts().groupby(level=0, group_keys=False).nlargest(5)

top_manufacturer_handsets_df = top_manufacturer_handsets.reset_index(name='Count')


fig = px.bar(
        top_manufacturer_handsets_df,
        x='Handset Type',
        y='Count',
        color='Handset Manufacturer',
        title='Top 5 Handsets per Top 3 Manufacturers',
        labels={'Handset Type': 'Handset Type', 'Count': 'Count'},
        category_orders={'Handset Type': top_manufacturer_handsets_df['Handset Type'].value_counts().head(5).index},
    )

fig.update_layout(
        barmode='group',
        xaxis=dict(title='Handset Type'),
        yaxis=dict(title='Count'),
        legend=dict(title='Handset Manufacturer', orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),
    )

top_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index.tolist()
top_handsets = df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')
top_handsets = top_handsets[top_handsets['Handset Manufacturer'].isin(top_manufacturers)]
top_handsets = top_handsets.sort_values(by='count', ascending=False).groupby('Handset Manufacturer').head(5)

fig_top_handsets = px.bar(top_handsets, x='Handset Type', y='count', color='Handset Manufacturer',
                          title='Top 5 Handsets per Top 3 Manufacturers')
fig_top_handsets.update_layout(xaxis_title='Handset Type', yaxis_title='Count', barmode='group')




left_column, middle_column, right_column = st.columns(3)
with left_column:
            fig_handsets = plot_top_handsets(df)
    st.plotly_chart(fig_handsets)
with middle_column:
        fig_manufacturers= plot_top_manufacturers(df)
    st.plotly_chart(fig_manufacturers)
with right_column:
            figmh = plot_top_handsets_per_manufacturer(df)
    st.plotly_chart(figmh)
    
st.markdown("---")


left_column, middle_column, right_column = st.columns(3)
with left_column:
            st.subheader("Top xDR Sessions & Duration Per User")
        user_session_duration = df.groupby('MSISDN/Number')['Dur. (ms)'].sum().reset_index()

        user_session_duration.columns = ['MSISDN/Number', 'Total_Session_Duration']

        st.table(user_session_duration.head(10))
with middle_column:
    st.subheader("Top DL and UL Per User")
    user_total_data = df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index()

        user_total_data.columns = ['MSISDN/Number', 'Total_DL_Data', 'Total_UL_Data']

        st.table(user_total_data.head(10))
with right_column:
            plot_top_app_usage = plot_top_app_usage(df)
    st.plotly_chart(plot_top_app_usage)
    
st.markdown("---")

left_column, middle_column, right_column = st.columns(3)
with left_column:
    st.subheader("Session Frequency")
    sessions_df = df[['MSISDN/Number', 'Start', 'End']]

        sessions_df['Start'] = pd.to_datetime(sessions_df['Start'])
    sessions_df['End'] = pd.to_datetime(sessions_df['End'])

        sessions_df['Session_Duration_Minutes'] = (sessions_df['End'] - sessions_df['Start']).dt.total_seconds() / 60

        session_frequency = sessions_df.groupby('MSISDN/Number')['Start'].count().reset_index()
    session_frequency.columns = ['MSISDN/Number', 'Session_Frequency']

        

        st.table(session_frequency.sort_values(by="Session_Frequency", ascending=False).head(10))
with middle_column:
    sessions_df = df[['MSISDN/Number', 'Start', 'End']]

        sessions_df['Start'] = pd.to_datetime(sessions_df['Start'])
    sessions_df['End'] = pd.to_datetime(sessions_df['End'])

        sessions_df['Session_Duration_Minutes'] = (sessions_df['End'] - sessions_df['Start']).dt.total_seconds() / 60

        fig = px.histogram(sessions_df, x='Session_Duration_Minutes', nbins=30,
                    title='Session Duration Distribution',
                    labels={'Session_Duration_Minutes': 'Session Duration (Minutes)', 'count': 'Number of Sessions'},
                    color_discrete_sequence=['skyblue'])

        st.plotly_chart(fig)
with right_column:
    sessions_df = df[['MSISDN/Number', 'Total DL (Bytes)', 'Total UL (Bytes)']]

        sessions_df['Total_Traffic_Bytes'] = sessions_df['Total DL (Bytes)'] + sessions_df['Total UL (Bytes)']

        fig = px.histogram(sessions_df, x='Total_Traffic_Bytes', nbins=30,
                    title='Total Traffic Distribution',
                    labels={'Total_Traffic_Bytes': 'Total Traffic (Bytes)', 'count': 'Number of Sessions'},
                    color_discrete_sequence=['skyblue'])

        st.plotly_chart(fig)
    
st.markdown("---")

st.subheader("Aggregate network parameters per customer")
@st.cache_data
def aggre_user():
    network_data = df[['MSISDN/Number', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',
                    'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Handset Type', 'Avg Bearer TP DL (kbps)',
                    'Avg Bearer TP UL (kbps)']]

            user_aggregated_network = network_data.groupby('MSISDN/Number').agg({
            'TCP DL Retrans. Vol (Bytes)': lambda x: x.mean(),
            'TCP UL Retrans. Vol (Bytes)': lambda x: x.mean(),
            'Avg RTT DL (ms)': lambda x: x.mean(),
            'Avg RTT UL (ms)': lambda x: x.mean(),
            'Handset Type': lambda x: x.mode().iloc[0] if not x.mode().empty else None,
            'Avg Bearer TP DL (kbps)': lambda x: x.mean(),
            'Avg Bearer TP UL (kbps)': lambda x: x.mean()
        })

            user_aggregated_network = user_aggregated_network.rename(columns={
            'TCP DL Retrans. Vol (Bytes)': 'Avg_TCP_Retrans_DL',
            'TCP UL Retrans. Vol (Bytes)': 'Avg_TCP_Retrans_UL',
            'Avg RTT DL (ms)': 'Avg_RTT_DL',
            'Avg RTT UL (ms)': 'Avg_RTT_UL',
            'Avg Bearer TP DL (kbps)': 'Avg_Throughput_DL',
            'Avg Bearer TP UL (kbps)': 'Avg_Throughput_UL'
        })
    return user_aggregated_network
user_aggregated_network = aggre_user()
st.table(user_aggregated_network.head())


st.markdown("---")



left_column, middle_column, right_column = st.columns(3)
with left_column:
        experience_metrics = df[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',
                            'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']]

        experience_metrics = experience_metrics.fillna(experience_metrics.mean())

        scaler = StandardScaler()
    scaled_data = scaler.fit_transform(experience_metrics)

        kmeans = KMeans(n_clusters=3, random_state=42)
    df['Cluster'] = kmeans.fit_predict(scaled_data)

        cluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=experience_metrics.columns)

        st.subheader('Original Data for Clustering')
    st.table(experience_metrics.head())
with middle_column:
        st.subheader('Cluster Assignments')
    st.table(df[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',
                                'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'Cluster']].head())
with right_column:
        st.subheader('Cluster Centers')
    st.table(cluster_centers.head())


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import pairwise_distances_argmin_min

engagement_experience_metrics = df[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',
                                    'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']]

engagement_experience_metrics = engagement_experience_metrics.fillna(engagement_experience_metrics.mean())

scaler = StandardScaler()
scaled_metrics = scaler.fit_transform(engagement_experience_metrics)

engagement_scores = pairwise_distances_argmin_min(scaled_metrics, kmeans.cluster_centers_[0].reshape(1, -1))[1]
experience_scores = pairwise_distances_argmin_min(scaled_metrics, cluster_centers.iloc[cluster_centers['Avg Bearer TP DL (kbps)'].idxmin()].values.reshape(1, -1))[1]

df['Engagement Score'] = engagement_scores
df['Experience Score'] = experience_scores

df['Satisfaction Score'] = df[['Engagement Score', 'Experience Score']].mean(axis=1)


features = ['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',
            'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']

df[features] = df[features].fillna(df[features].mean())

X_train, X_test, y_train, y_test = train_test_split(df[features], df['Satisfaction Score'], test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LinearRegression()

model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


engagement_experience_scores = df[['Engagement Score', 'Experience Score']]

engagement_experience_scores = engagement_experience_scores.fillna(engagement_experience_scores.mean())

scaler = StandardScaler()
scaled_scores = scaler.fit_transform(engagement_experience_scores)


kmeans_2 = KMeans(n_clusters=2, random_state=42)
df['Cluster_2'] = kmeans_2.fit_predict(scaled_scores)


cluster_aggregated_scores = df.groupby('Cluster_2').agg({
    'Satisfaction Score': 'mean',
    'Experience Score': 'mean'
}).reset_index()

st.subheader("Aggregate of average Engagement, Experience and Satisfaction score per user")
st.table(df[['MSISDN/Number', 'Engagement Score', 'Experience Score', 'Satisfaction Score']].sort_values(by='Satisfaction Score',ascending=False).head())
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt
import pandas as pd

from sqlalchemy import create_engine



database_name = 'week1'

table_name = 'xdr_data'



connection_params = {

    "host": "localhost",

    "user": "postgres",

    "password": "lhtdss",

    "port": "5432",

    "database": database_name

}



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")




sql_query = 'SELECT * FROM xdr_data '



telecom = pd.read_sql(sql_query, con=engine)

telecom.head(7)

telecom.columns.tolist()


print(f" There are {telecom.shape[0]} rows and {telecom.shape[1]} columns")

def percent_missing(m):



    
    totalCells = np.prod(m.shape)



    
    missingCount = m.isnull().sum()



    
    totalMissing = missingCount.sum()



    
    print("The Diabetes dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")



percent_missing(df)

telecom.isna().sum() 

telecom[[ 

 'Bearer Id',

 'Handset Manufacturer',

 'Handset Type', 'Start',

 'Start ms',

 'End',

 'End ms',

 'Dur. (ms)',

 'Social Media DL (Bytes)',

 'Social Media UL (Bytes)',

 'Google DL (Bytes)',

 'Google UL (Bytes)',

 'Email DL (Bytes)',

 'Email UL (Bytes)',

 'Youtube DL (Bytes)',

 'Youtube UL (Bytes)',

 'Netflix DL (Bytes)',

 'Netflix UL (Bytes)',

 'Gaming DL (Bytes)',

 'Gaming UL (Bytes)',

 'Other DL (Bytes)',

 'Other UL (Bytes)',

 'Total UL (Bytes)',

 'Total DL (Bytes)' ]].isna().sum()



telecom.nunique()
telecom.dtypes

telecom[[ 

 'Bearer Id',

 'Handset Manufacturer',

 'Handset Type', 'Start',

 'Start ms',

 'End',

 'End ms',

 'Dur. (ms)',

 'Social Media DL (Bytes)',

 'Social Media UL (Bytes)',

 'Google DL (Bytes)',

 'Google UL (Bytes)',

 'Email DL (Bytes)',

 'Email UL (Bytes)',

 'Youtube DL (Bytes)',

 'Youtube UL (Bytes)',

 'Netflix DL (Bytes)',

 'Netflix UL (Bytes)',

 'Gaming DL (Bytes)',

 'Gaming UL (Bytes)',

 'Other DL (Bytes)',

 'Other UL (Bytes)',

 'Total UL (Bytes)',

 'Total DL (Bytes)' ]].head(20)

def plot_top_values_histogram(column, top_n=10):

    
    value_counts = column.value_counts()



    
    top_values = value_counts.head(top_n)



    
    plt.bar(top_values.index, top_values.values)

    

    if top_n >= 5:

        plt.xticks(rotation=45, ha='right')



    
    plt.xlabel('Unique Values')

    plt.ylabel('Count')

    plt.title(f'Histogram of Top {top_n} Values Counts for {column.name}')



    
    plt.show()



telecom[["Handset Manufacturer", "Handset Type"]].nunique()
value_counts = telecom["Handset Type"].value_counts()


top_ten_values = value_counts.head(10)

print(top_ten_values)
plot_top_values_histogram(telecom['Handset Type'], top_n=10)
value_counts = telecom["Handset Manufacturer"].value_counts()

top_three_values = value_counts.head(3)


print(top_three_values)
plot_top_values_histogram(telecom['Handset Manufacturer'], top_n=3)














manufacturer_column = 'Handset Manufacturer'

handset_column = 'Handset Type'




grouped_data = telecom.groupby(manufacturer_column)[handset_column].value_counts()




top_manufacturers = telecom[manufacturer_column].value_counts().head(3).index




top_handsets_per_manufacturer = {}




for manufacturer in top_manufacturers:

    
    top_handsets = grouped_data[manufacturer].head(5)

    top_handsets_per_manufacturer[manufacturer] = top_handsets




for manufacturer, top_handsets in top_handsets_per_manufacturer.items():

    print(f"Top 5 handsets for {manufacturer}:\n{top_handsets}\n")










def plot_hist(df:pd.DataFrame, column:str, color:str)->None:

    
    
    sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)

    plt.title(f'Distribution of {column}', size=20, fontweight='bold')

    plt.show()
plot_hist(telecom,'top_3_Handset_Manufacturers','blue' )
value_counts_df = value_counts.reset_index()




value_counts_df.columns = ['Unique_Values', 'Count']




print(value_counts_df.head(7))

unique_values = telecom["Handset Manufacturer"].unique().value_counts()




print(unique_values)

%load_ext sql
%sql postgresql://postgres:lhtdss@localhost:5432/week1
%sql SELECT * FROM xdr_data LIMIT 10
%%sql SELECT "Bearer Id"

FROM xdr_data LIMIT 10
Repository Structure: '
' ├── setup.py
├── user_overview_plots.py
├── data_loader.py
├── database_operations
│   ├── mysql_integration.py
│   └── data_export.py
├── satisfaction_metrics
│   ├── regression_model.py
│   ├── k_means_satisfaction.py
│   └── satisfaction_scores.py
├── data_exploration
│   ├── exploration_data_analysis.py
│   └── visualizations.py
├── engagement_metrics
│   ├── k_means_clustering.py
│   └── engagement_analysis.py
├── experience_visualizations.py
├── README.md
├── user_overview_analysis.py
├── bivariate_analysis.py
├── engagement_visualizations.py
├── model_deployment
│   ├── mlops_tools
│   └── docker_setup
├── experience_metrics
│   ├── experience_analysis.py
│   └── experience_clustering.py
└── requirements.txt
 '
'  

from doctest import debug_script
from json import load
from pydoc import describe
from quopri import decodestring
import pandas as pd
import psycopg2

def loadData():
    conn = psycopg2.connect(
        host="localhost",
        port="5432",
        database="tellco",
        user="postgres",
        password="root"
    )
        cursor = conn.cursor('my_cursor_name', withhold=True)
    cursor.execute("SELECT * FROM xdr_data")
            batch_size = 1000
    rows = cursor.fetchmany(batch_size)
    
    data = [] 

    while rows:
                data += rows
                rows = cursor.fetchmany(batch_size)
        if not rows:
            break
        if len(data) >= 50000:
            columns = [desc[0] for desc in cursor.description]
            df = pd.DataFrame(data, columns=columns)
            return df

    columns = [desc[0] for desc in cursor.description]
    cursor.close()
    conn.close()  
    df = pd.DataFrame(data, columns=columns) 
    print(len(data))
    
    return (df)
import pandas.io.sql as sqlio
import psycopg2
from psycopg2 import sql
from sqlalchemy import create_engine
import pandas as pd

def db_connection_sqlalchemy():
        database_name='telecom'
    table_name='xdr_data'
    connection_params={
        "host":"localhost",
        "user":"postgres",
        "password":"admin",
        "port":"5432",
        "database":database_name
    }
    engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
    return engine

def db_read_table_sqlalchemy(engine,table_name):
        sql_query=f'SELECT * FROM {table_name}'
    df = pd.read_sql(sql_query, con= engine)
    return df    

def db_connection_psycopg():
         pgconn=psycopg2.connect(
        dbname="telecom",
        user="postgres",
        password="admin",
        host="localhost",
        port="5432"
    )
    return pgconn   

def db_read_table_psycopg(pgconn,table_name):
          sql_query=f'SELECT * FROM {table_name}'
     df=sqlio.read_sql_query(sql_query,pgconn)
     return df
import pandas as pd

from script import db_connection


alchemyConn=db_connection.db_connection_sqlalchemy()


pgconn=db_connection.db_connection_psycopg()

print(alchemyConn)

print(pgconn)

dfa = db_connection.db_read_table_sqlalchemy(alchemyConn, 'xdr_data')

dfa.head()

dfa = db_connection.db_read_table_psycopg(pgconn, 'xdr_data')

dfa.head()

df.info()
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import sys

sys.path.append('../')


import warnings

warnings.filterwarnings('ignore')

from script import db_connection


alchemyConn=db_connection.db_connection_sqlalchemy()


pgconn=db_connection.db_connection_psycopg()

print(alchemyConn)

print(pgconn)

dfa = db_connection.db_read_table_sqlalchemy(alchemyConn, 'xdr_data')

dfa.head()

dfg = db_connection.db_read_table_psycopg(pgconn, 'xdr_data')

dfg.head()

dfa.info()

dfa.nunique()

dfa.Start.unique()

dfa.Start.nunique()

dfa.isnull().sum()

(dfa.isnull().sum()/(len(dfa)))*100

dfa.describe().T

dfa['Total UL (Bytes)'].describe().T

dfa.describe(include='all').T




def percent_missing(dfa):

    
    totalCells = np.product(dfa.shape)



    
    missingCount = dfa.isnull().sum()



    
    totalMissing = missingCount.sum()



    
    print("The telecom dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")



percent_missing(dfa)


sessions = len(dfa.columns)

print("Number of xDR sessions:", sessions)

dfa['Google DL (Bytes)'].hist()





num_cols=dfa.select_dtypes(include=np.number).columns.tolist()


num_cols.skew()

session_durations = dfa['Dur. (ms)'].count()



print("Number of session durations:", session_durations)

total_dl_data = dfa['Total DL (Bytes)'].sum()

total_ul_data = dfa['Total UL (Bytes)'].sum()



print("Total Download Data:", total_dl_data, "bytes")

print("Total Upload Data:", total_ul_data, "bytes")

total_social_media_dl = dfa['Social Media DL (Bytes)'].sum()

total_social_media_ul = dfa['Social Media UL (Bytes)'].sum()

total_google_dl = dfa['Google DL (Bytes)'].sum()

total_google_ul = dfa['Google UL (Bytes)'].sum()

total_email_dl = dfa['Email DL (Bytes)'].sum()

total_email_ul = dfa['Email UL (Bytes)'].sum()

total_youtube_dl = dfa['Youtube DL (Bytes)'].sum()

total_youtube_ul = dfa['Youtube UL (Bytes)'].sum()

total_netflix_dl = dfa['Netflix DL (Bytes)'].sum()

total_netflix_ul = dfa['Netflix UL (Bytes)'].sum()

total_gaming_dl = dfa['Gaming DL (Bytes)'].sum()

total_gaming_ul = dfa['Gaming UL (Bytes)'].sum()

total_other_dl = dfa['Other DL (Bytes)'].sum()

total_other_ul = dfa['Other UL (Bytes)'].sum()



print("Total Social Media Data (DL):", total_social_media_dl, "bytes")

print("Total Social Media Data (UL):", total_social_media_ul, "bytes")

print("Total Google Data (DL):", total_google_dl, "bytes")

print("Total Google Data (UL):", total_google_ul, "bytes")

print("Total Email Data (DL):", total_email_dl, "bytes")

print("Total Email Data (UL):", total_email_ul, "bytes")

print("Total YouTube Data (DL):", total_youtube_dl, "bytes")

print("Total YouTube Data (UL):", total_youtube_ul, "bytes")

print("Total Netflix Data (DL):", total_netflix_dl, "bytes")

print("Total Netflix Data (UL):", total_netflix_ul, "bytes")

print("Total Gaming Data (DL):", total_gaming_dl, "bytes")

print("Total Gaming Data (UL):", total_gaming_ul, "bytes")

print("Total Other Data (DL):", total_other_dl, "bytes")

print("Total Other Data (UL):", total_other_ul, "bytes")


num_cols = dfa.select_dtypes(include=np.number).columns.tolist()

cat_cols=dfa.select_dtypes(include=['object']).columns

for col in num_cols:

    print(col)

    print('Skew :', round(dfa[col].skew(), 2))

    plt.figure(figsize = (15, 4))

    plt.subplot(1, 2, 1)

    dfa[col].hist(grid=False)

    plt.ylabel('count')

    plt.subplot(1, 2, 2)

    sns.boxplot(x=dfa[col])

    plt.show()



application_vars = ['Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)', 'Youtube UL (Bytes)',

                       'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)','Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

                       'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']



dfa['DL+UL (Bytes)']=dfa['Total UL (Bytes)']+dfa['Total DL (Bytes)']


for var in application_vars:

    subset = dfa[[var, 'DL+UL (Bytes)']]

    plt.figure(figsize=(13, 17))

    sns.pairplot(subset)

    plt.title(f'Pair Plot for {var} with Total UL')

    plt.show()

dfa['Total Duration'] = dfa['Dur. (ms)'] + dfa['Dur. (ms).1'] + dfa['Activity Duration DL (ms)'] + dfa['Activity Duration UL (ms)']




sorted_dfa = dfa.sort_values('Total Duration', ascending=False)




decile_size = len(sorted_dfa) // 5



sorted_dfa['Decile Class'] = pd.qcut(sorted_dfa.index, 5, labels=False)

sorted_dfa['Decile Class'] = sorted_dfa['Decile Class'].apply(lambda x: x + 1)




decile_data = sorted_dfa.groupby('Decile Class')[['DL+UL (Bytes)']].sum()




print(decile_data)


data_list = ['Social Media UL (Bytes)', 'Social Media DL (Bytes)',

             'Google UL (Bytes)', 'Google DL (Bytes)',

             'Email UL (Bytes)', 'Email DL (Bytes)',

             'Youtube UL (Bytes)', 'Youtube DL (Bytes)',

             'Netflix UL (Bytes)', 'Netflix DL (Bytes)',

             'Gaming UL (Bytes)', 'Gaming DL (Bytes)',

             'Other UL (Bytes)', 'Other DL (Bytes)']




subset = dfa[data_list]




correlation_matrix = subset.corr()




print(correlation_matrix)



session_duration = dfa.groupby('MSISDN/Number')['Dur. (ms)'].sum()

top_10_session_duration = session_duration.nlargest(10)



print("\nTop 10 Customers by Session Duration:")

print(top_10_session_duration)
dfa_clean=dfa[['Bearer Id','Start']]

dfa_clean.info()

dfa_clean.to_sql('clean_data',alchemyConn)
Repository Structure: '
' ├── readme.md
├── requirements.txt
└── ML_Model
    ├── readme.md
    └── streamlit_telecom.py
 '
' Commit History: 
{"insertions": [0, 2, 1, 1, 2], "deletions": [14, 0, 1, 2, 1], "lines": [14, 2, 2, 3, 3], "committed_datetime": ["2023-12-16 20:30:22", "2023-12-16 20:35:48", "2023-12-16 20:36:24", "2023-12-16 20:37:17", "2023-12-16 20:38:43"], "commit_count": 5} 
 Content: 
https://colab.research.google.com/drive/1WBBIvW670fn0Fzz86qQfynN3USb8vS-n?usp=sharing


This project encompasses a comprehensive machine learning pipeline tailored for the telecom dataset. It is designed to cater to a diverse audience, including customers and businesses, with a primary focus on investigating customer behavior. The analyses conducted aim to enhance engagement, satisfaction, and reduce retention.


**Data Preparation:** Rigorous preprocessing steps are implemented to ensure data quality and readiness for analysis.

- **Cleaning:** Addresses missing values, outliers, and ensures data consistency.
- **Feature Engineering:** Involves creating new features to improve model performance.
- **Customer Behavior Analysis:** Conducts an in-depth investigation into customer behavior, providing valuable insights for business strategy.

**Engagement and Satisfaction Enhancement:** Strategies are implemented to boost customer engagement and satisfaction levels.

**Retention Reduction Strategies:** Measures are employed to minimize customer retention and strengthen long-term customer relationships.


Provide concise instructions on how to install and set up the project on a local machine.


This pipeline is designed to be user-friendly, offering a seamless experience for users interested in harnessing the power of machine learning for telecom-related insights. Whether you are a customer seeking improved services or a business owner aiming for sustainable growth, this project caters to your needs.


Detailed explanations and steps for data preparation are provided, ensuring data quality and readiness for analysis.


Addressing missing values, outliers, and ensuring data consistency.


Creating new features to enhance the predictive power of the model.


Insights into the analysis of customer behavior are provided, empowering users with actionable information.


Strategies and implementations are outlined, aimed at boosting customer engagement and satisfaction.


Proven strategies to minimize customer retention and strengthen long-term relationships.


Guidelines for contributors who wish to participate in enhancing the project.

 

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import numpy as np

from sklearn.cluster import KMeans

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from scipy.stats import pearsonr

from sklearn.metrics import pairwise_distances_argmin_min

from sklearn.metrics import silhouette_score



%run db_connection.ipynb
import warnings

warnings.filterwarnings('ignore')
df = pd.read_sql(sql_query, con= engine)
df.shape
df.info()
df.isnull().sum().sum()
df.isnull().sum().sort_values(ascending=False)

def percent_missing(df):

    
    totalCells = np.product(df.shape)



    
    missingCount = df.isnull().sum()



    
    totalMissing = missingCount.sum()



    
    print("The telecom dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")



percent_missing(df)

df.isnull().sum().sort_values(ascending=False)/150001*100

def fix_missing_bymode(df, col):

    df[col] = df[col].fillna(df[col].mode()[0])

    return df[col]



def fix_missing_bfill(df, col):

    df[col] = df[col].fillna(method='bfill')

    return df[col]
df_clean['Start'] = fix_missing_bymode(df, 'Start')

df_clean['End '] = fix_missing_bymode(df, 'End')

df_clean['Last Location Name'] = fix_missing_bymode(df, 'Last Location Name')

df_clean['Handset Manufacturer'] = fix_missing_bymode(df, 'Handset Manufacturer')

df_clean['Handset Type'] = fix_missing_bymode(df, 'Handset Type')

df_clean.fillna(method='ffill', inplace=True)
df_clean2 = df

df_clean2.fillna(method='ffill', inplace=True)
df_clean.isnull().sum()

df_clean = df.drop(['Nb of sec with 37500B < Vol UL', 'Nb of sec with 6250B < Vol UL < 37500B', 

                    'Nb of sec with 125000B < Vol DL', 

                    'Nb of sec with 31250B < Vol DL < 125000B','Nb of sec with 1250B < Vol UL < 6250B',

                    'Nb of sec with 6250B < Vol DL < 31250B',

                    'HTTP UL (Bytes)','HTTP DL (Bytes)'], axis=1)

df_clean.shape
selected_numeric_vars = [

    'Dur. (ms)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',

    'Activity Duration DL (ms)', 'Activity Duration UL (ms)', 'Social Media DL (Bytes)',

    'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

    'Other DL (Bytes)', 'Other UL (Bytes)', 'Total UL (Bytes)', 'Total DL (Bytes)']
lami - check this line

sns.pairplot(df[selected_numeric_vars])

plt.suptitle('Pair Plot of Selected Numeric Variables', y=1.02)

plt.show()

print("This takes too much time")
lami - check this line


correlation_matrix = df[selected_numeric_vars].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')

plt.title('Correlation Heatmap of Selected Numeric Variables')

plt.show()
df_headset_type = df_clean['Handset Type'].value_counts(ascending=False)

df_headset_type.iloc[:10]
df_headset_type.iloc[:10].plot.bar(x="Total Handset Used", y="Handset Type", title="Top 10 Handset Type Widely used by user")
df_headset_m = df_clean['Handset Manufacturer'].value_counts(ascending=False)

df_headset_m.iloc[:3]
df_headset_m.iloc[:3].plot(kind="bar")

plt.xlabel("Handset Manufacturer")

plt.ylabel("Total Handset Used")

plt.title("Top 3 Handset used by Manufacturer")

handset_df = df_clean[['Handset Manufacturer', 'Handset Type']]




manufacturer_handset_counts = handset_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')




top_3_manufacturers = manufacturer_handset_counts.groupby('Handset Manufacturer')['Count'].sum().nlargest(3).index




top_3_manufacturer_handsets = manufacturer_handset_counts[manufacturer_handset_counts['Handset Manufacturer'].isin(top_3_manufacturers)]




top_5_handsets_per_manufacturer = top_3_manufacturer_handsets.groupby('Handset Manufacturer').apply(lambda x: x.nlargest(5, 'Count')).reset_index(drop=True)




plt.figure(figsize=(12, 8))

sns.barplot(x='Count', y='Handset Type', hue='Handset Manufacturer', data=top_5_handsets_per_manufacturer, palette='viridis')

plt.title('Top 5 Handsets per Top 3 Manufacturers')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.show()
user_aggregated = df_clean.groupby('MSISDN/Number').agg({

    'MSISDN/Number': 'count',                    
    'Dur. (ms).1': 'sum',                    
    'Total DL (Bytes)': 'sum',               
    'Total UL (Bytes)': 'sum',               
    'Social Media DL (Bytes)': 'sum',        
    'Social Media UL (Bytes)': 'sum',        
    'Google DL (Bytes)': 'sum',              
    'Google UL (Bytes)': 'sum',              
    'Email DL (Bytes)': 'sum',               
    'Email UL (Bytes)': 'sum',               
    'Youtube DL (Bytes)': 'sum',             
    'Youtube UL (Bytes)': 'sum',             
    'Netflix DL (Bytes)': 'sum',             
    'Netflix UL (Bytes)': 'sum',             
    'Gaming DL (Bytes)': 'sum',              
    'Gaming UL (Bytes)': 'sum',              
    'Other DL (Bytes)': 'sum',               
    'Other UL (Bytes)': 'sum'                
})




user_aggregated = user_aggregated.rename(columns={

    'MSISDN/Number': 'Number_of_xDR_sessions',

    'Dur. (ms).1': 'Session_duration',

    'Total DL (Bytes)': 'Total_DL_data',

    'Total UL (Bytes)': 'Total_UL_data',

    'Social Media DL (Bytes)': 'Social_Media_DL_data',

    'Social Media UL (Bytes)': 'Social_Media_UL_data',

    'Google DL (Bytes)': 'Google_DL_data',

    'Google UL (Bytes)': 'Google_UL_data',

    'Email DL (Bytes)': 'Email_DL_data',

    'Email UL (Bytes)': 'Email_UL_data',

    'Youtube DL (Bytes)': 'Youtube_DL_data',

    'Youtube UL (Bytes)': 'Youtube_UL_data',

    'Netflix DL (Bytes)': 'Netflix_DL_data',

    'Netflix UL (Bytes)': 'Netflix_UL_data',

    'Gaming DL (Bytes)': 'Gaming_DL_data',

    'Gaming UL (Bytes)': 'Gaming_UL_data',

    'Other DL (Bytes)': 'Other_DL_data',

    'Other UL (Bytes)': 'Other_UL_data'

})




user_aggregated.sort_values(by="Number_of_xDR_sessions", ascending=False).head()

columns = ['Bearer Id','Dur. (ms)', 'Social Media DL (Bytes)','Social Media UL (Bytes)','Google DL (Bytes)',

           'Google UL (Bytes)','Email DL (Bytes)','Email UL (Bytes)','Youtube DL (Bytes)','Youtube UL (Bytes)',

           'Netflix DL (Bytes)','Netflix UL (Bytes)','Gaming DL (Bytes)','Gaming UL (Bytes)','Other DL (Bytes)',

           'Total UL (Bytes)','Total DL (Bytes)']

df_user_aggregation = df_clean[columns]
df_user_aggregation.shape
df_user_aggregation.describe()


quantitative_variables = [

    'Dur. (ms)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)',

    'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

    'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',

    'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)',

    '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)',

    'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)',

    '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)',

    'HTTP DL (Bytes)', 'HTTP UL (Bytes)',

    'Activity Duration DL (ms)', 'Activity Duration UL (ms)',

    'Nb of sec with 125000B < Vol DL', 'Nb of sec with 1250B < Vol UL < 6250B',

    'Nb of sec with 31250B < Vol DL < 125000B', 'Nb of sec with 37500B < Vol UL',

    'Nb of sec with 6250B < Vol DL < 31250B', 'Nb of sec with 6250B < Vol UL < 37500B',

    'Nb of sec with Vol DL < 6250B', 'Nb of sec with Vol UL < 1250B',

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)',

    'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',

    'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)',

    'Total UL (Bytes)', 'Total DL (Bytes)']




dispersion_params = df[quantitative_variables].agg([

    'mean', 'median', 'std', 'min', 'max', lambda x: x.quantile(0.75) - x.quantile(0.25)])




dispersion_params





quantitative_variables_to_plot = ['Dur. (ms)', 'Avg Bearer TP DL (kbps)', 'TCP DL Retrans. Vol (Bytes)',

                                  'Social Media DL (Bytes)', 'Youtube DL (Bytes)', 'Total UL (Bytes)']



for variable in quantitative_variables_to_plot:

    plt.figure(figsize=(8, 5))

    plt.hist(df[variable], bins=30, color='skyblue', edgecolor='black')

    plt.title(f'Histogram of {variable}')

    plt.xlabel(variable)

    plt.ylabel('Frequency')

    plt.show()






selected_columns = [

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

    'Other DL (Bytes)', 'Other UL (Bytes)',

    'Total UL (Bytes)', 'Total DL (Bytes)'

]

selected_data = df[selected_columns]




correlation_matrix = selected_data.corr()




total_correlations = correlation_matrix[['Total UL (Bytes)', 'Total DL (Bytes)']]




print(total_correlations)





applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']



for app in applications:

    plt.figure(figsize=(10, 6))

    sns.scatterplot(x=f'{app} DL (Bytes)', y='Total DL (Bytes)', data=df_clean, label=f'{app} DL')

    sns.scatterplot(x=f'{app} UL (Bytes)', y='Total UL (Bytes)', data=df_clean, label=f'{app} UL')

    plt.title(f'Relationship between {app} and Total DL+UL Data')

    plt.xlabel(f'Total {app} Data')

    plt.ylabel('Total DL+UL Data')

    plt.legend()

    plt.show()



all_app_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)', 

             'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)','Netflix DL (Bytes)', 

             'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)', 

             'Total UL (Bytes)', 'Total DL (Bytes)']



df_clean['Dur. (ms)'] = df_clean['Dur. (ms)'] / 1000




user_summary = df_clean.groupby('MSISDN/Number').agg({

    'Dur. (ms)': 'sum',

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()




user_summary['Duration Decile'] = pd.qcut(user_summary['Dur. (ms)'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False, duplicates='drop')




data_per_decile = user_summary.groupby('Duration Decile').agg({

    'Total UL (Bytes)': 'sum',

    'Total DL (Bytes)': 'sum'

}).reset_index()




data_per_decile
plt.figure(figsize=(10, 6))



plt.bar(data_per_decile['Duration Decile'], data_per_decile['Total DL (Bytes)'], label='Total DL Data', alpha=0.7)

plt.bar(data_per_decile['Duration Decile'], data_per_decile['Total UL (Bytes)'], label='Total UL Data')



plt.xlabel('Decile Class')

plt.ylabel('Total Data (Bytes)')

plt.title('Total UL and DL Data per Decile Class')

plt.legend()

plt.show()


selected_columns = [

    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

    'Google DL (Bytes)', 'Google UL (Bytes)',

    'Email DL (Bytes)', 'Email UL (Bytes)',

    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

    'Other DL (Bytes)', 'Other UL (Bytes)'

]




selected_data = df[selected_columns]




correlation_matrix = selected_data.corr()




print(correlation_matrix)






sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

plt.show()





selected_columns_PCA = ['Dur. (ms)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)',

                    'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                    'Google DL (Bytes)', 'Google UL (Bytes)',

                    'Email DL (Bytes)', 'Email UL (Bytes)',

                    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                    'Other DL (Bytes)', 'Other UL (Bytes)',

                    'Total UL (Bytes)', 'Total DL (Bytes)']




data_for_pca = df_clean[selected_columns_PCA]




scaler = StandardScaler()

scaled_data = scaler.fit_transform(data_for_pca)




pca = PCA()

pca_result = pca.fit_transform(scaled_data)




explained_variance_ratio = pca.explained_variance_ratio_




cumulative_explained_variance = explained_variance_ratio.cumsum()

plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o')

plt.xlabel('Number of Principal Components')

plt.ylabel('Cumulative Explained Variance')

plt.title('Cumulative Explained Variance vs. Number of Principal Components')

plt.show()


sessions_df = df_clean[['MSISDN/Number', 'Start', 'End']]




sessions_df['Start'] = pd.to_datetime(sessions_df['Start'])

sessions_df['End'] = pd.to_datetime(sessions_df['End'])




sessions_df['Session_Duration_Minutes'] = (sessions_df['End'] - sessions_df['Start']).dt.total_seconds() / 60




session_frequency = sessions_df.groupby('MSISDN/Number')['Start'].count().reset_index()

session_frequency.columns = ['MSISDN/Number', 'Session_Frequency']




session_frequency.sort_values(by="Session_Frequency", ascending=False).head(10)
sessions_df = df[['MSISDN/Number', 'Start', 'End']]




sessions_df['Start'] = pd.to_datetime(sessions_df['Start'])

sessions_df['End'] = pd.to_datetime(sessions_df['End'])




sessions_df['Session_Duration_Minutes'] = (sessions_df['End'] - sessions_df['Start']).dt.total_seconds() / 60




print(sessions_df[['MSISDN/Number', 'Session_Duration_Minutes']])




plt.figure(figsize=(10, 6))

sns.histplot(sessions_df['Session_Duration_Minutes'], bins=30, kde=False, color='skyblue')

plt.title('Session Duration Distribution')

plt.xlabel('Session Duration (Minutes)')

plt.ylabel('Number of Sessions')

plt.show()
sessions_df = df[['MSISDN/Number', 'Total DL (Bytes)', 'Total UL (Bytes)']]




sessions_df['Total_Traffic_Bytes'] = sessions_df['Total DL (Bytes)'] + sessions_df['Total UL (Bytes)']




print(sessions_df[['MSISDN/Number', 'Total_Traffic_Bytes']])




plt.figure(figsize=(10, 6))

sns.histplot(sessions_df['Total_Traffic_Bytes'], bins=30, kde=False, color='skyblue')

plt.title('Total Traffic Distribution')

plt.xlabel('Total Traffic (Bytes)')

plt.ylabel('Number of Sessions')

plt.show()
df_clean_2 = df.drop(['Nb of sec with 37500B < Vol UL','Nb of sec with 6250B < Vol UL < 37500B'], axis=1)

df_clean_2.shape
df_clean_2.isnull().sum().sort_values(ascending=False)/150001*100

network_data = df[['MSISDN/Number', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',

                   'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Handset Type', 'Avg Bearer TP DL (kbps)',

                   'Avg Bearer TP UL (kbps)']]




user_aggregated_network = network_data.groupby('MSISDN/Number').agg({

    'TCP DL Retrans. Vol (Bytes)': lambda x: x.mean(),

    'TCP UL Retrans. Vol (Bytes)': lambda x: x.mean(),

    'Avg RTT DL (ms)': lambda x: x.mean(),

    'Avg RTT UL (ms)': lambda x: x.mean(),

    'Handset Type': lambda x: x.mode().iloc[0] if not x.mode().empty else None,

    'Avg Bearer TP DL (kbps)': lambda x: x.mean(),

    'Avg Bearer TP UL (kbps)': lambda x: x.mean()

})




user_aggregated_network = user_aggregated_network.rename(columns={

    'TCP DL Retrans. Vol (Bytes)': 'Avg_TCP_Retrans_DL',

    'TCP UL Retrans. Vol (Bytes)': 'Avg_TCP_Retrans_UL',

    'Avg RTT DL (ms)': 'Avg_RTT_DL',

    'Avg RTT UL (ms)': 'Avg_RTT_UL',

    'Avg Bearer TP DL (kbps)': 'Avg_Throughput_DL',

    'Avg Bearer TP UL (kbps)': 'Avg_Throughput_UL'

})




network_data['Avg_TCP_Retrans_DL'] = user_aggregated_network['Avg_TCP_Retrans_DL'].fillna(user_aggregated_network['Avg_TCP_Retrans_DL'].mean())

network_data['Avg_TCP_Retrans_UL'] = user_aggregated_network['Avg_TCP_Retrans_UL'].fillna(user_aggregated_network['Avg_TCP_Retrans_UL'].mean())

network_data['Avg_RTT_DL'] = user_aggregated_network['Avg_RTT_DL'].fillna(user_aggregated_network['Avg_RTT_DL'].mean())

network_data['Avg_RTT_UL'] = user_aggregated_network['Avg_RTT_UL'].fillna(user_aggregated_network['Avg_RTT_UL'].mean())

network_data['Handset Type'] = user_aggregated_network['Handset Type'].fillna(user_aggregated_network['Handset Type'].mode().iloc[0] if not user_aggregated_network['Handset Type'].mode().empty else None)

network_data['Avg_Throughput_DL'] = user_aggregated_network['Avg_Throughput_DL'].fillna(user_aggregated_network['Avg_Throughput_DL'].mean())

network_data['Avg_Throughput_UL'] = user_aggregated_network['Avg_Throughput_UL'].fillna(user_aggregated_network['Avg_Throughput_UL'].mean())

user_aggregated_network.head()



tcp_column = 'Avg Bearer TP DL (kbps)'

rtt_column = 'Avg RTT DL (ms)'

throughput_column = 'TCP DL Retrans. Vol (Bytes)'




top_tcp = df_clean_t4.nlargest(10, tcp_column)[[tcp_column]]

top_rtt = df_clean_t4.nlargest(10, rtt_column)[[rtt_column]]

top_throughput = df_clean_t4.nlargest(10, throughput_column)[[throughput_column]]




bottom_tcp = df_clean_t4.nsmallest(10, tcp_column)[[tcp_column]]

bottom_rtt = df_clean_t4.nsmallest(10, rtt_column)[[rtt_column]]

bottom_throughput = df_clean_t4.nsmallest(10, throughput_column)[[throughput_column]]




most_frequent_tcp = df_clean_t4[tcp_column].value_counts().nlargest(10).reset_index()

most_frequent_rtt = df_clean_t4[rtt_column].value_counts().nlargest(10).reset_index()

most_frequent_throughput = df_clean_t4[throughput_column].value_counts().nlargest(10).reset_index()




most_frequent_tcp.columns = [tcp_column, 'Frequency']

most_frequent_rtt.columns = [rtt_column, 'Frequency']

most_frequent_throughput.columns = [throughput_column, 'Frequency']




print("Top TCP Values:")

print(top_tcp)



print("\nBottom TCP Values:")

print(bottom_tcp)



print("\nMost Frequent TCP Values:")

print(most_frequent_tcp)



print("\nTop RTT Values:")

print(top_rtt)



print("\nBottom RTT Values:")

print(bottom_rtt)



print("\nMost Frequent RTT Values:")

print(most_frequent_rtt)



print("\nTop Throughput Values:")

print(top_throughput)



print("\nBottom Throughput Values:")

print(bottom_throughput)



print("\nMost Frequent Throughput Values:")

print(most_frequent_throughput)



throughput_tcp_data = df[['Handset Type', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                          'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']]




average_throughput_per_handset = throughput_tcp_data.groupby('Handset Type').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean',

    'TCP DL Retrans. Vol (Bytes)': 'mean',

    'TCP UL Retrans. Vol (Bytes)': 'mean'

})

average_throughput_per_handset
df_clean.dropna(inplace=True)




experience_columns = ['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                      'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',

                      'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                      'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)',

                      'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                      'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                      'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                      'Other DL (Bytes)', 'Other UL (Bytes)']




experience_data = df_clean[experience_columns]




scaler = StandardScaler()

experience_data_scaled = scaler.fit_transform(experience_data)




kmeans = KMeans(n_clusters=3, random_state=42)

df_clean['Cluster'] = kmeans.fit_predict(experience_data_scaled)




cluster_description = df_clean.groupby('Cluster')[experience_columns].mean()



print("Cluster Descriptions:")

print(cluster_description)

engagement_experience_metrics = df_clean[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                                    'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']]




engagement_experience_metrics = engagement_experience_metrics.fillna(engagement_experience_metrics.mean())




scaler = StandardScaler()

scaled_metrics = scaler.fit_transform(engagement_experience_metrics)




engagement_scores = pairwise_distances_argmin_min(scaled_metrics, kmeans.cluster_centers_[0].reshape(1, -1))[1]

experience_scores = pairwise_distances_argmin_min(scaled_metrics, cluster_centers.iloc[cluster_centers['Avg Bearer TP DL (kbps)'].idxmin()].values.reshape(1, -1))[1]




df_clean['Engagement Score'] = engagement_scores

df_clean['Experience Score'] = experience_scores




print(df_clean[['MSISDN/Number', 'Engagement Score', 'Experience Score']])

df_clean['Satisfaction Score'] = df_clean[['Engagement Score', 'Experience Score']].mean(axis=1)




top_satisfied_customers = df_clean.sort_values(by='Satisfaction Score', ascending=False).head(10)

print("Top 10 Satisfied Customers:")

print(top_satisfied_customers[['MSISDN/Number', 'Satisfaction Score']])
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler







features = ['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

            'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']




df_clean[features] = df_clean[features].fillna(df_clean[features].mean())




X_train, X_test, y_train, y_test = train_test_split(df_clean[features], df_clean['Satisfaction Score'], test_size=0.2, random_state=42)




scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)




model = LinearRegression()




model.fit(X_train_scaled, y_train)




y_pred = model.predict(X_test_scaled)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)

engagement_experience_scores = df_clean[['Engagement Score', 'Experience Score']]




engagement_experience_scores = engagement_experience_scores.fillna(engagement_experience_scores.mean())




scaler = StandardScaler()

scaled_scores = scaler.fit_transform(engagement_experience_scores)




kmeans_2 = KMeans(n_clusters=2, random_state=42)

df_clean['Cluster_2'] = kmeans_2.fit_predict(scaled_scores)

cluster_aggregated_scores = df_clean.groupby('Cluster_2').agg({

    'Satisfaction Score': 'mean',

    'Experience Score': 'mean'

}).reset_index()
import pandas as pd
from sqlalchemy import create_engine

dbname = "telecom"
user = "postgres"
password = "12345"
host = "localhost"
port = "5432"

connection_string = f"postgresql://{user}:{password}@{host}:{port}/{dbname}"

    engine = create_engine(connection_string)

    query = "SELECT * FROM xdr_data;"

df = pd.read_sql(query, engine)

telecomdata = df.to_csv('./data/telecom.csv')
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

import os, sys

sys.path.insert(0,'../scripts/')

from process import *

import warnings

warnings.filterwarnings('ignore')

telcom_df = pd.read_csv('../data/telecom.csv', na_values=['?',"n.a.","NA","n/a", "na", None])

telcom_df.drop("Unnamed: 0",axis=1,inplace=True)
telcom_df.head()

discribe = get_df_discribe(telcom_df)



discribe
telcom_df_information = get_df_information(telcom_df)

telcom_df.shape
telcom_df.columns
view_df_null_count = get_df_null_count(telcom_df)

view_df_null_count
telecom_missing = get_df_percent_missing(telcom_df)

telecom_missing
telecom_column_percentage = get_missing_colum_percentage(telcom_df)



telecom_column_percentage
telecom_clean = telcom_df.drop(['TCP DL Retrans. Vol (Bytes)','TCP UL Retrans. Vol (Bytes)', 'HTTP DL (Bytes)','HTTP UL (Bytes)','Nb of sec with 1250B < Vol UL < 6250B', 'Nb of sec with 6250B < Vol UL < 37500B', 'Nb of sec with 6250B < Vol DL < 31250B', 'Nb of sec with 125000B < Vol DL', 'Nb of sec with 37500B < Vol UL', 'Nb of sec with 31250B < Vol DL < 125000B'], axis=1)

telecom_clean.shape
telecom_clean.info()



telecom_clean = telecom_clean.dropna(subset=['Bearer Id', 'Start', 'End', 'IMSI', 'MSISDN/Number'])


telecom_clean.isna().sum()
missing_columns_lists = ['DL TP < 50 Kbps (%)','50 Kbps < DL TP < 250 Kbps (%)',

'250 Kbps < DL TP < 1 Mbps (%)','DL TP > 1 Mbps (%)','UL TP < 10 Kbps (%)','10 Kbps < UL TP < 50 Kbps (%)','50 Kbps < UL TP < 300 Kbps (%)','UL TP > 300 Kbps (%)','Last Location Name','Avg RTT DL (ms)','Avg RTT UL (ms)','Nb of sec with Vol DL < 6250B','Nb of sec with Vol UL < 1250B']



for value in missing_columns_lists:

  if(value!="Last Location Name"):

    telecom_clean[value] = telecom_clean[value].fillna(telecom_clean[value].mean())

  else:

    telecom_clean[value] = telecom_clean[value].fillna(telecom_clean[value].mode()[0])
telecom_clean.info()

telecom_clean.to_csv("../data/telecom_clean_data.csv")




from sklearn.preprocessing import MinMaxScaler



minmax_scaler = MinMaxScaler()




original_data = pd.DataFrame(np.random.exponential(200, size=2000))



original_data.sample(5)
count, bins, ignored = plt.hist(original_data, 14)

plt.show()


def scaler(df):

    scaled_data = minmax_scaler.fit_transform(df)



    
    fig, ax = plt.subplots(1,2, figsize=(10, 6))

    sns.histplot(original_data, ax=ax[0])

    ax[0].set_title("Original Data")

    sns.histplot(scaled_data, ax=ax[1])

    ax[1].set_title("Scaled data")

    

scaler(original_data)
from sklearn.preprocessing import Normalizer



def normalizer(df):

    norm = Normalizer()

    
    normalized_data = norm.fit_transform(df)



    
    fig, ax=plt.subplots(1,2, figsize=(10, 6))

    sns.histplot(df, ax=ax[0])

    ax[0].set_title("Original Data")

    sns.histplot(normalized_data[0], ax=ax[1])

    ax[1].set_title("Normalized data")



normalizer(original_data)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


def get_df_percent_missing(df: pd.DataFrame) -> str:
    totalCells = np.product(df.shape)
    missingCount = df.isnull().sum()
    totalMissing = missingCount.sum()
    return f"The telecom contains {round(((totalMissing/totalCells) * 100), 2)}% missing values."

def get_df_discribe(df:pd.DataFrame)->pd.DataFrame:
    return df.describe()

def get_df_null_count(df:pd.DataFrame)->pd.DataFrame:
    return df.isna().sum()

def get_df_information(df:pd.DataFrame)->pd.DataFrame:
    return df.info()

def get_missing_colum_percentage(df: pd.DataFrame) -> pd.DataFrame:
    num_missing = df.isnull().sum()
    num_rows = df.shape[0]

    data = {
        'num_missing': num_missing, 
        'percent_missing (%)': [round(x, 2) for x in num_missing / num_rows * 100]
    }

    stats = pd.DataFrame(data)

        return stats[stats['num_missing'] != 0]


def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str,title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.scatterplot(data = df, x=x_col, y=y_col)
    plt.title(f'{title}')
    plt.xticks(fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()

def plot_heatmap(df:pd.DataFrame, title:str, cbar=False)->None:
    plt.figure(figsize=(12, 7))
    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )
    plt.title(title, size=18, fontweight='bold')
    plt.show()

def plot_hist(df:pd.DataFrame, column:str, color:str)->None:
            sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()

def mult_hist(sr, rows, cols, title_text, subplot_titles, interactive=False):
    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)
    for i in range(rows):
        for j in range(cols):
            x = ["-> " + str(i) for i in sr[i+j].index]
            fig.add_trace(go.Bar(x=x, y=sr[i+j].values), row=i+1, col=j+1)
    fig.update_layout(showlegend=False, title_text=title_text)
    if(interactive):
        fig.show()
    else:
        return Image(pio.to_image(fig, format='png', width=1200))
Telecommunication Analysis

This project centers on a thorough analysis of a telecommunication dataset to evaluate growth opportunities and provide a recommendation on whether TellCo is a viable candidate for acquisition or divestment.

The dataset encompasses crucial information about customer demographics and network activities. Through exploratory data analysis (EDA), we will clean and explore the data, deriving key insights and trends.

The deliverables include an interactive web-based dashboard for visual exploration and a comprehensive written report outlining findings and strategic recommendations. Stakeholders can leverage these insights to make informed decisions regarding TellCo's potential for growth or divestment.
import pandas as pd

import os, sys

import numpy as np

import matplotlib.pylab as plt

import seaborn as sns

import statistics as st

sys.path.insert(0,'../scripts/')

telecom_df = pd.read_csv('../data/telecom_clean_data.csv')

telecom_df.drop("Unnamed: 0",axis=1,inplace=True)

pd.set_option('display.float_format', lambda x: '{:,.0f}'.format(x))
telecom_df.head(5)
telecom_df.head(5)

handset_type= telecom_df['Handset Type'].value_counts()[0:10]

handset_type.plot.bar(x='Handset Type', title="Top 10 handsets used by customers", stacked=True, color='
handset_type

Handset_manufacturers = telecom_df['Handset Manufacturer'].value_counts()[0:3]

Handset_manufacturers.plot.bar(x='Handset Manufacturer', title="Top 3 handset manufacturers", stacked=True, color='
Handset_manufacturers
top_apple = telecom_df.loc[telecom_df['Handset Manufacturer'] == 'Apple']

top_apple = top_apple.groupby(['Handset Manufacturer', 'Handset Type']).agg({'Handset Type': ['count']})

top_apple.columns = ['count']

top_apple=top_apple.nlargest(5, 'count')

top_apple.plot.bar(y='count', title="Top 5 Apple Handsets", stacked=True, color='
top_apple
top_samsung = telecom_df.loc[telecom_df['Handset Manufacturer'] == 'Samsung']

top_samsung = top_samsung.groupby(['Handset Manufacturer', 'Handset Type']).agg({'Handset Type': ['count']})

top_samsung.columns = ['count']

top_samsung=top_samsung.nlargest(5, 'count')

top_samsung.plot.bar(y='count', title="Top 5 Samsung Handsets", stacked=True, color='
top_samsung
top_huawei = telecom_df.loc[telecom_df['Handset Manufacturer'] == 'Huawei']

top_huawei = top_huawei.groupby(['Handset Manufacturer', 'Handset Type']).agg({'Handset Type': ['count']})

top_huawei.columns = ['count']

top_huawei=top_huawei.nlargest(5, 'count')

top_huawei.plot.bar(y='count', title="Top 5 Huawei Handsets", stacked=True, color='
top_huawei














xDR_session_per_user = telecom_df.groupby('MSISDN/Number')['Bearer Id'].sum()

xDR_session_per_user
session_duration_per_user = telecom_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()

session_duration_per_user

total_ul = telecom_df.groupby('MSISDN/Number')['Total UL (Bytes)'].sum()

total_dl = telecom_df.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()

print(f'total upload:\n{total_ul}\n\ntotal download:\n{total_dl}\n\ntotal upload + download: {total_dl+total_ul}')


total_google = telecom_df['Google DL (Bytes)'] + telecom_df['Google UL (Bytes)']

total_email = telecom_df['Email DL (Bytes)'] + telecom_df['Email UL (Bytes)']

total_gaming = telecom_df['Gaming DL (Bytes)'] + telecom_df['Gaming UL (Bytes)']



total_youtube = telecom_df['Youtube DL (Bytes)'] + telecom_df['Youtube UL (Bytes)']

total_netflix = telecom_df['Netflix DL (Bytes)'] + telecom_df['Netflix UL (Bytes)']

total_social = telecom_df['Social Media DL (Bytes)'] + telecom_df['Social Media UL (Bytes)']

total_other = telecom_df['Other DL (Bytes)'] + telecom_df['Other UL (Bytes)']




telecom_df['total_google'] = total_google

telecom_df['total_email'] = total_email

telecom_df['total_gaming'] = total_gaming



telecom_df['total_youtube'] = total_youtube

telecom_df['total_netflix'] = total_netflix

telecom_df['total_social'] = total_social

telecom_df['total_other'] = total_other





total_data_volume_per_user_google = telecom_df.groupby('MSISDN/Number')['total_google'].sum()

print(f'{total_data_volume_per_user_google}\n')




total_data_volume_per_user_email = telecom_df.groupby('MSISDN/Number')['total_email'].sum()

print(f'{total_data_volume_per_user_email}\n')




total_data_volume_per_user_gaming = telecom_df.groupby('MSISDN/Number')['total_gaming'].sum()

print(f'{total_data_volume_per_user_gaming}\n')




total_data_volume_per_user_youtube = telecom_df.groupby('MSISDN/Number')['total_youtube'].sum()

print(f'{total_data_volume_per_user_youtube}\n')




total_data_volume_per_user_netflix = telecom_df.groupby('MSISDN/Number')['total_netflix'].sum()

print(f'{total_data_volume_per_user_netflix}\n') 




total_data_volume_per_user_social = telecom_df.groupby('MSISDN/Number')['total_social'].sum()

print(f'{total_data_volume_per_user_social}\n')




total_data_volume_per_user_other = telecom_df.groupby('MSISDN/Number')['total_other'].sum()

print(f'{total_data_volume_per_user_other}\n')

relevant_features = ['Dur. (ms)', 'Activity Duration DL (ms)', 'Activity Duration UL (ms)', 'Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)', 'Total UL (Bytes)', 'Total DL (Bytes)', 'total_google', 'total_email', 'total_gaming', 'total_youtube', 'total_netflix', 'total_social', 'total_other']





print("Mean\n", telecom_df[relevant_features].mean(), '\n')

print("Median\n", telecom_df[relevant_features].median(), '\n')



mode_series = pd.Series(telecom_df[relevant_features].mode().values[0], index=relevant_features)

print("Mode\n", mode_series, '\n')



print("Highest\n", telecom_df[relevant_features].max(), '\n')

print("lowest\n", telecom_df[relevant_features].min(), '\n')



sub_relevant_features = ['MSISDN/Number','Dur. (ms)','Total UL (Bytes)', 'Total DL (Bytes)', 'total_google','total_email','total_gaming','total_youtube','total_netflix','total_social', 'total_other']



telecom_df[sub_relevant_features].describe()







telecom_df[['Dur. (ms).1']].describe()
plot_hist(telecom_df, 'Dur. (ms).1', 'dodgerblue')
telecom_df[['Total UL (Bytes)']].describe()
plot_hist(telecom_df, 'Total UL (Bytes)', '
telecom_df[['Total DL (Bytes)']].describe()
plot_hist(telecom_df, 'Total DL (Bytes)', '
telecom_df["Social_Media_Total_Data"].describe()
plot_hist(telecom_df, 'Social_Media_Total_Data', '

telecom_df[['Google_Total_Data']].describe()
plot_hist(telecom_df, 'Google_Total_Data', '
telecom_df[['Email_Total_Data']].describe()

plot_hist(telecom_df, 'Email_Total_Data', '
telecom_df[['Youtube_Total_Data']].describe()

plot_hist(telecom_df, 'Youtube_Total_Data', '


telecom_df[['Netflix_Total_Data']].describe()

plot_hist(telecom_df, 'Netflix_Total_Data', color="
plot_hist(telecom_df, 'Gaming_Total_Data', 'indigo')




telecom_df[['Other_Total_Data']].describe()
plot_hist(telecom_df, 'Other_Total_Data', 'sample_data = telecom_df.sample(frac=0.000099)

print(sample_data.shape)

sum_column = sample_data["Total DL (Bytes)"] + sample_data["Total UL (Bytes)"]
telecom_df["Youtube_Total_Data"]=telecom_df["Youtube DL (Bytes)"]+telecom_df["Youtube UL (Bytes)"]

telecom_df["Google_Total_Data"]=telecom_df["Google DL (Bytes)"]+telecom_df["Google UL (Bytes)"]

telecom_df["Email_Total_Data"]=telecom_df["Email DL (Bytes)"]+telecom_df["Email UL (Bytes)"]

telecom_df["Social_Media_Total_Data"]=telecom_df["Social Media DL (Bytes)"]+telecom_df["Social Media UL (Bytes)"]

telecom_df["Netflix_Total_Data"]=telecom_df["Netflix DL (Bytes)"]+telecom_df["Netflix UL (Bytes)"]

telecom_df["Gaming_Total_Data"]=telecom_df["Gaming DL (Bytes)"]+telecom_df["Gaming UL (Bytes)"]

telecom_df["Other_Total_Data"]=telecom_df["Other DL (Bytes)"]+telecom_df["Other UL (Bytes)"]

telecom_df["Total UL and DL"]=telecom_df["Total UL (Bytes)"]+telecom_df["Total DL (Bytes)"]

columns = ['MSISDN/Number', 'Youtube_Total_Data', 'Google_Total_Data', 'Email_Total_Data','Social_Media_Total_Data', 'Netflix_Total_Data', 'Gaming_Total_Data', 'Other_Total_Data', 'Total UL and DL']



user_ratio_usage= telecom_df[columns].groupby('MSISDN/Number').sum()

user_ratio_usage.head(10)
apps_columns = ['Dur. (ms)','MSISDN/Number', 'Youtube_Total_Data', 'Google_Total_Data', 'Email_Total_Data','Social_Media_Total_Data', 'Netflix_Total_Data', 'Gaming_Total_Data', 'Other_Total_Data', 'Total UL and DL']

user_ratio_usage = telecom_df[apps_columns]



user_ratio_usage.columns
plot_scatter(user_ratio_usage,'Gaming_Total_Data', 'Total UL and DL', 'Total Data Vs. Gaming Data usage (MegaBytes)')

plot_scatter(user_ratio_usage.sample(10000), 'Youtube_Total_Data', 'Total UL and DL', 'Total Data Vs. Youtube_Total_Data (MegaBytes)')



plot_scatter(user_ratio_usage.sample(10000), 'Email_Total_Data', 'Total UL and DL', 'Total Data Vs. Email_Total_Data (MegaBytes)')



plot_scatter(user_ratio_usage.sample(10000), 'Social_Media_Total_Data', 'Total UL and DL', 'Total Data Vs. Social_Media_Total_Data (MegaBytes)')



plot_scatter(user_ratio_usage.sample(10000), 'Netflix_Total_Data', 'Total UL and DL', 'Total Data Vs. Netflix_Total_Data (MegaBytes)')

plot_scatter(user_ratio_usage.sample(10000), 'Other_Total_Data', 'Total UL and DL', 'Total Data Vs. Other_Total_Data ')




telecom_df['total_data'] = telecom_df["Total DL (Bytes)"] + telecom_df["Total UL (Bytes)"]




telecom_df['decile'] = pd.qcut(telecom_df['Dur. (ms)'],10, duplicates='drop')



ax=telecom_df.groupby('decile')['total_data'].sum().head(5).plot(kind='bar', xticks=[0,1,2,3,4])

ax.set_xticklabels(['First','Second','Third','Fourth','Fifth'])

ax.set_ylabel('Data Durations')

columns = ['Youtube_Total_Data', 'Google_Total_Data', 'Email_Total_Data','Social_Media_Total_Data', 'Netflix_Total_Data', 'Gaming_Total_Data', 'Other_Total_Data', 'Total UL and DL']

corr = user_ratio_usage[columns].corr()

corr

plot_heatmap(corr, "Correlation of Usage of User Data Volume")


numeric_df = user_ratio_usage.select_dtypes(include='float64') 
numeric_df.describe()
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(numeric_df)

scaled_data = scaler.transform(numeric_df)

from sklearn.decomposition import PCA

pca = PCA(n_components=10)

pca.fit(scaled_data)

x_pca = pca.transform(scaled_data)

plt.figure(figsize=(8,6))

plt.scatter(x_pca[:,4],x_pca[:,1],c=user_ratio_usage['Total UL and DL'],cmap='rainbow')

plt.xlabel('First principal component')

plt.ylabel('Second Principal Component')

from sklearn.decomposition import PCA

pca = PCA().fit(numeric_df)

plt.plot(np.cumsum(pca.explained_variance_ratio_))

plt.xlim(0,7.1)

plt.xlabel('Number of components')

plt.ylabel('Cumulative explained variance')



from sklearn.decomposition import PCA 

sklearn_pca = PCA(n_components=6)

Y_sklearn = sklearn_pca.fit_transform(numeric_df)

Y_sklearn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.io as pio
from IPython.display import Image
from plotly.subplots import make_subplots
def get_df_percent_missing(df: pd.DataFrame) -> str:
    totalCells = np.product(df.shape)
    missingCount = df.isnull().sum()
    totalMissing = missingCount.sum()
    return f"The telecom contains {round(((totalMissing/totalCells) * 100), 2)}% missing values."

def get_df_discribe(df:pd.DataFrame)->pd.DataFrame:
    return df.describe()

def get_df_null_count(df:pd.DataFrame)->pd.DataFrame:
    return df.isna().sum()

def get_df_information(df:pd.DataFrame)->pd.DataFrame:
    return df.info()

def get_missing_colum_percentage(df: pd.DataFrame) -> pd.DataFrame:
    num_missing = df.isnull().sum()
    num_rows = df.shape[0]

    data = {
        'num_missing': num_missing, 
        'percent_missing (%)': [round(x, 2) for x in num_missing / num_rows * 100]
    }

    stats = pd.DataFrame(data)

        return stats[stats['num_missing'] != 0]


def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str,title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.scatterplot(data = df, x=x_col, y=y_col)
    plt.title(f'{title}')
    plt.xticks(fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()

def plot_heatmap(df:pd.DataFrame, title:str, cbar=False)->None:
    plt.figure(figsize=(12, 7))
    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )
    plt.title(title, size=18, fontweight='bold')
    plt.show()

def plot_hist(df:pd.DataFrame, column:str, color:str)->None:
            sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()

def mult_hist(sr, rows, cols, title_text, subplot_titles, interactive=False):
    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)
    for i in range(rows):
        for j in range(cols):
            x = ["-> " + str(i) for i in sr[i+j].index]
            fig.add_trace(go.Bar(x=x, y=sr[i+j].values), row=i+1, col=j+1)
    fig.update_layout(showlegend=False, title_text=title_text)
    if(interactive):
        fig.show()
    else:
        return Image(pio.to_image(fig, format='png', width=1200))
import numpy as np

import matplotlib.pyplot as plt

%matplotlib inline

import math

import seaborn as sns

import plotly.io as pio

from plotly import *

import plotly.express as px

import plotly.graph_objects as go  

import pandas as pd

from IPython.display import Image

from sklearn import preprocessing

from sklearn.cluster import KMeans

from scipy.spatial.distance import cdist

from sklearn.preprocessing import StandardScaler, normalize

import os,sys

import kaleido

from plotly.subplots import make_subplots

sys.path.insert(0,'../scripts/')

from process import *
df = pd.read_csv('../data/telecom_clean_data.csv', na_values=['?',"n.a.","NA","n/a", "na", None])

df.drop("Unnamed: 0",axis=1,inplace=True)

pd.set_option('display.float_format', lambda x: '{:,.0f}'.format(x))

df.head(5)




users_data = df.groupby('MSISDN/Number')




users_sessions= users_data['Bearer Id'].count()

users_sessions.head(10)





users_sessions= users_data['Dur. (ms)'].sum()

users_sessions

df["Total Uploads"]=df["Google UL (Bytes)"]+df["Email UL (Bytes)"]+df["Social Media UL (Bytes)"]+df["Youtube UL (Bytes)"]+df["Netflix UL (Bytes)"]+df["Gaming UL (Bytes)"]+df["Other UL (Bytes)"]

df["Total Downloads"]=df["Google DL (Bytes)"]+df["Email DL (Bytes)"]+df["Social Media DL (Bytes)"]+df["Youtube DL (Bytes)"]+df["Netflix DL (Bytes)"]+df["Gaming DL (Bytes)"]+df["Other DL (Bytes)"]

df['Total UL and DL']=df['Total DL (Bytes)']+df["Total UL (Bytes)"]








df["Youtube_Total_Data"]=df["Youtube DL (Bytes)"]+df["Youtube UL (Bytes)"]

df["Google_Total_Data"]=df["Google DL (Bytes)"]+df["Google UL (Bytes)"]

df["Email_Total_Data"]=df["Email DL (Bytes)"]+df["Email UL (Bytes)"]

df["Social_Media_Total_Data"]=df["Social Media DL (Bytes)"]+df["Social Media UL (Bytes)"]

df["Netflix_Total_Data"]=df["Netflix DL (Bytes)"]+df["Netflix UL (Bytes)"]

df["Gaming_Total_Data"]=df["Gaming DL (Bytes)"]+df["Gaming UL (Bytes)"]

df["Other_Total_Data"]=df["Other DL (Bytes)"]+df["Other UL (Bytes)"]

users = df[['MSISDN/Number', 'Bearer Id', 'Dur. (ms).1', 'Total UL and DL']].copy().rename(columns={'Dur. (ms).1': 'time_duration'})

users

users = users.groupby('MSISDN/Number').agg({'Bearer Id': 'count', 'time_duration': 'sum', 'Total UL and DL': 'sum'})

users = users.rename(columns={'Bearer Id': 'sessions'})

users.head(10)

users.nlargest(10, 'time_duration')


users.nlargest(10, 'time_duration')
sessions = users.nlargest(10, "sessions")['sessions']

duration = users.nlargest(10, "time_duration")['time_duration']

data_volume = users.nlargest(10, "Total UL and DL")['Total UL and DL']



mult_hist([sessions, duration, data_volume], 1,3, "User metrix", ['sessions', 'time_duration','Total UL and DL'])

users.boxplot()
scaler = StandardScaler()

scaled_array = scaler.fit_transform(users)

pd.DataFrame(scaled_array).head(5)



data_normalized = normalize(scaled_array)

pd.DataFrame(data_normalized).head(5)



data_normalized
kmeans = KMeans(n_clusters=3, random_state=0).fit(data_normalized)

kmeans.labels_
users.insert(0, 'Cluster', kmeans.labels_)

users.head(5)
fig = px.scatter(users, x='Total UL and DL', y="time_duration", color='Cluster', size='sessions')

Image(pio.to_image(fig, format='png', width=1200))





cluster1 = users[users["Cluster"]==0]

cluster1.describe()
cluster1 = users[users["Cluster"]==1]

cluster1.describe()


apps_df = df.groupby('MSISDN/Number').agg({'Gaming_Total_Data': 'sum', 'Youtube_Total_Data': 'sum', 'Netflix_Total_Data': 'sum',\

                     'Google_Total_Data': 'sum', 'Email_Total_Data': 'sum', 'Social_Media_Total_Data': 'sum', 'Other_Total_Data': 'sum'})

apps_df.head(10)
Gaming_Data = apps_df.nlargest(10, "Gaming_Total_Data")['Gaming_Total_Data']

Youtube_Data = apps_df.nlargest(10, "Youtube_Total_Data")['Youtube_Total_Data']

Netflix_Data = apps_df.nlargest(10, "Netflix_Total_Data")['Netflix_Total_Data']

Google_Data = apps_df.nlargest(10, "Google_Total_Data")['Google_Total_Data']

Email_Data = apps_df.nlargest(10, "Email_Total_Data")['Email_Total_Data']

Social_Media = apps_df.nlargest(10, "Social_Media_Total_Data")['Social_Media_Total_Data']

Other_Data = apps_df.nlargest(10, "Other_Total_Data")['Other_Total_Data']





mult_hist([Gaming_Data, Youtube_Data, Netflix_Data], 1,

          3, "User metrix", ["Gaming_Data", "Youtube_Data", "Netflix_Data"])



top_3 = apps_df.sum()

type(top_3)



top_3
import matplotlib.pyplot as plt




applications = ['Gaming', 'Youtube', 'Netflix', 'Google', 'Email', 'Social Media', 'Other']

data_values = [63910124731666, 3362537620065, 3360563670772, 1159544186717, 335503000148, 271478798103, 63770726326023]




sorted_data = sorted(zip(applications, data_values), key=lambda x: x[1], reverse=True)




top_applications = [app for app, _ in sorted_data[:3]]

top_values = [value for _, value in sorted_data[:3]]




plt.figure(figsize=(8, 6))




plt.bar(top_applications, top_values, color=['blue', 'green', 'orange'])

plt.title('Top 3 Most Used Applications')

plt.xlabel('Applications')

plt.ylabel('Total Data (in bytes)')




for i, value in enumerate(top_values):

    plt.text(i, value + 0.002 * max(top_values), f'{value:,}', ha='center')




plt.show()












def choose_kmeans(df: pd.DataFrame, num: int):

  distortions = []

  inertias = []

  K = range(1, num)

  for k in K:

    kmeans = KMeans(n_clusters=k, random_state=0).fit(df)

    distortions.append(sum(

        np.min(cdist(df, kmeans.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])

    inertias.append(kmeans.inertia_)



  return (distortions, inertias)

distortions, inertias = choose_kmeans(data_normalized, 1)
fig = make_subplots(

    rows=1, cols=2, subplot_titles=("Distortion", "Inertia")

)

fig.add_trace(go.Scatter(x=np.array(range(1, 15)), y=distortions), row=1, col=1)

fig.add_trace(go.Scatter(x=np.array(range(1, 15)), y=inertias), row=1, col=2)

fig.update_layout(title_text="The Elbow Method")


Image(pio.to_image(fig, format='png', width=1200))



kmeans = KMeans(n_clusters=4, random_state=0).fit(data_normalized)

users["cluster"]= kmeans.labels_

users
import pickle
with open("../models/users.pkl", "wb") as f:

    pickle.dump(kmeans, f)
import numpy as np

import matplotlib.pyplot as plt

%matplotlib inline

import math

import seaborn as sns

import plotly.io as pio

from plotly import *

import plotly.express as px

import plotly.graph_objects as go  

import pandas as pd

from sklearn import preprocessing

from sklearn.cluster import KMeans

from scipy.spatial.distance import cdist

from sklearn.preprocessing import StandardScaler, normalize

from IPython.display import Image

import os,sys

import kaleido

from plotly.subplots import make_subplots

sys.path.insert(0,'../scripts/')

from process import *
telecom_df = pd.read_csv('../data/telecom.csv')

telecom_df.drop("Unnamed: 0",axis=1,inplace=True)

pd.set_option('display.float_format', lambda x: '{:,.0f}'.format(x))
telecom_df.head()













df = telecom_df[['MSISDN/Number','Handset Type','Avg RTT DL (ms)','Avg RTT UL (ms)',

                 'TCP DL Retrans. Vol (Bytes)','TCP UL Retrans. Vol (Bytes)',

                 'Avg Bearer TP DL (kbps)','Avg Bearer TP UL (kbps)']]

df['MSISDN/Number'].fillna(value=df['MSISDN/Number'].mean(), inplace=True)



df.head(10)

df['Avg RTT DL (ms)'].fillna(value=df['Avg RTT DL (ms)'].mean(), inplace=True)

df['Avg Bearer TP DL (kbps)'].fillna(value=df['Avg Bearer TP DL (kbps)'].mean(), inplace=True)

df['Avg Bearer TP UL (kbps)'].fillna(value=df['Avg Bearer TP UL (kbps)'].mean(), inplace=True)

df['TCP DL Retrans. Vol (Bytes)'].fillna(value=df['TCP DL Retrans. Vol (Bytes)'].mean(), inplace=True)

df['TCP UL Retrans. Vol (Bytes)'].fillna(value=df['TCP UL Retrans. Vol (Bytes)'].mean(), inplace=True)

df['Avg Bearer TP DL (kbps)'].fillna(value=df['Avg Bearer TP DL (kbps)'].mean(), inplace=True)

df['Avg Bearer TP UL (kbps)'].fillna(value=df['Avg Bearer TP UL (kbps)'].mean(), inplace=True)

df['Total_Avg_RTT'].fillna(value=df['Total_Avg_RTT'].mean(),inplace=True)



plt.figure(figsize=(12, 7))

sns.boxplot(data=df, x='Avg RTT DL (ms)')

plt.title("box plot for Avg RTT DL (ms)", size=20)

plt.show()





plt.figure(figsize=(12, 7))

sns.boxplot(data=df, x='Avg RTT UL (ms)')

plt.title("box plot for Avg RTT UL (ms)", size=20)

plt.show()
plt.figure(figsize=(12, 7))

sns.boxplot(data=df, x='Avg Bearer TP DL (kbps)')

plt.title("TCP DL Retrans. Vol (Bytes)", size=20)

plt.show()
df['Avg RTT DL (ms)'] = np.where(df['Avg RTT DL (ms)'] > 220, 54, df['Avg RTT DL (ms)'])

df['Avg RTT UL (ms)'].describe()

df['Avg RTT DL (ms)'].describe()

avg_tp_uldl_columns = [

    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)'

]

avg_tp_uldl = df[avg_tp_uldl_columns].sum()

handsets_data = df.groupby('Handset Type')





handsets_tp = handsets_data[avg_tp_uldl_columns].mean().sum(axis=1)

handsets_tp.plot()



df['Total_Avg_RTT'] = df['Avg RTT DL (ms)'] + \

                                      df['Avg RTT UL (ms)']



df['Total_Avg_Bearer_TP'] = df['Avg Bearer TP DL (kbps)'] + \

                                      df['Avg Bearer TP DL (kbps)']



df['Total_Avg_TCP'] = df['TCP DL Retrans. Vol (Bytes)'] + \

                                      df['TCP UL Retrans. Vol (Bytes)']



df['Total_Avg_RTT'].fillna(value=df['Total_Avg_RTT'].mean(),inplace=True)










sorted_by_tcp = df.sort_values(

    'Total_Avg_TCP', ascending=False)

top_10 = sorted_by_tcp.head(10)['Total_Avg_TCP']

last_10 = sorted_by_tcp.tail(10)['Total_Avg_TCP']

most_10 = df['Total_Avg_TCP'].value_counts().head(10)
def mult_hist(sr, rows, cols, title_text, subplot_titles, interactive=False):

    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)

    for i in range(rows):

        for j in range(cols):

            x = ["-> " + str(i) for i in sr[i+j].index]

            fig.add_trace(go.Bar(x=x, y=sr[i+j].values), row=i+1, col=j+1)

    fig.update_layout(showlegend=False, title_text=title_text)

    if(interactive):

        fig.show()

    else:

        return Image(pio.to_image(fig, format='png', width=1200))

mult_hist([top_10, last_10, most_10], 1,

          3, "TCP values in the dataset", ['Top 10', 'Last 10', 'Most 10'])
sorted_by_RTT = df.sort_values(

    'Total_Avg_RTT', ascending=False)

top_10 = sorted_by_RTT.head(10)['Total_Avg_RTT']

last_10 = sorted_by_RTT.tail(10)['Total_Avg_RTT']

most_10 = df['Total_Avg_RTT'].value_counts().head(10)





mult_hist([top_10, last_10, most_10], 1,

          3, "RTT values in the dataset", ['Top 10', 'Last 10', 'Most 10'])

sorted_by_Bearer_TP = df.sort_values(

    'Total_Avg_Bearer_TP', ascending=False)

top_10 = sorted_by_Bearer_TP.head(10)['Total_Avg_Bearer_TP']

last_10 = sorted_by_Bearer_TP.tail(10)['Total_Avg_Bearer_TP']

most_10 = df['Total_Avg_Bearer_TP'].value_counts().head(10)
mult_hist([top_10, last_10, most_10, top_10], 1,

          3, "Throughput values in the dataset", ['Top 10', 'Last 10', 'Most 10'])



handset_type_agg = df.groupby('Handset Type').agg(

    {'Total_Avg_Bearer_TP': 'mean', 'Total_Avg_TCP': 'mean'})

handset_type_agg.head()



sorted_by_tp = handset_type_agg.sort_values('Total_Avg_Bearer_TP', ascending=False)

top_tp = sorted_by_tp['Total_Avg_Bearer_TP']



sns.distplot(top_tp)

pd.DataFrame(top_tp.describe()) 
metrics = df[[

    "Total_Avg_RTT",

    "Total_Avg_Bearer_TP",

    "Total_Avg_TCP"]].copy()

metrics.head()
scaler = StandardScaler()

scaled_array = scaler.fit_transform(metrics)

pd.DataFrame(scaled_array).head()


data_normalized = normalize(scaled_array)

pd.DataFrame(data_normalized).head(5)

kmeans = KMeans(n_clusters=3, random_state=0).fit(data_normalized)

kmeans.labels_
df.insert(0, 'Cluster', kmeans.labels_)

df.head(10)

import pickle

with open("../models/user_exp.pkl", "wb") as f:

    pickle.dump(kmeans, f)
import os
import sys
import streamlit as st
import os, sys
sys.path.insert(0, './dashboard')

from multiapp import MultiApp
import model, user_engagement_analysis, user_experience_analytics, user_satisfaction_analysis,user_overview_analysis

st.set_page_config(page_title="Telecom User Data Visualization", layout="wide")

app = MultiApp()

st.sidebar.markdown("""
""")

app.add_app("user_overview",user_overview_analysis.app)
app.add_app("user_engagement", user_engagement_analysis.app)
app.add_app("experience_analytics", user_experience_analytics.app)
app.add_app("satisfaction_analysis", user_satisfaction_analysis.app)
app.add_app("Model", model.app)

app.run()
import streamlit as st
import pandas as pd
import os
import sys
import matplotlib.pylab as plt
sys.path.insert(0,'../scripts/')
df_session = pd.read_csv('data/top10_user_session.csv')

def app():

    st.title("Telecommunication User Engagement Analysis")

    df_email = pd.read_csv('data/top10_email_users.csv')
    df_game = pd.read_csv('data/top10_gameApp_users.csv')
    df_google = pd.read_csv('data/top10_google_users.csv')
    df_netflix = pd.read_csv('data/top10_netflix_users.csv')
    df_otherAct = pd.read_csv('data/top10_otherAct_users.csv')
    df_social = pd.read_csv('data/top10_socialMedia_users.csv')
    df_youtube = pd.read_csv('data/top10_youtube_users.csv')
    df_session = pd.read_csv('data/top10_user_session.csv')
    df_DLUL = pd.read_csv('data/top10_DLUL_users.csv')

    st.header("Data transfers and overall data usage correlation.")
    st.image('data/corellation.png')
    st.markdown(
        'There is a correlation between data transfers and total data usage in games and other apps.')

    st.header("Top 10 Users Engaged Per Application")
    st.subheader("Email App users")
    st.dataframe(df_email)
    st.bar_chart(df_email.Email_Total_Data)

    st.subheader("Game App users")
    st.dataframe(df_game)
    st.bar_chart(df_game.Gaming_Total_Data)

    st.subheader("Google App users")
    st.dataframe(df_google)
    st.bar_chart(df_google.Google_Total_Data)

    st.subheader("Netflix App users")
    st.dataframe(df_netflix)
    st.bar_chart(df_netflix.Netflix_Total_Data)

    st.subheader("Other App users")
    st.dataframe(df_otherAct)
    st.bar_chart(df_otherAct.Other_Total_Data)

    st.subheader("Social Media App users")
    st.dataframe(df_social)
    st.bar_chart(df_social.Social_Media_Total_Data)

    st.subheader("Youtube App users")
    st.dataframe(df_youtube)
    st.bar_chart(df_youtube.Youtube_Total_Data)

    st.subheader("Top 10 users based on session count")
    df_session = df_session.drop(columns=['Unnamed: 0'])
    st.dataframe(df_session)
    st.bar_chart(df_session['Dur. (ms)'])

    st.subheader("Top 10 users based on download and upload count")
    st.dataframe(df_DLUL)
    st.bar_chart(df_DLUL['Total UL and DL'])

    st.header("3 groups k-means clustering")
    st.image('data/engclusters.png')
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import os
import sys
import seaborn as sns
import numpy as np
import streamlit as st
import pandas as pd
import plotly.express as px

def app():

    st.title("Telecommunication User Over View Analysis")
    df_over_view = pd.read_csv('data/telecom_clean_data.csv')

    st.header("Top 10 handsets used by the customers")
    st.bar_chart(df_over_view['Handset Type'].value_counts()[0:10])

    st.header("Top 3 handset manufacturers")
    st.bar_chart(df_over_view['Handset Manufacturer'].value_counts()[0:3])

    st.header("Top 5 handsets per top 3 handset manufacturer")
    


    top_apple = df_over_view.loc[df_over_view['Handset Manufacturer'] == 'Apple']

    top_apple = df_over_view.loc[df_over_view['Handset Manufacturer'] == 'Apple']

    top_apple = top_apple.groupby(['Handset Manufacturer', 'Handset Type']).agg({'Handset Type': ['count']})
    top_apple.columns = ['count']

    top_apple = top_apple.nlargest(5, 'count')


    fig, ax = plt.subplots()
    top_apple.plot.bar(y='count', ax=ax, stacked=True, color='    ax.set_title("Top 5 Apple Handsets")
    ax.set_xlabel("Handset Type")
    ax.set_ylabel("Count")

    st.pyplot(fig)

    st.header('Top 5 Samsung Handsets')

    top_samsung = df_over_view.loc[df_over_view['Handset Manufacturer'] == 'Samsung']
    top_samsung = top_samsung.groupby(['Handset Manufacturer', 'Handset Type']).agg({'Handset Type': ['count']})
    top_samsung.columns = ['count']
    top_samsung = top_samsung.nlargest(5, 'count')

    fig, ax = plt.subplots()
    top_samsung.plot.bar(y='count', ax=ax, stacked=True, color='    plt.xlabel("Handset Type")
    plt.ylabel("Count")
    st.pyplot(fig)



    st.header('Top 5 Huawei Handsets')

    top_huawei = df_over_view.loc[df_over_view['Handset Manufacturer'] == 'Huawei']

    top_huawei = top_huawei.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')

    top_huawei = top_huawei.nlargest(5, 'count')



    fig, ax = plt.subplots()
    ax.bar(top_huawei['Handset Type'], top_huawei['count'], color='    ax.set_title("Top 5 Huawei Handsets")
    ax.set_xlabel("Handset Type")
    ax.set_ylabel("Count")

    st.pyplot(fig)

    
    st.set_option('deprecation.showPyplotGlobalUse', False)


    st.title('number of xDR sessions')

    column_to_plot = 'Dur. (ms).1' 
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()


    st.header('Total Upload')


    column_to_plot = 'Total UL (Bytes)' 
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()



    st.header('Total Downloads Bytes')

    column_to_plot = 'Total DL (Bytes)' 
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()


    st.header('Social Media Total Upload and Download Data')

    column_to_plot = 'Social_Media_Total_Data' 
    df_over_view['Social_Media_Total_Data'] = df_over_view['Social Media DL (Bytes)'] + df_over_view['Social Media UL (Bytes)']
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()


    st.header('Google Total Data')

    column_to_plot = 'Google_Total_Data' 
    df_over_view['Google_Total_Data'] = df_over_view['Google DL (Bytes)'] + df_over_view['Google UL (Bytes)']
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()

    st.header('Total Email Data')

    column_to_plot = 'Email_Total_Data' 
    df_over_view['Email_Total_Data'] = df_over_view['Email DL (Bytes)'] + df_over_view['Email UL (Bytes)']
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()



    st.header('Total Youtube Data')

    column_to_plot = 'Youtube_Total_Data' 
    df_over_view['Youtube_Total_Data'] = df_over_view['Youtube DL (Bytes)'] + df_over_view['Youtube UL (Bytes)']
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()


    st.header('Total Netflix Data')

    column_to_plot = 'Netflix_Total_Data' 
    df_over_view['Netflix_Total_Data'] = df_over_view['Netflix DL (Bytes)'] + df_over_view['Netflix UL (Bytes)']
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()



    st.header('Total Gaming Data')

    column_to_plot = 'Gaming_Total_Data' 
    df_over_view['Gaming_Total_Data'] = df_over_view['Gaming DL (Bytes)'] + df_over_view['Gaming UL (Bytes)']
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()

    st.header('Other Total Data')

    column_to_plot = 'Other_Total_Data' 
    df_over_view["Other_Total_Data"]=df_over_view["Other DL (Bytes)"]+df_over_view["Other UL (Bytes)"]
    plt.figure(figsize=(8, 6))
    sns.histplot(df_over_view[column_to_plot], color='dodgerblue', kde=True)
    plt.title(f'Distrubution of {column_to_plot}')
    plt.xlabel(column_to_plot)
    plt.ylabel('Frequency')
    st.pyplot()
    



  
    st.header('Total Data Vs Gaming Data Usage')

    x_column = df_over_view["Gaming_Total_Data"]=df_over_view["Gaming DL (Bytes)"]+df_over_view["Gaming UL (Bytes)"]
    y_column = df_over_view["Total UL and DL"]=df_over_view["Total UL (Bytes)"]+df_over_view["Total DL (Bytes)"]


    st.scatter_chart(df_over_view[['Gaming_Total_Data', 'Total UL and DL']].sample(1000))


    st.header('Total Data Vs Youtube Total Data')

    x_column = df_over_view["Youtube_Total_Data"]=df_over_view["Youtube DL (Bytes)"]+df_over_view["Youtube UL (Bytes)"]
    y_column = df_over_view["Total UL and DL"]=df_over_view["Total UL (Bytes)"]+df_over_view["Total DL (Bytes)"]


    st.scatter_chart(df_over_view[['Youtube_Total_Data', 'Total UL and DL']].sample(1000))




    st.header('Total Data Vs Email Total Data')

    x_column = df_over_view["Email_Total_Data"]=df_over_view["Email DL (Bytes)"]+df_over_view["Email UL (Bytes)"]
    y_column = df_over_view["Total UL and DL"]=df_over_view["Total UL (Bytes)"]+df_over_view["Total DL (Bytes)"]


    st.scatter_chart(df_over_view[['Email_Total_Data', 'Total UL and DL']].sample(1000))

    st.header('Total Data Vs Social Media Total Data')

    x_column = df_over_view["Social_Media_Total_Data"]=df_over_view["Social Media DL (Bytes)"]+df_over_view["Social Media UL (Bytes)"]
    y_column = df_over_view["Total UL and DL"]=df_over_view["Total UL (Bytes)"]+df_over_view["Total DL (Bytes)"]


    st.scatter_chart(df_over_view[['Social_Media_Total_Data', 'Total UL and DL']].sample(1000))


    st.header('Total Data Vs Netflix Data')

    x_column = df_over_view["Netflix_Total_Data"]=df_over_view["Netflix DL (Bytes)"]+df_over_view["Netflix UL (Bytes)"]
    y_column = df_over_view["Total UL and DL"]=df_over_view["Total UL (Bytes)"]+df_over_view["Total DL (Bytes)"]


    st.scatter_chart(df_over_view[['Netflix_Total_Data', 'Total UL and DL']].sample(1000))


    st.header('Total Data Vs Other_Total_Data')

    x_column = df_over_view['Other_Total_Data'] = df_over_view["Other DL (Bytes)"] + df_over_view['Other UL (Bytes)']
    y_column = df_over_view["Total UL and DL"]=df_over_view["Total UL (Bytes)"]+df_over_view["Total DL (Bytes)"]


    st.scatter_chart(df_over_view[['Other_Total_Data', 'Total UL and DL']].sample(1000))



    columns = ['Youtube_Total_Data', 'Google_Total_Data', 'Email_Total_Data','Social_Media_Total_Data', 'Netflix_Total_Data', 'Gaming_Total_Data', 'Other_Total_Data', 'Total UL and DL']
    corr = df_over_view[columns].corr()
    
    st.title("Correlation of Usage of User Data Volume")

    st.write("Correlation Matrix:")
    st.write(corr)
import sys

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns
import numpy as np

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from sklearn.impute import SimpleImputer

from sklearn.cluster import KMeans
sys.path.append('script')

sys.path.append('analysis')
from script import dbconn

from analysis import data_preprocessing

from analysis  import user_overview_analysis

from analysis import user_engagement_analysis
df_telecom = data_preprocessing.load_data_from_postgres()

df_telecom.head(5)
df_telecom.isnull().sum()
df1 = data_preprocessing.load_data_from_sqlalchemy()

df1.info()
df_clean = data_preprocessing.clear_data(df1)

df_clean.head()
df_clean.info()
final_data = data_preprocessing.read_data()

final_data.head(10)
final_data.describe()
user_overview_analysis.history_graph(df_clean)
df_clean.isnull().sum()
final_data.describe()
user_overview_analysis.top_handsets_identifier(final_data)
user_overview_analysis.top_manfacturers(final_data)
user_overview_analysis.handsets_per_manufacturers(final_data)
user_overview_analysis.user_aggregation(final_data)
display(user_overview_analysis.total_user_aggregation(final_data).head(10))
data_description = final_data.dtypes.reset_index()

data_description.columns = ['Variable', 'Data Type']



data_description
user_overview_analysis.dispersion_parameters(final_data).head(20)
user_overview_analysis.histogram_of_session_duration(final_data)
user_overview_analysis.bivariate_analysis(final_data)
user_overview_analysis.correlation_matrix(final_data)
user_overview_analysis.principal_variance(final_data)
user_engagement_analysis.user_engagement_metrics(final_data)
user_engagement_analysis.normalized_engagment_metrics(final_data)
user_engagement_analysis.app_traffic(final_data)
user_engagement_analysis.top_apps_per_user_engagement(final_data)
user_engagement_analysis.KElbowVisualizer_on_normalized_metrics(final_data)
import pandas.io.sql as sqlio
import psycopg2
from psycopg2 import sql
from sqlalchemy import create_engine
import pandas as pd


def db_connection_sqlalchemy():

    engine = create_engine('postgresql+psycopg2://postgres:1234@localhost/telecom')

    return engine

def db_read_table_sqlalchemy(engine, table_name):
    query = f'SELECT * FROM {table_name}'
    df= pd.read_sql_query(query, engine)
    return df


def db_connection_psycopg():
    
    pgconn = psycopg2.connect(dbname="telecom",user="postgres",password="1234",host="localhost",port="5432")
    return pgconn


def db_read_table_psycopg(pgconn, table_name):
    sql = f'SELECT * FROM {table_name}'
    df = sqlio.read_sql_query(sql, pgconn)
    return df

def db_write_table_psycopg(pgconn, tablename, df):
    pass

def db_delete_table_pyscopg():
    cursor = pgconn.cursor()
    
    drop_table_query = sql.SQL("DROP TABLE IF EXISTS {} CASCADE").format(sql.Identifier(table_name))

    cursor.execute(drop_table_query)

    pgconn.commit()

    print(f"Table `{table_name}` has been successfully deleted.")

    if cursor:
        cursor.close()

def marketing_recommendation(df,self):
    interpretation = "Based on the user overview analysis, the marketing team should focus on:"
    
        top_handsets_list = top_handsets(df).index.tolist()
    interpretation += f"\n- Promoting the top handsets: {', '.join(top_handsets_list)}"

        top_manufacturers_list = top_manufacturers(df).index.tolist()
    interpretation += f"\n- Collaborating with the top manufacturers: {', '.join(top_manufacturers_list)}"

        for manufacturer in top_manufacturers_list:
        top_handsets_list = top_handsets_per_manufacturer(df, manufacturer).index.tolist()
        interpretation += f"\n- Highlighting top handsets for {manufacturer}: {', '.join(top_handsets_list)}"

    return interpretation
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import joblib
import pymysql

def assign_engagement_experience_scores(df, clustering_results, features_for_clustering):
  
    scaler = StandardScaler()
    user_data_for_clustering = df[features_for_clustering].copy()
    user_data_for_clustering_scaled = scaler.fit_transform(user_data_for_clustering)

    less_engaged_cluster_center = clustering_results.groupby('Cluster')[features_for_clustering].mean().iloc[0]
    engagement_scores = np.linalg.norm(user_data_for_clustering_scaled - less_engaged_cluster_center, axis=1)


    worst_experience_cluster_center = clustering_results.groupby('Cluster')[features_for_clustering].mean().idxmax()
    experience_scores = np.linalg.norm(user_data_for_clustering_scaled - worst_experience_cluster_center, axis=1)

    df['Engagement Score'] = engagement_scores
    df['Experience Score'] = experience_scores

    return df

def calculate_satisfaction_score(df):
 
    df['Satisfaction Score'] = (df['Engagement Score'] + df['Experience Score']) / 2
    return df

def top_satisfied_customers(df, n=10):
 
    top_satisfied = df.nlargest(n, 'Satisfaction Score')
    return top_satisfied

def build_regression_model(df, target_column='Satisfaction Score'):
    
    features = ['Engagement Score', 'Experience Score']
    X = df[features]
    y = df[target_column]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = LinearRegression()
    model.fit(X_train, y_train)

   
    y_pred = model.predict(X_test)

   
    mse = mean_squared_error(y_test, y_pred)

    return model, mse

def kmeans_clustering_2(df, features, n_clusters=2):
    
    scaler = StandardScaler()
    data_for_clustering = df[features].copy()
    data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df['Cluster 2'] = kmeans.fit_predict(data_for_clustering_scaled)

   
    cluster_summary = df.groupby('Cluster 2')[features].mean()

    return df, cluster_summary

def aggregate_scores_per_cluster(df, cluster_column='Cluster 2'):
   
    agg_per_cluster = df.groupby(cluster_column)['Satisfaction Score', 'Experience Score'].mean()
    return agg_per_cluster

def export_to_mysql(df, table_name='satisfaction_scores', host='localhost', user='root', password='password', database='telecom_analysis'):
   
    connection = pymysql.connect(host=host, user=user, password=password, database=database)
    cursor = connection.cursor()

    
    create_table_query = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        `MSISDN/Number` INT PRIMARY KEY,
        `Engagement Score` FLOAT,
        `Experience Score` FLOAT,
        `Satisfaction Score` FLOAT,
        `Cluster 2` INT
    );
    """
    cursor.execute(create_table_query)

   
    insert_query = f"""
    INSERT INTO {table_name} (`MSISDN/Number`, `Engagement Score`, `Experience Score`, `Satisfaction Score`, `Cluster 2`)
    VALUES (%s, %s, %s, %s, %s);
    """

    for _, row in df.iterrows():
        cursor.execute(insert_query, (row['MSISDN/Number'], row['Engagement Score'], row['Experience Score'], row['Satisfaction Score'], row['Cluster 2']))

    connection.commit()
    connection.close()

if __name__ == "__main__":
    
    features_for_clustering = ['TCP Retransmission', 'RTT', 'Throughput']

    df = assign_engagement_experience_scores(df, clustering_results, features_for_clustering)
   
    print(df[['MSISDN/Number', 'Engagement Score', 'Experience Score']].head())

   
    df = calculate_satisfaction_score(df)
    top_satisfied = top_satisfied_customers(df, n=10)
    print("\nTask 5.2 - Top Satisfied Customers:")
    print(top_satisfied[['MSISDN/Number', 'Satisfaction Score']])

   
    regression_model, mse = build_regression_model(df)
    print("\nTask 5.3 - Regression Model Evaluation:")
    print(f"Mean Squared Error: {mse}")
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt

def clean_data(df):
    
    df_cleaned = df.copy()
    df_cleaned['TCP Retransmission'] = df_cleaned['TCP Retransmission'].fillna(df_cleaned['TCP Retransmission'].mean())
    df_cleaned['RTT'] = df_cleaned['RTT'].fillna(df_cleaned['RTT'].mean())
    df_cleaned['Handset Type'] = df_cleaned['Handset Type'].fillna(df_cleaned['Handset Type'].mode()[0])
    df_cleaned['Throughput'] = df_cleaned['Throughput'].fillna(df_cleaned['Throughput'].mean())
    
   
    return df_cleaned

def aggregate_user_experience(df):
   
    user_experience_agg = df.groupby('MSISDN/Number').agg({
        'TCP Retransmission': 'mean',
        'RTT': 'mean',
        'Handset Type': 'first',          'Throughput': 'mean'
    }).reset_index()

    return user_experience_agg

def top_bottom_frequent_values(df, column, n=10):
   
    top_values = df[column].nlargest(n)
    bottom_values = df[column].nsmallest(n)
    frequent_values = df[column].value_counts().nlargest(n)

    return top_values, bottom_values, frequent_values

def compute_and_report_distribution(df, column_group, column_value, title):
    
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=column_group, y=column_value, data=df)
    plt.title(title)
    plt.show()

def kmeans_clustering(df, features, n_clusters=3):
    
    scaler = StandardScaler()
    data_for_clustering = df[features].copy()
    data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df['Cluster'] = kmeans.fit_predict(data_for_clustering_scaled)

   
    cluster_summary = df.groupby('Cluster')[features].mean()
    
    return df, cluster_summary
Repository Structure: '
' ├── percent_miss_each.py
├── util
│   └── utils.py
├── filmode.py
├── .gitignore
├── top
│   ├── top10.py
│   ├── top3.py
│   └── top5ofeachtop3.py
├── .ipynb_checkpoints
│   ├── tasks_nante-checkpoint.ipynb
│   └── tasks_nante_telecom-checkpoint.ipynb
├── Users_behaviour
│   ├── sum_of_two_data.py
│   ├── sum_of_many_couple.py
│   ├── session.py
│   └── total_session.py
├── notebook
│   └── tasks_nante_telecom.ipynb
├── percent_miss.py
└── nante_create_database.py
 '
' Commit History: 
{"insertions": [537], "deletions": [70], "lines": [607], "committed_datetime": ["2023-12-16 08:53:44"], "commit_count": 1} 
 Content: 
%reload_ext autoreload

%autoreload 2




import matplotlib.pyplot as plt

import matplotlib


import pandas as pd



from sqlalchemy import create_engine



database_name = 'telecom'

table_name = 'xdr_data'



connection_params ={"host": "localhost", "user": "postgres", "password":"myPassword", "port":"5432","database":database_name}

engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")

                       


sql_query='SELECT * FROM xdr_data Limit 300010'


df = pd.read_sql(sql_query,con=engine)

df.head(5)
df.info()
df.shape

len(df.columns)
len(df)

df.columns.tolist()
df.dtypes
df['Start'].dtypes

len(df['Bearer Id'].unique())


def get_data_float (datafr):

    fl_col = []

    for col in df.columns :

        if datafr[col].dtypes == 'float64' :

            fl_col = fl_col + [col]

    
    print ("number of float column = " , len(fl_col))

    return fl_col




def get_data_obj (datafr):

    obj_col = []

    for col in df.columns :

        if datafr[col].dtypes == 'O' :

            obj_col = obj_col + [col]

    
    print ("number of object column = " , len(obj_col))

    return obj_col
dtob = get_data_obj (df)

print(dtob)
dtfl = get_data_float (df)

print(dtfl)

df.describe()

df['Dur. (ms)'].describe()

df.isna().sum()


df.isna().sum().sum()

df['Dur. (ms)'].isna().sum()
import numpy as np




def percent_miss (data):

    

    
    T = np.product(data.shape)

    

    
    M = data.isna().sum().sum()

    

    
    P = round((M/T)*100,2)

    

    print ("The dataframe telecom has ", P , "%" , " missing values.")

    return P

    

percent_miss(df)

def percent_miss_each (datafr):

    a = (datafr.isna().sum() / len(df) )*100

    return a



percent_miss_each(df)

df[fl_col].skew(axis=0)
df['MSISDN/Number'].hist()
df['Dur. (ms)'].hist()
import seaborn as sns



sns.displot(data=df, x=df['Dur. (ms)'], kind='hist', log_scale=(10,10))

df_clean1 = df.drop(['TCP DL Retrans. Vol (Bytes)','TCP UL Retrans. Vol (Bytes)', 'HTTP DL (Bytes)','HTTP UL (Bytes)','Nb of sec with 125000B < Vol DL','Nb of sec with 1250B < Vol UL < 6250B','Nb of sec with 31250B < Vol DL < 125000B','Nb of sec with 37500B < Vol UL' ,'Nb of sec with 6250B < Vol DL < 31250B'  ,'Nb of sec with 6250B < Vol UL < 37500B'    ],axis=1)

df_clean1.shape









def filmode (datafr) :

    

    for col in datafr.columns :

        datafr[col] = datafr[col].fillna(datafr[col].mode()[0])

    return datafr



df_clean = filmode(df_clean1)

df_clean.shape
print (df_clean.head(5))

df_clean.isna().sum().sum()



def top10(datafr,IDcol,col):  
    
    UnId = datafr[IDcol].unique()

    

    

    
    UnHandset = datafr[col].unique()

    

    
    numb_use=[]

    for handset in UnHandset : 

        numb = len(datafr[datafr[col]==handset].index.tolist())

        numb_use = numb_use +[numb]



    df_handset = pd.DataFrame()

    df_handset['Handset type'] = UnHandset

    df_handset['number of users'] = numb_use 

 

    
    maxi_use = sorted(df_handset['number of users'], reverse = True)[:11]

   

    
    tops =[]

    for i in maxi_use : 

        row = df_handset[df_handset['number of users']== i].index.tolist()

        row = row[0]

        top = df_handset['Handset type'][row]

        tops=tops+[top]

    print("The top 10 of handset type are : ")

    return(tops)



    

top10(df_clean,'Bearer Id','Handset Type')   

def top3(datafr,IDcol,col):  


    
    UnManuf = datafr[col].unique() 
    numb_manu = len(UnManuf)                            


    
    numb_use_manu=[]

    for manu in UnManuf : 

        numb = len(datafr[datafr[col]==manu].index.tolist())

        numb_use_manu = numb_use_manu +[numb]



    
    df_manuf = pd.DataFrame()

    df_manuf['Handset Manufacturer'] = UnManuf

    df_manuf['Number of customers'] = numb_use_manu

    


    
    num_top_use = sorted(df_manuf['Number of customers'], reverse=True)[:3]

    


    
    top_manu =[]

    for num in num_top_use : 

        row = df_manuf[df_manuf['Number of customers']== num].index.tolist()

        row = row[0] 
        top = df_manuf['Handset Manufacturer'][row]

        top_manu=top_manu+[top]

    print("The top 3 manufacturers are :")

    return(top_manu)



top3(df_clean,'Bearer Id','Handset Manufacturer')

    






def findtop5 (datafr,manufcol,handsetcol,manuf):  

    

    
    row =datafr[datafr[manufcol]==manuf].index.tolist() 
    handset_list =[]

    for index in row :

        handset = datafr[handsetcol][index]

        handset_list = handset_list + [handset]

        handset_list =handset_list

    df_handset_and_manuf = pd.DataFrame()

    df_handset_and_manuf['Handset'] = handset_list

    handset_list = df_handset_and_manuf['Handset'].unique()  
    nb_hs_type = len( handset_list)

    

    
    
    numb_use_hs=[]

    for hs in handset_list : 

        numb = len(df_handset_and_manuf[df_handset_and_manuf['Handset']==hs].index.tolist())

        numb_use_hs = numb_use_hs +[numb]

        

        
    df_hs = pd.DataFrame()

    df_hs['Handset Type'] = handset_list

    df_hs['Number of users'] = numb_use_hs

    

    
    num_top_hs = sorted(df_hs['Number of users'], reverse=True)[:5]



    
    top_hs =[]

    for num in num_top_hs : 

        row = df_hs[df_hs['Number of users']== num].index.tolist()

        row = row[0] 
        top = df_hs['Handset Type'][row]

        top_hs=top_hs+[top]

    

    
    n=len(handset_list)

    

    print("There are ", n , "handset type of ", manuf , " and the top 5 handset types are : ", top_hs)   

    return(top_hs)




def top5ofeach(datafr,IDcol,manufcol,handsetcol) :

    top_manu = top3(datafr,IDcol,manufcol)

    for manuf in top_manu :

        a = findtop5(datafr,manufcol,handsetcol,manuf)

    return(a)

    

top5ofeach(df_clean, 'Bearer Id','Handset Manufacturer', 'Handset Type')










def session_num(datafr,usersIDcol, data) :

    

    
    userlist = datafr[usersIDcol].unique()

    

    
    Ss_num_ls = []

    for ID in userlist :

        row = datafr[datafr[usersIDcol]==ID].index.tolist()

        numb=len(row)

        Ss_num_ls = Ss_num_ls + [numb]

    

    
    df_Ss_num = pd.DataFrame()

    df_Ss_num[usersIDcol] = userlist

    df_Ss_num[data] = Ss_num_ls

    return df_Ss_num   

Session_bhv = session_num(df_clean,'Bearer Id', 'Number of xDR sessions')

print(Session_bhv.head(5))

Session_bhv = session_num(df_clean,'IMEI', 'Number of xDR sessions')

print(Session_bhv.head(5))

Session_bhv = session_num(df_clean,'IMSI', 'Number of xDR sessions')

print(Session_bhv.head(5))

Session_bhv = session_num(df_clean,'MSISDN/Number', 'Number of xDR sessions')

print(Session_bhv.head(5))

def table_user (datafr,usersIDcol, data) : 

    
    userlist = datafr[usersIDcol].unique()

    
    data1 = []

    for ID in userlist :

        row = datafr[datafr[usersIDcol]==ID].index.tolist()

        tot = 0

        for i in row : 

            row_n= i

            value_n = datafr[data][row_n]

            tot=tot+value_n

        data1 = data1 + [tot]

        

    
    df_behave = pd.DataFrame()

    df_behave[usersIDcol] = userlist

    df_behave[data] = data1

    return df_behave    

Dur_bhv_ID = table_user(df_clean,'Bearer Id','Dur. (ms)' )

print(Dur_bhv_ID.head(5))

Dur_bhv_ID.shape

Dur_bhv_IMEI = table_user(df_clean,'IMEI','Dur. (ms)' )

print(Dur_bhv_IMEI.head(5))

Dur_bhv_IMEI.shape

Dur_bhv_IMSI = table_user(df_clean,'IMSI','Dur. (ms)' )

print(Dur_bhv_IMSI.head(5))

Dur_bhv_IMSI.shape

Dur_bhv_MSISDN = table_user(df_clean,'MSISDN/Number','Dur. (ms)' )

print(Dur_bhv_MSISDN.head(5))

Dur_bhv_MSISDN.shape

def sumofdata (datafr,usersIDcol, data1, data2):

    
    DLT = table_user(datafr,usersIDcol,data1 )



    
    ULT = table_user(datafr,usersIDcol,data2 )



    
    ST = DLT[data1] + ULT[data2]



    
    df_DL_UL = pd.DataFrame()

    df_DL_UL[usersIDcol] =DLT[usersIDcol]

    df_DL_UL['Total of '+ data1 +' and '+data2] = ST

    return df_DL_UL 
tot_dl_ul_ID = sumofdata (df_clean,'Bearer Id', 'Total DL (Bytes)', 'Total UL (Bytes)')

print(tot_dl_ul_ID.head(5))

tot_dl_ul_IMEI = sumofdata (df_clean,'IMEI', 'Total DL (Bytes)', 'Total UL (Bytes)')

print(tot_dl_ul_IMEI.head(5))
tot_dl_ul_IMSI = sumofdata (df_clean,'IMSI', 'Total DL (Bytes)', 'Total UL (Bytes)')

print(tot_dl_ul_IMSI.head(5))
tot_dl_ul_MSIDN = sumofdata (df_clean,'MSISDN/Number', 'Total DL (Bytes)', 'Total UL (Bytes)')

print(tot_dl_ul_IMSI.head(5))





Applications =[['Social Media DL (Bytes)','Social Media UL (Bytes)'],['Youtube DL (Bytes)','Youtube UL (Bytes)'],['Netflix DL (Bytes)','Netflix UL (Bytes)'],['Google DL (Bytes)','Google UL (Bytes)'],['Email UL (Bytes)','Email DL (Bytes)'],['Gaming UL (Bytes)','Gaming DL (Bytes)'],['Total UL (Bytes)','Total DL (Bytes)']]



def tot_per_app (datafr,usersIDcol, Applist):

    

    
    dt_appli = pd.DataFrame()

    userlist = datafr[usersIDcol].unique()

    dt_appli[usersIDcol]= userlist

    for appli in Applist :

        tot_vol_ap = sumofdata(datafr,usersIDcol, appli[0], appli[1])

        
        dt_appli = pd.merge(dt_appli, tot_vol_ap)

    return dt_appli

Tot_dat_ID =tot_per_app (df_clean,'Bearer Id',Applications)

Tot_dat_ID.head(5) 

Tot_dat_IMEI =tot_per_app (df_clean,'IMEI',Applications)

Tot_dat_IMEI.head(5) 

Tot_dat_IMSI =tot_per_app (df_clean,'IMSI',Applications)

Tot_dat_IMSI.head(5) 

Tot_dat_MSISDN =tot_per_app (df_clean,'MSISDN/Number',Applications)

Tot_dat_MSISDN.head(5)  

Repository Structure: '
' └── Home.ipynb
 '
' Commit History: 
{"insertions": [774, 245, 637, 39, 100, 383], "deletions": [14, 19, 515, 19, 0, 284], "lines": [788, 264, 1152, 58, 100, 667], "committed_datetime": ["2023-12-16 21:05:05", "2023-12-16 21:17:12", "2023-12-16 21:52:57", "2023-12-16 21:57:26", "2023-12-16 22:19:51", "2023-12-16 22:36:20"], "commit_count": 6} 
 Content: 
import pandas as pd

import pandas.io.sql as sqilo

from sqlalchemy import create_engine

import psycopg2

from psycopg2 import sql



database_name = 'telecom'

table_name= 'xdr_data'

connection_params = { "host": "localhost", "user": "postgres", "password": "123",

"port": "5432", "database": database_name}

engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")


sql_query = 'SELECT * FROM xdr_data'

df = pd.read_sql(sql_query, con= engine)

df.info()
df.head()
import pandas as pd

import numpy as np

from IPython.display import Image

import seaborn as sns

import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings('ignore')
df.head()
df['Handset Type'].describe()

def percent_missing(df):



    
    totalCells = np.product(df.shape)



    
    missingCount = df.isnull().sum()



    
    totalMissing = missingCount.sum()



    
    print("The dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")

percent_missing(df)

df.isna().sum()

percent_missing(df['TCP UL Retrans. Vol (Bytes)'])
percent_missing(df['TCP UL Retrans. Vol (Bytes)'])
percent_missing(df['HTTP DL (Bytes)'])
percent_missing(df['HTTP DL (Bytes)'])

df_clean = df.drop(['Nb of sec with 37500B < Vol UL', 'Nb of sec with 6250B < Vol UL < 37500B', 'Nb of sec with 31250B < Vol DL < 125000B', 'Nb of sec with 1250B < Vol UL < 6250B', 'Nb of sec with 125000B < Vol DL', 'Nb of sec with 6250B < Vol DL < 31250B', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)' ,'HTTP DL (Bytes)', 'HTTP UL (Bytes)'], axis=1)

df_clean.shape
df_clean.isna().sum()
df['Dur. (ms)'].hist()
import seaborn as sns

sns.displot(data=df, x=df['Dur. (ms)'])



def plot_hist(df:pd.DataFrame, column:str, color:str)->None:

    
    
    sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)

    plt.title(f'Distribution of {column}', size=20, fontweight='bold')

    plt.show()



def plot_count(df:pd.DataFrame, column:str) -> None:

    plt.figure(figsize=(12, 7))

    sns.countplot(data=df, x=column)

    plt.title(f'Distribution of {column}', size=20, fontweight='bold')

    plt.show()



def plot_bar(df:pd.DataFrame, x_col:str, y_col:str, title:str, xlabel:str, ylabel:str)->None:

    plt.figure(figsize=(12, 7))

    sns.barplot(data = df, x=x_col, y=y_col)

    plt.title(title, size=20)

    plt.xticks(rotation=75, fontsize=14)

    plt.yticks( fontsize=14)

    plt.xlabel(xlabel, fontsize=16)

    plt.ylabel(ylabel, fontsize=16)

    plt.show()



def plot_heatmap(df:pd.DataFrame, title:str, cbar=False)->None:

    plt.figure(figsize=(12, 7))

    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )

    plt.title(title, size=18, fontweight='bold')

    plt.show()



def plot_box(df:pd.DataFrame, x_col:str, title:str) -> None:

    plt.figure(figsize=(12, 7))

    sns.boxplot(data = df, x=x_col)

    plt.title(title, size=20)

    plt.xticks(rotation=75, fontsize=14)

    plt.show()



def plot_box_multi(df:pd.DataFrame, x_col:str, y_col:str, title:str) -> None:

    plt.figure(figsize=(12, 7))

    sns.boxplot(data = df, x=x_col, y=y_col)

    plt.title(title, size=20)

    plt.xticks(rotation=75, fontsize=14)

    plt.yticks( fontsize=14)

    plt.show()



def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:

    plt.figure(figsize=(12, 7))

    sns.scatterplot(data = df, x=x_col, y=y_col, hue=hue, style=style)

    plt.title(title, size=20)

    plt.xticks(fontsize=14)

    plt.yticks( fontsize=14)

    plt.show()







pd.options.display.float_format = format_float

handset_counts = df['Handset Type'].value_counts()

top_10_handsets = handset_counts.head(10)

print("Top 10 handsets ")

print(top_10_handsets)
value_counts = df['Handset Type'].value_counts()[:10]

top_10_handsets = value_counts.index.tolist()





top_10_df = pd.DataFrame(value_counts)

top_10_df.plot(kind= 'bar',rot=80)

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.title('Top 10 Handsets')

plt.show(10)

handset_manufacturer=df['Handset Manufacturer'].value_counts()

top_3_handset_manufacturers=handset_manufacturer.head(3)

print("The top  3 handset manufacturers ")

print(top_3_handset_manufacturers)
value_counts = df['Handset Manufacturer'].value_counts()[:3]

top_3_manufacturers = value_counts.index.tolist()



top_3_df = pd.DataFrame(value_counts)

top_3_df.plot(kind='bar', rot=45)



plt.xlabel('Handset Manufacturer')

plt.ylabel('Count')

plt.title('Top 3 Handset Manufacturers')

plt.show()


top3_handsetManufacturers=handset_manufacturer.head(3).index

filtered_df = df[df['Handset Manufacturer'].isin(top3_handsetManufacturers)]

columns_of_interest=['Handset Manufacturer', 'Handset Type']

top_handsets_per_manufacturer = filtered_df.groupby(columns_of_interest).size().reset_index(name='count')

top_handsets_per_manufacturer = top_handsets_per_manufacturer.sort_values(by=['Handset Manufacturer', 'count'], ascending=[True, False])

top5_handsets_per_manufacturer = top_handsets_per_manufacturer.groupby('Handset Manufacturer').head(5)



print("Top 5 handsets per top 3 handset manufacturers and their counts")

print(top5_handsets_per_manufacturer)
df_clean['Total UL (Bytes)'].min()
plot_hist(df_clean, "Total UL (Bytes)", "blue")
plot_hist(df_clean, "Total DL (Bytes)", "blue")
plot_hist(df_clean, "UL TP > 300 Kbps (%)", "green")
plot_hist(df_clean, "End ms", "red")
df.head()

aggregated_data = df.groupby("MSISDN/Number").agg({"Total DL (Bytes)": "sum"})

aggregated_data['Total DL (GB)'] = aggregated_data['Total DL (Bytes)'] / 1073741824

plt.figure(figsize=(10, 6))

plt.plot(aggregated_data.index, aggregated_data['Total DL (GB)'])

plt.xlabel('MSISDN/Number')

plt.ylabel('Total Dowloaded GB')

plt.title('Total DL (GB) per MSISDN/Number')

plt.xticks(rotation=90)

plt.grid(True)

plt.show()
df.head()

aggregated_data = df.groupby("MSISDN/Number").agg({"Total UL (Bytes)": "sum"})

aggregated_data['Total UL (GB)'] = aggregated_data['Total UL (Bytes)'] / 1073741824

plt.figure(figsize=(10, 6))

plt.plot(aggregated_data.index, aggregated_data['Total UL (GB)'])

plt.xlabel('MSISDN/Number')

plt.ylabel('Total UL (GB)')

plt.title('Total UL (GB) per MSISDN/Number')

plt.xticks(rotation=90)

plt.grid(True)

plt.show()
df.head()

aggregated_data = df.groupby("MSISDN/Number").agg({"count"})

aggregated_data.head(10)

aggregated_data.plot()

plt.ylabel("No of Xdr Sessions")

plt.xlabel("Users")

plt.show() 

import numpy as np
import pandas as pd
from scipy import stats

class PreProcess:
    def __init__(self, data):
        self.data = data

    def remove_duplicate(self):
        return self.data.drop_duplicates()

    def remove_missing(self):
        '''
        This function removes all columns 
        with missing values of above 30%
        '''
        self.data = self.data.dropna(thresh=len(self.data)*0.7, axis=1)
        return self.data

    def convert_to_timestamp(self, column_name):
        '''
        This function converts the date column to timestamp
        '''
        self.data[column_name] = pd.to_datetime(self.data[column_name])
        return self.data
    
    def impute_mean_numeric(self):
        '''
        This function imputes mean values to missing values in numeric columns
        '''
        numeric_columns = self.data.select_dtypes(include=['float64', 'number']).columns
        self.data.loc[:, numeric_columns] = self.data.loc[:, numeric_columns].apply(lambda x: x.fillna(x.mean()))
        return self.data

    
    def impute_mean_datetime(self):
        '''
        This function imputes mean values to missing values in datetime columns
        '''
        datetime_columns = self.data.select_dtypes(include=['datetime64']).columns

        for col in datetime_columns:
            mean_timestamp = self.data[col].mean().timestamp()
            self.data.loc[:, col] = self.data[col].fillna(pd.to_datetime(mean_timestamp, unit='s'))

        return self.data
    
    def impute_mode_categorical(self):
        '''
        This function imputes mode values to missing values in categorical columns
        '''
        categorical_columns = self.data.select_dtypes(include=['object']).columns

        for col in categorical_columns:
            self.data.loc[:, col] = self.data[col].fillna(self.data[col].mode()[0])

        return self.data

    def remove_outliers(self):
        '''
        This function removes outliers from the dataset for numeric fields
        '''
        numeric_columns = self.data.select_dtypes(include=['float64', 'number']).columns
        z_scores = np.abs(stats.zscore(self.data[numeric_columns]))
        filtered_data = self.data[(z_scores < 3).all(axis=1)]
        self.data = filtered_data
        return filtered_data
%load_ext autoreload

%autoreload 2

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import os

import sys
import warnings

warnings.filterwarnings('ignore')

rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.preprocessing import PreProcess

from src.overview import Overview

from src.outlier import Outlier

from db_connection.database import Database
conn = Database(host='localhost',port=5432,user='basilel',dbname='telecom',password='Basi')

conn.connect()
query = "SELECT * FROM xdr_data;"

df_data = pd.read_sql(query, conn.connection)

df_data.to_csv('../data/uncleanData.csv', index=False)
df = df_data.copy()
df.head()
df.shape


print(f" There are {df.shape[0]} rows and {df.shape[1]} columns")
df.describe()

df.columns.tolist()

df.isna().sum()
df.info()
duplicated_entries = df[df.duplicated()]

duplicated_entries.shape
df.hist(bins=80, figsize=(30,25))
overview = Overview(df)
percent_missing = overview.percent_missing(df)
df['MSISDN/Number'].plot(kind='density')

print('This distribution has skew', df['MSISDN/Number'].skew())

print('This distribution has kurtosis', df['MSISDN/Number'].kurt())
preprocess = PreProcess(df)
df = preprocess.clean_feature_name(df)
df.columns
df.dtypes
%load_ext autoreload

%autoreload 2

df = preprocess.drop_duplicates(df)
df_c, df_before_filling, missing_cols = preprocess.drop_variables(df)

print(missing_cols)

cols, df_single, num_cols = preprocess.fill_numerical_variables(df)
print(len(missing_cols))

df_cols, df_single, cat_cols = preprocess.fill_categorical_variables(df, cols, num_cols, df_single)

df_single.columns[df_single.isnull().mean() > 0] 
df_single.info()
df_single.isna().sum().nlargest(10)
df_single.head()
df_single.columns
df.columns

















df.to_csv('../data/cleaned_data2.csv', index=False)
import os

import sys

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

import numpy as np
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src import utils

from src.plots import Plot

from src.preprocessing import PreProcess

from src.overview import Overview
df = pd.read_csv('../data/cleaned_data_outliers.csv')
plots = Plot()

preprocess = PreProcess(df)

overview = Overview(df)

handset_counts = df['Handset Type'].value_counts().head(10)

plots.plot_bar(x=handset_counts.values, y=handset_counts.index, xlabel='Number of Users', title='Top 10 Handsets by Users',ylabel='Handset Type')
manufacturer_counts = df['Handset Manufacturer'].value_counts().head(3)

plots.plot_pie(manufacturer_counts, title='Top 3 Handset Manufacturers by Users')
top_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index




df_top_manufacturers = df[df['Handset Manufacturer'].isin(top_manufacturers)]




top_handsets_per_manufacturer = (

    df_top_manufacturers

    .groupby('Handset Manufacturer')['Handset Type']

    .value_counts()

    .groupby(level=0, group_keys=False)

    .nlargest(5)

    .reset_index(name='Count')

)



plots.plot_bar(x=top_handsets_per_manufacturer['Count'], y=top_handsets_per_manufacturer['Handset Type'], xlabel='Number of Users', title='Top 5 Handsets per Top 3 Handset Manufacturers',ylabel='Handset Type',palette='viridis')


xDR_sess_agg = df['MSISDN/Number'].value_counts()

print(xDR_sess_agg.nlargest(10))
df.groupby('MSISDN/Number')['Dur. (ms)'].sum().nlargest(10)

df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().nlargest(10, 'Total DL (Bytes)')
df["social_media"] = df["Social Media DL (Bytes)"] + df['Social Media UL (Bytes)']

df["google"] = df["Google DL (Bytes)"] + df["Google UL (Bytes)"]

df['email'] = df["Email DL (Bytes)"] + df["Email UL (Bytes)"]

df['youtube'] = df["Youtube DL (Bytes)"] + df["Youtube UL (Bytes)"]

df['netflix'] = df["Netflix DL (Bytes)"] + df["Netflix UL (Bytes)"]

df["gaming"] = df["Gaming DL (Bytes)"] + df["Gaming UL (Bytes)"]

df['other'] = df["Other DL (Bytes)"]+df["Other UL (Bytes)"]

df['total_data'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']


quantitative_vars = df.select_dtypes(include=['float64', 'int64'])




dispersion_parameters = pd.DataFrame({

    'Mean': quantitative_vars.mean(),

    'Std Deviation': quantitative_vars.std(),

    'Min': quantitative_vars.min(),

    '25th Percentile (Q1)': quantitative_vars.quantile(0.25),

    'Median (Q2)': quantitative_vars.median(),

    '75th Percentile (Q3)': quantitative_vars.quantile(0.75),

    'Max': quantitative_vars.max(),

    'IQR': quantitative_vars.quantile(0.75) - quantitative_vars.quantile(0.25)

})




print(dispersion_parameters)

correlation_coefficients = quantitative_vars.corr(method='pearson')

correlation_coefficients
plots.plot_hist(df=df, column='Dur. (ms)', title='Distribution of Session Durations')
plots.plot_hist(df=df, column='Bearer Id', title='Distribution of Total Data')
plots.plot_hist(df=df, column='total_data',title='Distribution of Total Data')
plots.plot_hist(df=df, column='social_media', title='Distribution of Social Media Data')
plots.plot_hist(df=df, column='google', title='Distribution of Google Data')
plots.plot_hist(df=df, column='email', title='Distribution of Email Data')
plots.plot_hist(df=df, column='youtube', title='Distribution of Youtube Data')
plots.plot_hist(df=df, column='gaming', title='Distribution of Gaming Data')

plots.plot_scatter(df, df['google'], df['total_data'], 'Total Data Vs. Google', None, None)

plots.plot_scatter(df,  df['google'], df['Dur. (ms)'], 'Content Duration Vs. Google', None, None)

plots.plot_scatter(df,  df['youtube'], df['Dur. (ms)'], 'Content Duration Vs. YouTube', None, None)

plots.plot_scatter(df,  df['netflix'], df['Dur. (ms)'], 'Content Duration Vs. Netflix', None, None)

plots.plot_scatter(df, df['netflix'], df['total_data'], 'Total Data Vs. Netflix', None, None)

plots.plot_scatter(df, df['gaming'], df['Dur. (ms)'], 'Total Duration Vs. Gaming', None, None)
sns.relplot(data=df, x="Handset Manufacturer", y="google", hue=None, kind="line")
sns.relplot(x="total_data", y="google", hue="Handset Manufacturer", data=df);
preprocess = PreProcess(df)
df2 = df.copy()

df3 = pd.read_csv('../data/cleaned_data2.csv')
feature_to_use = df3[['msisdn/number', 'social_media', 'google', 'email', 'youtube', 'netflix',

                                 'gaming',  'other', 'total_data', 'handset_manufacturer']]
feature_to_use.head()
total_count_app = pd.DataFrame()

google_total = feature_to_use.sum()[1]

email_total = feature_to_use.sum()[2]

youtube_total = feature_to_use.sum()[3]

netflix_total = feature_to_use.sum()[4]

gaming_total = feature_to_use.sum()[5]

other_total = feature_to_use.sum()[6]
total_count_app['app'] = ['google', 'email', 'youtube', 'netflix', 'gaming', 'other']

total_count_app['total'] = [google_total, email_total, youtube_total, netflix_total, gaming_total,  other_total]
total_count_app.head(6)
total_count_app.head()
total_count_app
plots.plot_bar(x=total_count_app['app'], y=total_count_app['total'], title="Total data usage per app", xlabel="Application", ylabel="Total data volume")

df.columns
var_tr = df[['MSISDN/Number', 'Dur. (ms)', 'total_data']]
var_tr_agg = var_tr.groupby('MSISDN/Number').agg({'Dur. (ms)':'sum', 'total_data': 'sum'})
var_tr_agg.shape
var_tr_agg_decile = overview.get_decile(var_tr_agg, 'Dur. (ms)', 5, ['First Decile', 'Second Decile', 'Third Decile', 'Fourth Decile', 'Fifth Decile'])
var_tr_agg_decile.head()
plots.plot_bar(x=var_tr_agg_decile['deciles'], y=var_tr_agg_decile['total_data'] , title="Total data usage per decile", xlabel="Decile", ylabel="Total data volume")
corr_analysis = df3[['msisdn/number','social_media', 'google',

                    'email', 'youtube', 'netflix', 'gaming', 'other']]

corr_analysis_agg = corr_analysis.groupby('msisdn/number').agg({'social_media':'sum', 'google':'sum', 'email':'sum', 'youtube':'sum', 'netflix':'sum', 'gaming':'sum', 'other':'sum'})

plots.plot_heatmap(corr_analysis_agg, "Correlation between apps")

corr_analysis_agg.corr()

num_cols = df.select_dtypes(include=np.number).columns

cat_cols = list(set(df.columns) - set(num_cols))
df[num_cols].columns
num_final = [col for col in num_cols if col not in ['msisdn/number','bearer_id', 'start_ms', 'end_ms', 'imsi', 'imei']]
len(num_final)
len(df.columns)
def clean_dataset(df):

    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"

    df.dropna(inplace=True)

    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)

    return df[indices_to_keep].astype(np.float64)
df2 = df.copy()
df2 = clean_dataset(df2[num_final])
df2.shape
from sklearn.preprocessing import StandardScaler



scaler = StandardScaler()

scaler.fit(df2)

df_scaled = scaler.transform(df2)
df_scaled.shape
from sklearn.decomposition import PCA

pca_44 = PCA(n_components=44, random_state=42)

pca_44.fit(df_scaled)
x_pca_44 = pca_44.transform(df_scaled)
x_pca_44.shape 
sum(pca_44.explained_variance_ratio_)
np.cumsum(pca_44.explained_variance_ratio_ * 100)
plt.plot(np.cumsum(pca_44.explained_variance_ratio_))

plt.xlabel('Number of Components')

plt.ylabel('Cumulative Explained Variance')
print("Variance explained by first Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[0]))

print("Variance explained by  2 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[1]))

print("Variance explained by  3 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[2]))

print("Variance explained by  10 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[9]))

pca_65 = PCA(n_components=0.65, random_state=42)

pca_65.fit(df_scaled)

df_pca_95 = pca_65.transform(df_scaled)
df_pca_95.shape
import os

import sys

import numpy as np

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

from kneed import KneeLocator
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.plots import Plot
df = pd.read_csv('../data/cleaned_data_outliers.csv')
df.head(5)

column_name = 'msisdn/number'

value_to_match = 33663706799.0

mask = df[column_name] == value_to_match

df = df[~mask]
df.columns
df["social_media"] = df["social_media_dl_(bytes)"] + df['social_media_ul_(bytes)']

df["google"] = df["google_dl_(bytes)"] + df["google_ul_(bytes)"]

df['email'] = df["email_dl_(bytes)"] + df["email_ul_(bytes)"]

df['youtube'] = df["youtube_dl_(bytes)"] + df["youtube_ul_(bytes)"]

df['netflix'] = df["netflix_dl_(bytes)"] + df["netflix_ul_(bytes)"]

df["gaming"] = df["gaming_dl_(bytes)"] + df["gaming_ul_(bytes)"]

df['other'] = df["other_dl_(bytes)"]+df["other_ul_(bytes)"]

df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
df = df.rename( columns={'bearer_id': 'sessions'})
data_aggregate = df.groupby('msisdn/number').agg({'sessions': 'count', 'dur._(ms)': 'sum', 'total_data': 'sum'})
data_aggregate.head()

sessions = data_aggregate.nlargest(10, "sessions")['sessions']

duration = data_aggregate.nlargest(10, "dur._(ms)")['dur._(ms)']

total_data = data_aggregate.nlargest(10, "total_data")['total_data']
sesssions_df = pd.DataFrame()

sesssions_df['User_ID'] = sessions.index

sesssions_df['Sessions'] = sessions.values

durations_df = pd.DataFrame()

durations_df['User_ID'] = duration.index

durations_df['duration'] = duration.values

total_data_df = pd.DataFrame()

total_data_df['User_ID'] = total_data.index

total_data_df['total_data'] = total_data.values
durations_df.head()
f, axes = plt.subplots(1, 3, figsize=(25, 5))

ax1 = sns.barplot(data=sesssions_df, x='User_ID', orient='v', y='Sessions', ax=axes[0])

ax2 = sns.barplot(data=durations_df, x='User_ID',orient='v', y='duration', ax=axes[1])

ax3 = sns.barplot(data=total_data_df, x='User_ID',orient='v', y='total_data', ax=axes[2])

ax1.set_xticklabels(ax1.xaxis.get_majorticklabels(), rotation=45)

ax2.set_xticklabels(ax2.xaxis.get_majorticklabels(), rotation=45)

ax3.set_xticklabels(ax3.xaxis.get_majorticklabels(), rotation=45)

plt.plot()

from sklearn.preprocessing import Normalizer

from sklearn.cluster import KMeans
normalizer = Normalizer()
columns = ['sessions','dur._(ms)','total_data']

selected_columns = data_aggregate[columns]

normalized_data = normalizer.fit_transform(selected_columns)
normalized_user_data = pd.DataFrame(normalized_data, columns=columns)

normalized_user_data.head(5)
normalized_user_data.describe()
plt.figure(figsize=(15,6))

plt.subplot(1,2,1)

sns.violinplot(y=data_aggregate["dur._(ms)"])


plt.subplot(1,2,2)

sns.violinplot(y=data_aggregate["sessions"])


plt.show()

kmeans = KMeans(init="random",n_clusters=3,n_init=10,max_iter=300,random_state=42)

label = kmeans.fit_predict(normalized_user_data)

centroids = kmeans.cluster_centers_
lables_unique = np.unique(label)


plt.figure(figsize=(10,5))

plt.title('User K-Means Classification with 3 Groups (Standardized Data)')

for i in lables_unique:

    plt.scatter(normalized_user_data[label == i].iloc[:,0] , normalized_user_data[label == i].iloc[:,1], marker='o', label = i)

plt.scatter(centroids[:,0] , centroids[:,1],centroids[:,2], marker='x', color = 'black')

plt.legend()

plt.show()
normalized_user_data.head()
clustered_Df = pd.DataFrame()

clustered_Df['data_index'] = normalized_user_data.index.values

clustered_Df['cluster'] = kmeans.labels_
clustered_Df.cluster.value_counts()
u_eng = data_aggregate.copy()
u_eng.insert(0, 'cluster', kmeans.labels_)

u_eng.head(5)
cluster1 = u_eng[u_eng["cluster"]==0]

cluster1.describe()
cluster2 = u_eng[u_eng["cluster"] == 1]

cluster2.describe()
cluster3 = u_eng[u_eng["cluster"] == 2]

cluster3.describe()
cluster3.sessions.count()
data = [cluster1.sessions.count(), cluster2.sessions.count(), cluster3.sessions.count()]

keys = ['Cluster 1', 'Cluster 2', 'Cluster 3']


plot = Plot()

plot.plot_pie(data=data, label=keys, title="Cluster Distribution Count")
user_app_usage = df.groupby(

    'msisdn/number').agg({ 'social_media': 'sum', 'gaming': 'sum', 'youtube': 'sum', 'netflix': 'sum', 'google': 'sum', 'email': 'sum', 'other': 'sum'})

user_app_usage.reset_index(inplace=True)


user_app_usage = user_app_usage.drop('msisdn/number', axis=1)

user_app_usage
social_media = user_app_usage.nlargest(10, "social_media")['social_media']

gaming = user_app_usage.nlargest(10, "gaming")['gaming']

youtube = user_app_usage.nlargest(10, "youtube")['youtube']

netflix = user_app_usage.nlargest(10, "netflix")['netflix']

google = user_app_usage.nlargest(10, "google")['google']

email = user_app_usage.nlargest(10, "email")['email']

other = user_app_usage.nlargest(10, "other")['other']

social_media_df = pd.DataFrame()

social_media_df['User_ID'] = social_media.index

social_media_df['social_media'] = social_media.values

gaming_df = pd.DataFrame()

gaming_df['User_ID'] = gaming.index

gaming_df['gaming'] = gaming.values

youtube_df = pd.DataFrame()

youtube_df['User_ID'] = youtube.index

youtube_df['youtube'] = youtube.values



netflix_df = pd.DataFrame()

netflix_df['User_ID'] = netflix.index

netflix_df['netflix'] = netflix.values

google_df = pd.DataFrame()

google_df['User_ID'] = google.index

google_df['google'] = google.values

email_df = pd.DataFrame()

email_df['User_ID'] = email.index

email_df['email'] = email.values

others_df = pd.DataFrame()

others_df['User_ID'] = other.index

others_df['other'] = other.values
f, axes = plt.subplots(2, 4, figsize=(25, 15))

ax1 = sns.barplot(data=social_media_df, x='User_ID', orient='v', y='social_media', ax=axes[0,0], palette='Blues_d')

ax2 = sns.barplot(data=gaming_df, x='User_ID',orient='v', y='gaming', ax=axes[0,1], palette='Blues_d')

ax3 = sns.barplot(data=youtube_df, x='User_ID',orient='v', y='youtube', ax=axes[0,2], palette='Blues_d')

ax4 = sns.barplot(data=netflix_df, x='User_ID',orient='v', y='netflix', ax=axes[0,3], palette='Blues_d')

ax5 = sns.barplot(data=google_df, x='User_ID',orient='v', y='google', ax=axes[1,0], palette='Blues_d')

ax6 = sns.barplot(data=email_df, x='User_ID',orient='v', y='email', ax=axes[1,1], palette='Blues_d')

ax7 = sns.barplot(data=others_df, x='User_ID',orient='v', y='other', ax=axes[1,2], palette='Blues_d')

ax1.set_xticklabels(ax1.xaxis.get_majorticklabels(), rotation=45)

ax2.set_xticklabels(ax2.xaxis.get_majorticklabels(), rotation=45)

ax3.set_xticklabels(ax3.xaxis.get_majorticklabels(), rotation=45)

ax4.set_xticklabels(ax4.xaxis.get_majorticklabels(), rotation=45)

ax5.set_xticklabels(ax5.xaxis.get_majorticklabels(), rotation=45)

ax6.set_xticklabels(ax6.xaxis.get_majorticklabels(), rotation=45)

ax7.set_xticklabels(ax7.xaxis.get_majorticklabels(), rotation=45)

plt.plot()

top_used_applications = user_app_usage.sum()
top_used_applications.values
top_3_used = top_used_applications.nlargest(3)
top_3_used
plot.plot_bar(top_3_used, ["Netflix", "Email", "Gaming"], top_3_used.values, "Top 3 Used Applications", "Applications", "Usage Count")
inertias = []

for i in range(1,16):

    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)

    kmeans.fit(normalized_user_data)

    inertias.append(kmeans.inertia_)

plt.figure(figsize = (10,8))

plt.plot(range(1, 16), inertias, marker = 'o', linestyle = '--')

plt.xlabel('Number of Clusters')

plt.ylabel('Inertias')

plt.title('K-means Clustering')

plt.show()
kl = KneeLocator(range(0, 15), inertias, curve="convex", direction="decreasing")

kl.elbow
u_eng.shape
u_eng.to_csv('../data/user_eng.csv')
user_app_usage.to_csv('../data/normalized_eng.csv')
import joblib

joblib.dump(kmeans,'../models/user_engagement.pkl')
%load_ext autoreload

%autoreload 2

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import os

import sys
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.preprocessing import PreProcess

from src.overview import Overview

from src.outlier import Outlier

from src.plots import Plot
df = pd.read_csv('../data/cleaned_data2.csv')
outlier = Outlier(df)
df.head()
df.columns
pl = Plot()
pl.plot_box(df, 'dur._(ms)', 'Total Duration of the xDR (in ms)')
pl.plot_box(df, 'avg_rtt_dl_(ms)', 'Average Round Trip Time measurement Downlink direction (msecond)')
pl.plot_box(df, 'tcp_ul_retrans._vol_(bytes)', 'TCP volume of Uplink packets detected as retransmitted (bytes)')
display(outlier.outlier_overview(df, 'activity_duration_ul_(ms)'))
display(outlier.outlier_overview(df, 'tcp_ul_retrans._vol_(bytes)'))
%load_ext autoreload

%autoreload 2

df.dtypes
num_cols = df.select_dtypes(include=np.number).columns
dlist = ['bearer_id', 'start_ms', 'end_ms', 'imsi', 'msisdn/number', 'imei']
num_cols = [el for el in num_cols if el not in dlist]

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df[col])

    print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
df = outlier.handle_outliers(df, num_cols)

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df[col])

    print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
outlier_columns = []

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df[col])

    if len(sample_outliers) > 0:

        outlier_columns.append(col)
len(outlier_columns)
df_cop = df.copy()
from sklearn.preprocessing import MinMaxScaler



minmax_scaler = MinMaxScaler()




scaled_data = df.copy()



scaled_data.sample(5)
df_cop[outlier_columns].shape 
scaled_data.shape
scaled_data = minmax_scaler.fit_transform(scaled_data[outlier_columns])
scaled_data.shape
outlier_columns = []

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df_cop[col])

    if len(sample_outliers) > 0:

        outlier_columns.append(col)
len(outlier_columns)

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df_cop[col])

    print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
df_cop[num_cols].boxplot()
pl.plot_box(df_cop, 'dur._(ms)', 'Total Duration of the xDR (in ms) after outlier handling')
df_cop.describe()


df_cop.to_csv('../data/cleaned_data_outliers.csv', index=False)
import sys

import numpy as np
import pandas as pd


class Outlier:
    def __init__(self, df: pd.DataFrame):
        """Initialize the PreProcess class.

        Args:
            df (pd.DataFrame): dataframe to be preprocessed
        """
        try:
            self.df = df
        except Exception:
            sys.exit(1)

    
    def handle_outliers(self, df: pd.DataFrame, cols):
        """Handle outliers in the dataset.

        Args:
            df (pd.DataFrame): a dataframe to be preprocessed

        Returns:
            pd.DataFrame: the dataframe
        """
                        for col in cols:
                        df[col] = [np.log(x) for x in df[col]]
        return df

    def calculate_num_outliers_zscore(self, col):
        """Return the number of outliers for each numerical col.

        Args:
            col (pd.DataFrame): a dataframe to be analyzed
        """
                outliers = []

        thres = 3
        mean = np.mean(col)
        std = np.std(col)
                for i in col:
            z_score = (i-mean)/std
            if (np.abs(z_score) > thres):
                outliers.append(i)
        return outliers  
                        
    def calculate_num_outliers_iqr(self, df, cols):
        """Return the number of outliers for each col.

        Args:
            df (pd.DataFrame): a dataframe to be analyzed
            cols (list): list of columns to analyze
        """
        outliersTot = {}

        for col in cols:
            outliers = []
            df_sorted = df.sort_values(col)[col]
            q1 = np.percentile(df_sorted, 25)
            q3 = np.percentile(df_sorted, 75)

            IQR = q3 - q1
            lwr_bound = q1 - (1.5 * IQR)
            upr_bound = q3 + (1.5 * IQR)

            for i in df_sorted:
                if (i < lwr_bound or i > upr_bound):
                    outliers.append(i)

            outliersTot[col] = outliers

        return outliersTot


    def outlier_overview(self, df, col):
        """Get outlier overview.

        Args:
            df (pd.DataFrame): a dataframe to be analyzed
        """

                upper_limit = df[col].mean() + 3 * df['total_ul_(bytes)'].std()
        lower_limit = df[col].mean() - 3 * df['total_ul_(bytes)'].std()

                return df[~((df[col] < upper_limit) & (df[col] > lower_limit))]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sys


class Plot:
    def __init__(self) -> None:
        pass

    def plot_bar(self, x, y, xlabel,ylabel,title,palette=None) -> None:
        plt.figure(figsize=(12, 6))
        sns.barplot(x=x, y=y, palette="viridis")

        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)

        plt.show()

    def plot_count(self, df: pd.DataFrame, column: str) -> None:
        """Plot the count of the column.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.countplot(data=df, x=column)
        plt.title(f'Distribution of {column}', size=20, fontweight='bold')
        plt.show()


    def plot_heatmap(self, df: pd.DataFrame, title: str, cbar=False) -> None:
        """Plot Heat map of the dataset.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            title (str): title of chart.
        """
                plt.figure(figsize=(12, 7))
        sns.heatmap(df, annot=True, cmap='viridis', vmin=0,
                    vmax=1, fmt='.2f', linewidths=.7, cbar=cbar)
        plt.title(title, size=18, fontweight='bold')
        plt.show()

    def plot_box(self, df: pd.DataFrame, x_col: str, title: str) -> None:
        """Plot box chart of the column.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            x_col (str): column to be plotted.
            title (str): title of chart.
        """
        plt.figure(figsize=(12, 7))
        sns.boxplot(data=df, x=x_col)
        plt.title(title, size=20)
        plt.xticks(rotation=75, fontsize=14)
        plt.show()

    def plot_box_multi(self, df: pd.DataFrame, x_col: str, y_col: str, title: str) -> None:
        """Plot the box chart for multiple column.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.boxplot(data=df, x=x_col, y=y_col)
        plt.title(title, size=20)
        plt.xticks(rotation=75, fontsize=14)
        plt.yticks(fontsize=14)
        plt.show()

    def plot_scatter(self, df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:
        """Plot Scatter chart of the data.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.scatterplot(data=df, x=x_col, y=y_col, hue=hue, style=style)
        plt.title(title, size=20)
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        plt.show()

    def plot_pie(self, data, title, label) -> None:
        """Plot pie chart of the data.

        Args:
            data (list): Data to be plotted.
            labels (list): labels of the data.
            colors (list): colors of the data.
        """
        plt.style.context('seaborn-pastel')
        plt.figure(figsize=(8, 8))
        plt.pie(x=data, labels=label, autopct='%1.1f%%', startangle=140)

        plt.title(title)
        plt.show()

    def plot_hist(self, df: pd.DataFrame, column: str, title: str) -> None:
        """Plot histogram of the data.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.histplot(data=df, x=column, kde=True)
        plt.title(title, size=20)
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        plt.show()
import os

import sys

import numpy as np

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

%matplotlib inline
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.plots import Plot

from src.preprocessing import PreProcess
df = pd.read_csv('../data/cleaned_data_outliers.csv')
column_name = 'msisdn/number'

value_to_match = 33663706799.0

mask = df[column_name] == value_to_match

df = df[~mask]
df.head()
df.columns
df['avg_tcp_retrans'] = df['tcp_dl_retrans._vol_(bytes)'] + df['tcp_ul_retrans._vol_(bytes)']

df['avg_rrt'] = df['avg_rtt_dl_(ms)'] + df['avg_rtt_dl_(ms)']

df['avg_bearer_tp'] = df['avg_bearer_tp_dl_(kbps)'] + df['avg_bearer_tp_ul_(kbps)']

df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
preprocess = PreProcess(df)
agg_data = df.groupby('msisdn/number').agg({'avg_tcp_retrans':sum,'avg_rrt':sum,'avg_bearer_tp':sum, 'total_data':sum})

preprocess.replace_neg_inf_with_mean(agg_data)
sns.set_style('whitegrid')

sns.lmplot(x='avg_tcp_retrans',y='total_data',data=agg_data,palette='coolwarm',aspect=1)
plot = Plot()
plot.plot_scatter(agg_data,x_col='avg_rrt',y_col='total_data',title='Total Data vs Round Trip Time',hue=None,style=None)
plot.plot_scatter(agg_data,x_col='avg_bearer_tp',y_col='total_data',title='Total Data vs Average Bearer Throughput',hue=None,style=None)
column_name = 'avg_rrt'




top_values = agg_data[column_name].nlargest(10)




bottom_values = agg_data[column_name].nsmallest(10)




most_frequent_values = agg_data[column_name].value_counts().head(10)



rrt_result_table = pd.DataFrame({

    'Top Values': top_values.values,  
    'Bottom Values': bottom_values.values,

    'Most Frequent Values': most_frequent_values.index.values,  
    'Frequency': most_frequent_values.values

})

rrt_result_table

plot.plot_pie(rrt_result_table['Frequency'],title='Frequent Round Trip Time',label=rrt_result_table['Most Frequent Values'])

column_name = 'avg_rrt'




top_values = agg_data[column_name].nlargest(10)




bottom_values = agg_data[column_name].nsmallest(10)




most_frequent_values = agg_data[column_name].value_counts().head(10)



result_table = pd.DataFrame({

    'Top Values': top_values.values,  
    'Bottom Values': bottom_values.values,

    'Most Frequent Values': most_frequent_values.index.values,  
    'Frequency': most_frequent_values.values

})

result_table

column_name = 'avg_bearer_tp'




top_values = agg_data[column_name].nlargest(10)




bottom_values = agg_data[column_name].nsmallest(10)




most_frequent_values = agg_data[column_name].value_counts().head(10)



result_table = pd.DataFrame({

    'Top Values': top_values.values,  
    'Bottom Values': bottom_values.values,

    'Most Frequent Values': most_frequent_values.index.values,  
    'Frequency': most_frequent_values.values

})

result_table
manufacturer_agg = df.groupby('handset_type').agg({'avg_tcp_retrans':'mean','avg_rrt':'mean','avg_bearer_tp':'mean', 'total_data':sum})
manufacturer_agg.reset_index(inplace=True)

top_agg = manufacturer_agg.nlargest(10,columns='avg_bearer_tp')

plot.plot_bar(x=top_agg['handset_type'],y=top_agg['avg_bearer_tp'],xlabel='ReTransmission',ylabel='Manufacturer',title='Title')
top_agg = manufacturer_agg.nlargest(10,columns='avg_bearer_tp')

plot.plot_bar(x=top_agg['handset_type'],y=top_agg['avg_bearer_tp'],xlabel='Retransmission',ylabel='Manufacturer',title='Title')
from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans
scaler = StandardScaler()

scaled_features = scaler.fit_transform(agg_data[['avg_tcp_retrans','avg_rrt','avg_bearer_tp']])
kmeans = KMeans(n_clusters=3, random_state=42)

agg_data['Cluster'] = kmeans.fit_predict(scaled_features)
agg_data
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)

centroid_df = pd.DataFrame(cluster_centers, columns=['avg_tcp_retrans','avg_rrt','avg_bearer_tp'])
print("Cluster Centers:")

print(centroid_df)
cluster_counts = agg_data['Cluster'].value_counts()

print("\nCluster Counts:")

print(cluster_counts)
sns.set(style="whitegrid")




plt.figure(figsize=(10, 6))




plt.scatter(agg_data.loc[agg_data['Cluster'] == 0, 'avg_tcp_retrans'],

            agg_data.loc[agg_data['Cluster'] == 0, 'avg_rrt'],

            label='Cluster 0', alpha=0.8, s=50)




plt.scatter(agg_data.loc[agg_data['Cluster'] == 1, 'avg_tcp_retrans'],

            agg_data.loc[agg_data['Cluster'] == 1, 'avg_rrt'],

            label='Cluster 1', alpha=0.8, s=50)




plt.scatter(agg_data.loc[agg_data['Cluster'] == 2, 'avg_tcp_retrans'],

            agg_data.loc[agg_data['Cluster'] == 2, 'avg_rrt'],

            label='Cluster 2', alpha=0.8, s=50)




plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='X', s=200, color='red', label='Centroids')




plt.xlabel('Avg_TCP_Retransmission')

plt.ylabel('Avg_RTT')

plt.title('K-Means Clustering of Telecom Users')




plt.legend()




plt.show()

import joblib
joblib.dump(kmeans, '../models/kmeans_model.pkl')
import joblib
import os

import sys

import numpy as np

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

%matplotlib inline
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.plots import Plot

from src.preprocessing import PreProcess

from db_connection.database import Database
kmeans = joblib.load('../models/kmeans_model.pkl')

kmeans
df = pd.read_csv('../data/cleaned_data_outliers.csv')

df['avg_tcp_retrans'] = df['tcp_dl_retrans._vol_(bytes)'] + df['tcp_ul_retrans._vol_(bytes)']

df['avg_rrt'] = df['avg_rtt_dl_(ms)'] + df['avg_rtt_dl_(ms)']

df['avg_bearer_tp'] = df['avg_bearer_tp_dl_(kbps)'] + df['avg_bearer_tp_ul_(kbps)']

df["social_media"] = df["social_media_dl_(bytes)"] + df['social_media_ul_(bytes)']

df["google"] = df["google_dl_(bytes)"] + df["google_ul_(bytes)"]

df['email'] = df["email_dl_(bytes)"] + df["email_ul_(bytes)"]

df['youtube'] = df["youtube_dl_(bytes)"] + df["youtube_ul_(bytes)"]

df['netflix'] = df["netflix_dl_(bytes)"] + df["netflix_ul_(bytes)"]

df["gaming"] = df["gaming_dl_(bytes)"] + df["gaming_ul_(bytes)"]

df['other'] = df["other_dl_(bytes)"]+df["other_ul_(bytes)"]

df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
column_name = 'msisdn/number'

value_to_match = 33663706799.0

mask = df[column_name] == value_to_match

df = df[~mask]
preprocess = PreProcess(df)
agg_data = df.groupby('msisdn/number').agg({'avg_tcp_retrans':sum,'avg_rrt':sum,'avg_bearer_tp':sum})

preprocess.replace_neg_inf_with_mean(agg_data)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_features = scaler.fit_transform(agg_data[['avg_tcp_retrans','avg_rrt','avg_bearer_tp']])
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)

centroid_df = pd.DataFrame(cluster_centers, columns=['avg_tcp_retrans','avg_rrt','avg_bearer_tp'])
print("Cluster Centers:")

print(centroid_df)
from sklearn.metrics.pairwise import euclidean_distances




distances = euclidean_distances(scaled_features, cluster_centers)




less_engaged_cluster_index = 0




engagement_scores = distances[:, less_engaged_cluster_index]




agg_data['Experience_Score'] = engagement_scores


sns.set(style="whitegrid")




plt.figure(figsize=(10, 6))

sns.histplot(agg_data['Experience_Score'], bins=30, kde=True, color='skyblue')




plt.xlabel('Engagement Score')

plt.ylabel('Frequency')

plt.title('Distribution of Experience Scores')




plt.show()

user_engagement_kmeans = joblib.load('../models/user_engagement.pkl')
df = df.rename( columns={'bearer_id': 'sessions'})
data_aggregate = df.groupby('msisdn/number').agg({'sessions': 'count', 'dur._(ms)': 'sum', 'total_data': 'sum'})

data_aggregate.nlargest(5,'sessions')
scaler = StandardScaler()

scaled_features = scaler.fit_transform(data_aggregate[['sessions','dur._(ms)','total_data']])

cluster_centers = scaler.inverse_transform(user_engagement_kmeans.cluster_centers_)

centroid_df = pd.DataFrame(cluster_centers, columns=['sessions','dur._(ms)','total_data'])
from sklearn.metrics.pairwise import euclidean_distances




distances = euclidean_distances(scaled_features, cluster_centers)




less_engaged_cluster_index = 0




engagement_scores = distances[:, less_engaged_cluster_index]




data_aggregate['Engagement_Score'] = engagement_scores


sns.set(style="whitegrid")




plt.figure(figsize=(10, 6))

sns.histplot(data_aggregate['Engagement_Score'], bins=30, kde=True, color='skyblue')




plt.xlabel('Engagement Score')

plt.ylabel('Frequency')

plt.title('Distribution of Engagement Scores')




plt.show()
agg_data['Engagement_Score'] = data_aggregate['Engagement_Score'] 
agg_data['Satisfaction_Score'] = agg_data['Experience_Score'] + agg_data['Engagement_Score']
satified_customer = agg_data.nlargest(10,'Satisfaction_Score')

plt.figure(figsize=(10, 6))

sns.barplot(x=satified_customer.index, y="Satisfaction_Score", data=satified_customer, palette="Blues_d")

plt.title('Top 10 Satisfied Customers')

plt.xlabel('Customer ID')

plt.ylabel('Satisfaction Score')

plt.xticks(rotation=45)

plt.show()
agg_data

from sklearn.cluster import KMeans




kmeans = KMeans(n_clusters=2, random_state=42)




predictions = kmeans.fit_predict(agg_data[['Experience_Score','Engagement_Score']])

print(predictions)




agg_data['Cluster'] = predictions




agg_data.groupby('Cluster').mean()




plt.scatter(agg_data['Experience_Score'], agg_data['Engagement_Score'], c=agg_data['Cluster'], cmap='rainbow')

plt.show()

cluster_data = agg_data.groupby('Cluster').mean()


plt.figure(figsize=(10, 6))

sns.barplot(x=cluster_data.index, y="Satisfaction_Score", data=cluster_data, palette="Blues_d")

plt.title('Average Satisfaction Score per Cluster')

plt.xlabel('Cluster')

plt.ylabel('Satisfaction Score')

plt.xticks(rotation=45)

plt.show()
import pandas as pd
import numpy as np

class DataWrangler:
    def __init__(self, df):
        self.df = df

    def calculate_null_percentage(self):
        """
        Calculate the percentage of null values in each column of the DataFrame.

        Returns:
        - pd.Series: Percentage of null values for each column.
        """
        total_cells = self.df.size
        total_null_cells = self.df.isnull().sum().sum()
        null_percentage = (total_null_cells / total_cells) * 100
        return null_percentage

    def handle_duplicate_data(self):
        """
        Handle duplicate data in the DataFrame.

        Returns:
        - pd.DataFrame: DataFrame with duplicate data handled.
        """
        self.df.drop_duplicates(inplace=True)
        return self.df

    def replace_outliers_with_mean(self, columns):
        for column in columns:
                        z_scores = np.abs((self.df[column] - self.df[column].mean()) / self.df[column].std())
        
                        self.df[column] = np.where(z_scores > 3, self.df[column].mean(), self.df[column])

    def aggregate_data(self, group_by_columns, aggregation_functions):
        """
        Aggregate data in the DataFrame.

        Parameters:
        - group_by_columns (list): List of columns to group by.
        - aggregation_functions (dict): Dictionary of {column: aggregation function}.

        Returns:
        - pd.DataFrame: Aggregated DataFrame.
        """
        self.df = (
            self.df.groupby(group_by_columns).agg(aggregation_functions).reset_index()
        )
        return self.df

    def handle_categorical_data(self, encoding_method="one-hot", columns=None):
        """
        Handle categorical data in the DataFrame.

        Parameters:
        - encoding_method (str): Method for encoding categorical data ('one-hot', 'label', etc.).
        - columns (list): List of columns to encode.

        Returns:
        - pd.DataFrame: DataFrame with categorical data handled.
        """
        if columns is None:
            columns = self.df.select_dtypes(include="object").columns

        if encoding_method == "one-hot":
            self.df = pd.get_dummies(self.df, columns=columns, drop_first=True)
        elif encoding_method == "label":
                        pass
                return self.df

    def calculate_skewness(self):
        """
        Calculate skewness for numeric columns in a DataFrame.

        Returns:
        - pd.Series: Skewness values for each numeric column.
        """
        numeric_columns = self.df.select_dtypes(include=["float", "int"])
        skewness_values = numeric_columns.apply(lambda x: x.skew()).round(1)
        return skewness_values

    def get_numeric_columns(self):
        """
        Extract numeric columns from a DataFrame.

        Parameters:
        - df (pd.DataFrame): Input DataFrame.

        Returns:
        - pd.DataFrame: DataFrame containing only numeric columns.
        """
        numeric_columns = self.df.select_dtypes(include=["float", "int"])
        return numeric_columns

    def get_object_columns(self):
        """
        Extract object (string) columns from the DataFrame.

        Returns:
        - pd.Index: Index containing object column names.
        """
        object_columns = self.df.select_dtypes(include=["object"]).columns
        return object_columns

    def repl_numeric_columns(self):
        """
        Impute missing values in numeric columns based on skewness.

        Returns:
        - pd.DataFrame: DataFrame with missing values imputed.
        """

        for column_name in self.df:
            column_skew = self.df[column_name].skew().round()
            fill_value = self.df[column_name].median()

            self.df[column_name].fillna(fill_value, inplace=True)

        return self.df

    def calculate_categorical_mode(self, column_name):
        """
        Calculate the mode of a categorical column in a DataFrame.

        Parameters:
        - column_name (str): Name of the categorical column.

        Returns:
        - pd.Series: Mode(s) of the specified column.
        """
        category_mode = self.df[column_name].mode()
        return category_mode
import pandas as pd

import numpy as np

import psycopg2

import seaborn as sb

from sqlalchemy  import create_engine,MetaData, Table

database_name = 'tellcom'

table_name= 'xdr_data'



connection_params = { 

                    "host": "localhost",

                     "user": "postgres",

                     "password": "postgres",

                     "port": "5432",

                     "database": database_name

                   }



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")




sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query, con= engine)

df.head()

df.shape

sum(df.duplicated())


import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.wrangling import DataWrangler
data_wrangler = DataWrangler(df)
data_wrangler.calculate_null_percentage().round(2)

df.info()

numerical_df = data_wrangler.get_numeric_columns()

numerical_df.head()

numerical_df.info()
numerical_wrangler = DataWrangler(numerical_df)
skewnes_value = numerical_wrangler.calculate_skewness()
skewnes_value




df_numerical = numerical_wrangler.repl_numeric_columns()
df_numerical.head()
col_name = ['Dur. (ms)', 'Total DL (Bytes)', 'Total UL (Bytes)',

                         'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']
numerical_wrangler.replace_outliers_with_mean(col_name)
object_column_names = data_wrangler.get_object_columns()
categorical_df = data_wrangler.df[object_column_names]
categorical_df.head()
df_clean = pd.concat([df_numerical, categorical_df], axis=1)
df_clean.info()
df_clean.dropna(inplace=True)
df_clean.shape
df_clean.head()
df_clean.to_csv('../data/clean_data.csv',index=False)

!jupyter nbconvert <Part_II_Filename>.ipynb --to slides --post serve --no-input --no-prompt
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt
df_clean = pd.read_csv('../data/clean_data.csv')
df_clean.head()

df_clean.describe()


import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.visualizer import Plotter

handset_data = df_clean[['MSISDN/Number', 'Handset Type']]

handset_counts = handset_data['Handset Type'].value_counts().reset_index()

handset_counts.columns = ['Handset Type', 'Count']

top_ten_handsets = handset_counts.head(10)

top_ten_handsets
plotter_instance = Plotter(top_ten_handsets)
!jupyter nbconvert user_overview_analysis.ipynb --to slides --post serve --no-input --no-prompt

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Type', data=top_ten_handsets, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.title('Top Ten Handsets Used by Customers')

plt.show();

handsetM_data = df_clean['Handset Manufacturer']

handsetM_counts = handsetM_data.value_counts().reset_index()

handsetM_counts.columns = ['Handset Manufacturer', 'Count']

top_three_handsets_manf = handsetM_counts.head(3)

top_three_handsets_manf

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Manufacturer', data=top_three_handsets_manf, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Manufacturer')

plt.title('Top Three Handset Manufacturer')

plt.show();

handset_ocurrance_counts = df_clean.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')

top_manufacturers = handset_ocurrance_counts.groupby('Handset Manufacturer')['Count'].sum().nlargest(3).index

top_manufacturer_data = handset_ocurrance_counts[handset_ocurrance_counts['Handset Manufacturer'].isin(top_manufacturers)]

top_5_handsets_per_manufacturer = (

    top_manufacturer_data.groupby('Handset Manufacturer')

    .apply(lambda group: group.nlargest(5, 'Count'))

    .reset_index(drop=True)

)




top_5_handsets_per_manufacturer

sns.set(style="whitegrid")




plt.figure(figsize=(12, 8))

sns.barplot(x='Handset Manufacturer', y='Count', hue='Handset Type', data=top_5_handsets_per_manufacturer)

plt.title('Handset Types Count by Manufacturer')

plt.xlabel('Handset Manufacturer')

plt.ylabel('Count')

plt.xticks(rotation=45, ha='right')  
plt.legend(title='Handset Type', bbox_to_anchor=(1, 1))  
plt.show()












print(df_clean.columns.tolist())

columns_to_aggregate = ['Dur. (ms)', 'Total DL (Bytes)', 'Total UL (Bytes)',

                         'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']

user_aggregated_data = df_clean.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',  
    'Dur. (ms)': 'sum',  
    'Total DL (Bytes)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    'Social Media DL (Bytes)': 'sum', 'Social Media UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum', 'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum', 'Netflix UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum', 'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum', 'Email UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum', 'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum', 'Other UL (Bytes)': 'sum'

})






user_aggregated_data
user_aggregated_data['Session duration (s)'] = user_aggregated_data['Dur. (ms)'] / 1000

user_aggregated_data.head()

user_aggregated_data['Social Media Total (Bytes)'] = (

                                                        user_aggregated_data['Social Media DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Social Media UL (Bytes)']

                                                )

user_aggregated_data['Youtube Total (Bytes)'] = (

                                                        user_aggregated_data['Youtube DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Youtube UL (Bytes)']

                                                )

user_aggregated_data['Netflix Total (Bytes)'] = (

                                                        user_aggregated_data['Netflix DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Netflix UL (Bytes)']

                                                )

user_aggregated_data['Google Total (Bytes)'] = (

                                                        user_aggregated_data['Google DL (Bytes)'] 

                                                        + 

                                                        user_aggregated_data['Google UL (Bytes)']

                                                )

user_aggregated_data['Email Total (Bytes)'] = (

                                                        user_aggregated_data['Email DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Email UL (Bytes)']

                                                )

user_aggregated_data['Gaming Total (Bytes)'] = (

                                                        user_aggregated_data['Gaming DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Gaming UL (Bytes)']

                                                )

user_aggregated_data['Other Total (Bytes)'] = (

                                                        user_aggregated_data['Other DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Other UL (Bytes)']

                                                        )

user_aggregated_data.head()
user_aggregated_data.describe()









plt.hist(user_aggregated_data['Session duration (s)'], bins=30, edgecolor='black')

plt.title('Histogram of Session duration')

plt.xlabel('Session duration (ms)')

plt.ylabel('Frequency')

plt.show()


fig, ax = plt.subplots()


ax.boxplot(user_aggregated_data['Session duration (s)'])

ax.set_yscale('log')  
ax.set_title('Box Plot of Session duration')

ax.set_ylabel('Session duration (seccond)')

plt.show()
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

class Plotter:
    def __init__(self, data):
        self.data = data

    def plot_horizontal_bar_chart(self, x_values, y_values, title="Horizontal Bar Chart", xlabel="X Axis", ylabel="Y Axis"):
        plt.figure(figsize=(10, 6))
        sns.barplot(x=x_values, y=y_values, data=self.df, palette='viridis')
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.title(title)
        plt.show()

    def plot_heatmap(self, title="Heatmap"):
        plt.figure(figsize=(8, 6))
        sns.heatmap(self.data, annot=True, cmap="YlGnBu", fmt=".2f", linewidths=0.5)
        plt.title(title)
        plt.show()

    def plot_scatter_plot(self, x_values, y_values, title="Scatter Plot", xlabel="X Axis", ylabel="Y Axis"):
        plt.figure(figsize=(8, 6))
        plt.scatter(x_values, y_values)
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.show()
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt
df_clean = pd.read_csv('../data/clean_data.csv')
df_clean.head()

df_clean.describe()


import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.visualizer import Plotter

handset_data = df_clean[['MSISDN/Number', 'Handset Type']]

handset_counts = handset_data['Handset Type'].value_counts().reset_index()

handset_counts.columns = ['Handset Type', 'Count']

top_ten_handsets = handset_counts.head(10)

top_ten_handsets
plotter_instance = Plotter(top_ten_handsets)

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Type', data=top_ten_handsets, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.title('Top Ten Handsets Used by Customers')

plt.show();

handsetM_data = df_clean['Handset Manufacturer']

handsetM_counts = handsetM_data.value_counts().reset_index()

handsetM_counts.columns = ['Handset Manufacturer', 'Count']

top_three_handsets_manf = handsetM_counts.head(3)

top_three_handsets_manf

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Manufacturer', data=top_three_handsets_manf, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Manufacturer')

plt.title('Top Three Handset Manufacturer')

plt.show();

handset_ocurrance_counts = df_clean.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')

top_manufacturers = handset_ocurrance_counts.groupby('Handset Manufacturer')['Count'].sum().nlargest(3).index

top_manufacturer_data = handset_ocurrance_counts[handset_ocurrance_counts['Handset Manufacturer'].isin(top_manufacturers)]

top_5_handsets_per_manufacturer = (

    top_manufacturer_data.groupby('Handset Manufacturer')

    .apply(lambda group: group.nlargest(5, 'Count'))

    .reset_index(drop=True)

)




top_5_handsets_per_manufacturer

sns.set(style="whitegrid")




plt.figure(figsize=(12, 8))

sns.barplot(x='Handset Manufacturer', y='Count', hue='Handset Type', data=top_5_handsets_per_manufacturer)

plt.title('Handset Types Count by Manufacturer')

plt.xlabel('Handset Manufacturer')

plt.ylabel('Count')

plt.xticks(rotation=45, ha='right')  
plt.legend(title='Handset Type', bbox_to_anchor=(1, 1))  
plt.show()












print(df_clean.columns.tolist())

columns_to_aggregate = ['Dur. (ms)', 'Total DL (Bytes)', 'Total UL (Bytes)',

                         'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']

user_aggregated_data = df_clean.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',  
    'Dur. (ms)': 'sum',  
    'Total DL (Bytes)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    'Social Media DL (Bytes)': 'sum', 'Social Media UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum', 'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum', 'Netflix UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum', 'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum', 'Email UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum', 'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum', 'Other UL (Bytes)': 'sum'

})






user_aggregated_data
user_aggregated_data['Session duration (s)'] = user_aggregated_data['Dur. (ms)'] / 1000

user_aggregated_data.head()

user_aggregated_data['Social Media Total (Bytes)'] = (

                                                        user_aggregated_data['Social Media DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Social Media UL (Bytes)']

                                                )

user_aggregated_data['Youtube Total (Bytes)'] = (

                                                        user_aggregated_data['Youtube DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Youtube UL (Bytes)']

                                                )

user_aggregated_data['Netflix Total (Bytes)'] = (

                                                        user_aggregated_data['Netflix DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Netflix UL (Bytes)']

                                                )

user_aggregated_data['Google Total (Bytes)'] = (

                                                        user_aggregated_data['Google DL (Bytes)'] 

                                                        + 

                                                        user_aggregated_data['Google UL (Bytes)']

                                                )

user_aggregated_data['Email Total (Bytes)'] = (

                                                        user_aggregated_data['Email DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Email UL (Bytes)']

                                                )

user_aggregated_data['Gaming Total (Bytes)'] = (

                                                        user_aggregated_data['Gaming DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Gaming UL (Bytes)']

                                                )

user_aggregated_data['Other Total (Bytes)'] = (

                                                        user_aggregated_data['Other DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Other UL (Bytes)']

                                                        )

user_aggregated_data.head()
user_aggregated_data.describe()









plt.hist(user_aggregated_data['Session duration (s)'], bins=30, edgecolor='black')

plt.title('Histogram of Session duration')

plt.xlabel('Session duration (ms)')

plt.ylabel('Frequency')

plt.show()


fig, ax = plt.subplots()


ax.boxplot(user_aggregated_data['Session duration (s)'])

ax.set_yscale('log')  
ax.set_title('Box Plot of Session duration')

ax.set_ylabel('Session duration (seccond)')

plt.show()
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from IPython.display import Image

from sklearn.preprocessing import StandardScaler,MinMaxScaler

from sklearn.cluster import KMeans

import plotly.io as pio

from plotly.subplots import make_subplots

import plotly.graph_objects as go

from mpl_toolkits.mplot3d import Axes3D
df = pd.read_csv('../data/clean_data.csv')
df.head()
df.info()
df.columns.tolist()
tcp_retrans_cols = ['MSISDN/Number', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']

avg_tcp_retrans = df[tcp_retrans_cols].groupby('MSISDN/Number').mean()
avg_tcp_retrans
avg_tcp_retrans.info()
rtt_cols = ['MSISDN/Number', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']

avg_rtt = df[rtt_cols].groupby('MSISDN/Number').mean()
avg_rtt
handset_mode = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None)
handset_mode
throughput_cols = ['MSISDN/Number', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']

avg_throughput = df[throughput_cols].groupby('MSISDN/Number').mean()
avg_throughput

result_df = pd.concat([avg_tcp_retrans, avg_rtt, handset_mode, avg_throughput], axis=1)
result_df.head()

tcp_values = df[['TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']].stack().reset_index(level=1, drop=True)
top_tcp_values = tcp_values.nlargest(10)

print("Top TCP Values:")

print(top_tcp_values)
bottom_tcp_values = tcp_values.nsmallest(10)
print("\nBottom TCP Values:")

print(bottom_tcp_values)
most_frequent_tcp_values = tcp_values.value_counts().nlargest(10)
print("\nMost Frequent TCP Values:")

print(most_frequent_tcp_values)
rtt_values = df[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].stack().reset_index(level=1, drop=True)
top_rtt_values = rtt_values.nlargest(10)
print("\nTop RTT Values:")

print(top_rtt_values)
bottom_rtt_values = rtt_values.nsmallest(10)
print("\nBottom RTT Values:")

print(bottom_rtt_values)
most_frequent_rtt_values = rtt_values.value_counts().nlargest(10)
print("\nMost Frequent RTT Values:")

print(most_frequent_rtt_values)

throughput_values = df[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].stack().reset_index(level=1, drop=True)
top_throughput_values = throughput_values.nlargest(10)
print("\nTop Throughput Values:")

print(top_throughput_values)
bottom_throughput_values = throughput_values.nsmallest(10)
print("\nBottom Throughput Values:")

print(bottom_throughput_values)
most_frequent_throughput_values = throughput_values.value_counts().nlargest(10)
print("\nMost Frequent Throughput Values:")

print(most_frequent_throughput_values)

avg_throughput = df.groupby('Handset Type')['Avg Bearer TP DL (kbps)'].mean()




print("Average Throughput per Handset Type:")

print(avg_throughput)




avg_tcp_retransmission = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean()




print("Average TCP Retransmission per Handset Type:")

print(avg_tcp_retransmission)



experience_metrics = df[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'TCP DL Retrans. Vol (Bytes)']]
experience_metrics.info()
experience_metrics.head()

scaler = StandardScaler()

scaled_data = scaler.fit_transform(experience_metrics)

scaled_data

kmeans = KMeans(n_clusters=3, random_state=10)

df['Cluster'] = kmeans.fit_predict(scaled_data)
df['Cluster'].unique()
df1 = df[df.Cluster==0]

df2 = df[df.Cluster==1]

df3 = df[df.Cluster==2]
plt.scatter(df1['TCP DL Retrans. Vol (Bytes)'], df1['Avg Bearer TP DL (kbps)'], color='green', label='Avg Bearer TP DL (kbps)')

plt.scatter(df2['TCP DL Retrans. Vol (Bytes)'], df2['Avg Bearer TP DL (kbps)'], color='red', label='Avg Bearer TP DL (kbps)')

plt.scatter(df3['TCP DL Retrans. Vol (Bytes)'], df3['Avg Bearer TP DL (kbps)'], color='black', label='Avg Bearer TP DL (kbps)')



plt.xlabel('TCP DL Retransmission Volume (Bytes)')

plt.ylabel('Average Bearer Throughput DL (kbps)')

plt.legend()

plt.show();












experience_metrics.head()
Sessions Frequencyexperience_metrics.info()
experience_metrics.to_csv('../data/user_experience_metrics.csv',index=False)
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from IPython.display import Image

from sklearn.preprocessing import StandardScaler,MinMaxScaler

from sklearn.cluster import KMeans

import plotly.io as pio

from plotly.subplots import make_subplots

import plotly.graph_objects as go

from mpl_toolkits.mplot3d import Axes3D
df = pd.read_csv('../data/clean_data.csv')
df.head()
df.columns.tolist()
df.describe()

sessions_frequency = df.groupby('MSISDN/Number')['Bearer Id'].count()

sessions_frequency

session_duration = df.groupby('MSISDN/Number')['Dur. (ms)'].sum()

session_duration

df['Total Traffic (Bytes)'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']


total_traffic_per_user = df.groupby('MSISDN/Number')['Total Traffic (Bytes)'].sum()


print(total_traffic_per_user)













engagement_metrics = pd.DataFrame({

    'Sessions Frequency': sessions_frequency,

    'session_duration': session_duration,

    'Total Traffic (Bytes)': total_traffic_per_user

})




top_10_sessions_frequency = engagement_metrics['Sessions Frequency'].nlargest(10)

top_10_session_duration = engagement_metrics['session_duration'].nlargest(10)

top_10_total_traffic = engagement_metrics['Total Traffic (Bytes)'].nlargest(10)
top_10_sessions_frequency

def mult_hist(sr, rows, cols, title_text, subplot_titles, interactive=False):

    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)

    for i in range(rows):

        for j in range(cols):

            x = ["-> " + str(i) for i in sr[i+j].index]

            fig.add_trace(go.Bar(x=x, y=sr[i+j].values), row=i+1, col=j+1)

    fig.update_layout(showlegend=False, title_text=title_text)

    if(interactive):

        fig.show()

    else:

        return Image(pio.to_image(fig, format='png', width=1200))


mult_hist([top_10_session_duration, top_10_sessions_frequency,

           top_10_total_traffic], 1,3, "User metrix",

          ['Sessions Duration', 'Frequency Duration','Total Traffic']

         )

engagement_metrics.describe()
engagement_metrics['Total Traffic (Bytes)']

engagement_metrics['Sessions Frequency'] = (engagement_metrics['Sessions Frequency'] - engagement_metrics['Sessions Frequency'].min()) / (engagement_metrics['Sessions Frequency'].max() - engagement_metrics['Sessions Frequency'].min())

engagement_metrics['session_duration'] = (engagement_metrics['session_duration'] - engagement_metrics['session_duration'].min()) / (engagement_metrics['session_duration'].max() - engagement_metrics['session_duration'].min())

engagement_metrics['session_duration'] = (engagement_metrics['session_duration'] - engagement_metrics['session_duration'].min()) / (engagement_metrics['session_duration'].max() - engagement_metrics['session_duration'].min())

engagement_metrics['Total Traffic (Bytes)']['Total Traffic (Bytes)'] = (engagement_metrics['Total Traffic (Bytes)'] - engagement_metrics['Total Traffic (Bytes)'].min()) / (engagement_metrics['Total Traffic (Bytes)'].max() - engagement_metrics['Total Traffic (Bytes)'].min())


Km = KMeans(n_clusters=3, n_init=10)

Km
model_predicted =Km.fit_predict(engagement_metrics[['Sessions Frequency','session_duration','Total Traffic (Bytes)']])

model_predicted
engagement_metrics['clusters'] = model_predicted
engagement_metrics.head()
engagement_metrics['clusters'].unique()
df1 = engagement_metrics[engagement_metrics.clusters==0]

df2 = engagement_metrics[engagement_metrics.clusters==1]

df3 = engagement_metrics[engagement_metrics.clusters==2]
df1.head()
from DataCleaner import ProcessData

username = 'postgres'

password = '123'

host = 'localhost'

database = 'tel'

table_name = 'xdr_data'



data_processor = ProcessData(username, password, host, database, table_name)

processed_data = data_processor.process_data()
"""The first 10 rows of the dataframe """

processed_data.head(5)

for col in processed_data:

    missing_value_count = processed_data[col].isnull().sum()

    print(f"column '{col}': {missing_value_count} missing value")
import seaborn as sns

import matplotlib.pyplot as plt



def plot_top10_customer(dataframe):

    
    grouped = dataframe.groupby('MSISDN/Number')['Dur. (ms)'].sum().reset_index()



    
    top_ten_categories = grouped.nlargest(10, 'Dur. (ms)')



    
    plt.figure(figsize=(12, 6))

    sns.barplot(x='MSISDN/Number', y='Dur. (ms)', data=top_ten_categories, palette='Set2')

    plt.title('Top Ten Categories based on Dur. (ms) Values')

    plt.xlabel('MSISDN/Number')

    plt.ylabel('Sum of Dur. (ms)')

    plt.xticks(rotation=45) 

    plt.tight_layout()

    plt.show()




from univariate import univariate_analysis

univariate_analysis(processed_data, 'Bearer Id')
import seaborn as sns

import matplotlib.pyplot as plt



def plot_top_categories_by_bearer_id(dataframe):

    
    grouped = dataframe.groupby('MSISDN/Number')['Bearer Id'].count().reset_index()



    
    top_ten_categories = grouped.nlargest(10, 'Bearer Id')



    
    plt.figure(figsize=(12, 6))

    sns.barplot(x='MSISDN/Number', y='Bearer Id', data=top_ten_categories, palette='Set2')

    plt.title('Top Ten Categories based on Bearer Id Count')

    plt.xlabel('MSISDN/Number')

    plt.ylabel('No. of_total XDR session')

    plt.xticks(rotation=45) 
    plt.tight_layout()

    plt.show()



plot_top_categories_by_bearer_id(processed_data)
import seaborn as sns

import matplotlib.pyplot as plt



def plot_top_categories_by_sum_of_columns(dataframe):

    
    dataframe['Total_Bytes_Sum'] = dataframe['Total UL (Bytes)'] + dataframe['Total DL (Bytes)']



    
    grouped = dataframe.groupby('MSISDN/Number')['Total_Bytes_Sum'].sum().reset_index()



    
    top_ten_categories = grouped.nlargest(10, 'Total_Bytes_Sum')



    
    plt.figure(figsize=(12, 6))

    sns.barplot(x='MSISDN/Number', y='Total_Bytes_Sum', data=top_ten_categories, palette='Set2')

    plt.title('Top Ten Categories based on Sum of Total Bytes')

    plt.xlabel('MSISDN/Number')

    plt.ylabel('Sum of Total Bytes')

    plt.xticks(rotation=45) 

    plt.tight_layout()

    plt.show()

plot_top_categories_by_sum_of_columns(processed_data)
import seaborn as sns

import matplotlib.pyplot as plt



def plot_top_categories_by_handset_count(dataframe):

    
    handset_counts = dataframe['Handset Type'].value_counts()



    
    top_ten_categories = handset_counts.head(10)



    
    plt.figure(figsize=(12, 6))

    top_ten_categories.plot(kind='bar', color='skyblue')

    plt.title('Top Ten Handset Types based on Count')

    plt.xlabel('Handset Type')

    plt.ylabel('Count')

    plt.xticks(rotation=45) 
    plt.tight_layout()

    plt.show()



plot_top_categories_by_handset_count(processed_data)
def plot_top_huawei_handsets(dataframe):

    
    huawei_data = dataframe[dataframe['Handset Type'].str.contains('huawei', case=False)]



    
    huawei_counts = huawei_data['Handset Type'].value_counts().head(5)



    
    plt.figure(figsize=(10, 6))

    sns.barplot(x=huawei_counts.index, y=huawei_counts.values, palette='viridis')

    plt.title('Top 5 Huawei Handsets')

    plt.xlabel('Handset Model')

    plt.ylabel('Count')

    plt.xticks(rotation=45)

    plt.tight_layout()

    plt.show()
plot_top_huawei_handsets(processed_data)
def plot_top_apple_handsets(dataframe):

    
    apple_data = dataframe[dataframe['Handset Type'].str.contains('Apple', case=False)]



    
    apple_counts = apple_data['Handset Type'].value_counts().head(5)



    
    plt.figure(figsize=(10, 6))

    sns.barplot(x=apple_counts.index, y=apple_counts.values, palette='muted')

    plt.title('Top 5 Apple Handsets')

    plt.xlabel('Handset Model')

    plt.ylabel('Count')

    plt.xticks(rotation=45)

    plt.tight_layout()

    plt.show()
plot_top_apple_handsets(processed_data)
import seaborn as sns

import matplotlib.pyplot as plt



def plot_average_sum(processed_data, categorical_column, non_categorical_columns):

    
    grouped_data = processed_data.groupby(categorical_column)[non_categorical_columns].sum().mean(axis=1).reset_index()



    
    grouped_data.columns = [categorical_column, 'Avg Sum']



    
    plt.figure(figsize=(10, 6))

    sns.barplot(x=categorical_column, y='Avg Sum', data=grouped_data)

    plt.xlabel(categorical_column)

    plt.ylabel('Average of Sum of Non-Categorical Columns')

    plt.title(f'Average Sum of {", ".join(non_categorical_columns)} by {categorical_column}')

    plt.xticks(rotation=45)

    plt.tight_layout()

    plt.show()


categorical_column = 'MSISDN/Number'

non_categorical_columns = ['Avg RTT DL (ms)', 'Avg RTT UL (ms)']




plot_average_sum(processed_data, categorical_column, non_categorical_columns)

processed_data[['Avg RTT DL (ms)','Start']].head(15)
import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.preprocessing import LabelEncoder



class MissingValueFiller:

    def __init__(self, processed_data):

        self.processed_data = processed_data



    def fill_missing_column(self, column_name):

        
        if self.processed_data[column_name].isnull().sum() == 0:

            print(f"No missing values found in column '{column_name}'.")

            return self.processed_data

        

        
        self.processed_data = self.processed_data.dropna(subset=[column_name])

        

        
        exclude_cols = ['Avg RTT UL (ms)', 'MSISDN/Number', 'IMEI', 'TCP DL Retrans. Vol (Bytes)',

                        'TCP UL Retrans. Vol (Bytes)', 'Handset Manufacturer', 'Handset Type',

                        'Bearer Id', 'Start', 'End', 'Last Location Name', column_name]



        
        train_cols = [col for col in self.processed_data.columns if col not in exclude_cols]



        
        X = self.processed_data[train_cols]

        y = self.processed_data[column_name]



        
        if len(X) < 2:

            print(f"Insufficient data to predict missing values for column '{column_name}'.")

            return self.processed_data



        
        model = LinearRegression()



        
        model.fit(X, y)



        
        missing_data_indices = self.processed_data[self.processed_data[column_name].isnull()].index

        X_missing = self.processed_data.loc[missing_data_indices, train_cols]



        
        predicted_values = model.predict(X_missing)

        self.processed_data.loc[missing_data_indices, column_name] = predicted_values

        

        return self.processed_data



    def __call__(self):

        return self.processed_data


filler = MissingValueFiller(processed_data)

column_to_predict = 'Avg RTT DL (ms)'  
filled_data = filler.fill_missing_column(column_to_predict)

processed_data_filled = filler()
from scipy.interpolate import interp1d
from utility import DataProcessor
import pandas as pd
from datetime import datetime

class ProcessData:
    def __init__(self, username, password, host, database, table_name):
        self.username = username
        self.password = password
        self.host = host
        self.database = database
        self.table_name = table_name
""" columns to be interpolated based on appropriate mechanisms to ensure data integerity and conviniece analysis"""

        self.columns_to_interpolate =['Avg RTT DL (ms)', 'Avg RTT UL (ms)'
                                  ,'TCP DL Retrans. Vol (Bytes)',
                                 'TCP UL Retrans. Vol (Bytes)']
    
    def process_data(self):
        processor = DataProcessor(self.username, self.password, self.host, self.database, self.table_name)
        processed_df = processor.fetch_data()
""" 
    Dropping null values from the columns which are listed below, because the missing values from those columns

    indicates that the user xdr session is not recored accurately it might be the user on 'VPN' network for instance

    so the data measured for each attribute are not the actual data, or the other reason might be technical problem

"""
        col_val = ['Bearer Id', 'IMSI', 'Last Location Name', 'Last Location Name', 'IMEI',
                   'MSISDN/Number', 'Handset Manufacturer', 'Handset Type']
        for col in col_val:
            processed_df = processor.drop_null_rows(col)
"""
    we don't need ['HTTP DL (Bytes)', 'HTTP UL (Bytes)'] col to for our Expolraotry Data Analysis due to they are missing in
    
    large proportion compared to the data set we have
 
"""

        columns_to_drop = ['HTTP DL (Bytes)', 'HTTP UL (Bytes)']
        for col in columns_to_drop:
            processed_df = processor.drop_columns([col])
""" 
    those cols in cases variable are columns which are interelated with each other each columns 
    
    for a given interrelated columns they can not have non-zero value simultaneosly if one of 
    
    the attribute has non zero values we will assign 0 for the rest of the interrelated columns
"""
        cases = [
            {
                'related_cols': ['Nb of sec with 125000B < Vol DL', 'Nb of sec with 31250B < Vol DL < 125000B',
                                 'Nb of sec with 6250B < Vol DL < 31250B', 'Nb of sec with Vol DL < 6250B'],
                'target_cols': ['Nb of sec with 125000B < Vol DL', 'Nb of sec with 31250B < Vol DL < 125000B',
                                'Nb of sec with 6250B < Vol DL < 31250B', 'Nb of sec with Vol DL < 6250B']
            },
            {
                'related_cols': ['Nb of sec with 1250B < Vol UL < 6250B', 'Nb of sec with 37500B < Vol UL',
                                 'Nb of sec with 6250B < Vol UL < 37500B', 'Nb of sec with Vol UL < 1250B'],
                'target_cols': ['Nb of sec with 1250B < Vol UL < 6250B', 'Nb of sec with 37500B < Vol UL',
                                'Nb of sec with 6250B < Vol UL < 37500B', 'Nb of sec with Vol UL < 1250B']
            },
            {
                'related_cols': ['DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)',
                                 'DL TP > 1 Mbps (%)'],
                'target_cols': ['DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)',
                                'DL TP > 1 Mbps (%)']
            },
            {
                'related_cols': ['UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)', '50 Kbps < UL TP < 300 Kbps (%)',
                                 'UL TP > 300 Kbps (%)'],
                'target_cols': ['UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)', '50 Kbps < UL TP < 300 Kbps (%)',
                                'UL TP > 300 Kbps (%)']
            },
        ]
        col_interpolated = ['Avg RTT DL (ms)','Avg RTT UL (ms)']
        processed_df=  processor.interpolate_columns(col_interpolated,window_size=5)
        processed_df = processed_df
        for case in cases:
            related_cols = case['related_cols']
            target_cols = case['target_cols']
            processed_df = processor.assign_zero_based_on_condition(related_cols, target_cols)

        
        return processed_df
from scipy.interpolate import interp1d
import numpy as np
from utility import DataProcessor

class ProcessData:
    
    def interpolate_nonlinear(self, processed_df, column_name):
                non_null_indices = processed_df[column_name].notnull()

                x = np.where(non_null_indices)[0]          y = processed_df.loc[non_null_indices, column_name].values  
                interpolator = interp1d(x, y, kind='cubic')

                full_indices = np.arange(len(processed_df))

                interpolated_values = interpolator(full_indices)

                processed_df[column_name] = np.where(
            processed_df[column_name].isnull(),
            interpolated_values[processed_df.index],
            processed_df[column_name]
        )

        return processed_df

    def process_data(self):
        processor = DataProcessor(self.username, self.password, self.host, self.database, self.table_name)
        processed_df = processor.fetch_data()

        col_val = ['Bearer Id', 'IMSI', 'Last Location Name', 'Last Location Name', 'IMEI',
                   'MSISDN/Number', 'Handset Manufacturer', 'Handset Type']
        for col in col_val:
            processed_df = processor.drop_null_rows(col)

        columns_to_drop = ['HTTP DL (Bytes)', 'HTTP UL (Bytes)']
        for col in columns_to_drop:
            processed_df = processor.drop_columns([col])

        cases = [
            
        ]

        for case in cases:
            related_cols = case['related_cols']
            target_cols = case['target_cols']
            processed_df = processor.assign_zero_based_on_condition(related_cols, target_cols)
            
                column_to_interpolate = 'Your_Column_Name'
        processed_df = self.interpolate_nonlinear(processed_df, column_to_interpolate)

        return processed_df
import pandas as pd
from sqlalchemy import create_engine

class DataProcessor:
    def __init__(self, username, password, host, database, table_name):
        self.username = username
        self.password = password
        self.host = host
        self.database = database
        self.table_name = table_name
        self.df = None  
    def fetch_data(self):
        connection_str = f"postgresql+psycopg2://{self.username}:{self.password}@{self.host}/{self.database}"
        engine = create_engine(connection_str)

        query = f"SELECT * FROM {self.table_name}"

        self.df = pd.read_sql_query(query, engine)
        
        engine.dispose()
        return self.df  
    def drop_null_rows(self, column):
        if self.df is not None and column in self.df.columns:
            self.df.dropna(subset=[column], inplace=True)
        return self.df      
    def drop_columns(self, columns_to_drop):
        if self.df is not None:
            self.df.drop(columns=columns_to_drop, inplace=True, errors='ignore')
        return self.df 
    def assign_zero_based_on_condition(self, related_cols, target_cols):
        if self.df is not None:
            mask = self.df[target_cols].isnull().any(axis=1)
            for idx, row in self.df.iterrows():
                if any(pd.isnull(row[col]) for col in target_cols) and any(row[col] != 0 for col in related_cols):
                    for col in target_cols:
                        self.df.at[idx, col] = 0
        return self.df  

    def interpolate_columns(self, columns_to_interpolate, window_size=3):
        """
        Interpolate specified columns using a moving average and fill NaN values.

        Parameters:
        - columns_to_interpolate (list): List of column names to interpolate.
        - window_size (int): Size of the moving average window. we use small value to for resource management and processing time

        """
        df_interpolated = self.df.copy()

        for column in columns_to_interpolate:
            if column not in df_interpolated.columns:
                print(f"Column '{column}' not found in the DataFrame.")
                continue

            df_interpolated[column] = df_interpolated[column].rolling(window=window_size, min_periods=1).mean()

                df_interpolated = df_interpolated.ffill()

                self.df[columns_to_interpolate] = df_interpolated[columns_to_interpolate]

        return self.df
import pandas as pd

from sqlalchemy import create_engine

import numpy as p

import matplotlib.pyplot as plt

import seaborn as sns

from seaborn import scatterplot









database_name = 'telecom'

table_name= 'xdr_data'



connection_params = { "host": "localhost", "user": "postgres", "password": "admin",

                    "port": "5432", "database": database_name}



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")


sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query, con= engine)

df.info()

pd.set_option('display.max_rows', None)

df.head(10)

print(df.columns)

df.isna().sum()

df.describe()

print(df.shape)

top_10_of_handsets =df['Handset Type'].value_counts().head(10)

print("Top 10 Handsets:")

print(top_10_of_handsets)

top_3_manufacturers = df['Handset Manufacturer'].value_counts().head(3)



print("Top 3 Handset Manufacturers:")

print(top_3_manufacturers)

top_3_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index.tolist()

filtered_df = df[df['Handset Manufacturer'].isin(top_3_manufacturers)]

top_5_handsets_per_manufacturer = filtered_df.groupby('Handset Manufacturer')['Handset Type'].value_counts().groupby('Handset Manufacturer').head(5)



print("Top 5 Handsets per Top 3 of the Handset Manufacturers:")

print(top_5_handsets_per_manufacturer)

bottom_3_manufacturers = df['Handset Manufacturer'].value_counts().tail(3).index.tolist()

filtered_df = df[df['Handset Manufacturer'].isin(bottom_3_manufacturers)]

bottom_5_handsets_per_manufacturer = filtered_df.groupby('Handset Manufacturer')['Handset Type'].value_counts().groupby('Handset Manufacturer').tail(5)



print("Botttom 5 Handsets per bottom 3 of the Handset Manufacturers:")

print(bottom_5_handsets_per_manufacturer)













user_app_columns = ['Bearer Id', 'Handset Manufacturer', 'Handset Type', 'Social Media DL (Bytes)',

                    'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)',

                    'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                    'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                    'Other DL (Bytes)', 'Other UL (Bytes)']
df_user_apps = df[user_app_columns]



df_user_apps['Number of xDR Sessions'] = df_user_apps.groupby('Bearer Id')['Bearer Id'].transform('count')

result_columns = ['Bearer Id', 'Number of xDR Sessions']



result = df_user_apps[result_columns].drop_duplicates().reset_index(drop=True)

print(result)

session_dur_per_user = df.groupby('Bearer Id')['Dur. (ms)'].sum().reset_index(name='Session Duration (ms)')



print(session_dur_per_user)

total_data_per_user =pd.DataFrame (df.groupby('Bearer Id')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index())



print(total_data_per_user)

application_columns = ['Bearer Id', 'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                        'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)',

                        'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',

                         'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)'

                        

                       ]




total_data_per_user_app = df[application_columns].groupby('Bearer Id').sum().reset_index()



print(total_data_per_user_app)

num_col = df.select_dtypes (include = ['number']).columns

cat_col = df.select_dtypes (include = ['object']).columns



df_num = df[num_col]

df_cat = df[cat_col]



print("Numerical Columns:", num_col)

print("Categorical Columns:", cat_col)

df_num = df_num.fillna(df_num.mean())

df_num.isna().sum()



def replace_outliers_with_mean(df_num, threshold=3):

    for col in df_num.columns:

        z_scores = (df_num[col] - df_num[col].mean()) / df_num[col].std()

        outlier_mask = (z_scores > threshold) | (z_scores < -threshold)

        df_num[col][outlier_mask] = df_num[col].mean()

    return df_num



df_num = replace_outliers_with_mean(df_num)

df_num.describe()
correlation_matrix = df_num.corr()

print(correlation_matrix)




columns_to_plot = ['Dur. (ms)', 'Avg RTT DL (ms)',

                   'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',  

                   'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                    'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)',

                    'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                    'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',

                    'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)']




num_plots = len(columns_to_plot)

fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(10, 4 * num_plots))




for i, column in enumerate(columns_to_plot):

    axes[i].hist(df_num[column], bins=20, color='skyblue', edgecolor='black')

    axes[i].set_title(f'Histogram of {column}')

    axes[i].set_xlabel(column)

    axes[i].set_ylabel('Frequency')




plt.tight_layout()

plt.show()


columns_to_plot = ['Dur. (ms)', 'Avg RTT DL (ms)',

                   'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',

                   'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                   'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)',

                   'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                   'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',

                   'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)']




num_plots = len(columns_to_plot)

fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(10, 4 * num_plots))




for i, column in enumerate(columns_to_plot):

    axes[i].boxplot(df_num[column])

    axes[i].set_title(f'Box Plot of {column}')

    axes[i].set_xlabel(column)

    axes[i].set_ylabel('Value')




plt.tight_layout()

plt.show()
df_num['Total Bytes Sum'] = df_num['Total DL (Bytes)'] + df_num['Total UL (Bytes)']

print(df_num['Total Bytes Sum'])



application_columns2 = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                        'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)',

                        'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',

                         'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Other UL (Bytes)'

                        

                       ]





correlations = df_num[application_columns2 + ['Total Bytes Sum']]. corr()

correlations


for app_column in application_columns2:

    plt.figure(figsize=(8, 6))

    sns.scatterplot(x='Total Bytes Sum', y=app_column, data=df_num)  

    plt.title(f'Scatter Plot: {app_column} vs Total Bytes Sum')

    plt.xlabel('Total Bytes Sum')

    plt.ylabel(app_column)

    plt.show()

correlation_matrix = df_num[application_columns2 + ['Total Bytes Sum']].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')

plt.show()      

            
df_num['Total Duration'] = df_num['Dur. (ms)']




df_num['Duration Decile'] = pd.qcut(df_num['Total Duration'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False, precision=0)




decile_totals = df_num.groupby(['MSISDN/Number', 'Duration Decile'])['Total Bytes Sum'].sum().reset_index()




top_five_deciles = decile_totals.groupby('Duration Decile')['Total Bytes Sum'].sum().nlargest(5).index




top_five_deciles_data = decile_totals[decile_totals['Duration Decile'].isin(top_five_deciles)]




print(top_five_deciles_data)

subset_df = df_num[application_columns2]




correlation_matrix = subset_df.corr



print(correlation_matrix)
import numpy as np

import matplotlib.pyplot as plt

%matplotlib inline

import math

import seaborn as sns

import plotly.io as pio

from plotly import *

import plotly.express as px

from plotly.subplots import make_subplots

import plotly.graph_objects as go  

import pandas as pd

from IPython.display import Image

from sklearn import preprocessing

from sklearn.cluster import KMeans

from scipy.spatial.distance import cdist

from sklearn.preprocessing import StandardScaler, normalize

import os,sys

import kaleido

sys.path.insert(0,'../scripts/')



from process import *
df = pd.read_csv("telecom2.csv", na_values=['?',"n.a.","NA","n/a", "na", None])

df.drop("Unnamed: 0",axis=1,inplace=True)



df.head(5)






users_data = df.groupby('MSISDN/Number')




users_sessions= users_data['Bearer Id'].count()

users_sessions.head(10)





users_sessions= users_data['Dur. (ms)'].sum()

users_sessions

df["Total Uploads"]=df["Google UL (Bytes)"]+df["Email UL (Bytes)"]+df["Social Media UL (Bytes)"]+df["Youtube UL (Bytes)"]+df["Netflix UL (Bytes)"]+df["Gaming UL (Bytes)"]+df["Other UL (Bytes)"]

df["Total Downloads"]=df["Google DL (Bytes)"]+df["Email DL (Bytes)"]+df["Social Media DL (Bytes)"]+df["Youtube DL (Bytes)"]+df["Netflix DL (Bytes)"]+df["Gaming DL (Bytes)"]+df["Other DL (Bytes)"]

df['Total UL and DL']=df['Total DL (Bytes)']+df["Total UL (Bytes)"]








df["Youtube_Total_Data"]=df["Youtube DL (Bytes)"]+df["Youtube UL (Bytes)"]

df["Google_Total_Data"]=df["Google DL (Bytes)"]+df["Google UL (Bytes)"]

df["Email_Total_Data"]=df["Email DL (Bytes)"]+df["Email UL (Bytes)"]

df["Social_Media_Total_Data"]=df["Social Media DL (Bytes)"]+df["Social Media UL (Bytes)"]

df["Netflix_Total_Data"]=df["Netflix DL (Bytes)"]+df["Netflix UL (Bytes)"]

df["Gaming_Total_Data"]=df["Gaming DL (Bytes)"]+df["Gaming UL (Bytes)"]

df["Other_Total_Data"]=df["Other DL (Bytes)"]+df["Other UL (Bytes)"]

users = df[['MSISDN/Number', 'Bearer Id', 'Dur. (ms).1', 'Total UL and DL']].copy().rename(columns={'Dur. (ms).1': 'time_duration'})

users

users = users.groupby('MSISDN/Number').agg({'Bearer Id': 'count', 'time_duration': 'sum', 'Total UL and DL': 'sum'})

users = users.rename(columns={'Bearer Id': 'sessions'})

users.head(10)

users.nlargest(10, 'time_duration')


users.nlargest(10, 'time_duration')

sessions = users.nlargest(10, "sessions")['sessions']

duration = users.nlargest(10, "time_duration")['time_duration']

data_volume = users.nlargest(10, "Total UL and DL")['Total UL and DL']



mult_hist([sessions, duration, data_volume], 1,3, "User metrix", ['sessions', 'time_duration','Total UL and DL'])
users.boxplot()
scaler = StandardScaler()

scaled_array = scaler.fit_transform(users)

pd.DataFrame(scaled_array).head(5)



data_normalized = normalize(scaled_array)

pd.DataFrame(data_normalized).head(5)



data_normalized
kmeans = KMeans(n_clusters=3, random_state=0).fit(data_normalized)

kmeans.labels_
users.insert(0, 'Cluster', kmeans.labels_)

users.head(5)
fig = px.scatter(users, x='Total UL and DL', y="time_duration", color='Cluster', size='sessions')

Image(pio.to_image(fig, format='png', width=1200))





cluster1 = users[users["Cluster"]==0]

cluster1.describe()
cluster1 = users[users["Cluster"]==1]

cluster1.describe()


apps_df = df.groupby('MSISDN/Number').agg({'Gaming_Total_Data': 'sum', 'Youtube_Total_Data': 'sum', 'Netflix_Total_Data': 'sum',\

                     'Google_Total_Data': 'sum', 'Email_Total_Data': 'sum', 'Social_Media_Total_Data': 'sum', 'Other_Total_Data': 'sum'})

apps_df.head(10)
Gaming_Data = apps_df.nlargest(10, "Gaming_Total_Data")['Gaming_Total_Data']

Youtube_Data = apps_df.nlargest(10, "Youtube_Total_Data")['Youtube_Total_Data']

Netflix_Data = apps_df.nlargest(10, "Netflix_Total_Data")['Netflix_Total_Data']

Google_Data = apps_df.nlargest(10, "Google_Total_Data")['Google_Total_Data']

Email_Data = apps_df.nlargest(10, "Email_Total_Data")['Email_Total_Data']

Social_Media = apps_df.nlargest(10, "Social_Media_Total_Data")['Social_Media_Total_Data']

Other_Data = apps_df.nlargest(10, "Other_Total_Data")['Other_Total_Data']





mult_hist([Gaming_Data, Youtube_Data, Netflix_Data], 1,

          3, "User metrix", ["Gaming_Data", "Youtube_Data", "Netflix_Data"])



top_3 = apps_df.sum()

type(top_3)



top_3
import matplotlib.pyplot as plt




applications = ['Gaming', 'Youtube', 'Netflix', 'Google', 'Email', 'Social Media', 'Other']

data_values = [63910124731666, 3362537620065, 3360563670772, 1159544186717, 335503000148, 271478798103, 63770726326023]




sorted_data = sorted(zip(applications, data_values), key=lambda x: x[1], reverse=True)




top_applications = [app for app, _ in sorted_data[:3]]

top_values = [value for _, value in sorted_data[:3]]




plt.figure(figsize=(8, 6))




plt.bar(top_applications, top_values, color=['blue', 'green', 'orange'])

plt.title('Top 3 Most Used Applications')

plt.xlabel('Applications')

plt.ylabel('Total Data (in bytes)')




for i, value in enumerate(top_values):

    plt.text(i, value + 0.002 * max(top_values), f'{value:,}', ha='center')




plt.show()












def choose_kmeans(df: pd.DataFrame, num: int):

  distortions = []

  inertias = []

  K = range(1, num)

  for k in K:

    kmeans = KMeans(n_clusters=k, random_state=0).fit(df)

    distortions.append(sum(

        np.min(cdist(df, kmeans.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])

    inertias.append(kmeans.inertia_)



  return (distortions, inertias)

distortions, inertias = choose_kmeans(data_normalized, 1)
fig = make_subplots(

    rows=1, cols=2, subplot_titles=("Distortion", "Inertia")

)

fig.add_trace(go.Scatter(x=np.array(range(1, 15)), y=distortions), row=1, col=1)

fig.add_trace(go.Scatter(x=np.array(range(1, 15)), y=inertias), row=1, col=2)

fig.update_layout(title_text="The Elbow Method")


Image(pio.to_image(fig, format='png', width=1200))



kmeans = KMeans(n_clusters=4, random_state=0).fit(data_normalized)

users["cluster"]= kmeans.labels_

users
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

%matplotlib inline

import math

import seaborn as sns

import plotly.io as pio

from plotly import *

import plotly.express as px

from plotly.subplots import make_subplots

import plotly.graph_objects as go  

import pandas as pd

from IPython.display import Image

from sklearn import preprocessing

from sklearn.cluster import KMeans

from scipy.spatial.distance import cdist

from sklearn.preprocessing import StandardScaler, normalize

import os,sys

import kaleido

sys.path.insert('./script/')



from process import *
df = pd.read_csv("../data_csv/telecom2.csv", na_values=['?',"n.a.","NA","n/a", "na", None])

df.drop("Unnamed: 0",axis=1,inplace=True)



df.head(5)







users_data = df.groupby('MSISDN/Number')




users_sessions= users_data['Bearer Id'].count()

users_sessions.head(10)





users_sessions= users_data['Dur. (ms)'].sum()

users_sessions

df["Total Uploads"]=df["Google UL (Bytes)"]+df["Email UL (Bytes)"]+df["Social Media UL (Bytes)"]+df["Youtube UL (Bytes)"]+df["Netflix UL (Bytes)"]+df["Gaming UL (Bytes)"]+df["Other UL (Bytes)"]

df["Total Downloads"]=df["Google DL (Bytes)"]+df["Email DL (Bytes)"]+df["Social Media DL (Bytes)"]+df["Youtube DL (Bytes)"]+df["Netflix DL (Bytes)"]+df["Gaming DL (Bytes)"]+df["Other DL (Bytes)"]

df['Total UL and DL']=df['Total DL (Bytes)']+df["Total UL (Bytes)"]








df["Youtube_Total_Data"]=df["Youtube DL (Bytes)"]+df["Youtube UL (Bytes)"]

df["Google_Total_Data"]=df["Google DL (Bytes)"]+df["Google UL (Bytes)"]

df["Email_Total_Data"]=df["Email DL (Bytes)"]+df["Email UL (Bytes)"]

df["Social_Media_Total_Data"]=df["Social Media DL (Bytes)"]+df["Social Media UL (Bytes)"]

df["Netflix_Total_Data"]=df["Netflix DL (Bytes)"]+df["Netflix UL (Bytes)"]

df["Gaming_Total_Data"]=df["Gaming DL (Bytes)"]+df["Gaming UL (Bytes)"]

df["Other_Total_Data"]=df["Other DL (Bytes)"]+df["Other UL (Bytes)"]

users = df[['MSISDN/Number', 'Bearer Id', 'Dur. (ms).1', 'Total UL and DL']].copy().rename(columns={'Dur. (ms).1': 'time_duration'})

users

users = users.groupby('MSISDN/Number').agg({'Bearer Id': 'count', 'time_duration': 'sum', 'Total UL and DL': 'sum'})

users = users.rename(columns={'Bearer Id': 'sessions'})

users.head(10)

users.nlargest(10, 'time_duration')


users.nlargest(10, 'time_duration')

sessions = users.nlargest(10, "sessions")['sessions']

duration = users.nlargest(10, "time_duration")['time_duration']

data_volume = users.nlargest(10, "Total UL and DL")['Total UL and DL']



mult_hist([sessions, duration, data_volume], 1,3, "User metrix", ['sessions', 'time_duration','Total UL and DL'])
users.boxplot()
scaler = StandardScaler()

scaled_array = scaler.fit_transform(users)

pd.DataFrame(scaled_array).head(5)



data_normalized = normalize(scaled_array)

pd.DataFrame(data_normalized).head(5)



data_normalized
kmeans = KMeans(n_clusters=3, random_state=0).fit(data_normalized)

kmeans.labels_
users.insert(0, 'Cluster', kmeans.labels_)

users.head(5)
fig = px.scatter(users, x='Total UL and DL', y="time_duration", color='Cluster', size='sessions')

Image(pio.to_image(fig, format='png', width=1200))





cluster1 = users[users["Cluster"]==0]

cluster1.describe()
cluster1 = users[users["Cluster"]==1]

cluster1.describe()


apps_df = df.groupby('MSISDN/Number').agg({'Gaming_Total_Data': 'sum', 'Youtube_Total_Data': 'sum', 'Netflix_Total_Data': 'sum',\

                     'Google_Total_Data': 'sum', 'Email_Total_Data': 'sum', 'Social_Media_Total_Data': 'sum', 'Other_Total_Data': 'sum'})

apps_df.head(10)
Gaming_Data = apps_df.nlargest(10, "Gaming_Total_Data")['Gaming_Total_Data']

Youtube_Data = apps_df.nlargest(10, "Youtube_Total_Data")['Youtube_Total_Data']

Netflix_Data = apps_df.nlargest(10, "Netflix_Total_Data")['Netflix_Total_Data']

Google_Data = apps_df.nlargest(10, "Google_Total_Data")['Google_Total_Data']

Email_Data = apps_df.nlargest(10, "Email_Total_Data")['Email_Total_Data']

Social_Media = apps_df.nlargest(10, "Social_Media_Total_Data")['Social_Media_Total_Data']

Other_Data = apps_df.nlargest(10, "Other_Total_Data")['Other_Total_Data']





mult_hist([Gaming_Data, Youtube_Data, Netflix_Data], 1,

          3, "User metrix", ["Gaming_Data", "Youtube_Data", "Netflix_Data"])



top_3 = apps_df.sum()

type(top_3)



top_3
import matplotlib.pyplot as plt




applications = ['Gaming', 'Youtube', 'Netflix', 'Google', 'Email', 'Social Media', 'Other']

data_values = [63910124731666, 3362537620065, 3360563670772, 1159544186717, 335503000148, 271478798103, 63770726326023]




sorted_data = sorted(zip(applications, data_values), key=lambda x: x[1], reverse=True)




top_applications = [app for app, _ in sorted_data[:3]]

top_values = [value for _, value in sorted_data[:3]]




plt.figure(figsize=(8, 6))




plt.bar(top_applications, top_values, color=['blue', 'green', 'orange'])

plt.title('Top 3 Most Used Applications')

plt.xlabel('Applications')

plt.ylabel('Total Data (in bytes)')




for i, value in enumerate(top_values):

    plt.text(i, value + 0.002 * max(top_values), f'{value:,}', ha='center')




plt.show()












def choose_kmeans(df: pd.DataFrame, num: int):

  distortions = []

  inertias = []

  K = range(1, num)

  for k in K:

    kmeans = KMeans(n_clusters=k, random_state=0).fit(df)

    distortions.append(sum(

        np.min(cdist(df, kmeans.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])

    inertias.append(kmeans.inertia_)



  return (distortions, inertias)

distortions, inertias = choose_kmeans(data_normalized, 1)
fig = make_subplots(

    rows=1, cols=2, subplot_titles=("Distortion", "Inertia")

)

fig.add_trace(go.Scatter(x=np.array(range(1, 15)), y=distortions), row=1, col=1)

fig.add_trace(go.Scatter(x=np.array(range(1, 15)), y=inertias), row=1, col=2)

fig.update_layout(title_text="The Elbow Method")


Image(pio.to_image(fig, format='png', width=1200))



kmeans = KMeans(n_clusters=4, random_state=0).fit(data_normalized)

users["cluster"]= kmeans.labels_

users
import pandas as pd

import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from utils import DataCleaner, DatabaseEngine

db_engine = DatabaseEngine()

engine = db_engine.create()

df = pd.read_sql_table("xdr_data", con=engine)

df.head()



df.isnull().sum().sum()


cleaner = DataCleaner(df)

cleaned_df = cleaner.clean()




cleaned_df.to_sql("clean_xdr_data", con=engine, if_exists="replace", index=False)

clean_df = pd.read_sql_table("clean_xdr_data", con=engine)




clean_df.isnull().sum().sum()

filtered_handsets = clean_df[clean_df["Handset Type"] != "undefined"]

top_10_handsets = filtered_handsets["Handset Type"].value_counts().head(10)

pd.DataFrame(top_10_handsets)

top_3_manufacturers = clean_df["Handset Manufacturer"].value_counts().head(3)

pd.DataFrame(top_3_manufacturers)

top_3_manufacturers = ['Apple', 'Samsung', 'Huawei']



results = []



for manufacturer in top_3_manufacturers:

    top_5_handsets = clean_df[clean_df["Handset Manufacturer"] == manufacturer]["Handset Type"].value_counts().head(5)

    temp_df = pd.DataFrame({'Manufacturer': manufacturer, 'Handset Type': top_5_handsets.index, 'Count': top_5_handsets.values})

    results.append(temp_df)




results_df = pd.concat(results, ignore_index=True)



pd.DataFrame(results_df)
from urllib.parse import quote_plus
from pathlib import Path
from sqlalchemy import create_engine
from dotenv import dotenv_values


class DatabaseEngine:
    """
    A class used to create a SQLAlchemy engine object for the PostgreSQL database.

    ...

    Attributes
    ----------
    env_path : Path
        path to the .env file containing database configuration

    Methods
    -------
    create():
        Creates and returns a SQLAlchemy engine object for the PostgreSQL database.
    """

    def __init__(self):
        """
        Constructs all the necessary attributes for the DatabaseEngine object.
        """
        self.env_path = Path(__file__).parent / ".env"

    def create(self):
        """
        Creates and returns a SQLAlchemy engine object for the PostgreSQL database.

        Returns:
            engine: A SQLAlchemy engine object connected to the PostgreSQL database.

        Raises:
            Exception: If there is a problem connecting to the database.
        """
        env_values = dotenv_values(dotenv_path=self.env_path)

        db_password = env_values["POSTGRES_PASSWORD"]
        encoded_password = quote_plus(db_password)
        database = env_values["POSTGRES_DB"]
        user = env_values["POSTGRES_USER"]
        server = env_values["POSTGRES_SERVER"]

        engine = create_engine(
            f"postgresql://{user}:{encoded_password}@{server}/{database}"
        )
        try:
            with engine.connect() as connection_str:
                print(f'Successfully connected to the PostgreSQL "{database}" database')
        except Exception as ex:
            print(f"Sorry failed to connect: {ex}")

        return engine
class DataCleaner:
    """
    A class used to clean data in a DataFrame.

    ...

    Attributes
    ----------
    df : DataFrame
        a pandas DataFrame to be cleaned
    columns_to_exclude : list
        a list of column names to be excluded from certain operations

    Methods
    -------
    drop_null_rows():
        Drops rows that have a null value in any of the columns that have exactly one null value.
    fill_categorical():
        Fills missing categorical values with the mode of each column.
    fill_numerical():
        Fills missing numerical values with the mean of each column, excluding certain specified columns.
    fill_unknown():
        Fills missing values in certain specified columns with the string "Unknown".
    clean():
        Performs all the cleaning steps and returns the cleaned DataFrame.
    """

    def __init__(self, df):
        """
        Constructs all the necessary attributes for the DataCleaner object.

        Parameters
        ----------
            df : DataFrame
                a pandas DataFrame to be cleaned
        """
        self.df = df.copy()
        self.columns_to_exclude = ["Bearer Id", "IMSI", "MSISDN/Number", "IMEI"]

    def drop_null_rows(self):
        """
        Drops rows that have a null value in any of the columns that have exactly one null value.

        Returns
        -------
        self : object
            Returns self to allow chaining.
        """
        columns_with_one_null = self.df.columns[self.df.isnull().sum() == 1]
        self.df = self.df.dropna(subset=columns_with_one_null)
        return self

    def fill_categorical(self):
        """
        Fills missing categorical values with the mode of each column.

        Returns
        -------
        self : object
            Returns self to allow chaining.
        """
        categorical_columns = self.df.select_dtypes(include="object").columns
        self.df[categorical_columns] = self.df[categorical_columns].fillna(
            self.df[categorical_columns].mode().iloc[0]
        )
        return self

    def fill_numerical(self):
        """
        Fills missing numerical values with the mean of each column, excluding certain specified columns.

        Returns
        -------
        self : object
            Returns self to allow chaining.
        """
        numerical_columns = self.df.select_dtypes(include=["float64"]).columns
        numerical_columns = numerical_columns.drop(self.columns_to_exclude)
        self.df[numerical_columns] = self.df[numerical_columns].fillna(
            self.df[numerical_columns].mean()
        )
        return self

    def fill_unknown(self):
        """
        Fills missing values in certain specified columns with the string "Unknown".

        Returns
        -------
        self : object
            Returns self to allow chaining.
        """
        self.df[self.columns_to_exclude] = self.df[self.columns_to_exclude].fillna(
            "Unknown"
        )
        return self

    def clean(self):
        """
        Performs all the cleaning steps and returns the cleaned DataFrame.

        Returns
        -------
        DataFrame
            The cleaned DataFrame.
        """
        self.drop_null_rows().fill_categorical().fill_numerical().fill_unknown()
        return self.df
from .data_cleaning import DataCleaner
from .db_connection import DatabaseEngine
import pandas as pd

import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from utils import DatabaseEngine
db_engine = DatabaseEngine()

engine = db_engine.create()

clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
def process_user_info(clean_df):

    
    num_sessions = clean_df.groupby('MSISDN/Number')['Bearer Id'].count()



    
    session_duration = clean_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()



    
    total_DL = clean_df.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()

    total_UL = clean_df.groupby('MSISDN/Number')['Total UL (Bytes)'].sum()



    
    total_data_vol = clean_df.groupby('MSISDN/Number')[['Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                                                   'Google DL (Bytes)', 'Google UL (Bytes)',

                                                   'Email DL (Bytes)', 'Email UL (Bytes)',

                                                   'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                                                   'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                                                   'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                                                   'Other DL (Bytes)', 'Other UL (Bytes)']].sum()

    user_info = pd.concat([num_sessions, session_duration, total_DL, total_UL, total_data_vol], axis=1)

    user_info = user_info.rename(columns={

        'Bearer Id': 'Number of Sessions',

        'Dur. (ms)': 'Total Session Duration',

        'Total DL (Bytes)': 'Total Download Data',

        'Total UL (Bytes)': 'Total Upload Data',

        'Social Media DL (Bytes)': 'Social Media Download Data',

        'Social Media UL (Bytes)': 'Social Media Upload Data',

        'Google DL (Bytes)': 'Google Download Data',

        'Google UL (Bytes)': 'Google Upload Data',

        'Email DL (Bytes)': 'Email Download Data',

        'Email UL (Bytes)': 'Email Upload Data',

        'Youtube DL (Bytes)': 'Youtube Download Data',

        'Youtube UL (Bytes)': 'Youtube Upload Data',

        'Netflix DL (Bytes)': 'Netflix Download Data',

        'Netflix UL (Bytes)': 'Netflix Upload Data',

        'Gaming DL (Bytes)': 'Gaming Download Data',

        'Gaming UL (Bytes)': 'Gaming Upload Data',

        'Other DL (Bytes)': 'Other Download Data',

        'Other UL (Bytes)': 'Other Upload Data'

    })

    

    return user_info



filtered_df = clean_df[clean_df['MSISDN/Number'] != 'Unknown']

user_info = process_user_info(filtered_df)

pd.DataFrame(user_info)
import sys, os

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from utils import DatabaseEngine
db_engine = DatabaseEngine()

engine = db_engine.create()

clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
data_df = pd.read_csv('../data/user_info.csv')

df = pd.concat([clean_df, data_df], axis=1)

df.info()
df.describe()
exclude_columns = ['Bearer Id', 'IMSI', 'MSISDN/Number', 'IMEI']

column_list = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col not in exclude_columns]



for column in column_list:

    print(f"--- {column} ---")

    print(f"Range: {df[column].max()} - {df[column].min()}")

    print(f"Variance: {df[column].var()}")

    print(f"Standard Deviation: {df[column].std()}")

    print(f"IQR: {df[column].quantile(0.75) - df[column].quantile(0.25)}")

plt.hist(df['Total Download Data'], bins=20)

plt.xlabel('Total Download Data')

plt.ylabel('Frequency')

plt.title('Total Download Data')

plt.show()

plt.hist(df['Total Upload Data'], bins=20)

plt.xlabel('Total Uploaded Data')

plt.ylabel('Frequency')

plt.title('Total Uploaded Data')

plt.show()

labels = ['DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)']

sizes = [df[label].mean() for label in labels]



plt.pie(sizes, labels=labels, autopct='%1.1f%%')

plt.title('Traffic Distribution')

plt.axis('equal')

plt.show()

q1 = df['Avg RTT DL (ms)'].quantile(0.25)

q3 = df['Avg RTT DL (ms)'].quantile(0.75)

iqr = q3 - q1




lower_bound = q1 - 1.5 * iqr

upper_bound = q3 + 1.5 * iqr




df['Avg RTT DL (ms)'] = df['Avg RTT DL (ms)'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))




sns.boxplot(y=df['Avg RTT DL (ms)'])

plt.ylabel('Avg RTT DL (ms)')

plt.title('Average Round Trip Time for Download')

plt.show()


q1 = df['Avg RTT UL (ms)'].quantile(0.25)

q3 = df['Avg RTT UL (ms)'].quantile(0.75)

iqr = q3 - q1




lower_bound = q1 - 1.5 * iqr

upper_bound = q3 + 1.5 * iqr




df['Avg RTT UL (ms)'] = df['Avg RTT UL (ms)'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))






sns.boxplot(y=df['Avg RTT UL (ms)'])

plt.ylabel('Avg RTT UL (ms)')

plt.title('Average Round Trip Time for Upload')

plt.show()
activity_duration = df['Activity Duration DL (ms)']

avg_bearer_throughput = df['Avg Bearer TP DL (kbps)']



plt.figure(figsize=(10, 6))

plt.scatter(activity_duration, avg_bearer_throughput, color='blue', alpha=0.5)

plt.xlabel('Activity Duration DL (ms)')

plt.ylabel('Avg Bearer TP DL (kbps)')

plt.title('Average Bearer Throughput for Downlink vs. Activity Duration')

plt.tight_layout()

plt.show()


user_info = {

    'Total DL (Bytes)': 'Total Download Data',

    'Social Media DL (Bytes)': 'Social Media Download Data',

    'Google DL (Bytes)': 'Google Download Data',

    'Email DL (Bytes)': 'Email Download Data',

    'Youtube DL (Bytes)': 'Youtube Download Data',

    'Netflix DL (Bytes)': 'Netflix Download Data',

    'Gaming DL (Bytes)': 'Gaming Download Data',

    'Other DL (Bytes)': 'Other Download Data',

    

    'Total UL (Bytes)': 'Total Upload Data',

    'Social Media UL (Bytes)': 'Social Media Upload Data',

    'Google UL (Bytes)': 'Google Upload Data',

    'Email UL (Bytes)': 'Email Upload Data',

    'Youtube UL (Bytes)': 'Youtube Upload Data',

    'Netflix UL (Bytes)': 'Netflix Upload Data',

    'Gaming UL (Bytes)': 'Gaming Upload Data',

    'Other UL (Bytes)': 'Other Upload Data'

}




download_columns = [

    'Total DL (Bytes)',

    'Social Media DL (Bytes)',

    'Google DL (Bytes)',

    'Email DL (Bytes)',

    'Youtube DL (Bytes)',

    'Netflix DL (Bytes)',

    'Gaming DL (Bytes)',

    'Other DL (Bytes)'

]



upload_columns = [

    'Total UL (Bytes)',

    'Social Media UL (Bytes)',

    'Google UL (Bytes)',

    'Email UL (Bytes)',

    'Youtube UL (Bytes)',

    'Netflix UL (Bytes)',

    'Gaming UL (Bytes)',

    'Other UL (Bytes)'

]




download_data = df[download_columns].sum()

upload_data = df[upload_columns].sum()




plt.figure(figsize=(10, 6))

plt.bar(download_data.index.map(user_info), download_data.values, color='orange')

plt.title('Total Download Data for Different Categories')

plt.xlabel('Categories')

plt.ylabel('Download Data')

plt.xticks(rotation=45)

plt.tight_layout()

plt.show()




plt.figure(figsize=(10, 6))

plt.bar(upload_data.index.map(user_info), upload_data.values, color='green')

plt.title('Total Upload Data for Different Categories')

plt.xlabel('Categories')

plt.ylabel('Upload Data')

plt.xticks(rotation=45)

plt.tight_layout()

plt.show()

df['Decile Class'] = pd.qcut(df['Total Session Duration'], 10, labels=False)




df_top_five = df[df['Decile Class'] >= 5].copy()




df_top_five.loc[:, 'Total Data'] = df_top_five['Total Download Data'] + df_top_five['Total Upload Data']

total_data_per_decile = df_top_five.groupby('Decile Class')['Total Data'].sum()

pd.DataFrame(total_data_per_decile)

data_for_correlation = df[['Total DL (Bytes)', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',

                            'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)', 'Total UL (Bytes)', 'Social Media UL (Bytes)', 

                            'Google UL (Bytes)', 'Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']]






correlation_matrix = data_for_correlation.corr()



col_info = {

    'Total DL (Bytes)': 'Total Download Data',

    'Social Media DL (Bytes)': 'Social Media Download Data',

    'Google DL (Bytes)': 'Google Download Data',

    'Email DL (Bytes)': 'Email Download Data',

    'Youtube DL (Bytes)': 'Youtube Download Data',

    'Netflix DL (Bytes)': 'Netflix Download Data',

    'Gaming DL (Bytes)': 'Gaming Download Data',

    'Other DL (Bytes)': 'Other Download Data',

    

    'Total UL (Bytes)': 'Total Upload Data',

    'Social Media UL (Bytes)': 'Social Media Upload Data',

    'Google UL (Bytes)': 'Google Upload Data',

    'Email UL (Bytes)': 'Email Upload Data',

    'Youtube UL (Bytes)': 'Youtube Upload Data',

    'Netflix UL (Bytes)': 'Netflix Upload Data',

    'Gaming UL (Bytes)': 'Gaming Upload Data',

    'Other UL (Bytes)': 'Other Upload Data'

}



correlation_matrix = correlation_matrix.rename(columns=col_info, index=col_info)

correlation_matrix
plt.figure(figsize=(16, 12))

sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', linewidths=.05)

plt.title('Correlation Matrix')

plt.show()
import sys, os

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.metrics import silhouette_score

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from utils import DatabaseEngine
db_engine = DatabaseEngine()

engine = db_engine.create()

clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
data_df = pd.read_csv('../data/user_info.csv')

data_df.columns = ['MSISDN/Number_data' if col=='MSISDN/Number' else col for col in data_df.columns]
df = pd.concat([clean_df, data_df], axis=1)
df['Total Traffic'] = df['Total Download Data'] + df['Total Upload Data']

engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']



for metric in engagement_metrics:

    top_10_customers = df.nlargest(10, metric)[['MSISDN/Number', metric]]

    print(f"Top 10 customers for {metric}:")

    print(top_10_customers.to_string(index=False))

    print("\n")

def replace_with_mean(df, column):

    mean = df[column].mean()

    std = df[column].std()

    is_outlier = (df[column] - mean).abs() > 3*std

    df.loc[is_outlier, column] = mean

    return df
data_df['Total Traffic'] = data_df['Total Download Data'] + data_df['Total Upload Data']



engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']

for metric in engagement_metrics:

    data_df = replace_with_mean(data_df, metric)




scaler = StandardScaler()

normalized_data = scaler.fit_transform(data_df[engagement_metrics])




kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)

clusters = kmeans.fit_predict(normalized_data)

data_df['Engagement Cluster'] = clusters
data_df.head()
fig = plt.figure(figsize=(10, 7))

ax = fig.add_subplot(111, projection='3d')



scatter = ax.scatter(data_df['Number of Sessions'], 

                     data_df['Total Session Duration'], 

                     data_df['Total Traffic'], 

                     c=clusters, 

                     cmap='viridis')



ax.set_title('Customer Engagement Clusters')

ax.set_xlabel('Number of Sessions')

ax.set_ylabel('Total Session Duration')

ax.set_zlabel('Total Traffic')



cbar = plt.colorbar(scatter)

cbar.set_label('Cluster')



plt.show()
cluster_metrics = data_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])

pd.DataFrame(cluster_metrics)
cluster_metrics['Total Traffic'].plot(kind='bar', subplots=True, layout=(1,4), sharey=True, figsize=(20,5), title='Total Traffic Metrics by Cluster')

plt.yscale('log')

plt.show()
applications = {

    
    'Google': ['Google Download Data', 'Google Upload Data'],

    'Email': ['Email Download Data', 'Email Upload Data'],

    'Youtube': ['Youtube Download Data', 'Youtube Upload Data'],

    'Netflix': ['Netflix Download Data', 'Netflix Upload Data'],

    
    
}



for app, (dl, ul) in applications.items():

    data_df[app + ' Total Data'] = data_df[dl] + data_df[ul]




user_total_traffic = data_df.groupby('MSISDN/Number_data')[[app + ' Total Data' for app in applications]].sum()




for app in applications:

    top_users = user_total_traffic[app + ' Total Data'].nlargest(10)

    user_total_traffic.index = user_total_traffic.index.astype(int)

    print(f"Top 10 users for {app}:")

    print(pd.DataFrame(top_users).to_string())

    print("\n")
total_traffic_per_app = data_df[[app + ' Total Data' for app in applications]].sum()

top_3_apps = total_traffic_per_app.nlargest(3)



plt.figure(figsize=(10, 6))

plt.barh(top_3_apps.index, top_3_apps.values, color='skyblue')

plt.xlabel('Total Traffic')

plt.ylabel('Application')

plt.title('Top 3 Most Used Applications')

plt.gca().invert_yaxis()

plt.show()

engagement_metrics = data_df[['Number of Sessions', 'Total Session Duration', 'Total Traffic']]

scaler = StandardScaler()

engagement_metrics_scaled = scaler.fit_transform(engagement_metrics)




sse = []

for k in range(1, 11):

    kmeans = KMeans(n_clusters=k, n_init=10, random_state=1)

    kmeans.fit(engagement_metrics_scaled)

    sse.append(kmeans.inertia_)




plt.figure(figsize=(10, 6))

plt.plot(range(1, 11), sse, marker='o')

plt.xlabel('Number of clusters')

plt.ylabel('SSE')

plt.title('Elbow Method to Determine Optimal k')

plt.show()


kmeans = KMeans(n_clusters=2, n_init=10, random_state=1)

clusters = kmeans.fit_predict(engagement_metrics_scaled)

data_df['Elbow_Cluster'] = clusters

fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



for i in range(clusters.max()+1):

    cluster_data = data_df[data_df['Elbow_Cluster'] == i]

    ax.scatter(cluster_data['Number of Sessions'], 

               cluster_data['Total Session Duration'], 

               cluster_data['Total Traffic'], 

               label=f'Cluster {i}')



ax.set_xlabel('Number of Sessions')

ax.set_ylabel('Total Session Duration')

ax.set_zlabel('Total Traffic')

plt.legend()

plt.title('User Engagement Clusters')

plt.show()
import pandas as pd

import sys, os

import matplotlib.pyplot as plt



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from utils import DatabaseEngine

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.metrics import silhouette_score

from mpl_toolkits.mplot3d import Axes3D
db_engine = DatabaseEngine()

engine = db_engine.create()

clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
def process_user_info(clean_df):

    

    avg_tcp_retrans = clean_df.groupby('MSISDN/Number')[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].mean()



    avg_rtt = clean_df.groupby('MSISDN/Number')[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean()



    handset_type = clean_df.groupby('MSISDN/Number')['Handset Type'].agg(pd.Series.mode)



    avg_throughput = clean_df.groupby('MSISDN/Number')[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean()



    user_info = pd.concat([avg_tcp_retrans, avg_rtt, handset_type, avg_throughput], axis=1)

    

    return user_info



filtered_df = clean_df[clean_df['MSISDN/Number'] != 'Unknown']

user_info = process_user_info(filtered_df)

pd.DataFrame(user_info)

top10_tcp = user_info[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].sum(axis=1).nlargest(10)

bottom10_tcp = user_info[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].sum(axis=1).nsmallest(10)

most_frequent_tcp = user_info[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].sum(axis=1).value_counts().nlargest(10)
pd.DataFrame(top10_tcp, columns=['Total TCP Retransmission Volume (Bytes)'])
pd.DataFrame(bottom10_tcp, columns=['Total TCP Retransmission Volume (Bytes)'])
pd.DataFrame(most_frequent_tcp).rename_axis('Total TCP Retransmission Volume (Bytes)').rename(columns={0: 'Count'})

top10_rtt = user_info[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean(axis=1).nlargest(10)

bottom10_rtt = user_info[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean(axis=1).nsmallest(10)

most_frequent_rtt = user_info[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean(axis=1).value_counts().nlargest(10)
pd.DataFrame(top10_rtt, columns=['Total Average RTT (ms)'])
pd.DataFrame(bottom10_rtt, columns=['Total Average RTT (ms)'])
pd.DataFrame(most_frequent_rtt).rename_axis('Total Average RTT (ms)').rename(columns={0: 'Count'})

top10_throughput = user_info[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean(axis=1).nlargest(10)

bottom10_throughput = user_info[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean(axis=1).nsmallest(10)

most_frequent_throughput = user_info[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean(axis=1).value_counts().nlargest(10)
pd.DataFrame(top10_throughput, columns=['Total Average Throughput (kbps)'])
pd.DataFrame(bottom10_throughput, columns=['Total Average Throughput (kbps)'])
pd.DataFrame(most_frequent_throughput).rename_axis('Total Average Throughput (kbps)').rename(columns={0: 'Count'})
filtered_df = clean_df[clean_df['Handset Type'] != 'undefined']



avg_throughput_per_handset = filtered_df.groupby('Handset Type')[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean().mean(axis=1)



pd.DataFrame(avg_throughput_per_handset, columns = ['Average Throughput (kbps)'])
filtered_df = clean_df[clean_df['Handset Type'] != 'undefined']

avg_tcp_retrans_per_handset = filtered_df.groupby('Handset Type')[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].mean().mean(axis=1)

pd.DataFrame(avg_tcp_retrans_per_handset, columns = ['Average TCP Retransmission Volume (Bytes)'])

def replace_with_mean(df, column):

    mean = df[column].mean()

    std = df[column].std()

    is_outlier = (df[column] - mean).abs() > 3*std

    df.loc[is_outlier, column] = mean

    return df
clean_df['Total TCP'] = clean_df['TCP UL Retrans. Vol (Bytes)'] + clean_df['TCP DL Retrans. Vol (Bytes)']

clean_df['Total RTT'] = clean_df['Avg RTT DL (ms)'] + clean_df['Avg RTT UL (ms)']

clean_df['Total Throughput'] = clean_df['Avg Bearer TP DL (kbps)'] + clean_df['Avg Bearer TP UL (kbps)']



engagement_metrics = ['Total TCP', 'Total RTT', 'Total Throughput']

for metric in engagement_metrics:

    clean_df = replace_with_mean(clean_df, metric)




scaler = StandardScaler()

normalized_data = scaler.fit_transform(clean_df[engagement_metrics])




kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)

clusters = kmeans.fit_predict(normalized_data)

clean_df['Engagement Cluster'] = clusters
fig = plt.figure(figsize=(10, 7))

ax = fig.add_subplot(111, projection='3d')



scatter = ax.scatter(clean_df['Total TCP'], 

                     clean_df['Total RTT'], 

                     clean_df['Total Throughput'], 

                     c=clusters, 

                     cmap='viridis')



ax.set_title('Customer Experience Clusters')

ax.set_xlabel('Total TCP')

ax.set_ylabel('Total RTT')

ax.set_zlabel('Total Throughput')



cbar = plt.colorbar(scatter)

cbar.set_label('Cluster')



plt.show()
import pandas as pd

import sys, os

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.metrics.pairwise import euclidean_distances

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error

from scipy.stats.mstats import winsorize





rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from utils import DatabaseEngine

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.metrics import silhouette_score

from mpl_toolkits.mplot3d import Axes3D

from scipy.spatial import distance

import warnings




warnings.filterwarnings('ignore', message="Warning: 'partition' will ignore the 'mask' of the MaskedArray.")
db_engine = DatabaseEngine()

engine = db_engine.create()

clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
data_df = pd.read_csv('../data/user_info.csv')
df = pd.concat([clean_df, data_df], axis=1)
df['Total Traffic'] = df['Total Download Data'] + df['Total Upload Data']

engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']

def replace_with_mean(df, column):

    mean = df[column].mean()

    std = df[column].std()

    is_outlier = (df[column] - mean).abs() > 3*std

    df.loc[is_outlier, column] = mean

    return df
data_df['Total Traffic'] = data_df['Total Download Data'] + data_df['Total Upload Data']



engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']

for metric in engagement_metrics:

    data_df = replace_with_mean(data_df, metric)



scaler = StandardScaler()

normalized_data = scaler.fit_transform(data_df[engagement_metrics])



kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)

clusters = kmeans.fit_predict(normalized_data)



data_df['Engagement Cluster'] = clusters
avg_engagement = data_df.groupby('Engagement Cluster')[engagement_metrics].mean()



less_engaged_cluster = avg_engagement.mean(axis=1).idxmin()



centroid = kmeans.cluster_centers_[less_engaged_cluster]



data_df['Engagement Score'] = euclidean_distances(normalized_data, centroid.reshape(1, -1)).flatten()
data_df['Engagement Score'].describe()
clean_df['Total TCP'] = clean_df['TCP UL Retrans. Vol (Bytes)'] + clean_df['TCP DL Retrans. Vol (Bytes)']

clean_df['Total RTT'] = clean_df['Avg RTT DL (ms)'] + clean_df['Avg RTT UL (ms)']

clean_df['Total Throughput'] = clean_df['Avg Bearer TP DL (kbps)'] + clean_df['Avg Bearer TP UL (kbps)']



exp_engagement_metrics = ['Total TCP', 'Total RTT', 'Total Throughput']

for metric in exp_engagement_metrics:

    clean_df = replace_with_mean(clean_df, metric)



scaler = StandardScaler()

normalized_data = scaler.fit_transform(clean_df[exp_engagement_metrics])



kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)

exp_clusters = kmeans.fit_predict(normalized_data)



clean_df['Experiance Cluster'] = exp_clusters
exp_avg_engagement = clean_df.groupby('Experiance Cluster')[exp_engagement_metrics].mean()



exp_less_engaged_cluster = exp_avg_engagement.mean(axis=1).idxmin()



exp_centroid = kmeans.cluster_centers_[exp_less_engaged_cluster]



clean_df['Experiance Score'] = euclidean_distances(normalized_data, centroid.reshape(1, -1)).flatten()
clean_df['Experiance Score'].describe()
data_df['MSISDN/Number'] = data_df['MSISDN/Number'].astype(str)

clean_df['MSISDN/Number'] = clean_df['MSISDN/Number'].astype(str)



merged_df = pd.merge(data_df, clean_df, on='MSISDN/Number')



merged_df['Satisfaction Score'] = (merged_df['Experiance Score'] + merged_df['Engagement Score']) / 2



top_10_satisfied_users = merged_df.sort_values('Satisfaction Score', ascending=False).head(10)
merged_df['Satisfaction Score'].describe()
top_10_satisfied_customers = merged_df.sort_values('Satisfaction Score', ascending=False).head(10)



top_10_satisfied_customers = top_10_satisfied_customers[['MSISDN/Number', 'Satisfaction Score']]



print(top_10_satisfied_customers.to_string(index=False))
percentiles = merged_df['Satisfaction Score'].describe(percentiles=[0.25, 0.75])




bins = [percentiles['min'], percentiles['25%'], percentiles['75%'], percentiles['max']]




labels = ['low satisfaction', 'moderate satisfaction', 'high satisfaction']




merged_df['Satisfaction Category'] = pd.cut(merged_df['Satisfaction Score'], bins=bins, labels=labels, include_lowest=True)




category_counts = merged_df['Satisfaction Category'].value_counts()

print(category_counts)
category_percentages = category_counts / category_counts.sum() * 100



plt.figure(figsize=(10, 6))

plt.pie(category_percentages, labels=category_percentages.index, autopct='%1.1f%%', startangle=140)

plt.axis('equal')

plt.title('Satisfaction Score Categories')

plt.show()
X = merged_df[['Engagement Score', 'Experiance Score']]

y = merged_df['Satisfaction Score']




X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)




model = LinearRegression()




model.fit(X_train, y_train)




y_pred = model.predict(X_test)

new_data = {'Engagement Score': [1.5], 'Experiance Score': [1.2]}

new_df = pd.DataFrame(new_data)




new_pred = model.predict(new_df)



print(f'The predicted satisfaction score for the new customer is: {new_pred[0]}')
X = merged_df[['Engagement Score', 'Experiance Score']]

kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)

kmeans.fit(X)

merged_df['Cluster'] = kmeans.labels_
average_scores = merged_df.groupby('Cluster')[['Satisfaction Score', 'Experiance Score']].mean()



print(average_scores)
merged_df.to_sql('user_satisfaction', engine, if_exists='replace', index=False)
query = "SELECT * FROM user_satisfaction LIMIT 5;"

query_df = pd.read_sql_query(query, engine)

query_df
Repository Structure: '
' ├── .flake8
├── notebooks
│   └── telecom_analysis.ipynb
├── .pre-commit-config.yaml
├── src
│   ├── plot_utils.py
│   ├── loader.py
│   ├── __init__.py
│   ├── db_utils.py
│   ├── data_utils.py
│   └── utils.py
├── .gitignore
├── .github
│   └── workflows
├── Makefile
├── tests
│   └── __init__.py
├── README.md
├── .vscode
├── view_tree.py
├── docs
│   └── dbschema.md
├── style_guide.md
├── setup.cfg
├── pyproject.toml
└── requirements.txt
 '
'  

Commit History: 
{"insertions": [1046, 641, 2179], "deletions": [4, 1, 230], "lines": [1050, 642, 2409], "committed_datetime": ["2023-12-13 22:57:01", "2023-12-13 23:40:30", "2023-12-17 21:16:11"], "commit_count": 3} 
 Content: 
%reload_ext autoreload

%autoreload 2
import os, sys

import pprint


rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from src.db_utils import engine

import pandas as pd

import matplotlib.pylab as plt

import seaborn as sns

import src.data_utils as data_utils

import src.plot_utils as plot_utils

plt.style.use('ggplot')

pd.set_option('display.max_columns',200)

sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query,engine)

df.head()
df.shape
df.dtypes
df.describe()

df.isna().sum()

data_utils.percent_missing(df)
df.dtypes

df['Start'] =pd.to_datetime(df['Start'])

df['End'] =pd.to_datetime(df['End'])

df.columns = df.columns.str.replace(' ', '_')

df.columns

df = df[['Bearer_Id', 'Start', 

        
         'End', 'Dur._(ms)', 

       'MSISDN/Number',   'Avg_RTT_DL_(ms)',

       'Avg_RTT_UL_(ms)', 'Avg_Bearer_TP_DL_(kbps)', 'Avg_Bearer_TP_UL_(kbps)',

       'TCP_DL_Retrans._Vol_(Bytes)', 'TCP_UL_Retrans._Vol_(Bytes)',

       'DL_TP_<_50_Kbps_(%)', '50_Kbps_<_DL_TP_<_250_Kbps_(%)',

       '250_Kbps_<_DL_TP_<_1_Mbps_(%)', 'DL_TP_>_1_Mbps_(%)',

       'UL_TP_<_10_Kbps_(%)', '10_Kbps_<_UL_TP_<_50_Kbps_(%)',

       '50_Kbps_<_UL_TP_<_300_Kbps_(%)', 'UL_TP_>_300_Kbps_(%)',

       'HTTP_DL_(Bytes)', 'HTTP_UL_(Bytes)', 'Activity_Duration_DL_(ms)',

       'Activity_Duration_UL_(ms)', 'Dur._(ms).1', 'Handset_Manufacturer',

       'Handset_Type', 'Nb_of_sec_with_125000B_<_Vol_DL',

       'Nb_of_sec_with_1250B_<_Vol_UL_<_6250B',

       'Nb_of_sec_with_31250B_<_Vol_DL_<_125000B',

       'Nb_of_sec_with_37500B_<_Vol_UL',

       'Nb_of_sec_with_6250B_<_Vol_DL_<_31250B',

       'Nb_of_sec_with_6250B_<_Vol_UL_<_37500B',

       'Nb_of_sec_with_Vol_DL_<_6250B', 'Nb_of_sec_with_Vol_UL_<_1250B',

       'Social_Media_DL_(Bytes)', 'Social_Media_UL_(Bytes)',

       'Google_DL_(Bytes)', 'Google_UL_(Bytes)', 'Email_DL_(Bytes)',

       'Email_UL_(Bytes)', 'Youtube_DL_(Bytes)', 'Youtube_UL_(Bytes)',

       'Netflix_DL_(Bytes)', 'Netflix_UL_(Bytes)', 'Gaming_DL_(Bytes)',

       'Gaming_UL_(Bytes)', 'Other_DL_(Bytes)', 'Other_UL_(Bytes)',

       'Total_UL_(Bytes)', 'Total_DL_(Bytes)']]



top_10_handsets = df['Handset_Type'].value_counts().head(10)

ax = top_10_handsets.plot(kind='barh', title='Top 10 handsets')



ax.set_xlabel('Number of Handsets'); ax.set_ylabel('Handset Type')
top_3_handsets_manufacturers = df['Handset_Manufacturer'].value_counts().head(3)

ax = top_3_handsets_manufacturers.plot(kind='bar', title='Top 3 handset manufacturers')



ax.set_xlabel('Handset Manufacturer'); ax.set_ylabel('Number of Handsets')
df['Handset_Manufacturer']

filtered_df = df[df['Handset_Manufacturer'].isin(top_3_handsets_manufacturers.index.tolist())]




ax = filtered_df.groupby('Handset_Manufacturer')['Handset_Type']\

    .value_counts()\

    .groupby(level=0, group_keys=False)\

    .nlargest(5) \

    .plot(kind='barh', figsize=(12,5), title='Top 5 handsets for each manufacturer')



ax.set_xlabel('Number of Handsets')
df.head()






user_summary = df.groupby('MSISDN/Number').agg({

    'Bearer_Id': 'count',  
    'Dur._(ms)': 'sum',   
    'Total_UL_(Bytes)': 'sum',  
    'Total_DL_(Bytes)': 'sum',  
    'Social_Media_DL_(Bytes)': 'sum',  
    'Social_Media_UL_(Bytes)': 'sum',  
    
})



user_summary
sns.pairplot(data=user_summary,

             vars=['Total_DL_(Bytes)','Total_UL_(Bytes)'], 

             )
df.dtypes
print(df.shape)


cols_to_be_dropped= []

for col in df.columns:

    missing_percentage  = data_utils.percent_missing(df[col])

    if(missing_percentage > 30):

        print(f'{col} has => {missing_percentage} % missing')

        cols_to_be_dropped.append(col)

df_cleaned = df.drop(cols_to_be_dropped,axis=1)        

print(df_cleaned.shape)
df_cleaned.dtypes
df_cleaned.head(10)
df_cleaned.columns

df_cleaned['MSISDN/Number'].fillna(df_cleaned['MSISDN/Number'].mode()[0],inplace=True)

df_cleaned['Handset_Type'].fillna(df_cleaned['Handset_Type'].mode()[0],inplace=True)

df_cleaned['Handset_Manufacturer'].fillna(df_cleaned['Handset_Manufacturer'].mode()[0],inplace=True)

df_cleaned.isna().sum().sort_values(ascending=False)

df_cleaned['Avg_RTT_UL_(ms)'].fillna(df_cleaned['Avg_RTT_UL_(ms)'].median(),inplace=True)

df_cleaned['Avg_RTT_DL_(ms)'].fillna(df_cleaned['Avg_RTT_DL_(ms)'].median(),inplace=True)

df_cleaned['Nb_of_sec_with_Vol_UL_<_1250B'].fillna(df_cleaned['Nb_of_sec_with_Vol_UL_<_1250B'].median(),inplace=True)

df_cleaned['UL_TP_>_300_Kbps_(%)'].fillna(df_cleaned['UL_TP_>_300_Kbps_(%)'].median(),inplace=True)

df_cleaned.isna().sum().sort_values(ascending=False)
df_cleaned.describe()
plot_utils.plot_hist(df_cleaned, 'Avg_Bearer_TP_DL_(kbps)', 'blue')
df_cleaned['Avg_Bearer_TP_DL_(kbps)'].describe() 

 

import pandas as pd
import numpy as np
from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler


class Cleaner:
    def __init__(self):
        pass

    def drop_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        """
        drop columns
        """
        return df.drop(columns=columns)
    
    def drop_nan(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna()
    
    def drop_nan_column(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna(subset=[col])  
    
    def drop_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop duplicate rows
        """
        return df.drop_duplicates()
    
    def convert_to_datetime(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
        """
        convert column to datetime
        """
        df[col] = df[col].apply(pd.to_datetime)
        return df
    
    def convert_to_string(self, df: pd.DataFrame, col = list) -> pd.DataFrame:
        """
        convert columns to string
        """
        df[col] = df[col].astype(str)
        return df
    
    def remove_whitespace_column(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        remove whitespace from columns
        """
        return df.columns.str.replace(' ', '_').str.lower()
       
    def percent_missing(self, df: pd.DataFrame) -> float:
        """
        calculate the percentage of missing values from dataframe
        """
        totalCells = np.product(df.shape)
        missingCount = df.isnull().sum()
        totalMising = missingCount.sum()
        
        return round(totalMising / totalCells * 100, 2)
    
    def percent_missing_column(self, df: pd.DataFrame, col:str) -> float:
        """
        calculate the percentage of missing values for the specified column
        """
        try:
            col_len = len(df[col])
        except KeyError:
            print(f"{col} not found")
        missing_count = df[col].isnull().sum()
        
        return round(missing_count / col_len * 100, 2)
    
    def get_numerical_columns(self, df: pd.DataFrame) -> list:
        """
        get numerical columns
        """
        return df.select_dtypes(include=['number']).columns.to_list()
    
    def get_categorical_columns(self, df: pd.DataFrame) -> list:    
        """
        get categorical columns
        """
        return  df.select_dtypes(include=['object','datetime64[ns]']).columns.to_list()
    
    
    
    def fill_missing_values_categorical(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        
        categorical_columns = df.select_dtypes(include=['object','datetime64[ns]']).columns
        
        if method == "ffill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='ffill')
                
            return df
        
        elif method == "bfill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='bfill')
                
            return df
        
        elif method == "mode":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(df[col].mode()[0])
                
            return df
        else:
            print("Method unknown")
            return df
    
    def fill_missing_values_numeric(self, df: pd.DataFrame, method: str,columns: list =None) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        if(columns==None):
            numeric_columns = self.get_numerical_columns(df)
        else:
            numeric_columns=columns
        
        if method == "mean":
            for col in numeric_columns:
                df[col].fillna(df[col].mean(), inplace=True)
                
        elif method == "median":
            for col in numeric_columns:
                df[col].fillna(df[col].median(), inplace=True)
        else:
            print("Method unknown")
        
        return df
    
    def normalizer(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        normalize numerical columns
        """
        norm = Normalizer()
        return pd.DataFrame(norm.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def min_max_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        minmax_scaler = MinMaxScaler()
        return pd.DataFrame(minmax_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def standard_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        standard_scaler = StandardScaler()
        return pd.DataFrame(standard_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def handle_outliers(self, df:pd.DataFrame, col:str, method:str ='IQR') -> pd.DataFrame:
        """
        Handle Outliers of a specified column using Turkey's IQR method
        """
        df = df.copy()
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        
        lower_bound = q1 - ((1.5) * (q3 - q1))
        upper_bound = q3 + ((1.5) * (q3 - q1))
        if method == 'mode':
            df[col] = np.where(df[col] < lower_bound, df[col].mode()[0], df[col])
            df[col] = np.where(df[col] > upper_bound, df[col].mode()[0], df[col])
        
        elif method == 'median':
            df[col] = np.where(df[col] < lower_bound, df[col].median, df[col])
            df[col] = np.where(df[col] > upper_bound, df[col].median, df[col])
        else:
            df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
            df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
        
        return df
    
    def find_agg(self,df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
        
        new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
                            sort_values(by=col_name, ascending=order)[:top]
        
        return new_df

    def convert_bytes_to_megabytes(self,df, bytes_data):

        """
            This function takes the dataframe and the column which has the bytes values
            returns the megabytesof that value
            
            Args:
            -----
            df: dataframe
            bytes_data: column with bytes values
            
            Returns:
            --------
            A series
        """
        
        megabyte = 1*10e+5
        df[bytes_data] = df[bytes_data] / megabyte
        
        return df[bytes_data]
%reload_ext autoreload

%autoreload 2
import pandas as pd

import sys

sys.path.append('../script')

from data_extraction import pgdatabase

from utils import Cleaner

clean = Cleaner()

from plots import plots

plt = plots()

db = pgdatabase()

conn = db.connection()


data = db.get_dataframe_sql(conn, 'xdr_data')
data.shape

data.head()

data.info()
data.describe()
data.isnull().sum()

clean.missing_values_table(data)


data['TCP UL Retrans. Vol (Bytes)'].describe()
data['TCP DL Retrans. Vol (Bytes)'].describe()
data['HTTP UL (Bytes)'].describe()
data['HTTP DL (Bytes)'].describe()
percent = clean.percent_missing(data)

print(f'The data has {percent}% missing values')

col = ['Start ms', 'End ms']

data1 = clean.convert_to_datetime(data, col)

data.skew()















col = ['Nb of sec with 37500B < Vol UL',

    'Nb of sec with 6250B < Vol UL < 37500B',

    'Nb of sec with 1250B < Vol UL < 6250B',

    'Nb of sec with 125000B < Vol DL',

    'Nb of sec with 31250B < Vol DL < 125000B',

    'Nb of sec with 6250B < Vol DL < 31250B']



data2 = clean.impute_zero(data1, col)



col1 = ['TCP DL Retrans. Vol (Bytes)',

        'TCP UL Retrans. Vol (Bytes)',

        'Avg RTT DL (ms)',

        'Avg RTT UL (ms)',

        'HTTP DL (Bytes)',

        'HTTP UL (Bytes)']



data2 = clean.fill_missing_values_numeric(data1, 'mean', col1)


data2['Handset Type'] = data2['Handset Type'].fillna('unknown')

data2['Handset Manufacturer'] = data2['Handset Manufacturer'].fillna('unknown')
categorical_columns = data.select_dtypes(include=['object','datetime64[ns]']).columns



for col in categorical_columns:

                data2[col] = data2[col].fillna(data2[col].mode()[0])

cleand_data = clean.fill_missing_values_numeric(data2, 'median')
clean.missing_values_table(cleand_data)

plt.plot_box(cleand_data, 'Activity Duration UL (ms)', 'Active Duration UL Outliers')

plt.plot_box(cleand_data, 'Activity Duration DL (ms)', 'Active Duration DL Outliers')
numeric_columns = clean.get_numerical_columns(cleand_data)

data5 = data[numeric_columns]
indices = clean.detect_outliers(data5,6)

print(len(indices))
telecom_data = clean.handle_outliers(data5, indices, 'mean')
%reload_ext autoreload

%autoreload 2
from urllib.parse import quote_plus

from sqlalchemy import create_engine



password = 'kerod53@'




encoded_password = quote_plus(password)




db_string = f'postgresql://postgres:{encoded_password}@localhost:5432/Telecom'




engine = create_engine(db_string)
table_name = 'telecom_data'

 

cleand_data.to_sql(table_name, engine, index=False, if_exists='replace')
import pandas as pd
import numpy as np
from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler


class Cleaner:
    def __init__(self):
        pass

    def drop_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        """
        drop columns
        """
        return df.drop(columns=columns)
    
    def drop_nan(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna()
    
    def drop_nan_column(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna(subset=[col])  
    
    def drop_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop duplicate rows
        """
        return df.drop_duplicates()
    
    def convert_to_datetime(self, df: pd.DataFrame, col: list) -> pd.DataFrame:
        """
        convert column to datetime
        """
        df[col] = df[col].apply(pd.to_datetime)
        return df
    
    def convert_to_string(self, df: pd.DataFrame, col = list) -> pd.DataFrame:
        """
        convert columns to string
        """
        df[col] = df[col].astype(str)
        return df
    
    def remove_whitespace_column(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        remove whitespace from columns
        """
        return df.columns.str.replace(' ', '_').str.lower()
       
    def percent_missing(self, df: pd.DataFrame) -> float:
        """
        calculate the percentage of missing values from dataframe
        """
        totalCells = np.product(df.shape)
        missingCount = df.isnull().sum()
        totalMising = missingCount.sum()
        
        return round(totalMising / totalCells * 100, 2)
     
    def get_numerical_columns(self, df: pd.DataFrame) -> list:
        """
        get numerical columns
        """
        return df.select_dtypes(include=['float64']).columns.to_list()
    
    def get_categorical_columns(self, df: pd.DataFrame) -> list:    
        """
        get categorical columns
        """
        return  df.select_dtypes(include=['object','datetime64[ns]']).columns.to_list()
    
    def impute_zero(self, df: pd.DataFrame, column: list) -> pd.DataFrame:
        """
        imputes 0 inplace of NaN for a given columon(s)
        """
        df[column] = df[column].fillna(0)
        
        return df 

    def fill_missing_values_categorical(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        
        categorical_columns = df.select_dtypes(include=['object','datetime64[ns]']).columns
        
        if method == "ffill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='ffill')
                
            return df
        
        elif method == "bfill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='bfill')
                
            return df
        
        elif method == "mode":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(df[col].mode()[0])
                
            return df
        else:
            print("Method unknown")
            return df
    
    def fill_missing_values_numeric(self, df: pd.DataFrame, method: str,columns: list = None) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        if(columns==None):
            numeric_columns = df.select_dtypes(include=['float64','int64']).columns
        else:
            numeric_columns=columns
        
        if method == "mean":
            for col in numeric_columns:
                df[col].fillna(df[col].mean(), inplace=True)
                
        elif method == "median":
            for col in numeric_columns:
                df[col].fillna(df[col].median(), inplace=True)
        else:
            print("Method unknown")
        
        return df
    
    def normalizer(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        normalize numerical columns
        """
        norm = Normalizer()
        return pd.DataFrame(norm.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def min_max_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        minmax_scaler = MinMaxScaler()
        return pd.DataFrame(minmax_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def standard_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        standard_scaler = StandardScaler()
        return pd.DataFrame(standard_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def detect_outliers(self, df:pd.DataFrame, threshold: int) -> list:
        """
        detect the indices of outliers using Z-method 
        """
        z_scores = df.apply(lambda x: np.abs((x - x.mean()) / x.std()))
        tr = threshold
        outliers = np.where(z_scores > tr)
        outlier_indices = [(df.index[i], df.columns[j]) for i, j in zip(*outliers)]
        return outlier_indices
        

    def handle_outliers(self, df:pd.DataFrame, indices:list, method:str) -> pd.DataFrame:
        """
        Handle Outliers of a specified column using the Z method
        """
        if method == 'mean':
            for idx, col_name in indices:
                column_mean = df[col_name].mean()
                df.iloc[idx, df.columns.get_loc(col_name)] = column_mean
        
        elif method == 'mode':
            for idx, col_name in indices:
                column_mode = df[col_name].mode()
                df.iloc[idx, df.columns.get_loc(col_name)] = column_mode

        elif method == 'median':
            for idx, col_name in indices:
                column_median = df[col_name].median()
                df.loc[idx, col_name] = column_median
        else:
            print("Method unknown")
    
        return df
    
    def find_agg(self,df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
        
        new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
                            sort_values(by=col_name, ascending=order)[:top]
        
        return new_df

    def convert_bytes_to_megabytes(self,df, bytes_data):

        """
            This function takes the dataframe and the column which has the bytes values
            returns the megabytesof that value
            
            Args:
            -----
            df: dataframe
            bytes_data: column with bytes values
            
            Returns:
            --------
            A series
        """
        
        megabyte = 1*10e+5
        df[bytes_data] = df[bytes_data] / megabyte
        
        return df[bytes_data]
    
    def missing_values_table(self,df):
                mis_val = df.isnull().sum()

                mis_val_percent = 100 * df.isnull().sum() / len(df)

                mis_val_dtype = df.dtypes

                mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)

                mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})

                mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

                print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
            " columns that have missing values.")

                return mis_val_table_ren_columns
import sys

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

sys.path.append('../script')

from utils import Cleaner

from plots import *

from data_extraction import *



clean = Cleaner()

%reload_ext autoreload

%autoreload 2

conn = connection()

data = table_to_sql(conn, "telecom_data")
data.head()
data.info()
top_10 = data["Handset Type"].value_counts()[:10]

plt.figure(figsize= (10,5))

top_10.plot(kind="bar",title="top 10 handset types")

plt.show()
top_3 = data["Handset Manufacturer"].value_counts()[:3]

plt.figure(figsize=(10,5))

top_3.plot(kind="bar",title="top 3 handset manufacturers")

plt.show()
handsets_per_manufacturers=data[data["Handset Manufacturer"].str.contains("Apple|Samsung|Huawei") ][["Handset Manufacturer","Handset Type"]]

plt.figure(figsize=(10,5))

top_five_apple_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Apple')["Handset Type"].value_counts()[:5]

handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Apple')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Apple")

plt.show()
plt.figure(figsize=(10,5))

top_five_samsung_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Samsung')["Handset Type"].value_counts()[:5]

handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Samsung')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Samsung")

plt.show()
plt.figure(figsize=(10,5))

top_five_huawei_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Huawei')["Handset Type"].value_counts()[:5]

handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Huawei')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Huawei")

plt.show()

session=data.groupby(["MSISDN/Number"]).agg({"Bearer Id":"count"})

session = session.rename(columns={"Bearer Id":"XDR_sessions"})

session = session.sort_values(by=["XDR_sessions"],ascending=False)



session.head(10)

duration = data.groupby(["MSISDN/Number"]).agg({"Dur. (ms)":"sum"})

duration.sort_values(by=["Dur. (ms)"],ascending=False,inplace=True)

duration=duration.rename(columns={"Dur. (ms)":"total_duration(ms) "})

duration.head(10)

total_data = data.groupby(["MSISDN/Number"]).agg({"Total UL (Bytes)":"sum","Total DL (Bytes)":"sum"})

total_data["total_data(bytes)"]=total_data["Total UL (Bytes)"]+total_data["Total DL (Bytes)"]

total_data.sort_values(by=["total_data(bytes)"],ascending=False,inplace=True)

total_data.head(10)




data['social_media'] = data['Social Media DL (Bytes)'] + data['Social Media UL (Bytes)']

data['google'] = data['Google DL (Bytes)'] + data['Google UL (Bytes)']

data['email'] = data['Email DL (Bytes)'] + data['Email UL (Bytes)']

data['youtube'] = data['Youtube DL (Bytes)'] + data['Youtube UL (Bytes)']

data['netflix'] = data['Netflix DL (Bytes)'] + data['Netflix UL (Bytes)']

data['gaming'] = data['Gaming DL (Bytes)'] + data['Gaming UL (Bytes)']

data['other'] = data['Other DL (Bytes)'] + data['Other UL (Bytes)']

data['total_data'] = data['Total UL (Bytes)'] + data['Total DL (Bytes)']
app_total_data = data[['MSISDN/Number','social_media','google', 'email', 'youtube','netflix', 'gaming','other']].copy()

app_total_data.groupby('MSISDN/Number').sum().sample(10)






variables = data[['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'total_data', 'Total DL (Bytes)', 'social_media', 'google', 'email', 'youtube', 'netflix', 'gaming', 'other']].copy()

variables.groupby('MSISDN/Number').sum().sample(10)


non_graphical_univariate = variables.drop('MSISDN/Number', axis=1).describe()

non_graphical_univariate.transpose()

plot_hist(variables,'email','cornflowerblue')

plot_hist(variables,'gaming','cornflowerblue')

plot_hist(variables,'google','cornflowerblue')

plot_hist(variables,'netflix','cornflowerblue')

plot_hist(variables,'other','cornflowerblue')

plot_hist(variables,'social_media','cornflowerblue')

plot_hist(variables,'total_data','cornflowerblue')

plot_hist(variables,'youtube','cornflowerblue')

plot_hist(variables,'Total DL (Bytes)','cornflowerblue')

plot_hist(variables,'Total UL (Bytes)','cornflowerblue')

plot_hist(variables,'Dur. (ms)','cornflowerblue')


agg_data = variables.groupby('MSISDN/Number').sum()

agg_data.head(10)
plot_scatter(agg_data, 'social_media', 'total_data', 'social media data vs total data',  None,  None)

import seaborn as sns

plt.figure(figsize=(15,12))

plt.subplot(2,3,1,title="social media data vs total_data data")

sns.scatterplot(data=agg_data,x="social_media",y="total_data",hue=None,style=None)



plt.subplot(2,3,2,title="email data vs total_data data")

sns.scatterplot(data=agg_data,x="email",y="total_data",hue=None,style=None)



plt.subplot(2,3,3,title="gaming data vs total_data data")

sns.scatterplot(data=agg_data,x="gaming",y="total_data",hue=None,style=None)



plt.subplot(2,3,4,title="google data vs total_data data")

sns.scatterplot(data=agg_data,x="google",y="total_data",hue=None)



plt.subplot(2,3,5,title="netflix data vs total_data data")

sns.scatterplot(data=agg_data,x="netflix",y="total_data",hue="netflix")



plt.subplot(2,3,6,title="youtube data vs total_data data")

sns.scatterplot(data=variables,x="youtube",y="total_data",hue="youtube")



plt.show()


from sklearn.preprocessing import MinMaxScaler





scaled_explore_feature_df = variables[['MSISDN/Number', 'total_data', 'Dur. (ms)']]



scaled_explore_feature_df['Dur. (ms)'] = variables['Dur. (ms)'] /1000



scaled_explore_feature_df = scaled_explore_feature_df.rename(columns={'Dur. (ms)': 'duration'})



scaled_explore_feature_df_agg = scaled_explore_feature_df.groupby('MSISDN/Number').agg({'duration':'sum', 'total_data': 'sum'})



deciles = pd.qcut(scaled_explore_feature_df_agg['duration'], 5, labels=["1st_decile", "2nd_decile",

                                                      "3rd_decile", "4th_decile",

                                                      "5th_decile"])







explore_feature_df_with_decile = scaled_explore_feature_df_agg.copy()



explore_feature_df_with_decile['decile'] = deciles



explore_feature_df_with_decile_agg = explore_feature_df_with_decile.groupby('decile').agg({'total_data': 'sum',

                                                                                           'duration': 'sum'})

explore_feature_df_with_decile_agg
feature2 = variables.drop(['MSISDN/Number', 'Dur. (ms)'], axis=1)

feature2.corr(method='pearson')
plt.figure(figsize=(12,5))

sns.heatmap(feature2.corr(),cmap="YlGnBu")

plt.title("Application Data Correlation")

plt.show()
data_reduction = variables.drop(['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)'], axis=1)

data_reduction.head()

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA




scaler = StandardScaler()

scaled_explore_feature_df = scaler.fit_transform(data_reduction)




pca = PCA(n_components=2)

pca.fit(scaled_explore_feature_df)

pca_df = pd.DataFrame(pca.transform(scaled_explore_feature_df))




plt.figure(figsize=(8, 6))

plt.scatter(pca_df[0], pca_df[1])

plt.xlabel('PCA 1')

plt.ylabel('PCA 2')

plt.title('PCA')

plt.show()
pca_df.head()
%reload_ext autoreload

%autoreload 2
import pandas as pd
from sqlalchemy import create_engine
import psycopg2


def connection():
    """
    connects to my pg database
    """
    conn = psycopg2.connect(dbname = 'Telecom',
                            user = 'postgres',
                            password = 'kerod53@',
                            host = 'localhost',
                            port = '5432')
    return conn

def table_to_sql(conn, table_name:str) -> pd.DataFrame:
    query = f'SELECT * FROM public.{table_name}'
    data = pd.read_sql_query(query, conn)

    return data
my_parameters = {'dbname': 'Telecom',
                'user': 'postgres',
                'password':'kerod53@',
                'host':'localhost',
                'port': '5432'}

def connection2(self, parameters: dict = my_parameters):
    """
    Connects to the PostgreSQL database. for a given paramters:
    connection_params is a dictionary that define the following:
    {
        'dbname': 'your_database_name',
        'user': 'your_username',
        'password': 'your_password',
        'host': 'your_host',
        'port': 'your_port'
        }
    """
    try:
        conn = psycopg2.connect(**parameters)
        return conn
    except psycopg2.Error as e:
        print(f"Error: Unable to connect to the database. {e}")
        return None

def get_dataframe_sql(self, conn, table_name) -> pd.DataFrame:
    query = f'SELECT * FROM public.{table_name}'
    data = pd.read_sql_query(query, conn)
    return data

def create_engine(self, connection_params: dict = my_parameters):
    """
    creates engine using sqlalchemy for a given paramters:
    """
    engine = create_engine("postgresql://postgres:kerod53@@localhost:5432/Telecom")
    return engine

def write_dataframe_to_table(self, df: pd.DataFrame, table_name: str,engine)->None:
    """
    Writes a pandas dataframe to a new table in the PostgreSQL database.
    """
    df.to_sql(table_name, engine, index=False, if_exists='replace')
    print(f"Dataframe successfully written to the '{table_name}' table.")

def update_table_by_appending(df, table_name, connection_params = my_parameters):
    """
    Appends a pandas dataframe to an existing PostgreSQL table.
    """
    engine = create_engine(f"postgresql://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['dbname']}")
    df.to_sql(table_name, engine, index=False, if_exists='append')
    print(f"Dataframe successfully appended to the '{table_name}' table.")

def delete_table(table_name, connection_params = my_parameters):
    """
    Deletes a table from the PostgreSQL database.
    """
    connection = connect_to_database(connection_params)
    if connection:
        cursor = connection.cursor()
        cursor.execute(f"DROP TABLE IF EXISTS {table_name};")
        connection.commit()
        connection.close()
        print(f"Table '{table_name}' successfully deleted.")
    else:
        print("Error: Unable to connect to the database.")

def connect(self):
    conn = psycopg2.connect(dbname = 'Telecom',
                            user = 'postgres',
                            password = 'kerod53@',
                            host = 'localhost',
                            port = '5432')
    return conn

def get_data_sql(self, conn, table_name):
    query = f'SELECT * FROM public.{table_name}'
    data = pd.read_sql_query(query, conn)
    return data
import imp
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def plot_hist(df:pd.DataFrame, column:str, color:str)->None:
    plt.figure(figsize=(12, 7))
    sns.histplot(data=df, x=column, color=color, kde=True)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()

def plot_count(self,df:pd.DataFrame, column:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.countplot(data=df, x=column)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()
    
def plot_bar(self,df:pd.DataFrame, x_col:str, y_col:str, title:str, xlabel:str, ylabel:str)->None:
    plt.figure(figsize=(12, 7))
    sns.barplot(data = df, x=x_col, y=y_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.yticks( fontsize=14)
    plt.xlabel(xlabel, fontsize=16)
    plt.ylabel(ylabel, fontsize=16)
    plt.show()

def plot_heatmap(self,df:pd.DataFrame, title:str, cbar=False)->None:
    plt.figure(figsize=(12, 7))
    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )
    plt.title(title, size=18, fontweight='bold')
    plt.show()

def plot_box(self,df:pd.DataFrame, x_col:str, title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.boxplot(data = df, x=x_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.show()

def plot_box_multi(self,df:pd.DataFrame, x_col:str, y_col:str, title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.boxplot(data = df, x=x_col, y=y_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()

def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:
    plt.figure(figsize=(12, 7))
    sns.scatterplot(data = df, x=x_col, y=y_col, hue=hue, style=style)
    plt.title(title, size=20)
    plt.xticks(fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()
import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

import numpy as np

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from sklearn import preprocessing

from sklearn.cluster import KMeans

from sklearn.preprocessing import MinMaxScaler

from sklearn.preprocessing import Normalizer

import sys

sys.path.append('../script')

from plots import *

from data_extraction import *

conn = connection()
data = table_to_sql(conn, 'telecom_data')
data.head()

experiance = data[['Bearer Id', 'MSISDN/Number', 'Avg RTT DL (ms)','Avg RTT UL (ms)','TCP DL Retrans. Vol (Bytes)','TCP UL Retrans. Vol (Bytes)','Avg Bearer TP DL (kbps)','Avg Bearer TP UL (kbps)', 'Handset Type']]



experiance.head()

experiance['total_RTT'] = experiance['Avg RTT DL (ms)'] + experiance['Avg RTT UL (ms)']

experiance['total_TCP'] = experiance['TCP DL Retrans. Vol (Bytes)'] + experiance['TCP UL Retrans. Vol (Bytes)']

experiance['total_TP'] = experiance['Avg Bearer TP DL (kbps)'] + experiance['Avg Bearer TP UL (kbps)']



experiance.head()



agg_experiance = experiance.groupby('MSISDN/Number').agg({'total_RTT': 'mean', 'total_TCP': 'mean', 'total_TP': 'mean', 'Handset Type': 'first'})

agg_experiance.rename(columns={'total_RTT': 'avg_RTT', 'total_TCP': 'avg_TCP', 'total_TP': 'avg_TP'}, inplace=True)

agg_experiance.head()

top_10_RTT = agg_experiance.sort_values(by='avg_RTT', ascending=False)

print('The top 10 values of RTT per user are: \n', top_10_RTT['avg_RTT'].head(10))

print('---------------------------------------------------')

print('The bottom to values of RTT per user are: \n',top_10_RTT['avg_RTT'].tail(10))

print('---------------------------------------------------')

freq = agg_experiance['avg_RTT'].value_counts()

print('The most frequent values of RTT are: \n', freq.head(10))

top_10_TP = agg_experiance.sort_values(by='avg_TP', ascending=False)

print('The top 10 values of TP per user are: \n', top_10_TP['avg_TP'].head(10))

print('---------------------------------------------------')

print('The bottom to values of TP per user are: \n',top_10_TP['avg_TP'].tail(10))

print('---------------------------------------------------')

freq = agg_experiance['avg_TP'].value_counts()

print('The most frequent values of TP are: \n', freq.head(10))

top_10_tcp = agg_experiance.sort_values(by='avg_TCP', ascending=False)

print('The top 10 values of TCP per user are: \n', top_10_tcp['avg_TCP'].head(10))

print('---------------------------------------------------')

print('The bottom to values of TCP per user are: \n',top_10_tcp['avg_TCP'].tail(10))

print('---------------------------------------------------')

freq = agg_experiance['avg_TCP'].value_counts()

print('The most frequent values of TCP are: \n', freq.head(10))

freq = agg_experiance['Handset Type'].value_counts()

print('The most frequent handsets are: \n', freq.head(10))

avg_TP_by_handset_type=experiance.groupby('Handset Type').agg({'total_TP':'mean'})

avg_TP_by_handset_type.rename(columns={'total_TP':'avg_tp'},inplace=True)

avg_TP_by_handset_type.head()

avg_TCP_by_handset_type=experiance.groupby('Handset Type').agg({'total_TCP':'mean'})

avg_TCP_by_handset_type.rename(columns={'total_TCP':'avg_tcp'},inplace=True)

avg_TCP_by_handset_type.head()

experiance_cluster = experiance.groupby('MSISDN/Number').agg({'total_RTT': 'mean', 'total_TCP': 'mean', 'total_TP': 'mean'})

experiance_cluster.rename(columns={'total_RTT': 'avg_RTT', 'total_TCP': 'avg_TCP', 'total_TP': 'avg_TP'}, inplace=True)

experiance_cluster.head()

experiance_cluster.reset_index(inplace=True)

experiance_cluster.drop(columns=['MSISDN/Number'], inplace=False)

from sklearn.preprocessing import Normalizer



norm = Normalizer()

normilzed_data = norm.fit_transform(experiance_cluster)

normilized_data = pd.DataFrame(normilzed_data, columns=experiance_cluster.columns)

normilized_data.head()
kmeans = KMeans(n_clusters=3)

kmeans.fit(normilized_data)

labels = kmeans.labels_

agg_experiance['clusters'] = labels



agg_experiance.head()

agg_experiance[agg_experiance['clusters']==0].describe().transpose()

agg_experiance[agg_experiance['clusters']==1].describe().transpose()

agg_experiance[agg_experiance['clusters']==2].describe().transpose()

from urllib.parse import quote_plus

from sqlalchemy import create_engine



password = 'kerod53@'




encoded_password = quote_plus(password)




db_string = f'postgresql://postgres:{encoded_password}@localhost:5432/Telecom'




engine = create_engine(db_string)

table_name = 'user_experiance'

 

agg_experiance.to_sql(table_name, engine, index=False, if_exists='replace')
import pandas as pd










df = pd.read_csv("../data/data.csvcsv", sep=";")
with open("../data/data.csv", 'r') as file:

    lines = file.readlines()
print(f"The number of rows/lines is {len(lines)}")
print(lines[0]) 
print(lines[0].strip('\n').strip().strip(';').split(';')) lines_as_lists = [line.strip('\n').strip().strip(';').split(';') for line in lines]

len(lines_as_lists)
print(f"the number of fields in row 1 is {len(lines_as_lists[1])}, row 2 is {len(lines_as_lists[2])}")










no_field_max = 0



for row in lines_as_lists:

    if len(row) > no_field_max:

        no_field_max = len(row)



print(f"the maximum number of fields is {no_field_max}")

largest_n = int((no_field_max-4)/6)

print(f"the largest n = {largest_n}")





cols = lines_as_lists.pop(0)
track_cols = cols[:4]

trajectory_cols = ['track_id'] + cols[4:]



print(track_cols)

print(trajectory_cols)

track_info = []

trajectory_info = []



for row in lines_as_lists:

    track_id = row[0]



    
    track_info.append(row[:4]) 



    remaining_values = row[4:]

    
    trajectory_matrix = [ [track_id] + remaining_values[i:i+6] for i in range(0,len(remaining_values),6)]

    
    trajectory_info = trajectory_info + trajectory_matrix

df_track = pd.DataFrame(data= track_info,columns=track_cols)

df_track.columns = df_track.columns.str.strip()





df_track
df_trajectory = pd.DataFrame(data= trajectory_info,columns=trajectory_cols)

df_trajectory.columns = df_trajectory.columns.str.strip()



df_trajectory.head(20)
from sqlalchemy import create_engine

import pandas as pd

"""

a function that connect to the local database

"""

def create_conn():

    engine = None

    try:

        
        engine = create_engine('postgresql://postgres:new_password@localhost:5432/warehouse')

        print("Connection successful")

    except Exception as error:

        print(error)



    return engine



"""

a function that that accept engine, and table_name as an argument and return pandas data fream

"""

def fetch_data(engine, table_name):

    df = None

    try:

        
        df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)

    except Exception as error:

        print(error)



    return df



def load_data_to_db(df, table_name, engine):

    try:

        df.to_sql(table_name, engine, if_exists='append', index=False)

        print(f"Data loaded to {table_name} successfully.")

    except Exception as error:

        print(error)

conn = create_conn()

load_data_to_db(df_track, "traffic_track", conn )
load_data_to_db(df_trajectory, "traffic_trajectory", conn )
load_data_to_db(df_track, "traffic_track", conn )
load_data_to_db(df_track, "traffic_track", conn )
/opt/airflow/logs/scheduler/2024-01-03
[0m00:41:10.645073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd01aba7790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd018917be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd018917b80>]}


============================== 00:41:10.648459 | 30281428-7127-4978-b984-94e352ee28e3 ==============================
[0m00:41:10.648459 [info ] [MainThread]: Running with dbt=1.7.4
[0m00:41:10.649380 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'debug': 'False', 'profiles_dir': '/home/abrham/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt init dbt_warehouse', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:41:10.650108 [warn ] [MainThread]: [ConfigFolderDirectory]: Unable to parse dict {'dir': PosixPath('/home/abrham/.dbt')}
[0m00:41:10.650913 [info ] [MainThread]: Creating dbt configuration folder at 
[0m00:41:10.652813 [debug] [MainThread]: Starter project path: /home/abrham/Desktop/10x/week - 2/Week-2-Data-Engineering-Data-warehouse/warehous/lib/python3.10/site-packages/dbt/include/starter_project
[0m00:41:10.657764 [info ] [MainThread]: 
Your new dbt project "dbt_warehouse" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

[0m00:41:10.659278 [info ] [MainThread]: Setting up your profile.
[0m00:54:33.652250 [info ] [MainThread]: Profile dbt_warehouse written to /home/abrham/.dbt/profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.
[0m00:54:33.661795 [debug] [MainThread]: Resource report: {"command_name": "init", "command_success": true, "command_wall_clock_time": 803.0185, "process_user_time": 2.496967, "process_kernel_time": 0.132101, "process_mem_max_rss": "98048", "process_in_blocks": "56", "process_out_blocks": "80"}
[0m00:54:33.668224 [debug] [MainThread]: Command `dbt init` succeeded at 00:54:33.666593 after 803.02 seconds
[0m00:54:33.672021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd01aba7790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0189c7d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd01cf1bb80>]}
[0m00:54:33.676388 [debug] [MainThread]: Flushing usage events
[0m00:57:53.639719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f87523790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f84fb24d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f84fb0910>]}


============================== 00:57:53.644006 | a2ca3bd5-641d-4db0-a845-95bff79b6bd6 ==============================
[0m00:57:53.644006 [info ] [MainThread]: Running with dbt=1.7.4
[0m00:57:53.644962 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'profiles_dir': '/home/abrham/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:57:53.645760 [error] [MainThread]: Encountered an error:
Runtime Error
  No dbt_project.yml found at expected path /home/abrham/Desktop/10x/week - 2/Week-2-Data-Engineering-Data-warehouse/dbt_project.yml
  Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml
  
[0m00:57:53.647040 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.07079544, "process_user_time": 2.479447, "process_kernel_time": 0.143967, "process_mem_max_rss": "94100", "process_in_blocks": "5480", "process_out_blocks": "24", "command_success": false}
[0m00:57:53.648013 [debug] [MainThread]: Command `dbt run` failed at 00:57:53.647727 after 0.07 seconds
[0m00:57:53.648985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f87523790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f84fb1b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f84fb3760>]}
[0m00:57:53.651305 [debug] [MainThread]: Flushing usage events
[0m18:21:05.211164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff4181af790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff415ef3910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff415ef2e00>]}


============================== 18:21:05.216237 | 0def2429-1707-4558-8bca-859fd46e90cf ==============================
[0m18:21:05.216237 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:21:05.217188 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/abrham/.dbt', 'log_path': 'logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:21:05.218048 [info ] [MainThread]: dbt version: 1.7.4
[0m18:21:05.218885 [info ] [MainThread]: python version: 3.10.12
[0m18:21:05.219694 [info ] [MainThread]: python path: /home/abrham/Desktop/10x/week - 2/Week-2-Data-Engineering-Data-warehouse/warehous/bin/python
[0m18:21:05.220443 [info ] [MainThread]: os info: Linux-6.2.0-39-generic-x86_64-with-glibc2.35
[0m18:21:05.375146 [info ] [MainThread]: Using profiles dir at /home/abrham/.dbt
[0m18:21:05.376051 [info ] [MainThread]: Using profiles.yml file at /home/abrham/.dbt/profiles.yml
[0m18:21:05.376697 [info ] [MainThread]: Using dbt_project.yml file at /home/abrham/Desktop/10x/week - 2/Week-2-Data-Engineering-Data-warehouse/dbt_project.yml
[0m18:21:05.377532 [info ] [MainThread]: adapter type: postgres
[0m18:21:05.378279 [info ] [MainThread]: adapter version: 1.7.4
[0m18:21:05.379102 [info ] [MainThread]: Configuration:
[0m18:21:05.379742 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m18:21:05.380367 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m18:21:05.381061 [info ] [MainThread]: Required dependencies:
[0m18:21:05.381892 [debug] [MainThread]: Executing "git --help"
[0m18:21:05.385425 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:21:05.386935 [debug] [MainThread]: STDERR: "b''"
[0m18:21:05.387622 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m18:21:05.388294 [info ] [MainThread]: Connection:
[0m18:21:05.389138 [info ] [MainThread]:   host: localhost
[0m18:21:05.390078 [info ] [MainThread]:   port: 5432
[0m18:21:05.391106 [info ] [MainThread]:   user: postgres
[0m18:21:05.392061 [info ] [MainThread]:   database: warehouse
[0m18:21:05.392817 [info ] [MainThread]:   schema: tables
[0m18:21:05.393665 [info ] [MainThread]:   connect_timeout: 10
[0m18:21:05.394441 [info ] [MainThread]:   role: None
[0m18:21:05.395250 [info ] [MainThread]:   search_path: None
[0m18:21:05.396263 [info ] [MainThread]:   keepalives_idle: 0
[0m18:21:05.397168 [info ] [MainThread]:   sslmode: None
[0m18:21:05.397864 [info ] [MainThread]:   sslcert: None
[0m18:21:05.398944 [info ] [MainThread]:   sslkey: None
[0m18:21:05.399985 [info ] [MainThread]:   sslrootcert: None
[0m18:21:05.401016 [info ] [MainThread]:   application_name: dbt
[0m18:21:05.401926 [info ] [MainThread]:   retries: 1
[0m18:21:05.403149 [info ] [MainThread]: Registered adapter: postgres=1.7.4
[0m18:21:05.404572 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m18:21:05.405643 [debug] [MainThread]: Using postgres connection "debug"
[0m18:21:05.406569 [debug] [MainThread]: On debug: select 1 as id
[0m18:21:05.407144 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:21:05.461067 [debug] [MainThread]: SQL status: SELECT 1 in 0.0 seconds
[0m18:21:05.464212 [debug] [MainThread]: On debug: Close
[0m18:21:05.466239 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m18:21:05.467821 [info ] [MainThread]: [31m1 check failed:[0m
[0m18:21:05.469933 [info ] [MainThread]: Project loading failed for the following reason:
 project path </home/abrham/Desktop/10x/week - 2/Week-2-Data-Engineering-Data-warehouse/dbt_project.yml> not found

[0m18:21:05.473660 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_wall_clock_time": 0.32231817, "process_user_time": 2.843827, "process_kernel_time": 0.123818, "process_mem_max_rss": "102360", "process_in_blocks": "4144", "process_out_blocks": "24", "command_success": false}
[0m18:21:05.480153 [debug] [MainThread]: Command `dbt debug` failed at 18:21:05.479399 after 0.33 seconds
[0m18:21:05.481578 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m18:21:05.483165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff4181af790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff415e5e650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff415e06f50>]}
[0m18:21:05.484805 [debug] [MainThread]: Flushing usage events
[0m18:57:42.033049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdb079035b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdb05395db0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdb05396890>]}


============================== 18:57:42.038522 | 2ef08cbb-3e07-487f-ad7b-298600629359 ==============================
[0m18:57:42.038522 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:57:42.039827 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/abrham/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ls', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:57:42.041170 [error] [MainThread]: Encountered an error:
Runtime Error
  No dbt_project.yml found at expected path /home/abrham/Desktop/10x/week - 2/Week-2-Data-Engineering-Data-warehouse/dbt_project.yml
  Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml
  
[0m18:57:42.043109 [debug] [MainThread]: Resource report: {"command_name": "list", "command_wall_clock_time": 0.07379782, "process_user_time": 2.84518, "process_kernel_time": 0.212503, "process_mem_max_rss": "94000", "process_in_blocks": "52200", "process_out_blocks": "8", "command_success": false}
[0m18:57:42.044822 [debug] [MainThread]: Command `dbt ls` failed at 18:57:42.044350 after 0.08 seconds
[0m18:57:42.045855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdb079035b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdb05394190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdb05397a60>]}
[0m18:57:42.047275 [debug] [MainThread]: Flushing usage events
name: 'dbt_warehouse'
version: '1.0.0'
config-version: 2

profile: 'dbt_warehouse'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:           - "target"
  - "dbt_packages"



models:
  dbt_warehouse:
    materialized: table
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from src.extract_data import extract_data
from src.loading_data import create_conn
from src.loading_data import load_data_to_db

default_args = {
    'owner':'abrham',
    'retries': 5,
    'retry_delay':timedelta(minutes=2)
}

def extract_task():
    df_track, df_trajectory = extract_data("/home/abrham/Desktop/10x/week - 2/Week-2-Data-Engineering-Data-warehouse/data/data.csv")
    return df_track, df_trajectory

def load_data_task():
    df_track, df_trajectory= extract_task()
    engine = create_conn()
    load_data_to_db(df_track, "track", engine)
    load_data_to_db(df_trajectory, "trajectory", engine)

with DAG(
    dag_id= 'extract_and_load_the_row_data',
    default_args=default_args,
    description='dag with python operator',
    start_date=datetime(2023, 12, 22, 2),
    schedule_interval='@daily',
)as dag:
    extract_operator = PythonOperator(
        task_id= 'extract_task',
        python_callable=extract_task,
        dag=dag,
    )
    load_operator = PythonOperator(
        task_id='load_task',
        python_callable=load_data_task,
        dag=dag,
    )

extract_operator >> load_operator
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {
    'owner': 'abrham',
    'retries': 5,
    'retry_delay': timedelta(minutes=2)
}

with DAG(
    dag_id='dbt_dag',
    default_args=default_args,
    description='A DAG to run dbt commands',
    start_date=datetime(2021, 7, 29, 2),
    schedule_interval='@daily',
) as dag:

    dbt_run = BashOperator(
        task_id='dbt_run',
        bash_command='dbt run --profiles-dir ../dbt_warehouse'
    )

    dbt_test = BashOperator(
        task_id='dbt_test',
        bash_command='dbt test --profiles-dir ../dbt_warehouse'
    )

    dbt_run >> dbt_test
import pandas as pd

def extract_data(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()
    lines_as_lists = [line.strip('\n').strip().strip(';').split(';') for line in lines]

    no_field_max = 0
    for row in lines_as_lists:
        if len(row) > no_field_max:
            no_field_max = len(row)

    largest_n = int((no_field_max-4)/6)

    cols = lines_as_lists.pop(0)

    track_cols = cols[:4]
    trajectory_cols = ['track_id'] + cols[4:]

    track_info = []
    trajectory_info = []

    for row in lines_as_lists:
        track_id = row[0]
        track_info.append(row[:4]) 
        remaining_values = row[4:]
        trajectory_matrix = [ [track_id] + remaining_values[i:i+6] for i in range(0,len(remaining_values),6)]
        trajectory_info = trajectory_info + trajectory_matrix

    df_track = pd.DataFrame(data= track_info,columns=track_cols)
    df_track.columns = df_track.columns.str.strip()

    df_trajectory = pd.DataFrame(data= trajectory_info,columns=trajectory_cols)
    df_trajectory.columns = df_trajectory.columns.str.strip()

    return df_track, df_trajectory


if __name__ == "__main__":
    extract_data()
Repository Structure: '
' ├── docker-compose.yml
├── airflow
│   ├── airflow.cfg
│   └── dags
├── jupyter_notebook
│   └── Simple ETL.ipynb
├── dbt_app
│   ├── models
│   ├── analyses
│   ├── macros
│   ├── snapshots
│   ├── dbt_project.yml
│   ├── .gitignore
│   ├── seeds
│   ├── tests
│   └── README.md
├── redash
│   └── __init__.py
├── .gitignore
├── dockerfile
├── README.md
├── .dockerignore
└── requirements.txt
 '
' Commit History: 
{"insertions": [31], "deletions": [45], "lines": [76], "committed_datetime": ["2023-12-22 20:09:09"], "commit_count": 1} 
 Content: 
import os
from functools import wraps
import pandas as pd
from airflow.models import DAG
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator,ShortCircuitOperator
from dotenv import dotenv_values
from sqlalchemy import create_engine, inspect
from datetime import datetime,timedelta
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator

args = {
    "owner": "Airflow",
    "start_date": datetime(2023, 1, 1), 
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5)}


DATASET_URL = "D:/tenacademy/new/Airflow/data/data.csv"


CONFIG = dotenv_values(".env")

def connect_db():
    print("Connecting to DB")
    connection_uri = "postgresql://{}:{}@{}:{}/{}".format(
        CONFIG["POSTGRES_USER"],
        CONFIG["POSTGRES_PASSWORD"],
        CONFIG['POSTGRES_HOST'],
        CONFIG["POSTGRES_PORT"],
        CONFIG["POSTGRES_DB"],
    )

    engine = create_engine(connection_uri, pool_pre_ping=True)
    engine.connect()
    return engine



def extract_data(DATASET_URL):
    print(f"Reading dataset from {DATASET_URL}")
    df = pd.read_csv(DATASET_URL)
    return df



def check_table_exists(table_name, engine):
    if table_name in inspect(engine).get_table_names():
        print(f"{table_name!r} exists in the DB!")
    else:
        print(f"{table_name} does not exist in the DB!")


def load_to_db(df, table_name, engine):
    print(f"Loading dataframe to DB on table: {table_name}")
    df.to_sql(table_name, engine, if_exists="replace")


def tables_exists():
    db_engine = connect_db()
    print("Checking if tables exists")
    check_table_exists("df_trafic1", db_engine)
    db_engine.dispose()


def etl():
    db_engine = connect_db()
    raw_df = extract_data(DATASET_URL)
    raw_table_name = "df_trafic1"
    load_to_db(raw_df, raw_table_name, db_engine)
    db_engine.dispose()


DBT_CMD = "dbt"

def work_on_dbt():
    print("Work on bt")


dbt_seed = BashOperator(
    task_id='dbt_seed',
    bash_command=f"{DBT_CMD} seed --profiles-dir /dbt",
)

dbt_run = BashOperator(
    task_id='dbt_run',
    bash_command=f"{DBT_CMD} run --profiles-dir /dbt",
)

dbt_test = BashOperator(
    task_id='dbt_test',
    bash_command=f"{DBT_CMD} test --profiles-dir /dbt",
)

dbt_docs_generate = BashOperator(
    task_id='dbt_docs_generate',
    bash_command=f"{DBT_CMD} docs generate --profiles-dir /dbt",
)

DAG
with DAG('RAW-DATA-EXTRACTOR-AND-LOADER',start_date = datetime(2023, 1, 1), catchup=False, schedule_interval = '@daily') as dag:
  
    checking_db_connection = ShortCircuitOperator(
        task_id='Connecting_to_DB',
        python_callable=connect_db,
    )
    creating_db = ShortCircuitOperator(
        task_id='Reading_dataset',
        python_callable=extract_data,
    )
    creating_stations_table = ShortCircuitOperator(
        task_id='check_table_exists',
        python_callable=check_table_exists,
    )

    work_on_dbt = ShortCircuitOperator(
        task_id='work_on_dbt',
        python_callable=work_on_dbt,
    )


    dbt_tasks = [dbt_seed, dbt_run, dbt_test, dbt_docs_generate]

    checking_db_connection >> creating_db >> creating_stations_table >> work_on_dbt >> dbt_tasks
 

 

 

import pandas as pd
import numpy as np
import sqlite3
import DatabaseConnection from connection

DatabaseConnection.connect()

AUTOMOBILE_SCHEMA = "automobile_data_schema.sql"
TRAFFIC_SCHEMA = "traffic_schema.sql"

class DBFilter:
    def __init__(self, dataframe):
        self.df = dataframe

    def filter_numeric_columns(self, threshold=0):
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        filtered_df = self.df[numeric_columns].apply(lambda x: x[x > threshold])

        return filtered_df
    
    def load_data_from_db(self, db_path, sql_query):
        connection = sqlite3.connect(db_path)
        df = pd.read_sql_query(sql_query, connection)
        connection.close()
        return df

    def get_unique_values(self, column):
        unique_values = self.df[column].unique()
        return unique_values
    
    def most_repeated_value(self, column):
        return self.df[column].mode().values[0]

    def calculate_average(self, column):
        return self.df[column].mean()

    def close_connection(self):
                pass

def create_table():
    try:
        with engine.connect() as conn:
            for name in [TRAJECTORIES_SCHEMA,VEHICLE_SCHEMA]:
                
                with open(f'/opt/pgsql/{name}', "r") as file:
                    query = text(file.read())
                    conn.execute(query)
        print("Successfull")
    except Exception as e:
        print("Error creating table",e)
        sys.exit(e)



def insert_to_table(json_stream :str, table_name: str,from_file=False ):
    try:
        if not from_file:
            df = pd.read_json(json_stream)
        else:
                        with open(f'../temp_storage/{json_stream}','r') as file:
                data=file.readlines()
            dt=data[0]

            df=pd.DataFrame.from_dict(json.loads(dt))
            df.columns=df.columns.str.replace(' ','')

                                                df.dropna(inplace=True)
        with engine.connect() as conn:
            df.to_sql(name=table_name, con=conn, if_exists='append', index=False)

    except Exception as e:
        print(f"error while inserting to table: {e}")  
        sys.exit(e)
name: 'dbt_traffic'
version: '1.0.0'
config-version: 2

profile: 'dbt_traffic'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:           - "target"
  - "dbt_packages"



models:
  dbt_traffic:
        example:
      +materialized: view
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as pl

from sqlalchemy import create_engine, text

import psycopg2

import os

import csv

from io import StringIO
os.chdir('..')
from db.connection import DatabaseConnection
db_connection = DatabaseConnection()
db_connection.connect()

file_path = 'data/20181024_d1_0830_0900.csv'

import pandas as pd



data = pd.read_csv(file_path, sep="[,;:]", index_col=False, engine='python')



data
data2 = pd.read_csv(file_path, index_col=False, delimiter='; ')
data2
import os
import sys
from datetime import datetime
from datetime import timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

cwd=os.getcwd()
sys.path.append(f'../utils/')
sys.path.append(f'../db_pst/')
sys.path.append(f'../temp/')
sys.path.insert(0,os.path.abspath(os.path.dirname(__file__)))

from extractor import DataExtractor
import sql_preprocessor

data_extractor=DataExtractor()


def extract_data(ti):

    loaded_df_name=data_extractor.extract_data(file_name='20181024_d1_0830_0900.csv',return_json=True)
    traffic_file_name,automobile_file_name=loaded_df_name
   
    ti.xcom_push(key="traffic",value=traffic_file_name)
    ti.xcom_push(key="automobile",value=automobile_file_name)

def create_table():
    sql_preprocessor.create_table()

def populate__automobiles_table(ti):
    traffic_file_name = ti.xcom_pull(key="traffic",task_ids='extract_from_file')
    sql_preprocessor.insert_to_table(traffic_file_name, 'traffic',from_file=True)

def populate_traffic_table(ti):
    automobile_file_name = ti.xcom_pull(key="automobile",task_ids='extract_from_file')
    sql_preprocessor.insert_to_table(automobile_file_name, 'automobiles',from_file=True)

def clear_memory_automobile(ti):
    traffic_file_name = ti.xcom_pull(key="traffic",task_ids='extract_from_file')

    os.remove(f'../temp/{traffic_file_name}')

def clear_memory_traffic(ti):
    automobile_file_name = ti.xcom_pull(key="automobile",task_ids='extract_from_file')

    os.remove(f'../temp/{automobile_file_name}')

default_args = {
    'owner': 'Abel',
    'depends_on_past': False,
    'email': ['abel@abelbekele.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0
}

with DAG(
    dag_id='loading_pg',
    default_args=default_args,
    description='this loads our data to the database',
    start_date=datetime(2023,12,21,3),
    schedule_interval='@daily',
    catchup=False
) as dag:
    
    read_data = PythonOperator(
        task_id='extract_from_file',
        python_callable = extract_data,
    ) 
    
    create_tables = PythonOperator(
        task_id='create_table',
        python_callable = create_table
    )
    
    populate_automobiles = PythonOperator(
        task_id='load_automobile_data',
        python_callable = populate__automobiles_table
    )
    
    populate_traffic = PythonOperator(
        task_id='load_traffic_data',
        python_callable = populate_traffic_table
    ) 

    clear_temp_automobile_data = PythonOperator(
        task_id='delete_temp_automobile_files',
        python_callable = clear_memory_automobile
    )
    clear_temp_traffic_data = PythonOperator(
        task_id='delete_temp_traffic_files',
        python_callable = clear_memory_traffic
    )

    [read_data,create_tables]>>populate_automobiles>>populate_traffic>>[clear_temp_traffic_data,clear_temp_automobile_data]
CREATE TABLE IF NOT EXISTS automobiles
(
    "id" SERIAL NOT NULL,
    "track_id" TEXT NOT NULL,
    "lat" TEXT NOT NULL,
    "lon" TEXT DEFAULT NULL,
    "speed" TEXT DEFAULT NULL,
    "lon_acc" TEXT DEFAULT NULL,
    "lat_acc" TEXT DEFAULT NULL,
    "time" TEXT DEFAULT NULL,
    PRIMARY KEY ("id"),
    CONSTRAINT fk_traffic
        FOREIGN KEY("track_id") 
            REFERENCES traffic(track_id)
            ON DELETE CASCADE
    
);
import sys
import pandas as pd
from sqlalchemy import text
import json
from sqlalchemy import create_engine
import numpy as np

engine = create_engine('postgresql+psycopg2://airflow:airflow@192.168.137.112:2345/postgres')

AUTOMOBILE_SCHEMA = "automobile_data_schema.sql"
TRAFFIC_SCHEMA = "traffic_schema.sql"

class DBFilter:
    def __init__(self, dataframe):
        self.df = dataframe

    def filter_numeric_columns(self, threshold=0):
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        filtered_df = self.df[numeric_columns].apply(lambda x: x[x > threshold])

        return filtered_df
    
    def load_data_from_db(self, db_path, sql_query):
        connection = sqlite3.connect(db_path)
        df = pd.read_sql_query(sql_query, connection)
        connection.close()
        return df

    def get_unique_values(self, column):
        unique_values = self.df[column].unique()
        return unique_values
    
    def most_repeated_value(self, column):
        return self.df[column].mode().values[0]

    def calculate_average(self, column):
        return self.df[column].mean()

    def close_connection(self):
                pass

def create_table():
    try:
        with engine.connect() as conn:
            for name in [TRAFFIC_SCHEMA,AUTOMOBILE_SCHEMA]:
                
                with open(f'/opt/db_pst/{name}', "r") as file:
                    query = text(file.read())
                    conn.execute(query)
        print("Successfull")
    except Exception as e:
        print("Error creating table",e)
        sys.exit(e)



def insert_to_table(json_stream :str, table_name: str,from_file=False ):
    try:
        if not from_file:
            df = pd.read_json(json_stream)
        else:
            with open(f'../temp/{json_stream}','r') as file:
                data=file.readlines()
            dt=data[0]

            df=pd.DataFrame.from_dict(json.loads(dt))
            df.columns=df.columns.str.replace(' ','')

            df.dropna(inplace=True)
        with engine.connect() as conn:
            df.to_sql(name=table_name, con=conn, if_exists='append', index=False)

    except Exception as e:
        print(f"error while inserting to table: {e}")  
        sys.exit(e)
import pandas as pd
from logger import Logger
import sys
from datetime import datetime
class DataExtractor():
    
    def __init__(self)->None:
        try:
            self.logger=Logger().get_app_logger()
            self.logger.info('Data extractor object Initialized')
        except:
            pass
    
    def get_columns_and_rows(self,file_path)->tuple:
        try:
            with open(f'../data/{file_path}','r') as f:
                lines=f.readlines()

            columns=lines[0].replace('\n','').split(';')
            data=lines[1:100]
            return columns,data
        except Exception as e:
                        try:
                self.logger.error(f"Failed to read data: {e}")
            except:
                pass
            sys.exit(1)
    
    def chunk_list(self,list,chunk_size,default_first_val=None)->list:
        chunked_list=[]
        for i in range(0, len(list), chunk_size):
            if default_first_val:
                values=[default_first_val]
                values.extend(list[i:i+chunk_size])
                chunked_list.append(values)
            else:
                chunked_list.append(list[i:i+chunk_size])

        return chunked_list

    def prepare_data_for_pandas(self,columns,all_data,id_prefix)->tuple:
        try:
            traffic_cols=columns[:4]
            traffic_rows=[]

            timed_automobile_cols=['track_id']+columns[4:]
            timed_automobile_rows=[]

            for row in all_data:
                try:
                    items=row.replace('\n','').split(';')
                    items[0]=f"{id_prefix}_{items[0]}"
                    traffic_rows.append(items[:4])
                    timed_automobile_rows.extend(self.chunk_list(items[4:],6,items[0]))
                except Exception as e:
                                        try:
                        self.logger.error(f"Failed preparing data for pands at row {row}: {e}")
                    except:
                        pass
            
            return (traffic_cols,traffic_rows),(timed_automobile_cols,timed_automobile_rows)
        except Exception as e:
                        try:
                self.logger.error(f"Failed to prepare data for pandas: {e}")
            except:
                pass
    
    def prepare_data_frame(self,traffic_data:tuple,timed_automobile_data:tuple):

        try:
            traffic_cols,traffic_rows=traffic_data
            timed_automobile_cols,timed_automobile_rows=timed_automobile_data

            traffic_data=pd.DataFrame(columns=traffic_cols,data=traffic_rows)
            timed_automobile_data=pd.DataFrame(columns=timed_automobile_cols,data=timed_automobile_rows)

            return traffic_data,timed_automobile_data

        except Exception as e:
                        try:
                self.logger.error(f"Failed to prepare data frame: {e}")
            except:
                pass

    def extract_data(self,file_name:str,return_json=False)->pd.DataFrame:
        try:
                        id_prefix= f"{file_name.split('.')[0]}"
            columns,all_data=self.get_columns_and_rows(file_path=file_name)
            traffic_data, timed_automobile_data=self.prepare_data_for_pandas(columns=columns,all_data=all_data,id_prefix=id_prefix)
            if not return_json:
                return self.prepare_data_frame(traffic_data,timed_automobile_data)
            
            tr,vh= self.prepare_data_frame(traffic_data,timed_automobile_data)
            
            tr_file_name= str(datetime.today()).replace(' ','_')+"traffic.json"
            vh_file_name= str(datetime.today()).replace(' ','_')+"automobile_data.json"

            tr.to_json(f'../temp/{tr_file_name}',orient='records')
            vh.to_json(f'../temp/{vh_file_name}',orient='records')

            return tr_file_name,vh_file_name
        except Exception as e:
            print(e)
            try:
                self.logger.error(f"Failed to extract data: {e}")
            except:
                pass
    def separate_data(self,file_name:str,chunk_size:int = 100):
        pass
/opt/airflow/logs/scheduler/2023-12-24
import os
import sys
from datetime import datetime
from datetime import timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

cwd=os.getcwd()
sys.path.append(f'../utils/')
sys.path.append(f'../db_pst/')
sys.path.append(f'../temp/')
sys.path.insert(0,os.path.abspath(os.path.dirname(__file__)))

from extractor import DataExtractor
import sql_preprocessor

data_extractor=DataExtractor()


def extract_data(ti):

    loaded_df_name=data_extractor.extract_data(file_name='20181024_d1_0830_0900.csv',return_json=True)
    traffic_file_name,automobile_file_name=loaded_df_name
   
    ti.xcom_push(key="traffic",value=traffic_file_name)
    ti.xcom_push(key="automobile",value=automobile_file_name)

def create_table():
    sql_preprocessor.create_table()

def populate__automobiles_table(ti):
    traffic_file_name = ti.xcom_pull(key="traffic",task_ids='extract_from_file')
    sql_preprocessor.insert_to_table(traffic_file_name, 'traffic',from_file=True)

def populate_traffic_table(ti):
    automobile_file_name = ti.xcom_pull(key="automobile",task_ids='extract_from_file')
    sql_preprocessor.insert_to_table(automobile_file_name, 'automobiles',from_file=True)

def clear_memory_automobile(ti):
    traffic_file_name = ti.xcom_pull(key="traffic",task_ids='extract_from_file')

    os.remove(f'../temp/{traffic_file_name}')

def clear_memory_traffic(ti):
    automobile_file_name = ti.xcom_pull(key="automobile",task_ids='extract_from_file')

    os.remove(f'../temp/{automobile_file_name}')

default_args = {
    'owner': 'Abel',
    'depends_on_past': False,
    'email': ['abel@abelbekele.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0
}

with DAG(
    dag_id='loading_pg',
    default_args=default_args,
    description='this loads our data to the database',
    start_date=datetime(2023,12,21,3),
    schedule_interval='@daily',
    catchup=False
) as dag:
    
    read_data = PythonOperator(
        task_id='extract_from_file',
        python_callable = extract_data,
    ) 
    
    create_tables = PythonOperator(
        task_id='create_table',
        python_callable = create_table
    )
    
    populate_automobiles = PythonOperator(
        task_id='load_automobile_data',
        python_callable = populate__automobiles_table
    )
    
    populate_traffic = PythonOperator(
        task_id='load_traffic_data',
        python_callable = populate_traffic_table
    ) 

    clear_temp_automobile_data = PythonOperator(
        task_id='delete_temp_automobile_files',
        python_callable = clear_memory_automobile
    )
    clear_temp_traffic_data = PythonOperator(
        task_id='delete_temp_traffic_files',
        python_callable = clear_memory_traffic
    )

    [read_data,create_tables]>>populate_automobiles>>clear_temp_automobile_data,populate_automobiles>>populate_traffic>>clear_temp_traffic_data
{{ config(materialized='view') }}

with summary as (
  
    SELECT 
    type as "Automobile type",
    count(type) as "Automobile count",
    Round(AVG(Cast(traveled_d as numeric)),2) as "Avg distance traveled",
    Round(AVG(cast(avg_speed as numeric)),2) as "Avg speed by automobile"
    from traffic 
    GROUP BY type ORDER BY "Automobile count" ASC
  
)

SELECT * from summary
{{ config(materialized='view') }}

with timely_summary as (
    SELECT 
    time,
    Round(AVG(Cast(speed as numeric)),2) as "speed",
    Round(AVG(Cast(lat_acc as numeric)),2) as "lat_acc",
    Round(AVG(Cast(lon_acc as numeric)),2) as "lon_acc"
    from automobiles
    GROUP BY "time"
)


SELECT * FROM timely_summary
set -eu

REDASH_BASE_PATH=/opt/redash


create_directories() {
  if [ ! -e "$REDASH_BASE_PATH" ]; then
    sudo mkdir -p "$REDASH_BASE_PATH"
    sudo chown "$USER:" "$REDASH_BASE_PATH"
  fi

  if [ ! -e "$REDASH_BASE_PATH"/postgres-data ]; then
    mkdir "$REDASH_BASE_PATH"/postgres-data
  fi
}

create_config() {
  if [ -e "$REDASH_BASE_PATH"/env ]; then
    rm "$REDASH_BASE_PATH"/env
    touch "$REDASH_BASE_PATH"/env
  fi

  COOKIE_SECRET=$(pwgen -1s 32)
  SECRET_KEY=$(pwgen -1s 32)
  POSTGRES_PASSWORD=$(pwgen -1s 32)
  REDASH_DATABASE_URL="postgresql://postgres:${POSTGRES_PASSWORD}@postgres/postgres"

  cat <<EOF >"$REDASH_BASE_PATH"/env
PYTHONUNBUFFERED=0
REDASH_LOG_LEVEL=INFO
REDASH_REDIS_URL=redis://redis:6379/0
POSTGRES_PASSWORD=$POSTGRES_PASSWORD
REDASH_COOKIE_SECRET=$COOKIE_SECRET
REDASH_SECRET_KEY=$SECRET_KEY
REDASH_DATABASE_URL=$REDASH_DATABASE_URL
EOF
}

setup_compose() {
  REQUESTED_CHANNEL=stable
  LATEST_VERSION=$(curl -s "https://version.redash.io/api/releases?channel=$REQUESTED_CHANNEL" | json_pp | grep "docker_image" | head -n 1 | awk 'BEGIN{FS=":"}{print $3}' | awk 'BEGIN{FS="\""}{print $1}')

  cd "$REDASH_BASE_PATH"
  GIT_BRANCH="${REDASH_BRANCH:-master}"   curl -OL https://raw.githubusercontent.com/getredash/setup/"$GIT_BRANCH"/data/docker-compose.yml
  sed -ri "s/image: redash\/redash:([A-Za-z0-9.-]*)/image: redash\/redash:$LATEST_VERSION/" docker-compose.yml
  echo "export COMPOSE_PROJECT_NAME=redash" >>~/.profile
  echo "export COMPOSE_FILE=$REDASH_BASE_PATH/docker-compose.yml" >>~/.profile
  export COMPOSE_PROJECT_NAME=redash
  export COMPOSE_FILE="$REDASH_BASE_PATH"/docker-compose.yml
  sudo docker-compose run --rm server create_db
  sudo docker-compose up -d
}

create_directories
create_config
setup_compose
import datetime
import json
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import datetime
from airflow.utils.dates import timedelta


default_args = {
    'owner': 'Abel',
    'depends_on_past': False,
    'start_date': datetime(2023, 12, 23),
    'email': ['abel@abelbekele.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'dbt_dag',
    default_args=default_args,
    description='A dbt wrapper for airflow',
    schedule_interval=timedelta(days=1),
)



def load_manifest():
    local_filepath = "/opt/dbt/dbt_traffic/target/manifest.json"
    with open(local_filepath) as f:
        data = json.load(f)

    return data

def make_dbt_task(node, dbt_verb):
    """Returns an Airflow operator either run and test an individual model"""
    DBT_DIR = "/opt/dbt/dbt_traffic"
    GLOBAL_CLI_FLAGS = "--no-write-json"
    model = node.split(".")[-1]

    if dbt_verb == "run":
        dbt_task = BashOperator(
            task_id=node,
            bash_command=f"""
            cd {DBT_DIR} &&
            dbt {GLOBAL_CLI_FLAGS} {dbt_verb} --target prod --models {model}
            """,
            dag=dag,
        )

    elif dbt_verb == "test":
        node_test = node.replace("model", "test")
        dbt_task = BashOperator(
            task_id=node_test,
            bash_command=f"""
            cd {DBT_DIR} &&
            dbt {GLOBAL_CLI_FLAGS} {dbt_verb} --target prod --models {model}
            """,
            dag=dag,
        )

    return dbt_task

data = load_manifest()

dbt_tasks = {}
for node in data["nodes"].keys():
    if node.split(".")[0] == "model":
        node_test = node.replace("model", "test")

        dbt_tasks[node] = make_dbt_task(node, "run")
        dbt_tasks[node_test] = make_dbt_task(node, "test")

for node in data["nodes"].keys():
    if node.split(".")[0] == "model":

                node_test = node.replace("model", "test")
        dbt_tasks[node] >> dbt_tasks[node_test]

                for upstream_node in data["nodes"][node]["depends_on"]["nodes"]:

            upstream_node_type = upstream_node.split(".")[0]
            if upstream_node_type == "model":
                dbt_tasks[upstream_node] >> dbt_tasks[node]
{{ config(materialized='table') }}

with summary as (
    SELECT 
    type as "Automobile type",
    count(type) as "Automobile count",
    Round(AVG(Cast(traveled_d as numeric)),2) as "Avg distance traveled",
    Round(AVG(cast(avg_speed as numeric)),2) as "Avg speed by automobile"
    from traffic 
    GROUP BY type ORDER BY "Automobile count" ASC
)

SELECT * from summary
name: 'dwh_dbt'
version: '1.0.0'
config-version: 2

profile: 'dwh_dbt'

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"  clean-targets:           - "target"
  - "dbt_packages"



models:
  dwh_dbt:
        example:
      +materialized: view
import pandas as pd

import os, sys

import csv




rpath = os.path.abspath('../data/20181024_d1_0830_0900 (5).csv')



csv.field_size_limit(10000000) 


expanded_rows = []



with open(rpath, 'r') as csvfile:

    reader = csv.reader(csvfile)

    header = next(reader)



    for row in reader:

        values = row[0].split(';')  
        num_values = len(values)



        for i in range(1, num_values // 6 + 1):

            expanded_row = {

                'track_id': values[0],

                'type': values[1],

                'traveled_d': values[2],

                'avg_speed': values[3],

                'lat': values[6 * (i - 1) + 4],

                'lon': values[6 * (i - 1) + 5],

                'speed': values[6 * (i - 1) + 6],

                'lon_acc': values[6 * (i - 1) + 7],

                'lat_acc': values[6 * (i - 1) + 8],

                'time': values[6 * (i - 1) + 9]

            }



            expanded_rows.append(expanded_row)






df = pd.DataFrame(expanded_rows)




df.to_csv('expanded_data.csv', index=False) 



df.head(10)
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from dags.load_csv_files import load_csv

dbt_model_name = '03_average_distance_by_type'
dbt_project_dir = '/dbt'

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(0),      'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=1),
}

dag = DAG(
    'dbt_run_dag',
    default_args=default_args,
    description='DAG to run dbt models',
    schedule_interval=None,  )

run_dbt_model = BashOperator(
    task_id=f'run_dbt_{dbt_model_name}',
    bash_command=f'dbt run --models {dbt_model_name} --project-dir {dbt_project_dir}',
    dag=dag,
)


dag = DAG(
    dag_id='traffic_data_dag',
    description='DAG for processing traffic data with dbt',
    default_args=default_args,
    schedule_interval='@daily',
)

task_load_data = PythonOperator(
    task_id='load_data',
    python_callable=load_csv,
    dag=dag,
)

task_insert_data = BashOperator(
    task_id='insert_data',
    bash_command='dbt run --models=insert_data',
    dag=dag,
)

task_average_distance = BashOperator(
    task_id='average_distance',
    bash_command='dbt run --models=average_distance',
    dag=dag,
)

task_average_speed = BashOperator(
    task_id='average_speed',
    bash_command='dbt run --models=average_speed',
    dag=dag,
)

task_load_data >> task_insert_data
task_insert_data >> task_average_distance
task_insert_data >> task_average_speed
import pandas as pd
from sqlalchemy import create_engine, pool
import logging

logging.basicConfig(level=logging.info)
logger = logging.getLogger(__name__)


class DatabaseConnection:
    def __init__(self):
        self.db_url = "postgresql://airflow:airflow@postgres/airflow"
        self.engin = create_engine(self.db_url)
    
    def connect(self):
        try:
            connection = self.engin.connect()
            logger.info('connected succesfully ')
            return connection
        except:
            logger.error('conndection to database failed')
CREATE TABLE IF NOT EXISTS trajectory (
        "track_id" NUMERIC REFERENCES vehicle("track_id"),
        "lat" NUMERIC,
        "lon" NUMERIC,
        "speed" NUMERIC,
        "lon_acc" NUMERIC,
        "lat_acc" NUMERIC,
        "time" NUMERIC,
        PRIMARY KEY ("track_id", "time")
);
