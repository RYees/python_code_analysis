pip install python-dotenv

import os

from dotenv import load_dotenv

from langchain.chat_models import ChatOpenAI
 
load_dotenv()

chat = ChatOpenAI(
  openai_api_key=os.getenv("OPENAI_API_KEY"),
  model='gpt-3.5-turbo'

)
 
from langchain.schema import (
  SystemMessage,
  HumanMessage,
  AIMessage

)
 
messages = [
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="Hi AI, how are you today?"),
  AIMessage(content="I'm great thank you. How can I help you?"),
  HumanMessage(content="I'd like to understand string relativite by eninstein.")

]
 
res = chat(messages)

print(res.content)

messages.append(res)
 
prompt = HumanMessage(
  content="Why is the shortes distance between points in a space is a curved line'?"

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="What is so special about Llama 2?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="Can you tell me about the LLMChain in LangChain?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
llmchain_information = [
  "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",
  "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",
  "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]
 
source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"
 
augmented_prompt = f"""Using the contexts below, answer the query.
 
Contexts:

{source_knowledge}
 
Query: {query}"""

prompt = HumanMessage(
  content=augmented_prompt

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
import PyPDF2
 
with open('../data/10 Academy Cohort A - Weekly Challenge_ Week - 6.pdf', 'rb') as file:
  reader = PyPDF2.PdfReader(file)
  num_pages = len(reader.pages)
  text = ''
  for page_num in range(num_pages):
  page_text = reader.pages[page_num].extract_text()
  text += page_text
 
print(text)
 
import pinecone
 
pinecone.init(
  api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',
  environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time
 
index_name = 'llama-2-rag'
 
if index_name not in pinecone.list_indexes():
  pinecone.create_index(
  index_name,
  dimension=1536,
  metric='cosine'
  )
  while not pinecone.describe_index(index_name).status['ready']:
  time.sleep(1)
 
index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings
 
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [
  'this is the first chunk of text',
  'then another second chunk of text is here'

]
 
res = embed_model.embed_documents(texts)

len(res), len(res[0])
 
from tqdm.auto import tqdm  
data = dataset.to_pandas()  
batch_size = 100
 
for i in tqdm(range(0, len(data), batch_size)):
  i_end = min(len(data), i+batch_size)
  batch = data.iloc[i:i_end]
  ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]
  texts = [x['chunk'] for _, x in batch.iterrows()]
  embeds = embed_model.embed_documents(texts)
  metadata = [
  {'text': x['chunk'],
  'source': x['source'],
  'title': x['title']} for i, x in batch.iterrows()
  ]
  index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone
 
text_field = "text"  
vectorstore = Pinecone(
  index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"
 
vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):
  results = vectorstore.similarity_search(query, k=3)
  source_knowledge = "\n".join([x.page_content for x in results])
  augmented_prompt = f"""Using the contexts below, answer the query.
  Contexts:
  {source_knowledge}
  Query: {query}"""
  return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(
  content=augment_prompt(query)

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
prompt = HumanMessage(
  content="what safety measures were used in the development of llama 2?"

)
 
res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(
  content=augment_prompt(
  "what safety measures were used in the development of llama 2?"
  )

)
 
res = chat(messages + [prompt])

print(res.content)
import requests

from langchain.document_loaders import TextLoader

from langchain.text_splitter import CharacterTextSplitter  
from langchain.embeddings import OpenAIEmbeddings

import pinecone

from dotenv import load_dotenv, find_dotenv

from langchain.chat_models import ChatOpenAI

from langchain.prompts import ChatPromptTemplate

from langchain.schema.runnable import RunnablePassthrough

from langchain.schema.output_parser import StrOutputParser

def data_loader(file_path= '../prompts/Weekly_Challenge_Week_6.txt'):
  loader = TextLoader(file_path)
  documents = loader.load()
  text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  chunks = text_splitter.split_documents(documents)
  return chunks
data_loader()
chunks =  data_loader()
import os
 
pinecone.init(
  api_key=os.environ.get('7663f65c-a68e-4f31-a9a8-cf6d9e7bdac2'),
  environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time
 
index_name = '10-acadamy'
 
if index_name not in pinecone.list_indexes():
  pinecone.create_index(
  index_name,
  dimension=1536,
  metric='cosine'
  )
  while not pinecone.describe_index(index_name).status['ready']:
  time.sleep(1)
 
index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings
 
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
import pandas as pd

data = pd.DataFrame(chunks)
from tqdm.auto import tqdm  
import pandas as pd
 
data = pd.DataFrame([{'page_content': doc.page_content, 'source': doc.metadata['source']} for doc in chunks])
 
batch_size = 100
 
for i in tqdm(range(0, len(data), batch_size)):
  i_end = min(len(data), i + batch_size)
  batch = data.iloc[i:i_end]
  ids = [f"{i}-{source}" for i, source in enumerate(batch['source'], start=i)]
  texts = batch['page_content'].tolist()
  embeds = embed_model.embed_documents(texts)
  metadata = [{'text': text, 'source': source} for text, source in zip(texts, batch['source'])]
  index.upsert(vectors=zip(ids, embeds, metadata))
import time

import os

import pinecone
 
def create_retriever(chunks):
  load_dotenv(find_dotenv())
  pinecone.init(
  api_key=os.environ.get('7663f65c-a68e-4f31-a9a8-cf6d9e7bdac2'),
  environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'
  )
  index_name = "my-index"
  pinecone.create_index(
  index_name,
  dimension=1536,
  metric='cosine'
  )
  while not pinecone.describe_index(index_name).status['ready']:
  time.sleep(1)
  index = pinecone.Index(index_name)
  for chunk in chunks:
  embeddings = OpenAIEmbeddings().embed_query(chunk)
  index.upsert(ids=[chunk], vectors=embeddings)
  retriever = index.as_retriever()
  return retriever
chunks =  data_loader()

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
 
template = """You are an assistant for question-answering tasks.  
Use the following pieces of retrieved context to answer the question.  
If you don't know the answer, just say that you don't know.  
Use two sentences maximum and keep the answer concise.

Question: {question}  
Context: {context}  
Answer:

"""
 
prompt = ChatPromptTemplate.from_template(template)
 
rag_chain = (
  {"context": retriever,  "question": RunnablePassthrough()}   | prompt   | llm
  | StrOutputParser()  
)
/* Basic Reset */
* {
  margin: 5;
  padding: 5;
  box-sizing: border-box;
}

/* Body Styles */
body {
  font-family: 'Arial', sans-serif;
  background-color:   color:   line-height: 1.5;
  padding: 30px;
}

/* Header Styles */
header {
  background:   color: white;
  padding: 10px 0;
  text-align: center;
}

header h1 {
  margin: 0;
}

/* Navigation Menu Styles */
nav ul {
  list-style: none;
  background-color:   text-align: center;
  padding: 10px;
  margin: 0;
}

nav ul li {
  display: inline;
}

nav ul li a {
  text-decoration: none;
  color: white;
  padding: 10px 15px;
  display: inline-block;
}

nav ul li a:hover {
  background-color: }

/* Main Content Styles */
.container {
  width: 90%;
  margin: auto;
  overflow: hidden;
}

/* Button Styles */
button {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  cursor: pointer;
  border-radius: 4px;
}

button:hover {
  background-color: }

/* Form Styles */
form {
  background-color:   padding: 30px;
  border-radius: 5px;
  margin-left: 5%;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

form label {
  margin-bottom: 5px;
  display: block;
}

form input[type="text"],
form input[type="email"],
form input[type="password"] {
  width: 100%;
  padding: 8px;
  margin-bottom: 10px;
  border: 1px solid   border-radius: 4px;
}

form input[type="submit"] {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  border-radius: 4px;
  cursor: pointer;
}

form input[type="submit"]:hover {
  background-color: }

/* Footer Styles */
footer {
  background:   color: white;
  text-align: center;
  padding: 10px 0;
  margin-top: 20px;
}

/* Utility Classes */
.clearfix::after {
  content: "";
  display: table;
  clear: both;
}

/* Responsive Design */
@media screen and (max-width: 768px) {
  .container,
  nav ul li,
  nav ul li a {
  width: 100%;
  }
  nav ul li {
  display: block;
  }
}
<!DOCTYPE html>
<html>
<head>
  <title>Prompt Result</title>
  <style>
  body {
  font-family: Arial, sans-serif;
  background-color:   margin: 0;
  padding: 0;
  color:   }
  .container {
  width: 80%;
  margin: auto;
  overflow: hidden;
  }
  header {
  background:   color: white;
  padding-top: 30px;
  min-height: 70px;
  border-bottom:   }
  header a {
  color:   text-decoration: none;
  text-transform: uppercase;
  font-size: 16px;
  }
  header ul {
  padding: 0;
  margin: 0;
  list-style: none;
  overflow: hidden;
  }
  header li {
  float: left;
  display: inline;
  padding: 0 20px 0 20px;
  }
  header   float: left;
  }
  header   margin: 0;
  }
  header nav {
  float: right;
  margin-top: 10px;
  }
  header .highlight, header .current a {
  color:   font-weight: bold;
  }
  header a:hover {
  color:   font-weight: bold;
  }
  .scrollable-paragraph {
  width: 60%; /* Narrow width */
  max-height: 400px; /* Maximum height */
  margin: 20px auto; /* Centering the paragraph */
  overflow-y: scroll; /* Enables vertical scrolling */
  padding: 15px;
  border: 1px solid   background-color: white;
  }
  </style>
</head>
<body>
  <header>
  <div class="container">
  <div id="branding">
  <h1>Prompt Generator</h1>
  </div>
  <nav>
  <ul>
  <li class="current"><a href="{% url 'generate' %}">Home</a></li>
  </ul>
  </nav>
  </div>
  </header>
  <div class="container">
  <h2>Generated Prompt</h2>
  <div class="scrollable-paragraph">
  {{ generated_prompt }}
  </div>
  <a href="{% url 'generate' %}">Generate another prompt</a>
  </div>
</body>
</html>
/* Basic Reset */
* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

/* Body Styles */
body {
  font-family: 'Arial', sans-serif;
  background-color:   color:   line-height: 1.5;
  padding: 20px;
}

/* Header Styles */
header {
  background:   color: white;
  padding: 10px 0;
  text-align: center;
}

header h1 {
  margin: 0;
}

/* Navigation Menu Styles */
nav ul {
  list-style: none;
  background-color:   text-align: center;
  padding: 10px;
  margin: 0;
}

nav ul li {
  display: inline;
}

nav ul li a {
  text-decoration: none;
  color: white;
  padding: 10px 15px;
  display: inline-block;
}

nav ul li a:hover {
  background-color: }

/* Main Content Styles */
.container {
  width: 90%;
  margin: auto;
  overflow: hidden;
}

/* Button Styles */
button {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  cursor: pointer;
  border-radius: 4px;
}

button:hover {
  background-color: }

/* Form Styles */
form {
  background-color:   padding: 15px;
  border-radius: 5px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

form label {
  margin-bottom: 5px;
  display: block;
}

form input[type="text"],
form input[type="email"],
form input[type="password"] {
  width: 100%;
  padding: 8px;
  margin-bottom: 10px;
  border: 1px solid   border-radius: 4px;
}

form input[type="submit"] {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  border-radius: 4px;
  cursor: pointer;
}

form input[type="submit"]:hover {
  background-color: }

/* Footer Styles */
footer {
  background:   color: white;
  text-align: center;
  padding: 10px 0;
  margin-top: 20px;
}

/* Utility Classes */
.clearfix::after {
  content: "";
  display: table;
  clear: both;
}

/* Responsive Design */
@media screen and (max-width: 768px) {
  .container,
  nav ul li,
  nav ul li a {
  width: 100%;
  }
  nav ul li {
  display: block;
  }
}
from django.shortcuts import HttpResponse, render

from prompt_app.utils.prompt_gen import get_prompt
from .utils.embed_text import embed_text
from .utils.similarity import cosine_similarity

from .models import Prompt, TextChunk
 
def generate_prompt_from_vector(text_chunk, user_question):
  prompt = text_chunk + " \n based on the above data, give an answer to \
  the following question. restrict yourself to the above data only. \
  if you can't get an answer based on the data, you can feel free to \
  say i don't know. here is the question. \n" + user_question +\
  "DONOT add anything from yourself."
  return prompt
 
def generate_prompt(request):
  if request.method == 'POST':
  input_text = request.POST.get('input_text')
  embeded_question = embed_text([input_text])[0]
  highest_similarity = -1
  best_text_chunk = None
  for text_chunk in TextChunk.objects.all():
  similarity = cosine_similarity(embeded_question, text_chunk.embed)
  if similarity > highest_similarity:
  highest_similarity = similarity
  best_text_chunk = text_chunk.chunk
  if best_text_chunk is not None:
  generated_prompt = generate_prompt_from_vector(best_text_chunk, input_text)
  return render(request, 'prompt_app/prompt_result.html', {'generated_prompt': generated_prompt})
  else:
  return HttpResponse("No similar documents found.")
  else:
  return render(request, 'prompt_app/generate_prompt.html')
import fitz  
def extract_text_from_pdf(pdf_path):
  document = fitz.open(pdf_path)
  full_text = ""
  for page_num in range(len(document)):
  page = document.load_page(page_num)
  text = page.get_text()
  full_text += text
  document.close()
  return full_text
 
def chunk_text(text, words_per_chunk, overlap_size):
  words = text.split()
  chunks = []
  i = 0
  while i < len(words):
  end = i + words_per_chunk
  chunk = ' '.join(words[i:end])
  chunks.append(chunk)
  i = end - overlap_size if end - overlap_size > i else end
  return chunks
 
if __name__ == "__main__":
  pdf_path = "data/week6_data.pdf"
  extracted_text = extract_text_from_pdf(pdf_path)
  chunks = chunk_text(extracted_text, 150, 5)
  print(len(chunks))
from collections.abc import Iterable
from django.db import models
from .utils.chunk_pdf import extract_text_from_pdf, chunk_text
from .utils.embed_text import embed_text
import logging

logger = logging.getLogger(__name__)
 
class Prompt(models.Model):
  input_text = models.TextField()
  length_of_line = models.IntegerField(null=True)
  generated_prompt = models.TextField(null=True)
  def __str__(self):
  return f"Prompt: {self.input_text[:50]}..."  
class Document(models.Model):
  pdf_file = models.FileField(upload_to='pdfs/')
  created_at = models.DateTimeField(auto_now_add=True)
  updated_at = models.DateTimeField(auto_now=True)
  def __str__(self):
  return f"PDF File uploaded on {self.created_at}"
  def save(self, *args, **kwargs):
  super().save(*args, **kwargs)
  if self.pdf_file:   text = extract_text_from_pdf(self.pdf_file.path)
  chunks = chunk_text(text)
  embeds = embed_text(chunks)
  for chunk, embed in zip(chunks, embeds):
  TextChunk.objects.create(document=self, chunk=chunk, embed=embed)
  else:
  logger.error("No PDF file to process.")

class TextChunk(models.Model):
  document = models.ForeignKey(Document, on_delete=models.CASCADE)
  chunk = models.TextField()
  embed = models.JSONField(blank=True)
  created_at = models.DateTimeField(auto_now_add=True)
  updated_at = models.DateTimeField(auto_now=True)
from django.contrib import admin
 
from django.contrib import admin
from .models import Document, TextChunk

class DocumentAdmin(admin.ModelAdmin):
  list_display = ('pdf_file', 'created_at', 'updated_at')
  search_fields = ('pdf_file',)
  readonly_fields = ('created_at', 'updated_at')

class TextChunkAdmin(admin.ModelAdmin):
  list_display = ('chunk', 'embed', 'updated_at')
 
admin.site.register(Document, DocumentAdmin)
admin.site.register(TextChunk, TextChunkAdmin)
import fitz  
def extract_text_from_pdf(pdf_path):
  document = fitz.open(pdf_path)
  full_text = ""
  for page_num in range(len(document)):
  page = document.load_page(page_num)
  text = page.get_text()
  full_text += text
  document.close()
  return full_text
 
def chunk_text(text, words_per_chunk=500, overlap_size=20):
  words = text.split()
  chunks = []
  i = 0
  while i < len(words):
  end = i + words_per_chunk
  chunk = ' '.join(words[i:end])
  chunks.append(chunk)
  i = end - overlap_size if end - overlap_size > i else end
  return chunks
 
if __name__ == "__main__":
  pdf_path = "data/week6_data.pdf"
  extracted_text = extract_text_from_pdf(pdf_path)
  chunks = chunk_text(extracted_text, 150, 5)
  print(len(chunks))
from langchain.embeddings.openai import OpenAIEmbeddings
from dotenv import load_dotenv

from . chunk_pdf import extract_text_from_pdf, chunk_text

load_dotenv()

def embed_text(chunks):
  embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
  embeds = embed_model.embed_documents(chunks)
  return embeds
 
if __name__ == "__main__":
  pdf_path = "data/week6_data.pdf"
  text = extract_text_from_pdf(pdf_path)
  chunks = chunk_text(text, 150, 5)
  embeds = embed_text(chunks)
  print(embeds)
"""
Django settings for prompt_generator project.

Generated by 'django-admin startproject' using Django 5.0.1.

For more information on this file, see
https://docs.djangoproject.com/en/5.0/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/5.0/ref/settings/
"""

from pathlib import Path
import os

BASE_DIR = Path(__file__).resolve().parent.parent
 
SECRET_KEY = 'django-insecure-j1ut4aojbrfo13kkj
DEBUG = True

ALLOWED_HOSTS = []
 
INSTALLED_APPS = [
  'django.contrib.admin',
  'django.contrib.auth',
  'django.contrib.contenttypes',
  'django.contrib.sessions',
  'django.contrib.messages',
  'django.contrib.staticfiles',
  'prompt_app',
]

MIDDLEWARE = [
  'django.middleware.security.SecurityMiddleware',
  'django.contrib.sessions.middleware.SessionMiddleware',
  'django.middleware.common.CommonMiddleware',
  'django.middleware.csrf.CsrfViewMiddleware',
  'django.contrib.auth.middleware.AuthenticationMiddleware',
  'django.contrib.messages.middleware.MessageMiddleware',
  'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'prompt_generator.urls'

TEMPLATES = [
  {
  'BACKEND': 'django.template.backends.django.DjangoTemplates',
  'DIRS': [],
  'APP_DIRS': True,
  'OPTIONS': {
  'context_processors': [
  'django.template.context_processors.debug',
  'django.template.context_processors.request',
  'django.contrib.auth.context_processors.auth',
  'django.contrib.messages.context_processors.messages',
  ],
  },
  },
]

WSGI_APPLICATION = 'prompt_generator.wsgi.application'
 
DATABASES = {
  'default': {
  'ENGINE': 'django.db.backends.sqlite3',
  'NAME': BASE_DIR / 'db.sqlite3',
  }
}
 
AUTH_PASSWORD_VALIDATORS = [
  {
  'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
  },
  {
  'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
  },
  {
  'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
  },
  {
  'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
  },
]
 
LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_TZ = True
 
STATIC_URL = '/static/'
STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')
 
DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'
 
iLOGGING = {
  'version': 1,   'disable_existing_loggers': False,
  'handlers': {
  'file': {
  'level': 'DEBUG',
  'class': 'logging.FileHandler',
  'filename': '/path/to/your/logfile.log',
  },
  },
  'loggers': {
  'django': {
  'handlers': ['file'],
  'level': 'DEBUG',
  'propagate': True,
  },
  },
}
import os

from langchain.chat_models import ChatOpenAI

from langchain.schema import SystemMessage, HumanMessage,  AIMessage
 
OPENAI_KEY = os.getenv("OPENAI_API_KEY") 
chat = ChatOpenAI(
  openai_api_key=OPENAI_KEY,
  model='gpt-3.5-turbo'

)
 
messages = [
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="Hi AI, how are you today?"),
  AIMessage(content="I'm great thank you. How can I help you?"),
  HumanMessage(content="I'd like to understand string theory.")

]
 
res = chat(messages)

res
import os

from langchain.chains import RetrievalQA

from langchain_community.document_loaders import PyPDFLoader

from langchain.embeddings import OpenAIEmbeddings

from langchain.llms import OpenAI

from langchain.text_splitter import CharacterTextSplitter

from langchain.vectorstores import Chroma

def create_qa_model():
  loader = PyPDFLoader("./data/RAG.pdf")
  documents = loader.load()
  text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
  texts = text_splitter.split_documents(documents)
  print(len(texts))
  embeddings = OpenAIEmbeddings()
  db = Chroma.from_documents(texts, embeddings)
  retriever = db.as_retriever( search_kwargs={"k": 3})
  return  retriever
create_qa_model()
from langchain.chat_models import ChatOpenAI

from langchain.chains import RetrievalQA
 
retriver = create_qa_model()

primary_qa = ChatOpenAI(model_name='gpt-3.5-turbo-16k' )

qa_chain = RetrievalQA.from_chain_type(primary_qa,retriever = retriver, return_source_documents= True)
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from load_to_chroma import Load_VD
import os
 
class Chat:
  def __init__(self) -> None:
  pass
  def connect_openai():
  OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
  chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model="gpt-3.5-turbo-16k")
  return chat
  def qa_chain(self):
  retriver = Load_VD()
  primary_qa = self.connect_openai()
  qa_chain = RetrievalQA.from_chain_type(
  primary_qa, retriver, return_source_documents=True
  )
  return qa_chain
 
Chat
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
import os
 
def Load_VD(file):
  loader = PyPDFLoader(file)
  documents = loader.load()
  text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
  texts = text_splitter.split_documents(documents)
  print(len(texts))
  embeddings = OpenAIEmbeddings()
  db = Chroma.from_documents(texts, embeddings)
  retriever = db.as_retriever(search_kwargs={"k": 3})
  return retriever
!pip install pinecone-client langchain cohere
!pip install openai==0.27.1
from pinecone import pinecone

import os
 
from langchain.vectorstores import Pinecone

from langchain.embeddings.cohere import CohereEmbeddings

import openai

from langchain.embeddings.openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain.document_loaders import DirectoryLoader, TextLoader
 
openai_api_key = os.getenv("OPEN_API_KEY")

pinecone_api_key = os.getenv("PINECONE_API_KEY")

cohere_api_key = os.getenv("COHERE_API_KEY")
 
loader = TextLoader("/content/drive/MyDrive/10academy/rags/rags doc.txt")

document = loader.load()
doc = "/content/drive/MyDrive/10academy/rags/rags doc.txt"
with open(doc, 'r') as file:
  doc_content = file.read()
 
print(doc_content)

splitter = CharacterTextSplitter(
  chunk_size = 500,
  chunk_overlap = 100

)
split_docs = splitter.split_documents(document)

print(split_docs)

embeddings = CohereEmbeddings( model = "multilingual-22-12", cohere_api_key= cohere_api_key)

texts = [doc.page_content for doc in split_docs]
 
embedded = embeddings.embed(texts)

print(embedded)

key3 = os.getenv("OPEN_API_KEY")

openai.api_key=key3
MODEL = "text-embedding-ada-002"
 
res = openai.Embedding.create(
  input= texts,
  engine=MODEL, api_key = openai.api_key

)

res
from pinecone import Pinecone
 
api_key = os.getenv("PINECONE_API_KEY")
 
pc = Pinecone(api_key=api_key)

import time

from pinecone import ServerlessSpec
 
index_name = 'rag-cohere1'

existing_indexes = [
  index_info["name"] for index_info in pc.list_indexes()

]
 
if index_name not in existing_indexes:
  pc.create_index(
  index_name,
  dimension=768,   metric='cosine',
  spec=ServerlessSpec(
  cloud="aws",
  region="us-west-2"
  )
  )
  while not pc.describe_index(index_name).status['ready']:
  time.sleep(1)
 
index = pc.Index(index_name)

time.sleep(1)
 
index.describe_index_stats()
from uuid import uuid4
 
ids = [str(uuid4()) for _ in range(len(texts))]

documents = [{'id': id, 'values': vector} for id, vector in zip(ids, embedded)]

index.upsert(documents)
print(res.keys())

data_values = res['data']
 
for value in data_values[1].items():
  print(f"{value}")

print(data_values[1]['embedding'])

vector_data = []

for i, data in enumerate(data_values):
  embedding = data.get('embedding')
  if embedding is not None:
  embedding_list = np.array(embedding).tolist()
  vector_data.append({'id': str(i), 'values': embedding_list})
print(vector_data)
embeds = [record['embedding'] for record in res['data']]

len(embeds)
ids = str(range(len(embeds)))

to_upsert = zip(ids, embeds)

index = pc.Index("rag-cohere1")
 
index.upsert(vectors = vector_data )

query = "What is the business objective of the challenge?"
 
ques = openai.Embedding.create(input = query, engine=MODEL, api_key = openai.api_key

)['data'][0]['embedding']

print(ques)

index.fetch(["25"])
function App() {
  return (
  <>
  <h1 className="text-2xl font-bold underline">Hello world!</h1>
  </>
  );
}

export default App;
@tailwind base;
@tailwind components;
@tailwind utilities;
import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.tsx";
import "./index.css";
import { BrowserRouter } from "react-router-dom";

ReactDOM.createRoot(document.getElementById("root")!).render(
  <React.StrictMode>
  <BrowserRouter>
  <App />
  </BrowserRouter>
  </React.StrictMode>
);
import os

from langchain.chat_models import ChatOpenAI
 
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or "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"
 
chat = ChatOpenAI(
  openai_api_key="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",
  model='gpt-3.5-turbo'

)
 
from langchain.schema import (
  SystemMessage,
  HumanMessage,
  AIMessage

)
 
messages = [
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="Hi AI, how are you today?"),
  AIMessage(content="I'm great thank you. How can I help you?"),
  HumanMessage(content="I'd like to understand string theory.")

]
 
res = chat(messages)

res
print(res.content)

messages.append(res)
 
prompt = HumanMessage(
  content="Why do physicists believe it can produce a 'unified theory'?"

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="What is so special about Llama 2?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="Can you tell me about the LLMChain in LangChain?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
llmchain_information = [
  "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",
  "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",
  "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]
 
source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"
 
augmented_prompt = f"""Using the contexts below, answer the query.
 
Contexts:

{source_knowledge}
 
Query: {query}"""

prompt = HumanMessage(
  content=augmented_prompt

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
from datasets import load_dataset
 
dataset = load_dataset(
  "jamescalam/llama-2-arxiv-papers-chunked",
  split="train"

)
 
dataset
dataset[0]
 
import pinecone
 
pinecone.init(
  api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',
  environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time
 
index_name = 'llama-2-rag'
 
if index_name not in pinecone.list_indexes():
  pinecone.create_index(
  index_name,
  dimension=1536,
  metric='cosine'
  )
  while not pinecone.describe_index(index_name).status['ready']:
  time.sleep(1)
 
index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings
 
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [
  'this is the first chunk of text',
  'then another second chunk of text is here'

]
 
res = embed_model.embed_documents(texts)

len(res), len(res[0])
 
from tqdm.auto import tqdm  
data = dataset.to_pandas()  
batch_size = 100
 
for i in tqdm(range(0, len(data), batch_size)):
  i_end = min(len(data), i+batch_size)
  batch = data.iloc[i:i_end]
  ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]
  texts = [x['chunk'] for _, x in batch.iterrows()]
  embeds = embed_model.embed_documents(texts)
  metadata = [
  {'text': x['chunk'],
  'source': x['source'],
  'title': x['title']} for i, x in batch.iterrows()
  ]
  index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone
 
text_field = "text"  
vectorstore = Pinecone(
  index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"
 
vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):
  results = vectorstore.similarity_search(query, k=3)
  source_knowledge = "\n".join([x.page_content for x in results])
  augmented_prompt = f"""Using the contexts below, answer the query.
  Contexts:
  {source_knowledge}
  Query: {query}"""
  return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(
  content=augment_prompt(query)

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
prompt = HumanMessage(
  content="what safety measures were used in the development of llama 2?"

)
 
res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(
  content=augment_prompt(
  "what safety measures were used in the development of llama 2?"
  )

)
 
res = chat(messages + [prompt])

print(res.content)
%reload_ext autoreload

%autoreload 2
import os

from langchain.chat_models import ChatOpenAI

from dotenv import load_dotenv, find_dotenv

load_dotenv()

openai_key = os.getenv("OPENAI_API_KEY")  
chat = ChatOpenAI(
  openai_api_key=openai_key,   model='gpt-3.5-turbo'

)
from langchain.schema import (
  SystemMessage,
  HumanMessage,
  AIMessage

)
 
messages = [
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="Hi AI, how are you today?"),
  AIMessage(content="I'm great thank you. How can I help you?"),
  HumanMessage(content="I'd like to understand string theory.")

]
res = chat(messages)
 
res

messages.append(res)
 
prompt = HumanMessage(
  content="Why do physicists believe it can produce a 'unified theory'?"

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
import PyPDF2

import os  
with open("/home/hp/Documents/week 5/precision-RAG-prompt-tuning/prompt_generation/challenge.pdf",'rb') as file:
  pdf_reader = PyPDF2.PdfReader(file)
  num_pages = len(pdf_reader.pages)
  for page in range(num_pages):
  page_obj = pdf_reader.pages[page]
  extracted_text = page_obj.extract_text()
 
len(extracted_text)

chunk_size=50

overlap=20

def chunk_text(extracted_text, chunk_size, overlap):
  chunks = []
  text_length = len(extracted_text)
  for start in range(0, text_length, chunk_size - overlap):
  end = min(start + chunk_size, text_length)
  chunk = extracted_text[start:end]
  chunks.append(chunk)
  return chunks
 
chunks = chunk_text(extracted_text, chunk_size, overlap)
 
for i, chunk in enumerate(chunks):
  print(f"Chunk {i + 1}:", chunk)
chunks = chunk_text(extracted_text, chunk_size, overlap)
 
print(chunks[0])

len(chunks)
from pinecone import Pinecone
 
pc = Pinecone(api_key="cc6b9914-0016-4fa4-ab44-f82fe08434b9")

index = pc.Index("mekdes-index")
from langchain.embeddings.openai import OpenAIEmbeddings
 
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
res = embed_model.embed_documents(chunks)

len(res), len(res[0])

import time

import hashlib  
def generate_unique_id_for_chunk(chunk):
  timestamp = str(time.time())   chunk_hash = hashlib.sha256(chunk.encode()).hexdigest()   unique_id = f"{timestamp}_{chunk_hash}"
  return unique_id

for chunk, embedding in zip(chunks, res):
  document_id = generate_unique_id_for_chunk(chunk)   index.upsert(vectors=[(document_id, embedding)])
 
index_name = "mekdes-index"

query_vector = [0.1, 0.2, 0.3]  
top_k = 5  
import pinecone  
index = pinecone.Index(index_name, host='https://mekdes-index-nn3xpxm.svc.gcp-starter.pinecone.io')
 
from langchain.vectorstores import Pinecone
 
text_field = "text"  
vectorstore = Pinecone(
  index, embed_model.embed_query, text_field

)
def generate_prompt_from_vector(chunks, user_question):
  prompt = chunks + " \n based on the above data, give an answer to \
  the following question. restrict yourself to the above data only. \
  if you can't get an answer based on the data, you can feel free to \
  say i don't know. here is the question. \n" + user_question
  return prompt

import numpy as np
 
def cosine_similarity(embedding1, embedding2):
  return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
query = "What is so special about Llama 2?"
 
cosine_similarity(query, embedding)
def generate_prompt(request):
  if request.method == 'POST':
  input_text = request.POST.get('input_text')
  embeded_question = embed_text([input_text])[0]
  highest_similarity = -1
  best_text_chunk = None
  for text_chunk in chunks.objects.all():
  similarity = cosine_similarity(embeded_question, text_chunk.embed)
  if similarity > highest_similarity:
  highest_similarity = similarity
  best_text_chunk = text_chunk.chunk
  if best_text_chunk is not None:
  generated_prompt = generate_prompt_from_vector(best_text_chunk, input_text)
  return render(request, 'prompt_app/prompt_result.html', {'generated_prompt': generated_prompt})
  else:
  return HttpResponse("No similar documents found.")
  else:
  return render(request, 'prompt_app/generate_prompt.html')  
import os

from langchain.chat_models import ChatOpenAI
 
with open(r"C:\Users\HP\Desktop\week_six_Api_key.txt", 'r') as file:
  API_key = file.read().strip()

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or API_key
 
chat = ChatOpenAI(
  openai_api_key=API_key,
  model='gpt-3.5-turbo'

)
from langchain.schema import (
  SystemMessage,
  HumanMessage,
  AIMessage

)
 
messages = [
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="Hi AI, how are you today?"),
  AIMessage(content="I'm great thank you. How can I help you?"),
  HumanMessage(content="I'd like to understand this week 10 Academy challenge")

]
res = chat(messages)

res
print(res.content)

messages.append(res)
 
prompt = HumanMessage(
  content="What is first task of it"

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
week_six = ["Learning Outcomes of week six challeng",
  "Skills Development",
  "Prompt Engineering Proficiency: Gain expertise in crafting effective prompts that guide LLMs to desired outputs, understanding nuances and variations in language that impact model responses.",
  "Critical Analysis: Develop the ability to critically analyze and evaluate the effectiveness of different prompts based on their performance in varied scenarios.",
  "Technical Aptitude with LLMs: Enhance technical skills in using advanced language models like GPT-4 and GPT-3.5-Turbo, understanding their functionalities and capabilities.",
  "Problem-Solving and Creativity: Cultivate creative problem-solving skills by generating innovative prompts and test cases, addressing complex and varied objectives.",
  "Data Interpretation: Learn to interpret and analyze data from test cases and prompt evaluations, deriving meaningful insights from performance metrics.",
  "Knowledge Acquisition",
  "Understanding of Language Models: Acquire a deeper understanding of how LLMs function, including their strengths, limitations, and the principles behind their responses.",
  "Insights into Automated Evaluation Data Generation: Gain knowledge about the methodology and importance of creating test cases for evaluating prompt effectiveness.",
  "ELO Rating System and its Applications: Learn about the ELO rating system used for ranking prompts, understanding its mechanics and relevance in performance evaluation.",
  "Prompt Optimization Strategies: Understand various strategies for refining and optimizing prompts to achieve better alignment with specific goals and desired outcomes.",
  "Industry Best Practices: Familiarize with the best practices in prompt engineering within different industries, learning about real-world applications and challenges."
 
]
 
source_knowledge = "\n".join(week_six)
query = "Can you tell me about skills that week six develop?"
 
augmented_prompt = f"""Using the contexts below, answer the query.
 
Contexts:

{source_knowledge}
 
Query: {query}"""
 
prompt = HumanMessage(
  content=augmented_prompt

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content) 
from langchain_community.document_loaders import HuggingFaceDatasetLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.storage import LocalFileStore
 
from langchain_openai import ChatOpenAI, OpenAI

from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings

from langchain_community.vectorstores import Chroma

from langchain.prompts import ChatPromptTemplate

from langchain_core.output_parsers import StrOutputParser

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from tqdm.auto import tqdm

from langchain import text_splitter

from langchain import PromptTemplate

from langchain.prompts.chat import (
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,

)

from langchain.chains import LLMChain

from langchain.chains import RetrievalQA

from langchain.document_loaders import PyPDFLoader

from langchain.text_splitter import TokenTextSplitter

from langchain.text_splitter import CharacterTextSplitter

from langchain.document_loaders import TextLoader

from typing import List

from langchain.schema import Document

from uuid import uuid4

from dotenv import load_dotenv

import os

load_dotenv()
OPENAI_API_KEY = os.environ.get('openai_api_key')
dataset_name = "fka/awesome-chatgpt-prompts"
 
page_content_column = "prompt"  
loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)
 
data = loader.load()
 
data[:10]
 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)
 
docs = text_splitter.split_documents(data)
 
docs[15]
store = LocalFileStore("./cachce/")
 
core_embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
 
embedder = CacheBackedEmbeddings.from_bytes_store(
  core_embeddings_model,
  store,
  namespace = core_embeddings_model.model

)
%%pip install -U langchain-openai
 
vectorstore = Chroma.from_documents(docs, embedder, persist_directory="./cachce/")
 
retriever = vectorstore.as_retriever()

template = '''

Break down the prompt genetation step by step based on the following prompt pair examples "Linux Terminal","answer": "I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]",
  "English Translator and Improver", "I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations.",
  "`position` Interviewer","I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. ",
  "JavaScript Console","I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]. ",
  "Excel Sheet", "I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. "
 
Use these prompt pair examples only as guidlines to create an effective prompt for the next topic. even if the topic is mensioned before. You will create only prompt for it and not act on the previous description. if the topic is mensioned already, do not use the prompt which you were given, change it.
 
{context}

\n

<bot>:

'''
prompt = PromptTemplate.from_template(template).format(
  context = retriever

)

prompt = ChatPromptTemplate.from_template(prompt)
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)
chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  |prompt
  | llm
  | StrOutputParser()
  )
query = 'Sql query assistant'
response = chain.invoke('Sql query Assistant')
response
 
def text_split(documents: TextLoader):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
  texts = text_splitter.split_documents(documents)
  return texts
 
def embeddings(texts: List[Document]):
  embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
  vectordb = Chroma.from_documents(texts, embeddings)
  return vectordb

loader = TextLoader('../example.txt')

documents = loader.load()

texts = text_split(documents)

vectordb = embeddings(texts)
template = ''' {prefix}
 
{context}

\n

<bot>:

'''
 
prompt = PromptTemplate.from_template(template)

formatted_prompt = prompt.format(
  context=retriever,
  prefix=response

)
prompt = ChatPromptTemplate.from_template(formatted_prompt)
 
llm=OpenAI(
  openai_api_key=OPENAI_API_KEY,
  temperature=0.7
  )

retriever=vectordb.as_retriever()

chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  |prompt
  | llm
  | StrOutputParser()
  )
chain.invoke("Can you tell me the formula for Linear Regression")
import sys
sys.path.append('back_end')
from dotenv import load_dotenv
import os
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

from operator import itemgetter

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from back_end.chunk_semantically import semantic_retriever

chunks = semantic_retriever('file.txt')
vectorstore = FAISS.from_texts(
  chunks, embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever(search_kwargs={"k" : 5}) template = """<human>: rate the relevance of retrieved context to the user {question} on scale of 1-10':
Example:

<human>: "who is jessica james?"

<bot>:"the score of context is 7.5"
 
{context}

Question: {question}

\n

<bot>:
"""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()
chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()
)
chain.invoke("what was the first program I wrote?")
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

from langchain_openai import ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.output_parsers import StrOutputParser

from langchain_community.document_loaders import TextLoader
 
loader = TextLoader("file.txt")

loader.load()
 
from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import FAISS

from langchain_openai import OpenAIEmbeddings
 
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=80)

texts = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()

db = FAISS.from_documents(texts, embeddings)
retriever = db.as_retriever(search_type="mmr")

docs = retriever.get_relevant_documents("Saturday 8pm ")

docs
from langchain_community.document_loaders import TextLoader

from langchain_openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import Chroma
 
def data_retriever(file, query):
  raw_documents = TextLoader(file).load()
  text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
  documents = text_splitter.split_documents(raw_documents)
  db = Chroma.from_documents(documents, OpenAIEmbeddings())
  query = query
  docs = db.similarity_search(query)
  for i in range(min(4, len(docs))):   print(docs[i].page_content)
  return None

context = data_retriever('file.txt','brown')
from langchain_openai import ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate

from langchain.schema import StrOutputParser

from langchain_core.runnables import RunnablePassthrough
 
template = """Answer the question based only on the following context:
 
{context}
 
Question: brown

"""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()
 
def format_docs(docs):
  return "\n\n".join([d.page_content for d in docs])
 
chain = (
  {"context": retriever | format_docs, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()

)
 
chain.invoke("What did the president say about technology?")
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
def data_retriever(file, query):
  raw_documents = TextLoader(file).load()
  text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=100,
  chunk_overlap=20,
  length_function=len,
  is_separator_regex=False,
)
  documents = text_splitter.split_documents(raw_documents)
  db = Chroma.from_documents(documents, OpenAIEmbeddings())
  query = query
  docs = db.similarity_search(query)
  for i in range(min(4, len(docs))):   print(docs[i].page_content)
  return None
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
import combine_sentences
import re
def data_retriever(file, query):
  raw_documents = TextLoader(file).load()
  """
  splitting the text recursively for since it is the recommended one for generic text.
  It tries to split on them in order until the chunks are small enough
  """
  text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=1000,
  chunk_overlap=50,
  length_function=len,
  is_separator_regex=False,
)
  documents = text_splitter.split_documents(raw_documents)
  db = Chroma.from_documents(documents, OpenAIEmbeddings())
  query = query
  """
  Assigning score_threshold to 0.5 to retrieve document with above   50% of relevance score, and setting "K" == 5 to retrieve five chunks
  for a single query
  """
  retriever = db.as_retriever(
  search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.65, "k": 5}
)
  docs = retriever.get_relevant_documents(query)
  for i in range(min(4, len(docs))):   print(docs[i].page_content)
  return None
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

from langchain_openai import ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.output_parsers import StrOutputParser

from langchain_community.document_loaders import TextLoader
 
loader = TextLoader("file.txt")

loader.load()
 
from langchain_community.document_loaders import TextLoader

from langchain_openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import Chroma

from langchain.text_splitter import RecursiveCharacterTextSplitter

def data_retriever(file, query):
  raw_documents = TextLoader(file).load()
  text_splitter = RecursiveCharacterTextSplitter(
  chunk_size=1000,
  chunk_overlap=50,
  length_function=len,
  is_separator_regex=False,

)
  documents = text_splitter.split_documents(raw_documents)
  db = Chroma.from_documents(documents, OpenAIEmbeddings())
  query = query
  """Assigning score_threshold to 0.5 to retrieve document with above   50% of relevance score, and setting "K" == 5 to retrieve five chunks
  for a single query
  """
  retriever = db.as_retriever(
  search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.65, "k": 5}

)
  docs = retriever.get_relevant_documents(query)
  for i in range(min(4, len(docs))):   print(docs[i].page_content)
  return None

from langchain_community.document_loaders import TextLoader

from langchain_openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import Chroma
 
def data_retriever(file, query):
  raw_documents = TextLoader(file).load()
  text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
  documents = text_splitter.split_documents(raw_documents)
  db = Chroma.from_documents(documents, OpenAIEmbeddings())
  query = query
  docs = db.similarity_search(query)
  for i in range(min(4, len(docs))):   print(docs[i].page_content)
  return None

context = data_retriever('file.txt','brown')

with open('file.txt') as file:
  essay = file.read()

import re
 
single_sentences_list = re.split(r'(?<=[.?!])\s+', essay)

print (f"{len(single_sentences_list)} senteneces were found")
sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]
def combine_sentences(sentences, buffer_size=1):
  for i in range(len(sentences)):
  combined_sentence = ''
  for j in range(i - buffer_size, i):
  if j >= 0:
  combined_sentence += sentences[j]['sentence'] + ' '
  combined_sentence += sentences[i]['sentence']
  for j in range(i + 1, i + 1 + buffer_size):
  if j < len(sentences):
  combined_sentence += ' ' + sentences[j]['sentence']
  sentences[i]['combined_sentence'] = combined_sentence
  return sentences
 
sentences = combine_sentences(sentences)
sentences[5]
from langchain_openai.embeddings import OpenAIEmbeddings

oaiembeds = OpenAIEmbeddings()
embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])
sentences[0]['combined_sentence']
for i, sentence in enumerate(sentences):
  sentence['combined_sentence_embedding'] = embeddings[i]

from sklearn.metrics.pairwise import cosine_similarity
 
def calculate_cosine_distances(sentences):
  distances = []
  for i in range(len(sentences) - 1):
  embedding_current = sentences[i]['combined_sentence_embedding']
  embedding_next = sentences[i + 1]['combined_sentence_embedding']
  similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
  distance = 1 - similarity
  distances.append(distance)
  sentences[i]['distance_to_next'] = distance
  return distances, sentences
distances, sentences = calculate_cosine_distances(sentences)
import matplotlib.pyplot as plt
 
plt.plot(distances);
import numpy as np
 
plt.plot(distances);
 
y_upper_bound = .2

plt.ylim(0, y_upper_bound)

plt.xlim(0, len(distances))
 
breakpoint_percentile_threshold = 95

breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) 
plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-');
 
num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) 
plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f"{num_distances_above_theshold + 1} Chunks");
 
indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]  
colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

for i, breakpoint_index in enumerate(indices_above_thresh):
  start_index = 0 if i == 0 else indices_above_thresh[i - 1]
  end_index = breakpoint_index if i < len(indices_above_thresh) - 1 else len(distances)
  plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)
  plt.text(x=np.average([start_index, end_index]),
  y=breakpoint_distance_threshold + (y_upper_bound)/ 20,
  s=f"Chunk   rotation='vertical')
 
if indices_above_thresh:
  last_breakpoint = indices_above_thresh[-1]
  if last_breakpoint < len(distances):
  plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)
  plt.text(x=np.average([last_breakpoint, len(distances)]),
  y=breakpoint_distance_threshold + (y_upper_bound)/ 20,
  s=f"Chunk   rotation='vertical')
 
plt.title("PG Essay Chunks Based On Embedding Breakpoints")

plt.xlabel("Index of sentences in essay (Sentence Position)")

plt.ylabel("Cosine distance between sequential sentences")

plt.show()

start_index = 0
 
chunks = []
 
for index in indices_above_thresh:
  end_index = index
  group = sentences[start_index:end_index + 1]
  combined_text = ' '.join([d['sentence'] for d in group])
  chunks.append(combined_text)
  start_index = index + 1
 
if start_index < len(sentences):
  combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])
  chunks.append(combined_text)
 
for i, chunk in enumerate(chunks[:2]):
  buffer = 200
  print (f"Chunk   print (chunk[:buffer].strip())
  print ("...")
  print (chunk[-buffer:].strip())
  print ("\n")
from operator import itemgetter
 
from langchain_community.vectorstores import FAISS

from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
vectorstore = FAISS.from_texts(
  chunks, embedding=OpenAIEmbeddings()

)

retriever = vectorstore.as_retriever(search_kwargs={"k" : 5}) 
relevant_docs = retriever.get_relevant_documents("What are the challenges in evaluating Retrieval Augmented Generation pipelines?")

relevant_docs[0]

for doc in relevant_docs:
  print(doc.page_content)
  print('\n')
template = """<human>: Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':
 
{context}
 
Question: {question}
 
\n
 
<bot>:

"""
 
prompt = ChatPromptTemplate.from_template(template)
 
model = ChatOpenAI()
chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()

)
chain.invoke("what was the first program I wrote?")
from langchain.output_parsers import ResponseSchema

from langchain.output_parsers import StructuredOutputParser

from langchain.output_parsers import ResponseSchema

from langchain.output_parsers import StructuredOutputParser
 
question_schema = ResponseSchema(
  name="question",
  description="a question about the context."

)
 
question_response_schemas = [
  question_schema,

]

question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)
 
format_instructions = question_output_parser.get_format_instructions()

question_generation_llm = ChatOpenAI(model="gpt-3.5-turbo-1106")
 
bare_prompt_template = "{content}"
 
bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)
qa_template = """\

You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.
 
question: a question about the context.
 
Format the output as JSON with the following keys:

question
 
context: {context}

"""
 
prompt_template = ChatPromptTemplate.from_template(template=qa_template)
 
messages = prompt_template.format_messages(
  context=docs[0],
  format_instructions=format_instructions

)
 
question_generation_chain = bare_template | question_generation_llm
 
response = question_generation_chain.invoke({"content" : messages})
 
output_dict = question_output_parser.parse(response.content)

for k, v in output_dict.items():
  print(k)
  print(v)
import sys

from dotenv import load_dotenv

import os

from langchain.chains import LLMChain

from langchain.prompts import PromptTemplate

from langchain_openai import OpenAI

from operator import itemgetter

from langchain_community.vectorstores import FAISS

from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from back_end.chunk_semantically import semantic_retriever
 
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
 
chunks = semantic_retriever('file.txt')

vectorstore = FAISS.from_texts(
  chunks, embedding=OpenAIEmbeddings()

)

retriever = vectorstore.as_retriever(search_kwargs={"k" : 5}) 
template = """<human>:  
context: {context}
 
Question: {question}
 
\n
 
<bot>:rate the relevance of retrieved context to the user question on scale of 1-10'
 
"""
 
prompt = ChatPromptTemplate.from_template(template)
 
model = ChatOpenAI(temperature=0.75)

chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()

)

chain.invoke("what was the first program I wrote?")

template = """<human>: rate the relevance of retrieved context to the user {question} on scale of 1-10'
  and return ONLY the rating value make sure to evaluate each   question and context meticulously. I want the rating to be   consitent for a given {question} each time prompted, make sure   to rate them accurately. minimize the variation of the rating to
  0, so make sure each time prompted you rate them accurately
  you can use vector similarities beween the question and context
  to avoid the variation:
 
the output MUST ALWAYS BE in the following format.
 
example:
  "The relevance of the retrieved context is rating value "
 
{context}
 
Question: {question}
 
\n
 
<bot>:

"""
 
prompt = ChatPromptTemplate.from_template(template)
 
model = ChatOpenAI()

chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()

)

chain.invoke("what was the first program I wrote?")
template = """<human>: rate the relevance of retrieved context to the user {question} on scale of 1-10'
  and return ONLY the rating value make sure to evaluate each   question and context meticulously. I want the rating to be   consitent for a given {question} each time prompted, make sure   to rate them accurately. minimize the variation of the rating to
  0, so make sure each time prompted you rate them accurately
  you can use vector similarities beween the question and context
  to avoid the variation:
 
the output MUST ALWAYS BE in the following format.
 
example:
  "The relevance of the retrieved context is rating value "
 
{context}
 
Question: {question}
 
\n
 
<bot>:

"""
 
prompt = ChatPromptTemplate.from_template(template)
 
model = ChatOpenAI()

chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()

)

chain.invoke("what was the first program I wrote?")
template = """<human>: craft efficient prompt based on {question},   make sure to generate very EFFECTIVE and   PRACTICAL, the prompt should be clear and
  consize and strategic as well.  
{context}
 
Question: {question}
 
\n
 
<bot>:

"""
 
prompt = ChatPromptTemplate.from_template(template)
 
model = ChatOpenAI()

chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()

)

chain.invoke("What was the first program the author wrote on the IBM 1401 computer in 9th grade, and what were the limitations and challenges they encountered while using it?")
from evaluation.evaluation import monte_carlo_eval

from evaluation.evaluation import elo_eval

from evaluation.evaluation import elo_ratings_func
 
elo_rating=elo_eval(prompt)

elo_ratings_func(prompt,elo_rating)

monte_carlo_eval(prompt)
from api_endpoint import prompt_return

prompt_return("who is charlie jackson?")
def combine_sentences(sentences, buffer_size=1):
  for i in range(len(sentences)):
  combined_sentence = ''
  for j in range(i - buffer_size, i):
  if j >= 0:
  combined_sentence += sentences[j]['sentence'] + ' '
  combined_sentence += sentences[i]['sentence']
  for j in range(i + 1, i + 1 + buffer_size):
  if j < len(sentences):
  combined_sentence += ' ' + sentences[j]['sentence']
  sentences[i]['combined_sentence'] = combined_sentence
  return sentences
import os

import faiss

import tiktoken

import pandas as pd

import matplotlib.pyplot as plt

from dotenv import load_dotenv

from PyPDF2 import PdfReader
 
from langchain_community.document_loaders import PyPDFLoader

from langchain.document_loaders import DirectoryLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.text_splitter import CharacterTextSplitter
 
from langchain_openai import OpenAIEmbeddings
 
from langchain.vectorstores import FAISS

from langchain_community.vectorstores.weaviate import Weaviate

from langchain_community.vectorstores.pgvector import PGVector  
from langchain.chains.question_answering import load_qa_chain

from langchain.llms import OpenAI

from langchain.chains import ConversationalRetrievalChain
load_dotenv()
 
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
print(os.getcwd())

print(OPENAI_API_KEY)

pdfloader = PyPDFLoader('../data/gpt-4.pdf')  
pages = pdfloader.load_and_split()

print(len(pages), pages[0])
 
splitter = RecursiveCharacterTextSplitter(
  chunk_size = 800,
  chunk_overlap  = 200,
  length_function = len,

)
 
pages_chunks = splitter.split_documents(pages)

print(len(pages_chunks), pages_chunks[0])

pdfReader = PdfReader('../data/gpt-4.pdf')
 
from typing_extensions import Concatenate
 
raw_text = ''

for i, page in enumerate(pdfReader.pages):
  content = page.extract_text()
  if content:
  raw_text += content
 
text_splitter = CharacterTextSplitter(
  chunk_size = 800,
  chunk_overlap  = 200,
  length_function = len,

)

text_chunks = text_splitter.split_text(raw_text)  
type(raw_text), type(pdfReader), type(pdfloader), type(pages[0])
print(f"{type(text_chunks)}")

print(f"{len(text_chunks)}")

print(f"{text_chunks[0]}")

type(pages_chunks[0]), type(text_chunks[0])
embeddings = OpenAIEmbeddings(disallowed_special=())
 
db = FAISS.from_documents(pages_chunks, embeddings)
db
vector_store = FAISS.from_texts(text_chunks, embeddings)
vector_store

query = "what are the limitations of gpt-4 ?"

docs = db.similarity_search(query)

docs[0]
 
chain = load_qa_chain(OpenAI(temperature=0), chain_type="stuff")
 
query = "Who created transformers?"

docs = db.similarity_search(query)
 
chain.run(input_documents=docs, question=query)
from IPython.display import display

import ipywidgets as widgets
 
qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1), db.as_retriever())
chat_history = []
 
def on_submit(_):
  query = input_box.value
  input_box.value = ""
  if query.lower() == 'exit':
  print("Thank you for using the State of the Union chatbot!")
  return
  result = qa({"question": query, "chat_history": chat_history})
  chat_history.append((query, result['answer']))
  display(widgets.HTML(f'<b>User:</b> {query}'))
  display(widgets.HTML(f'<b><font color="blue">Chatbot:</font></b> {result["answer"]}'))
 
print("Welcome to the Transformers chatbot! Type 'exit' to stop.")
 
input_box = widgets.Text(placeholder='Please enter your question:')

input_box.on_submit(on_submit)
 
display(input_box)
client = OpenAI(api_key=OPENAI_API_KEY)
 
def generate_response(prompt):
  user_prompt = f'''"Break down the prompt generation step by step based on the following prompt pairs = "Linux Terminal","I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]"
  "English Translator and Improver","I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations."
  "`position` Interviewer","I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. "
  "JavaScript Console","I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]. "
  "Excel Sheet","I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. "
  "English Pronunciation Helper","I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies."
  "Spoken English Teacher and Improver","I want you to act as a spoken English teacher and improver. I will speak to you in English and you will reply to me in English to practice my spoken English. I want you to keep your reply neat, limiting the reply to 100 words. I want you to strictly correct my grammar mistakes, typos, and factual errors. I want you to ask me a question in your reply. Now let's start practicing, you could ask me a question first. Remember, I want you to strictly correct my grammar mistakes, typos, and factual errors."
  "Travel Guide","I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. "
  "Plagiarism Checker","I want you to act as a plagiarism checker. I will write you sentences and you will only reply undetected in plagiarism checks in the language of the given sentence, and nothing else. Do not write explanations on replies. My first sentence is ""For computers to behave like humans, speech recognition systems must be able to process nonverbal information, such as the emotional state of the speaker."""
  "Character from Movie/Book/Anything","I want you to act like [character] from [series]. I want you to respond and answer like [character] using the tone, manner and vocabulary [character] would use. Do not write any explanations. Only answer like [character]. You must know all of the knowledge of [character]. "
  "Advertiser","I want you to act as an advertiser. You will create a campaign to promote a product or service of your choice. You will choose a target audience, develop key messages and slogans, select the media channels for promotion, and decide on any additional activities needed to reach your goals. "
  use these topic, prompt pair examples only as guidlines to create an effective prompt for the next topic. even if the topic is mensioned before. You will create
  only prompt for it and not act on the previous description. if the topic is mensioned already,
  do not use the prompt which you were given, change it.
  "{prompt}"'''
  response = client.completions.create(
  model="gpt-3.5-turbo-instruct",
  prompt=user_prompt,   temperature=0.7,
  max_tokens=250,
  )
  return response.choices[0].text
app = gr.Interface(
  generate_response,
  title="Retrieval Augmented Generation",
  inputs="text",
  outputs="text",
  allow_flagging=False,
  examples=[["Prompt Generator"], ["a cmd prompt"], ['a translator'], ['an SQL generator'], ['an image generator']]

)
 
app.launch()
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
 
def get_pdf_text(pdf_docs):
  text = ""
  for pdf in pdf_docs:
  pdf_reader = PdfReader(pdf)
  for page in pdf_reader.pages:
  text += page.extract_text()
  return text
 
def get_text_chunks(text):
  text_splitter = RecursiveCharacterTextSplitter(
  separator="\n", chunk_size=1000, chunk_overlap=200, length_function=len
  )
  chunks = text_splitter.split_text(text)
  return chunks
 
def get_vectorstore(chunked_docs, text_chunks):
  embeddings = OpenAIEmbeddings()
  vectorstore = Chroma.from_documents(chunked_docs, embeddings)
  return vectorstore
 
def get_retriever(vectorstore):
  retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
  return retriever
from langchain_community.vectorstores.chroma import Chroma
from langchain_openai import OpenAIEmbeddings
 
class MyVectorStore:
  def __init__(self):
  pass
  def embed_text_and_return_vectorstore(self, text_chunks):
  embeddings = OpenAIEmbeddings()
  vectorstore = Chroma.from_texts(text_chunks, embeddings)
  return vectorstore
  def get_retriever(self, vectorstore):
  retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
  return retriever
import streamlit as st
from dotenv import load_dotenv
from htmlTemplates import css, bot_template

from utils.pdf_utils import MyPDF
from utils.text_splitter_utils import MyTextSplitter
from utils.vector_store_utils import MyVectorStore
from utils.langchain_utils import MyLangChain
import json
 
def get_conversation_chain(retriever):
  my_lang_chain = MyLangChain()
  return my_lang_chain.generate_prompts_chain(base_retriever=retriever)
 
def handle_userinput(user_question):
  if not st.session_state.conversation:
  st.error(f"Please enter document")
  return
  result = st.session_state.conversation.invoke(
  {
  "user_prompt": user_question,
  "num_of_prompts_to_generate": 5,
  }
  )
  prompts_generated = json.loads(result["response"].content)
  for message in prompts_generated:
  st.write(bot_template.replace("{{MSG}}", message), unsafe_allow_html=True)
 
def main():
  load_dotenv()
  st.set_page_config(page_title="Optimized Prompts", page_icon="")
  st.write(css, unsafe_allow_html=True)
  if "conversation" not in st.session_state:
  st.session_state.conversation = None
  if "chat_history" not in st.session_state:
  st.session_state.chat_history = None
  st.header("Get optimized prompts ")
  user_question = st.text_input("State your objective:")
  if user_question:
  handle_userinput(user_question)
  with st.sidebar:
  st.subheader("Your documents")
  pdf_docs = st.file_uploader(
  "Upload your PDFs here and click on 'Process'", accept_multiple_files=True
  )
  if st.button("Process"):
  with st.spinner("Processing"):
  pdf = MyPDF(pdf=pdf_docs)
  raw_text = pdf.get_pdf_text()
  text_splitter = MyTextSplitter(raw_text)
  text_chunks = text_splitter.get_text_chunks()
  my_vector_store = MyVectorStore()
  chroma_vector_store = my_vector_store.embed_text_and_return_vectorstore(
  text_chunks
  )
  retreiver = my_vector_store.get_retriever(chroma_vector_store)
  st.session_state.conversation = get_conversation_chain(retreiver)
 
if __name__ == "__main__":
  main()
OPENAI_API_KEY = ""
import os

os.environ["OPENAI_API_KEY"] = 'openai_api_key'
from langchain import PromptTemplate
 
demo_template = ''' I want you to act as acting financial advisor for people.  
In an easy way explain the basics of {financial_concept}'''
 
prompt = PromptTemplate(
  input_variables=['financial_concept'],
  template=demo_template
  )
 
prompt.format(financial_concept='income tax')
 
from langchain.llms import OpenAI

from langchain.chains import LLMChain
 
llm = OpenAI(temperature=0.7)

chain1 = LLMChain(llm=llm,prompt=prompt)
chain1.run('GDP')
chain1.run('economics')
chain1.run('deficit')
chain1.run('collateral')
import os

import openai

from OpenAIAPIKey import openapikey as OPENAI_API_KEY

os.environ["OPENAI_API_KEY"] = 'OPENAI_API_KEY'
 
openai.api_key = "OPENAI_API_KEY"
 
def generate_ai_powered_test_cases(prompt):
  test_cases = []
  for i in range(5):   response = openai.Client.create_completion(
  engine="text-davinci-003",
  prompt=prompt,
  max_tokens=200,   n=1,
  stop=None,
  temperature=0.8,   )
  test_cases.append(response.choices[0].text)
  return test_cases
 
prompt = "Write a test case to validate the login functionality of a website. Consider edge cases, security vulnerabilities, and different input combinations."
 
test_cases = generate_ai_powered_test_cases(prompt)
 
for test_case in test_cases:
  print("Test Case:", test_case)
  print("------------------")
import os

import openai

from OpenAIAPIKey import openapikey as OPENAI_API_KEY

os.environ["OPENAI_API_KEY"] = 'OPENAI_API_KEY'

openai.api_key = "OPENAI_API_KEY"

client = openai.Client()

def generate_test_cases_from_dataset(prompt, dataset):
  test_cases = []
  for example in dataset:
  response = openai.client.create_completion(   engine="text-davinci-003",
  prompt=f"{prompt}. Here's an example: {example}. Generate a similar code snippet with a different approach.",
  max_tokens=150,   n=1,
  stop=None,
  temperature=0.7,   )
  test_cases.append((response.choices[0].text, example))   return test_cases
 
dataset = ["num = num * 2", "num *= 2", "num += num", ...]
 
prompt = "Write a code snippet that doubles a given number in Python."
 
test_cases = generate_test_cases_from_dataset(prompt, dataset)
 
for generated_code, original_code in test_cases:
  print("Generated Code:", generated_code)
  print("Original Code:", original_code)
  print("------------------")
import random
def monte_carlo_eval(prompt):
  response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']
  scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}
  trials = 100
  total_score = 0
  for _ in range(trials):
  response = random.choice(response_types)
  total_score += scores[response]
  return total_score / trials
 
def elo_eval(prompt, base_rating=1500):
  outcomes = ['win', 'loss', 'draw']
  outcome = random.choice(outcomes)
  K = 30   R_base = 10 ** (base_rating / 400)
  R_opponent = 10 ** (1600 / 400)   expected_score = R_base / (R_base + R_opponent)
  actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]
  new_rating = base_rating + K * (actual_score - expected_score)
  return new_rating
def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):
  """
  Update Elo ratings for a list of prompts based on simulated outcomes.
  Parameters:
  prompts (list): List of prompts to be evaluated.
  elo_ratings (dict): Current Elo ratings for each prompt.
  K (int): Maximum change in rating.
  opponent_rating (int): Fixed rating of the opponent for simulation.
  Returns:
  dict: Updated Elo ratings.
  """
  for prompt in prompts:
  outcome = random.choice(['win', 'loss', 'draw'])
  actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]
  R_base = 10 ** (elo_ratings[prompt] / 400)
  R_opponent = 10 ** (opponent_rating / 400)
  expected_score = R_base / (R_base + R_opponent)
  elo_ratings[prompt] += K * (actual_score - expected_score)
  return elo_ratings
 
prompts = ["Who founded OpenAI?",   "What was the initial goal of OpenAI?",
  "What did OpenAI release in 2016?",   "What project did OpenAI showcase in 2018?",
  "How did the AI agents in OpenAI Five work together?"
  ]

elo_ratings = {prompt: 1500 for prompt in prompts}  
for _ in range(10):   elo_ratings = elo_ratings_func(prompts, elo_ratings)
 
sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)
 
for prompt in sorted_prompts:
  print(f"{prompt}: {elo_ratings[prompt]}")
 
def evaluate_prompt(main_prompt, test_cases):
  evaluations = {}
  evaluations['main_prompt'] = {
  'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),
  'Elo Rating Evaluation': elo_eval(main_prompt)
  }
  for idx, test_case in enumerate(test_cases):
  evaluations[f'test_case_{idx+1}'] = {
  'Monte Carlo Evaluation': monte_carlo_eval(test_case),
  'Elo Rating Evaluation': elo_eval(test_case)
  }
  return evaluations
main_prompt = "why we use OepenAI?"

test_cases = ["Who founded OpenAI?",   "What was the initial goal of OpenAI?",
  "What did OpenAI release in 2016?",   "What project did OpenAI showcase in 2018?",
  "How did the AI agents in OpenAI Five work together?"
  ]

result = evaluate_prompt(main_prompt, test_cases)

print(result)
 
import requests

from langchain.document_loaders import TextLoader

from langchain.text_splitter import CharacterTextSplitter  
from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import Weaviate

import weaviate

from weaviate.embedded import EmbeddedOptions

from dotenv import load_dotenv,find_dotenv

from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import Weaviate

import weaviate

from weaviate.embedded import EmbeddedOptions

from dotenv import load_dotenv,find_dotenv
 
from langchain.chat_models import ChatOpenAI

from langchain.prompts import ChatPromptTemplate

from langchain.schema.runnable import RunnablePassthrough

from langchain.schema.output_parser import StrOutputParser

def data_loader(file_path= 'prompts/context.txt'):
  loader = TextLoader(file_path)
  documents = loader.load()
  text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  chunks = text_splitter.split_documents(documents)
  return chunks
def create_retriever(chunks):
  load_dotenv(find_dotenv())
  client = weaviate.Client(
  embedded_options = EmbeddedOptions()
  )
  vectorstore = Weaviate.from_documents(
  client = client,   documents = chunks,
  embedding = OpenAIEmbeddings(),
  by_text = False
  )
  retriever = vectorstore.as_retriever()
  return retriever
chunks
chunks =  data_loader()

retriever = create_retriever(chunks)
 
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
 
template = """You are an assistant for question-answering tasks.  
Use the following pieces of retrieved context to answer the question.  
If you don't know the answer, just say that you don't know.  
Use two sentences maximum and keep the answer concise.

Question: {question}  
Context: {context}  
Answer:

"""
 
prompt = ChatPromptTemplate.from_template(template)
 
rag_chain = (
  {"context": retriever,  "question": RunnablePassthrough()}   | prompt   | llm
  | StrOutputParser()  
)
from datasets import Dataset
 
questions = ["Who founded OpenAI?",   "What was the initial goal of OpenAI?",
  "What did OpenAI release in 2016?",
  ]

ground_truths = [["Sam Altman, Elon Musk, Ilya Sutskever and Greg Brockman"],
  ["To advance digital intelligence in a way that benefits humanity"],
  ["OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms"]]

answers = []

contexts = []
 
for query in questions:
  answers.append(rag_chain.invoke(query))
  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])
 
data = {
  "question": questions,   "answer": answers,   "contexts": contexts,   "ground_truths": ground_truths 
}
 
dataset = Dataset.from_dict(data)
from ragas import evaluate

from ragas.metrics import (
  faithfulness,
  answer_relevancy,
  context_recall,
  context_precision,

)
 
result = evaluate(
  dataset = dataset,   metrics=[
  context_precision,
  context_recall,
  faithfulness,
  answer_relevancy,
  ],

)
 
df = result.to_pandas()
df
import os
import sys
from dotenv import load_dotenv
load_dotenv(".env")

class OPENAI_KEYS:
  def __init__(self):
  self.OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '') or None
 
class VECTORDB_KEYS:
  def __init__(self):
  self.VECTORDB_API_KEY = os.environ.get('VECTORDB_API_KEY', '') or None
  self.VECTORDB_URL = os.environ.get('VECTORDB_URL', '') or None
  self.VECTORDB_MODEL = os.environ.get('VECTORDB_MODEL', '') or None
 
def _get_openai_keys() -> OPENAI_KEYS:
  return OPENAI_KEYS()
 
def _get_vectordb_keys() -> VECTORDB_KEYS:
  return VECTORDB_KEYS()
 
def get_env_manager() -> dict:
  openai_keys = _get_openai_keys().__dict__
  vectordb_keys = _get_vectordb_keys().__dict__
  return {
  'openai_keys': openai_keys,
  'vectordb_keys': vectordb_keys,
  }
