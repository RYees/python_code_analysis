from sqlalchemy import create_engine
 
print(__name__)
def connect_to_postgres():
  database_name = 'telecom'
  table_name= 'xdr_data'
  connection_params = { "host": "localhost", "user": "postgres", "password": "postgres",
  "port": "5432", "database": database_name}
  engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
  return engine
 
if __name__ == "__main__":
  connect_to_postgres()
import pandas as pd
from postgres_connection import connect_to_postgres
 
def get_df():
  df = pd.read_csv('data.csv')
  return df
import pandas as pd
from process_data import  get_df
import numpy as np
import seaborn as sns
pd.set_option('display.float_format', lambda x: '%.0f' % x)
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
 
df = get_df()
 
Handset_counts = df['Handset Type'].value_counts()
Handset_counts = Handset_counts .reset_index()
Handset_counts.columns = ['Handset Type', 'Count']
Handset_counts.head(10)
manufacturers_counts = df['Handset Manufacturer'].value_counts()
manufacturers_counts = manufacturers_counts .reset_index()
manufacturers_counts.columns = ['top 3 handset manufacturers', 'Count']
manufacturers_counts.head(3)
df.columns
 
filtered_df = df[df['Handset Manufacturer'].isin(['Apple', 'Samsung', 'Huawei'])]
filtered_df.iloc[0]
Handset_counts = filtered_df['Handset Type'].value_counts()
Handset_counts = Handset_counts .reset_index()
Handset_counts.columns = ['top 5 handsets', 'Count']
Handset_counts.head(5)
 
df['Start'] = pd.to_datetime(df['Start'])
df['End'] = pd.to_datetime(df['End'])
 
df['Session Duration (s)'] = (df['End'] - df['Start']).dt.total_seconds()
 
applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']
 
user_aggregated = df.groupby('MSISDN/Number').agg({
  'Bearer Id': 'count',   'Session Duration (s)': 'sum',   'Total DL (Bytes)': 'sum',   'Total UL (Bytes)': 'sum',   **{f'{app} DL (Bytes)': 'sum' for app in applications},   **{f'{app} UL (Bytes)': 'sum' for app in applications}  
})
 
for app in applications:
  user_aggregated[f'{app} (Total Bytes)'] = user_aggregated[f'{app} DL (Bytes)'] + user_aggregated[f'{app} UL (Bytes)']
  user_aggregated.drop([f'{app} DL (Bytes)', f'{app} UL (Bytes)'], axis=1, inplace=True)
 
user_aggregated.rename(columns={'Bearer Id': 'Number of xDR sessions'}, inplace=True)
 
user_aggregated.head(10)
missing_values = df.isnull()
missing_values
missing_values_count = df.isnull().sum()
missing_values_count
 
non_numeric_columns = df.select_dtypes(exclude=['number']).columns
 
df_numeric = df.drop(non_numeric_columns, axis=1)
 
cleaned_data = df_numeric.fillna(df_numeric.mean())
 
selected_columns = [
  "Dur. (ms)",
  "Avg Bearer TP DL (kbps)",
  "Avg Bearer TP UL (kbps)",
  "Social Media DL (Bytes)",
  "Social Media UL (Bytes)",
  "Google DL (Bytes)",
  "Google UL (Bytes)",
  "Email DL (Bytes)",
  "Email UL (Bytes)",
  "Youtube DL (Bytes)",
  "Youtube UL (Bytes)",
  "Netflix DL (Bytes)",
  "Netflix UL (Bytes)",
  "Gaming DL (Bytes)",
  "Gaming UL (Bytes)",
  "Total UL (Bytes)",
  "Total DL (Bytes)",
  "Other DL (Bytes)",
  "Other UL (Bytes)"
]
 
displayed_data = cleaned_data[selected_columns]
 
displayed_data.describe()
 
quantitative_columns = displayed_data.select_dtypes(include=['number']).columns
 
dispersion_data = pd.DataFrame(index=quantitative_columns, columns=['Range', 'Variance', 'Standard Deviation', 'IQR'])
 
for column in quantitative_columns:
  data = displayed_data[column].dropna()   data_range = data.max() - data.min()
  data_variance = data.var()
  data_std_dev = data.std()
  data_iqr = data.quantile(0.75) - data.quantile(0.25)
  dispersion_data.loc[column] = [data_range, data_variance, data_std_dev, data_iqr]
 
dispersion_data
import matplotlib.pyplot as plt
 
numeric_columns = displayed_data.select_dtypes(include='number').columns
 
for column in numeric_columns:
  plt.figure(figsize=(8, 5))
  plt.hist(df[column].dropna(), bins=30, color='skyblue', edgecolor='black')
  plt.title(f'Histogram of {column}')
  plt.xlabel(column)
  plt.ylabel('Frequency')
  plt.show()
 
import seaborn as sns
import matplotlib.pyplot as plt
 
applications = [
  'Social Media',
  'Google',
  'Email',
  'Youtube',
  'Netflix',
  'Gaming',
  'Other',
]
 
cleaned_data['Total Data (DL+UL)'] = cleaned_data['Total UL (Bytes)'] + cleaned_data['Total DL (Bytes)']
 
plt.figure(figsize=(14, 10))
for app in applications:
  sns.scatterplot(x=cleaned_data[app + ' DL (Bytes)'], y=cleaned_data[app + ' UL (Bytes)'], label=app)
 
plt.xlabel('DL Data (Bytes)')
plt.ylabel('UL Data (Bytes)')
plt.title('Scatter Plot of DL vs. UL Data for Each Application')
plt.legend()
plt.show()
 
df['Start'] = pd.to_datetime(df['Start'])
df['End'] = pd.to_datetime(df['End'])
 
df['Session Duration (s)'] = (df['End'] - df['Start']).dt.total_seconds()
 
df['Total Data (DL+UL)'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']
 
df['Duration Decile'] = pd.qcut(df.groupby('MSISDN/Number')['Session Duration (s)'].transform('sum'), q=10, labels=False, duplicates='drop')
 
decile_data = df.groupby('Duration Decile')['Total Data (DL+UL)'].sum().reset_index()
 
decile_data = decile_data.sort_values(by='Total Data (DL+UL)', ascending=False)
 
print(decile_data)
 
columns_of_interest = [
  'Social Media DL (Bytes)',
  'Google DL (Bytes)',
  'Email DL (Bytes)',
  'Youtube DL (Bytes)',
  'Netflix DL (Bytes)',
  'Gaming DL (Bytes)',
  'Other DL (Bytes)',
  'Social Media UL (Bytes)',
  'Google UL (Bytes)',
  'Email UL (Bytes)',
  'Youtube UL (Bytes)',
  'Netflix UL (Bytes)',
  'Gaming UL (Bytes)',
  'Other UL (Bytes)',
]
 
correlation_data = cleaned_data[columns_of_interest]
 
correlation_matrix = correlation_data.corr()
 
correlation_matrix
app_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',
  'Google DL (Bytes)', 'Google UL (Bytes)',
  'Email DL (Bytes)', 'Email UL (Bytes)',
  'Youtube DL (Bytes)', 'Youtube UL (Bytes)',
  'Netflix DL (Bytes)', 'Netflix UL (Bytes)',
  'Gaming DL (Bytes)', 'Gaming UL (Bytes)',
  'Other DL (Bytes)', 'Other UL (Bytes)']
 
for app in ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other', 'Total']:
  cleaned_data[f'{app} Total Bytes'] = cleaned_data[f'{app} DL (Bytes)'] + cleaned_data[f'{app} UL (Bytes)']
 
total_bytes_columns = [f'{app} Total Bytes' for app in ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']]
total_bytes_data = cleaned_data[total_bytes_columns]
 
corr_matrix_total_bytes = total_bytes_data.corr()
 
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix_total_bytes, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix for Total Bytes')
plt.show()
 
all_columns_for_pca = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',
  'Google DL (Bytes)', 'Google UL (Bytes)',
  'Email DL (Bytes)', 'Email UL (Bytes)',
  'Youtube DL (Bytes)', 'Youtube UL (Bytes)',
  'Netflix DL (Bytes)', 'Netflix UL (Bytes)',
  'Gaming DL (Bytes)', 'Gaming UL (Bytes)',
  'Other DL (Bytes)', 'Other UL (Bytes)',
  'Total DL (Bytes)', 'Total UL (Bytes)',
  'Avg RTT DL (ms)', 'Avg RTT UL (ms)',
  'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',
  'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',
  'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)',
  '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)',
  'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)',
  '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)',
  'HTTP DL (Bytes)', 'HTTP UL (Bytes)',
  'Activity Duration DL (ms)', 'Activity Duration UL (ms)',
  'Nb of sec with 125000B < Vol DL', 'Nb of sec with 1250B < Vol UL < 6250B',
  'Nb of sec with 31250B < Vol DL < 125000B', 'Nb of sec with 37500B < Vol UL',
  'Nb of sec with 6250B < Vol DL < 31250B', 'Nb of sec with 6250B < Vol UL < 37500B',
  'Nb of sec with Vol DL < 6250B', 'Nb of sec with Vol UL < 1250B']
 
data_for_pca = cleaned_data[all_columns_for_pca]
 
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data_for_pca)
 
pca = PCA(10)
principal_components = pca.fit_transform(scaled_data)
 
explained_variance_ratio = pca.explained_variance_ratio_
 
plt.figure(figsize=(12, 6))
plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.8, align='center')
plt.step(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), where='mid')
plt.title('Explained Variance vs. Number of Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.show()
 
principal_components  = pd.DataFrame(principal_components)
principal_components
! pip install scikit-learn
import streamlit as st
from streamlit.logger import get_logger
from process_data import get_df
import matplotlib.pyplot as plt
import seaborn as sns  
LOGGER = get_logger(__name__)
df = get_df()
df.to_csv('data.csv')
def run():
  st.set_page_config(
  page_title="10Academy",
  page_icon="ðŸ‘‹",
  )
  st.write("
  Handset_counts = df['Handset Type'].value_counts()
  Handset_counts = Handset_counts .reset_index()
  Handset_counts.columns = ['Handset Type', 'Count']
  manufacturers_counts = df['Handset Manufacturer'].value_counts()
  manufacturers_counts = manufacturers_counts .reset_index()
  manufacturers_counts.columns = ['top 3 handset manufacturers', 'Count']
  filtered_df = df[df['Handset Manufacturer'].isin(['Apple', 'Samsung', 'Huawei'])]
  Handset_counts = filtered_df['Handset Type'].value_counts()
  Handset_counts = Handset_counts .reset_index()
  Handset_counts.columns = ['top 5 handsets', 'Count']
  numeric_columns = df.select_dtypes(include=['number']).columns
  for column in numeric_columns:
  fig, ax = plt.subplots(figsize=(8, 5))
  ax.hist(df[column].dropna(), bins=30, color='skyblue', edgecolor='black')
  ax.set_title(f'Histogram of {column}')
  ax.set_xlabel(column)
  ax.set_ylabel('Frequency')
  st.pyplot(fig)
 
if __name__ == "__main__":
  run()
import pandas as pd
from process_data import  get_df
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
 
pd.set_option('display.float_format', lambda x: '%.0f' % x)
 
df = get_df()
df
 
analytics_columns = [
  'MSISDN/Number',
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)',
  'Handset Type',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
]
 
num_columns = [
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
]
 
df = df[analytics_columns]
 
df_cleaned_handset=df.dropna(subset=['Handset Type'])
 
df_cleaned_handset  
lower_percentile = 1
upper_percentile = 99
 
lower_bounds = df_cleaned_handset[num_columns].quantile(lower_percentile / 100)
upper_bounds = df_cleaned_handset[num_columns].quantile(upper_percentile / 100)
 
for col in num_columns:
  outliers = (df_cleaned_handset[col] < lower_bounds[col]) | (df_cleaned_handset[col] > upper_bounds[col])
  if col  == 'Avg Bearer TP DL (kbps)':
  df_cleaned_handset[col] = df_cleaned_handset[col].mask(outliers, df_cleaned_handset[col].mean())
 
df_cleaned_handset
 
df_clean_nan = df_cleaned_handset.copy()
df_clean_nan[num_columns] = df_clean_nan[num_columns].fillna(df_clean_nan[num_columns].mean())
df_clean_nan
 
df_clean_nan['Sum RTT'] = df_clean_nan['Avg RTT DL (ms)'] + df_clean_nan['Avg RTT UL (ms)']
df_clean_nan['Sum TCP Retrans. Vol (Bytes)'] = df_clean_nan['TCP DL Retrans. Vol (Bytes)'] + df_clean_nan['TCP UL Retrans. Vol (Bytes)']
df_clean_nan['Sum Bearer TP'] = df_clean_nan['Avg Bearer TP DL (kbps)'] + df_clean_nan['Avg Bearer TP UL (kbps)']
 
df_clean_nan = df_clean_nan.rename(columns={
  'Avg RTT DL (ms)': 'Avg RTT DL',
  'Avg RTT UL (ms)': 'Avg RTT UL',
  'TCP DL Retrans. Vol (Bytes)': 'TCP Retrans. Vol DL',
  'TCP UL Retrans. Vol (Bytes)': 'TCP Retrans. Vol UL',
  'Avg Bearer TP DL (kbps)': 'Avg Bearer TP DL',
  'Avg Bearer TP UL (kbps)': 'Avg Bearer TP UL',
})
 
df_clean_nan
important_columns = [
  'MSISDN/Number',
  'Handset Type',
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP'
]
 
num_important_columns  = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP'
]
 
df_important = df_clean_nan[important_columns]
df_important
 
num_important_columns = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP'
]
 
grouped_df = df_important.groupby('MSISDN/Number')[num_important_columns].sum().reset_index()
 
grouped_df.head(10)
 
top_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].nlargest(10)
bottom_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].nsmallest(10)
most_frequent_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].mode()
 
print("Top TCP Values:")
print(top_tcp_values)
 
print("\nBottom TCP Values:")
print(bottom_tcp_values)
 
print("\nMost Frequent TCP Values:")
print(most_frequent_tcp_values)
 
top_rtt_values = df_important['Sum RTT'].nlargest(10)
bottom_rtt_values = df_important['Sum RTT'].nsmallest(10)
most_frequent_rtt_values = df_important['Sum RTT'].mode()
 
print("Top RTT Values:")
print(top_rtt_values)
 
print("\nBottom RTT Values:")
print(bottom_rtt_values)
 
print("\nMost Frequent RTT Values:")
print(most_frequent_rtt_values)
 
top_throughput_values = df_important['Sum Bearer TP'].nlargest(10)
bottom_throughput_values = df_important['Sum Bearer TP'].nsmallest(10)
most_frequent_throughput_values = df_important['Sum Bearer TP'].mode()
 
print("Top Throughput Values:")
print(top_throughput_values)
 
print("\nBottom Throughput Values:")
print(bottom_throughput_values)
 
print("\nMost Frequent Throughput Values:")
print(most_frequent_throughput_values)
 
average_throughput_per_handset = df_important.groupby('Handset Type')['Sum Bearer TP'].mean().reset_index()
print("Distribution of Average Throughput per Handset Type:")
print(average_throughput_per_handset)
 
avg_tcp_r_per_handset = df_important.groupby('Handset Type')['Sum TCP Retrans. Vol (Bytes)'].mean().reset_index()
print("Distribution of Average TCP Retransmission per Handset Type:")
avg_tcp_r_per_handset
 
experience_metrics = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP',
]
 
df_cluster = grouped_df[experience_metrics]
 
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_cluster)
 
kmeans = KMeans(n_clusters=3, random_state=42)
df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)
 
cluster_means = df_cluster.groupby('Cluster').mean()
print(cluster_means)
 
experience_metrics = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP',
]
 
df_cluster = grouped_df[experience_metrics].dropna()
 
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_cluster)
 
kmeans = KMeans(n_clusters=3, random_state=42)
df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)
 
sns.set(style="whitegrid")
 
sns.pairplot(df_cluster, hue="Cluster", palette="Set1", height=3, diag_kind="kde")
plt.show()
 
experience_metrics = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP',
]
 
df_cluster = grouped_df[experience_metrics].dropna()
 
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_cluster)
 
kmeans = KMeans(n_clusters=3, random_state=42)
df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)
 
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
scatter = ax.scatter(
  df_cluster['Sum RTT'],
  df_cluster['Sum TCP Retrans. Vol (Bytes)'],
  df_cluster['Sum Bearer TP'],
  c=df_cluster['Cluster'],
  cmap='viridis',
  s=50,
  alpha=0.6,
  edgecolors='w'
)
 
ax.set_xlabel('Sum RTT')
ax.set_ylabel('Sum TCP Retrans. Vol (Bytes)')
ax.set_zlabel('Sum Bearer TP')
ax.set_title('K-Means Clustering of User Experiences')
 
legend1 = ax.legend(*scatter.legend_elements(), title='Clusters')
ax.add_artist(legend1)
 
plt.show()
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
def set_seaborn_style():
  """
  Set a custom Seaborn style.
  """
  sns.set(style="whitegrid")
def plot_histogram_kde(data, title, x_label, y_label, color='skyblue'):
  """
  Plot a histogram with KDE.
  Parameters:
  - data: Series or array-like
  Data to be plotted.
  - title: str
  Plot title.
  - x_label: str
  Label for the x-axis.
  - y_label: str
  Label for the y-axis.
  - color: str, optional
  Color for the plot.
  Returns:
  - None
  """
  plt.figure(figsize=(10, 6))
  sns.histplot(data, kde=True, color=color, edgecolor='black')
  plt.title(title, fontsize=16)
  plt.xlabel(x_label, fontsize=14)
  plt.ylabel(y_label, fontsize=14)
  plt.xticks(fontsize=12)
  plt.yticks(fontsize=12)
  plt.grid(axis='y', linestyle='--', alpha=0.7)
  plt.show()
def plot_boxplot(data, title, x_label, color='lightcoral'):
  """
  Plot a boxplot.
  Parameters:
  - data: Series or array-like
  Data to be plotted.
  - title: str
  Plot title.
  - x_label: str
  Label for the x-axis.
  - color: str, optional
  Color for the plot.
  Returns:
  - None
  """
  plt.figure(figsize=(8, 6))
  sns.boxplot(x=data, color=color)
  plt.title(title, fontsize=16)
  plt.xlabel(x_label, fontsize=14)
  plt.xticks(fontsize=12)
  plt.yticks(fontsize=12)
  plt.grid(axis='y', linestyle='--', alpha=0.7)
  plt.show()
def plot_time_series(data, time_column, title, x_label, y_label, color='skyblue'):
  """
  Plot a time series using Matplotlib.
  Parameters:
  - data: DataFrame
  Data containing a time series.
  - time_column: str
  Column representing the time.
  - title: str
  Plot title.
  - x_label: str
  Label for the x-axis.
  - y_label: str
  Label for the y-axis.
  - color: str, optional
  Color for the plot.
  Returns:
  - None
  """
  plt.figure(figsize=(12, 6))
  sns.lineplot(data=data.resample('D').size(), color=color, marker='o')
  plt.title(title, fontsize=16)
  plt.xlabel(x_label, fontsize=14)
  plt.ylabel(y_label, fontsize=14)
  plt.xticks(fontsize=12)
  plt.yticks(fontsize=12)
  plt.grid(axis='y', linestyle='--', alpha=0.7)
  plt.show()
def plot_countplot(data, x_column, title, x_label, rotation=45, color='skyblue'):
  """
  Plot a countplot using Seaborn.
  Parameters:
  - data: DataFrame
  Data to be plotted.
  - x_column: str
  Column for the x-axis.
  - title: str
  Plot title.
  - x_label: str
  Label for the x-axis.
  - rotation: int, optional
  Rotation angle for x-axis labels.
  - color: str, optional
  Color for the plot.
  Returns:
  - None
  """
  plt.figure(figsize=(10, 6))
  sns.countplot(x=data[x_column], color=color)
  plt.title(title, fontsize=16)
  plt.xlabel(x_label, fontsize=14)
  plt.xticks(rotation=rotation, fontsize=12)
  plt.yticks(fontsize=12)
  plt.grid(axis='y', linestyle='--', alpha=0.7)
  plt.show()
def create_table(table_data):
  """
  Create a table using Plotly.
  Parameters:
  - table_data: DataFrame
  Data for the table.
  Returns:
  - None
  """
  table = go.Figure(data=[go.Table(
  header=dict(values=list(table_data.columns),
  fill_color='lightblue',
  align='center',
  font=dict(color='black', size=14)),
  cells=dict(values=[table_data['Handset Manufacturer'], table_data['Handset Type'], table_data['count']],
  fill=dict(color=['white', 'lightcyan', 'lightcyan']),
  align='center',
  font=dict(color='black', size=12)))
  ])
  table.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=0, b=0))
  table.show()
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as pl
from sqlalchemy import create_engine, text
from scipy.stats import zscore
import psycopg2
import os
import plotly.graph_objects as go
import plotly.express as px
os.chdir('..')
from db.connection import DatabaseConnection
from db.sql_preprocessor import DBFilter
from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot
db_connection = DatabaseConnection()
db_connection.connect()
query = "SELECT * FROM xdr_data"
df = db_connection.execute_query(query)
df.head()
df.columns
missing_values = df.isnull().sum()
print(missing_values)
duplicates = df.duplicated()
print("Number of duplicate rows:", duplicates.sum())
top_handsets = df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')
top_handsets = top_handsets.sort_values(by='count', ascending=False).head(10)
table_data = top_handsets[['Handset Manufacturer', 'Handset Type', 'count']]
 
table = go.Figure(data=[go.Table(
  header=dict(values=list(table_data.columns),
  fill_color='lightblue',
  align='center',
  font=dict(color='black', size=14)),
  cells=dict(values=[table_data['Handset Manufacturer'], table_data['Handset Type'], table_data['count']],
  fill=dict(color=['lightcyan', 'lightcyan', 'lightcyan']),
  align='center',
  font=dict(color='black', size=12)))
])
 
table.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=0, b=0))
 
table.show()
top_handsets = df.query("`Handset Manufacturer` != 'undefined' and `Handset Type` != 'undefined'")[['Handset Manufacturer', 'Handset Type']]
top_handsets = top_handsets.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')
top_handsets = top_handsets.sort_values(by='count', ascending=False).head(10)
top_handsets[['Handset Manufacturer', 'Handset Type', 'count']]
fig = px.bar(top_handsets, x='Handset Type', y='count', color='Handset Manufacturer',
  labels={'count': 'Count', 'Handset Type': 'Handset Type'},
  title='Top Handsets by Manufacturer and Type',
  template='plotly_white',   color_discrete_sequence=px.colors.qualitative.Set1)  
fig.update_layout(
  xaxis=dict(title='Handset Type'),
  yaxis=dict(title='Count'),
  legend=dict(title='Manufacturer'),
  barmode='group',
  showlegend=True
)
 
fig.show()
top_manufacturers = df[df['Handset Manufacturer'] != 'undefined']
top_manufacturers = top_manufacturers['Handset Manufacturer'].value_counts().head(3).reset_index(name='count')
top_manufacturers.columns = ['Handset Manufacturer', 'count']
top_manufacturers
len(top_manufacturers)
total_rows = len(df)
top_manufacturers['percentage'] = (top_manufacturers['count'] / total_rows) * 100
 
print(top_manufacturers)
total_handsets = df['Handset Type'].count()
 
print("Total number of handsets:", total_handsets)
top_manufacturers = df[df['Handset Manufacturer'] != 'undefined']
top_manufacturers = top_manufacturers['Handset Manufacturer'].value_counts().head(3).index
filtered_df = df[df['Handset Manufacturer'].isin(top_manufacturers)]
top_handsets_per_manufacturer = (
  filtered_df.groupby(['Handset Manufacturer', 'Handset Type'])
  .size()
  .reset_index(name='count')
  .sort_values(by=['Handset Manufacturer', 'count'], ascending=[True, False])
  .groupby('Handset Manufacturer')
  .head(5)
)
top_handsets_per_manufacturer
import pandas as pd
import numpy as np
import sqlite3
class DBFilter:
  def __init__(self, dataframe):
  self.df = dataframe
  def filter_numeric_columns(self, threshold=0):
  numeric_columns = self.df.select_dtypes(include=[np.number]).columns
  filtered_df = self.df[numeric_columns].apply(lambda x: x[x > threshold])
  return filtered_df
  def load_data_from_db(self, db_path, sql_query):
  connection = sqlite3.connect(db_path)
  df = pd.read_sql_query(sql_query, connection)
  connection.close()
  return df
  def get_unique_values(self, column):
  unique_values = self.df[column].unique()
  return unique_values
  def most_repeated_value(self, column):
  return self.df[column].mode().values[0]
  def calculate_average(self, column):
  return self.df[column].mean()
  def close_connection(self):
  pass
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv
import pandas as pd
class DatabaseConnection:
  def __init__(self):
  load_dotenv()
  self.username = os.getenv("DB_USERNAME")
  self.password = os.getenv("DB_PASSWORD")
  self.host = os.getenv("DB_HOST")
  self.port = os.getenv("DB_PORT")
  self.database = os.getenv("DB_DATABASE")
  if None in (self.username, self.password, self.host, self.port, self.database):
  raise ValueError("One or more database credentials are missing.")
  self.connection_url = f"postgresql+psycopg2://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}"
  self.engine = None
  self.connection = None
  def connect(self):
  try:
  self.engine = create_engine(self.connection_url)
  self.connection = self.engine.connect()
  print("Connected to the database.")
  except Exception as e:
  print(f"Error connecting to the database: {str(e)}")
  def execute_query(self, query):
  try:
  df = pd.read_sql_query(query, self.connection)
  return df
  except Exception as e:
  print(f"Error executing query: {str(e)}")
  def execute_update_query(self, query):
  try:
  self.connection.execute(query)
  print("Query executed successfully.")
  except Exception as e:
  print(f"Error executing query: {str(e)}")
  def close_connection(self):
  try:
  self.connection.close()
  print("Connection closed.")
  except Exception as e:
  print(f"Error closing connection: {str(e)}")
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sqlalchemy import create_engine, text
from scipy.stats import zscore
import psycopg2
import os
import plotly.express as px
import random
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import tabulate
import plotly.graph_objects as go
from scipy.spatial import distance
from sklearn.metrics import pairwise_distances_argmin_min
from functools import reduce
from tabulate import tabulate
from scipy.stats.mstats import winsorize
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import RidgeCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
os.chdir('..')
from db.connection import DatabaseConnection
from db.sql_preprocessor import DBFilter
from src.utils import missing_values_table, find_agg, convert_bytes_to_megabytes, fix_outlier, remove_outliers
from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot, create_table
db_connection = DatabaseConnection()
set_seaborn_style()
db_connection.connect()
query = "SELECT * FROM xdr_data"
df = db_connection.execute_query(query)
df.columns
features = ['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']
df['Start'] = pd.to_datetime(df['Start'])
df['End'] = pd.to_datetime(df['End'])
 
df['Session Duration (ms)'] = (df['End'] - df['Start']).dt.total_seconds() * 1000
 
df['Total Traffic (Bytes)'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']
 
user_engagement = df.groupby('MSISDN/Number').agg({
  'Start': 'count',   'Session Duration (ms)': 'mean',   'Total Traffic (Bytes)': 'sum'  
})
 
user_engagement = user_engagement.rename(columns={
  'Start': 'Sessions Frequency',
  'Session Duration (ms)': 'Average Session Duration (ms)',
  'Total Traffic (Bytes)': 'Total Traffic (Bytes)'
})
 
print(user_engagement)
scaler = StandardScaler()
normalized_engagement = scaler.fit_transform(user_engagement)
 
kmeans = KMeans(n_clusters=3, random_state=42)
user_engagement['Cluster'] = kmeans.fit_predict(normalized_engagement)
 
print("Customers Classified into 3 Groups of Engagement:")
print(user_engagement)
kmeans = KMeans(n_clusters=3, random_state=42)
user_engagement['Cluster'] = kmeans.fit_predict(normalized_engagement)
 
less_engaged_cluster = user_engagement.groupby('Cluster')['Sessions Frequency'].mean().idxmin()
 
distances = pairwise_distances_argmin_min(normalized_engagement, kmeans.cluster_centers_)[1]
 
user_engagement.reset_index(inplace=True)
 
user_engagement['Engagement Score'] = distances if less_engaged_cluster == 0 else -distances
print("Customers with Engagement Scores:")
print(user_engagement[['MSISDN/Number', 'Engagement Score']])
top_10_engaged_customers = user_engagement['Engagement Score'].sort_values(ascending=False).head(10)
 
top_10_df = pd.DataFrame({'Engagement Score': top_10_engaged_customers.values})
 
top_10_df.reset_index(inplace=True, drop=True)
 
fig = px.bar(top_10_df, x=top_10_df.index + 1, y='Engagement Score', title='Top 10 Engaged Customers')
 
fig.update_layout(xaxis_title='Rank', yaxis_title='Engagement Score')
 
fig.show()
top_10_engaged_customers
grouped_data_throughput = df.groupby('MSISDN/Number').agg({
  'Avg Bearer TP DL (kbps)': 'mean',
  'Avg Bearer TP UL (kbps)': 'mean'
}).reset_index()
 
grouped_data_throughput['Avg Bearer TP DL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP DL (kbps)'].mean(), inplace=True)
grouped_data_throughput['Avg Bearer TP UL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP UL (kbps)'].mean(), inplace=True)
 
print(grouped_data_throughput)
grouped_data_rtt = df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean().reset_index()
 
grouped_data_rtt['Avg RTT DL (ms)'].fillna(grouped_data_rtt['Avg RTT DL (ms)'].mean(), inplace=True)
 
mean_value_rtt = grouped_data_rtt['Avg RTT DL (ms)'].mean()
std_dev_rtt = grouped_data_rtt['Avg RTT DL (ms)'].std()
 
outlier_threshold_rtt = 3
 
grouped_data_rtt['Avg RTT DL (ms)'] = grouped_data_rtt['Avg RTT DL (ms)'].apply(
  lambda x: mean_value_rtt if abs(x - mean_value_rtt) > outlier_threshold_rtt * std_dev_rtt else x
)
 
print(grouped_data_rtt)
grouped_data_retrans = df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()
 
grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].fillna(grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].mean(), inplace=True)
 
mean_value = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].mean()
std_dev = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].std()
 
outlier_threshold = 3
 
grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'] = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].apply(
  lambda x: mean_value if abs(x - mean_value) > outlier_threshold * std_dev else x
)
 
print(grouped_data_retrans)
grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()
 
grouped_data_tcp = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()
 
merged_data = pd.merge(grouped_data_handset, grouped_data_tcp, on='Handset Type')
 
print(merged_data)
consolidated_data = pd.merge(grouped_data_handset, grouped_data_retrans, on='MSISDN/Number')
consolidated_data = pd.merge(consolidated_data, grouped_data_rtt, on='MSISDN/Number')
consolidated_data = pd.merge(consolidated_data, grouped_data_throughput, on='MSISDN/Number')
consolidated_data = pd.merge(consolidated_data, user_engagement, on='MSISDN/Number')
 
print(consolidated_data.dtypes)
 
features_for_clustering = ['TCP DL Retrans. Vol (Bytes)', 'Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']
 
consolidated_data.dropna(subset=features_for_clustering, inplace=True)
 
scaler = StandardScaler()
consolidated_data_scaled = scaler.fit_transform(consolidated_data[features_for_clustering])
 
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
consolidated_data['cluster'] = kmeans.fit_predict(consolidated_data_scaled)
 
consolidated_data['euclidean_distance'] = consolidated_data.apply(
  lambda row: distance.euclidean(row[features_for_clustering], kmeans.cluster_centers_[row['cluster']]),
  axis=1
)
 
consolidated_data['experience_score'] = 1 / (1 + consolidated_data['euclidean_distance'])
 
print(consolidated_data[['MSISDN/Number', 'experience_score']])
 
top_10_experience_customers = consolidated_data.sort_values(by='experience_score', ascending=False).head(10)
 
top_10_experience_df = pd.DataFrame({'experience_score': top_10_experience_customers['experience_score'].values})
 
top_10_experience_df.reset_index(inplace=True, drop=True)
 
fig = px.bar(top_10_experience_df, x=top_10_experience_df.index + 1, y='experience_score', title='Top 10 Customers by Experience Score')
 
fig.update_layout(xaxis_title='Rank', yaxis_title='Experience Score')
 
fig.show()
top_10_experience_customers = consolidated_data.sort_values(by='experience_score', ascending=False).head(10)
 
columns_to_include = [
  'MSISDN/Number',
  'experience_score',
]
 
result_df = top_10_experience_customers[columns_to_include]
 
print(result_df)
top_10_experience_scores
columns_to_handle_outliers = ['Engagement Score', 'experience_score']
 
def handle_outliers_iqr(consolidated_data, columns):
  for column in columns:
  Q1 = consolidated_data[column].quantile(0.25)
  Q3 = consolidated_data[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_limit = Q1 - 1.5 * IQR
  upper_limit = Q3 + 1.5 * IQR
  consolidated_data[column] = consolidated_data[column].apply(lambda x: max(lower_limit, min(x, upper_limit)))
  return consolidated_data
 
consolidated_data = handle_outliers_iqr(consolidated_data, columns_to_handle_outliers)
consolidated_data['satisfaction_score'] = (user_engagement['Engagement Score'] + consolidated_data['experience_score']) / 2
 
top_satisfied_customers = consolidated_data.sort_values(by='satisfaction_score', ascending=False)
 
top_10_satisfied_customers = top_satisfied_customers.head(10)
print("Top 10 Satisfied Customers:")
print(top_10_satisfied_customers[['MSISDN/Number', 'satisfaction_score']])
consolidated_data
sorted_data = consolidated_data.sort_values(by='satisfaction_score', ascending=False)
 
top_10 = sorted_data.head(10)
 
fig = go.Figure(data=[go.Table(
  header=dict(values=list(top_10.columns),
  fill_color='paleturquoise',
  align='left'),
  cells=dict(values=[top_10[col] for col in top_10.columns],
  fill_color='lavender',
  align='left'))
])
 
fig.show()
fig1 = px.scatter(consolidated_data, x='satisfaction_score', y='Engagement Score',
  title='Satisfaction Score vs Engagement Score',
  labels={'satisfaction_score': 'Satisfaction Score',
  'engagement_score': 'Engagement Score'})
 
fig2 = px.scatter(consolidated_data, x='satisfaction_score', y='experience_score',
  title='Satisfaction Score vs Experience Score',
  labels={'satisfaction_score': 'Satisfaction Score',
  'experience_score': 'Experience Score'})
 
fig3 = px.scatter(consolidated_data, x='Engagement Score', y='experience_score',
  title='Engagement Score vs Experience Score',
  labels={'engagement_score': 'Engagement Score',
  'experience_score': 'Experience Score'})
 
fig1.show()
fig2.show()
fig3.show()
consolidated_data
max_satisfaction_score = consolidated_data['satisfaction_score'].max()
 
low_satisfaction_threshold = 0.1
 
0 * max_satisfaction_score
moderate_satisfaction_threshold = 0.25 * max_satisfaction_score
high_satisfaction_threshold = 0.50 * max_satisfaction_score
 
consolidated_data['Satisfaction Level'] = pd.cut(consolidated_data['satisfaction_score'],
  bins=[-float('inf'), low_satisfaction_threshold, moderate_satisfaction_threshold, high_satisfaction_threshold, float('inf')],
  labels=['Low Satisfied', 'Moderately Satisfied', 'Satisfied', 'Highly Satisfied'])
 
satisfaction_counts = consolidated_data['Satisfaction Level'].value_counts()
 
satisfaction_percentage = (satisfaction_counts / len(df)) * 100
 
fig = px.pie(satisfaction_percentage, names=satisfaction_percentage.index, values=satisfaction_percentage.values,
  title='Percentage Distribution of Satisfaction Levels',
  labels={'index': 'Satisfaction Level', 'values': 'Percentage of Individuals'})
 
fig.show()
columns_to_handle_outliers = ['Engagement Score', 'experience_score', 'satisfaction_score']
 
def handle_outliers_iqr(consolidated_data, columns):
  for column in columns:
  Q1 = consolidated_data[column].quantile(0.25)
  Q3 = consolidated_data[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_limit = Q1 - 1.5 * IQR
  upper_limit = Q3 + 1.5 * IQR
  consolidated_data[column] = consolidated_data[column].apply(lambda x: max(lower_limit, min(x, upper_limit)))
  return consolidated_data
 
consolidated_data = handle_outliers_iqr(consolidated_data, columns_to_handle_outliers)
consolidated_data.to_csv('consolidated_data.csv', index=False)
top_satisfied_customers = consolidated_data.sort_values(by='satisfaction_score', ascending=False)
 
top_10_satisfied_customers = top_satisfied_customers.head(10)
print("Top 10 Satisfied Customers:")
print(top_10_satisfied_customers[['MSISDN/Number', 'satisfaction_score']])
max_satisfaction_score = consolidated_data['satisfaction_score'].max()
 
low_satisfaction_threshold = 0.25 * max_satisfaction_score
moderate_satisfaction_threshold = 0.50 * max_satisfaction_score
high_satisfaction_threshold = 0.75 * max_satisfaction_score
 
consolidated_data['Satisfaction Level'] = pd.cut(consolidated_data['satisfaction_score'],
  bins=[-float('inf'), low_satisfaction_threshold, moderate_satisfaction_threshold, high_satisfaction_threshold, float('inf')],
  labels=['Low Satisfied', 'Moderately Satisfied', 'Satisfied', 'Highly Satisfied'])
 
satisfaction_counts = consolidated_data['Satisfaction Level'].value_counts()
 
satisfaction_percentage = (satisfaction_counts / len(df)) * 100
 
fig = px.pie(satisfaction_percentage, names=satisfaction_percentage.index, values=satisfaction_percentage.values,
  title='Percentage Distribution of Satisfaction Levels',
  labels={'index': 'Satisfaction Level', 'values': 'Percentage of Individuals'})
 
fig.show()
regression_features = ['Engagement Score', 'experience_score']
 
regression_data = consolidated_data.dropna(subset=regression_features)
 
X_train, X_test, y_train, y_test = train_test_split(
  regression_data[regression_features],
  regression_data['satisfaction_score'],
  test_size=0.2, random_state=42
)
 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
 
model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)
 
model.fit(X_train_scaled, y_train)
 
y_pred = model.predict(X_test_scaled)
 
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
 
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
 
consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])
 
print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])
print(f'Cross-validated R-squared scores: {cv_scores}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Satisfaction Score')
plt.ylabel('Predicted Satisfaction Score')
plt.title('Actual vs. Predicted Satisfaction Score')
plt.show()
scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
print(f'Cross-validated R-squared scores: {scores}')
regression_features = ['Engagement Score', 'experience_score']
 
regression_data = consolidated_data.dropna(subset=regression_features)
 
X_train, X_test, y_train, y_test = train_test_split(
  regression_data[regression_features],
  regression_data['satisfaction_score'],
  test_size=0.2, random_state=42
)
 
model = RandomForestRegressor(n_estimators=100, random_state=42)
 
model.fit(X_train, y_train)
 
y_pred = model.predict(X_test)
 
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
 
consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])
 
print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])
 
cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f'Cross-validated R-squared scores: {cv_scores}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Satisfaction Score')
plt.ylabel('Predicted Satisfaction Score')
plt.title('Actual vs. Predicted Satisfaction Score')
plt.show()
regression_features = ['Engagement Score', 'experience_score']
 
regression_data = consolidated_data.dropna(subset=regression_features)
 
X_train, X_test, y_train, y_test = train_test_split(
  regression_data[regression_features],
  regression_data['satisfaction_score'],
  test_size=0.2, random_state=42
)
 
model = GradientBoostingRegressor(n_estimators=100, random_state=42)
 
model.fit(X_train, y_train)
 
y_pred = model.predict(X_test)
 
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
 
consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])
 
print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])
 
cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f'Cross-validated R-squared scores: {cv_scores}')
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
clustering_features = ['Engagement Score', 'experience_score']
 
clustering_data = consolidated_data.dropna(subset=clustering_features)
 
scaler = StandardScaler()
clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])
 
kmeans = KMeans(n_clusters=2, random_state=42)
clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)
 
print("Results of K-means Clustering (k=2):")
print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])
clustering_features = ['Engagement Score', 'experience_score']
 
clustering_data = consolidated_data.dropna(subset=clustering_features)
 
scaler = StandardScaler()
clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])
 
kmeans = KMeans(n_clusters=3, random_state=42)
clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)
 
print("Results of K-means Clustering (k=2):")
print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])
clustering_features = ['Engagement Score', 'experience_score']
 
clustering_data = consolidated_data.dropna(subset=clustering_features)
 
scaler = StandardScaler()
clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])
 
kmeans = KMeans(n_clusters=4, random_state=42)
clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)
 
print("Results of K-means Clustering (k=2):")
print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])
cluster_aggregation = clustering_data.groupby('cluster').agg({
  'satisfaction_score': 'mean',
  'experience_score': 'mean'
}).reset_index()
 
print("Average Scores per Cluster:")
print(cluster_aggregation)
create_table_query = """
CREATE TABLE user_scores (
  user_id VARCHAR(255),
  engagement_score FLOAT,
  experience_score FLOAT,
  satisfaction_score FLOAT
);
"""
db_connection.execute_query(create_table_query)
user_scores_df = consolidated_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'satisfaction_score']]
user_scores_df.to_sql('user_scores', con=db_connection.engine, index=False, if_exists='append')
 
db_connection.close_connection()
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sqlalchemy import create_engine, text
from scipy.stats import zscore
import psycopg2
import os
import plotly.express as px
import random
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import tabulate
import plotly.graph_objects as go
os.chdir('..')
from db.connection import DatabaseConnection
from db.sql_preprocessor import DBFilter
from src.utils import missing_values_table, find_agg, convert_bytes_to_megabytes, fix_outlier, remove_outliers
from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot, create_table
db_connection = DatabaseConnection()
set_seaborn_style()
db_connection.connect()
query = "SELECT * FROM xdr_data"
df = db_connection.execute_query(query)
missing_values_df = missing_values_table(df)
print("Missing Values in df:")
print(missing_values_df)
grouped_data = df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()
 
grouped_data['TCP DL Retrans. Vol (Bytes)'].fillna(grouped_data['TCP DL Retrans. Vol (Bytes)'].mean(), inplace=True)
 
mean_value = grouped_data['TCP DL Retrans. Vol (Bytes)'].mean()
std_dev = grouped_data['TCP DL Retrans. Vol (Bytes)'].std()
 
outlier_threshold = 3
 
grouped_data['TCP DL Retrans. Vol (Bytes)'] = grouped_data['TCP DL Retrans. Vol (Bytes)'].apply(
  lambda x: mean_value if abs(x - mean_value) > outlier_threshold * std_dev else x
)
 
print(grouped_data)
 grouped_data_rtt = df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean().reset_index()
 
grouped_data_rtt['Avg RTT DL (ms)'].fillna(grouped_data_rtt['Avg RTT DL (ms)'].mean(), inplace=True)
 
mean_value_rtt = grouped_data_rtt['Avg RTT DL (ms)'].mean()
std_dev_rtt = grouped_data_rtt['Avg RTT DL (ms)'].std()
 
outlier_threshold_rtt = 3
 
grouped_data_rtt['Avg RTT DL (ms)'] = grouped_data_rtt['Avg RTT DL (ms)'].apply(
  lambda x: mean_value_rtt if abs(x - mean_value_rtt) > outlier_threshold_rtt * std_dev_rtt else x
)
 
print(grouped_data_rtt)
grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()
 
grouped_data_handset['Handset Type'].fillna(grouped_data_handset['Handset Type'].mode().iat[0], inplace=True)
 
print(grouped_data_handset)
grouped_data_throughput = df.groupby('MSISDN/Number').agg({
  'Avg Bearer TP DL (kbps)': 'mean',
  'Avg Bearer TP UL (kbps)': 'mean'
}).reset_index()
 
grouped_data_throughput['Avg Bearer TP DL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP DL (kbps)'].mean(), inplace=True)
grouped_data_throughput['Avg Bearer TP UL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP UL (kbps)'].mean(), inplace=True)
 
print(grouped_data_throughput)
top_10_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].nlargest(10)
 
bottom_10_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)
 
most_frequent_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)
 
print("Top 10 TCP values:")
print(top_10_tcp_values)
 
print("\nBottom 10 TCP values:")
print(bottom_10_tcp_values)
 
print("\nMost frequent TCP values:")
print(most_frequent_tcp_values)
 
top_10_data = pd.DataFrame({'Top 10 TCP Values': top_10_tcp_values.values}, index=top_10_tcp_values.index)
bottom_10_data = pd.DataFrame({'Bottom 10 TCP Values': bottom_10_tcp_values.values}, index=bottom_10_tcp_values.index)
most_frequent_data = pd.DataFrame({'Frequency': most_frequent_tcp_values.values}, index=most_frequent_tcp_values.index)
 
most_frequent_tcp_values = most_frequent_tcp_values[~(most_frequent_tcp_values.index == "16853393.739320666")]
 
fig, axes = plt.subplots(3, 1, figsize=(12, 15))
 
bottom_10_data.plot(kind='bar', ax=axes[1], legend=False)
axes[1].set_ylabel('TCP DL Retrans. Vol (Bytes)')
axes[1].set_title('Bottom 10 TCP Values')
 
plt.show()
bottom_10_data.plot(kind='bar', ax=axes[1], legend=False)
axes[1].set_ylabel('TCP DL Retrans. Vol (Bytes)')
axes[1].set_title('Bottom 10 TCP Values')
 
most_frequent_data.plot(kind='bar', ax=axes[2], legend=False)
axes[2].set_xlabel('Index')
axes[2].set_ylabel('Frequency')
axes[2].set_title('Most Frequent TCP Values')
 
plt.show()
data
top_10_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].nlargest(10)
 
bottom_10_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].nsmallest(10)
 
most_frequent_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].value_counts().head(10)
 
print("Top 10 RTT values:")
print(top_10_rtt_values)
 
print("\nBottom 10 RTT values:")
print(bottom_10_rtt_values)
 
print("\nMost frequent RTT values:")
print(most_frequent_rtt_values)
top_10_rtt_data = pd.DataFrame({'Top 10 RTT Values': top_10_rtt_values.values}, index=top_10_rtt_values.index)
bottom_10_rtt_data = pd.DataFrame({'Bottom 10 RTT Values': bottom_10_rtt_values.values}, index=bottom_10_rtt_values.index)
 
fig, axes = plt.subplots(2, 1, figsize=(12, 10))
 
bottom_10_rtt_data.plot(kind='bar', ax=axes[1], legend=False, color='skyblue')
axes[1].set_xlabel('Index')
axes[1].set_ylabel('Avg RTT DL (ms)')
axes[1].set_title('Bottom 10 RTT Values')
 
plt.show()
top_10_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].nlargest(10)
 
bottom_10_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].nsmallest(10)
 
most_frequent_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].value_counts().head(10)
 
top_10_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].nlargest(10)
 
bottom_10_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].nsmallest(10)
 
most_frequent_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].value_counts().head(10)
 
print("Top 10 Throughput values (DL):")
print(top_10_throughput_dl_values)
 
print("\nBottom 10 Throughput values (DL):")
print(bottom_10_throughput_dl_values)
 
print("\nMost frequent Throughput values (DL):")
print(most_frequent_throughput_dl_values)
 
print("\nTop 10 Throughput values (UL):")
print(top_10_throughput_ul_values)
 
print("\nBottom 10 Throughput values (UL):")
print(bottom_10_throughput_ul_values)
 
print("\nMost frequent Throughput values (UL):")
print(most_frequent_throughput_ul_values)
top_10_throughput_dl_data = pd.DataFrame({'Top 10 Throughput DL Values': top_10_throughput_dl_values.values}, index=top_10_throughput_dl_values.index)
bottom_10_throughput_dl_data = pd.DataFrame({'Bottom 10 Throughput DL Values': bottom_10_throughput_dl_values.values}, index=bottom_10_throughput_dl_values.index)
 
fig, axes = plt.subplots(2, 1, figsize=(12, 10))
 
bottom_10_throughput_dl_data.plot(kind='bar', ax=axes[1], legend=False, color='purple')
axes[1].set_xlabel('Index')
axes[1].set_ylabel('Avg Bearer TP DL (kbps)')
axes[1].set_title('Bottom 10 Throughput DL Values')
 
plt.show()
grouped_throughput = df.groupby('Handset Type').agg({
  'Avg Bearer TP DL (kbps)': 'mean',
  'Avg Bearer TP UL (kbps)': 'mean'
}).reset_index()
 
print(grouped_throughput)
grouped_throughput_handset = pd.merge(grouped_data_handset, grouped_data_throughput, on='MSISDN/Number')
 
print(grouped_throughput_handset)
 
grouped_throughput_handset = grouped_throughput_handset.groupby('Handset Type').agg({
  'Avg Bearer TP DL (kbps)': 'mean',
  'Avg Bearer TP UL (kbps)': 'mean'
}).reset_index()
 
print(grouped_throughput_handset)
merged_data = pd.merge(grouped_data_handset, grouped_data, on='MSISDN/Number')
 
print(merged_data)
 
grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()
 
grouped_data_tcp = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()
 
merged_data = pd.merge(grouped_data_handset, grouped_data_tcp, on='Handset Type')
 
print(merged_data)
top_10_mean_tcp_values = merged_data.nlargest(10, 'TCP DL Retrans. Vol (Bytes)')
 
plt.figure(figsize=(12, 6))
 
plt.bar(top_10_mean_tcp_values['Handset Type'], top_10_mean_tcp_values['TCP DL Retrans. Vol (Bytes)'], color='blue')
 
plt.xlabel('Handset Type')
plt.ylabel('Mean TCP DL Retrans. Vol (Bytes)')
plt.title('Top 10 Mean TCP DL Retrans. Vol (Bytes) for Each Handset Type')
 
plt.xticks(rotation=90)
 
plt.show()
top_10_mean_tcp_values
import pandas as pd
import numpy as np
import psycopg2
import matplotlib.pyplot as plt
import seaborn as sns
from psycopg2 import sql
from sqlalchemy import create_engine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
connection_params = {
  "host": "localhost",
  "user": "postgres",
  "password": "post33
  "port": "5432"
}
db_name = 'telecom'
connection_params["database"] = db_name
engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
db_conn = engine.connect()
df = pd.read_sql("select * from \"xdr_data\"", db_conn);
df
df.isna().sum()
percent_missing = df.isna().sum() * 100 / len(df)
missing_percent = pd.DataFrame({'column_name': df.columns,'percent_missing': percent_missing})
missing_percent.sort_values('percent_missing', inplace=True)
missing_percent
columns_to_remove = missing_percent[missing_percent['percent_missing'] >= 30].index.tolist()
columns_to_remove
columns_to_remove = [col for col in columns_to_remove if col not in ['TCP UL Retrans. Vol (Bytes)',
  'TCP DL Retrans. Vol (Bytes)']]
 
clean_df = df.drop(columns_to_remove,axis=1)
clean_df.isna().sum()
clean_df['TCP DL Retrans. Vol (Bytes)'] = clean_df['TCP DL Retrans. Vol (Bytes)'].fillna(method='bfill')
clean_df['TCP UL Retrans. Vol (Bytes)'] = clean_df['TCP UL Retrans. Vol (Bytes)'].fillna(method='bfill')
clean_df['Avg RTT DL (ms)'] = clean_df['Avg RTT DL (ms)'].fillna(method='ffill')
clean_df['Avg RTT UL (ms)'] = clean_df['Avg RTT UL (ms)'].fillna(method='ffill')
clean_df.isna().sum()
clean_df['Handset Type'] = clean_df['Handset Type'].fillna('undefined')
clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].fillna('undefined')
clean_df.isna().sum()
clean_df.dropna(inplace=True)
clean_df = clean_df.drop_duplicates()
clean_df.rename(columns={'Dur. (ms)': 'Dur (s)', 'Dur. (ms).1': 'Dur (ms)'}, inplace=True)
clean_df.drop(['Dur (s)'], axis=1, inplace=True)
clean_df.isna().sum()
clean_df.dtypes
clean_df['Bearer Id'] = clean_df['Bearer Id'].astype("int64")
clean_df['MSISDN/Number'] = clean_df['MSISDN/Number'].astype("int64")
clean_df
clean_df['Last Location Name'] = clean_df['Last Location Name'].astype("string")
clean_df['Handset Type'] = clean_df['Handset Type'].astype("string")
clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].astype("string")
clean_df.dtypes
clean_df.hist(bins=60,figsize=(20,10))
clean_df.columns
clean_df['Handset Type'].value_counts()
pd.DataFrame(clean_df['Handset Type'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
clean_df['Handset Manufacturer'].value_counts()
pd.DataFrame(clean_df['Handset Manufacturer'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
samsung_manu = clean_df[clean_df['Handset Manufacturer'] == 'Samsung']
pd.DataFrame(samsung_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
apple_manu = clean_df[clean_df['Handset Manufacturer'] == 'Apple']
pd.DataFrame(apple_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
huawei_manu = clean_df[clean_df['Handset Manufacturer'] == 'Huawei']
pd.DataFrame(huawei_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
clean_df.columns
def fix_outliers(df: pd.DataFrame):
  for col in df.select_dtypes('float64').columns.tolist():
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower = Q1 - (IQR * 1.5)
  upper = Q3 + (IQR * 1.5)
  df[col] = np.where(df[col] > upper, upper, df[col])
  df[col] = np.where(df[col] < lower, lower, df[col])
  return df
 
def hist(df:pd.DataFrame, column:str, color:str='orange')->None:
  sns.displot(data=df, x=column, color=color, kde=True, height=6, aspect=2)
  plt.title(f'Distribution of {column}', size=20, fontweight='bold')
  plt.show()
def scatter(df: pd.DataFrame, x_col: str, y_col: str) -> None:
  plt.figure(figsize=(12, 7))
  sns.scatterplot(data = df, x=x_col, y=y_col)
  plt.title(f'{x_col} Vs. {y_col}\n', size=20)
  plt.xticks(fontsize=14)
  plt.yticks( fontsize=14)
  plt.show()
clean_df = fix_outliers(clean_df)
 
clean_df['Dur (ms)'].plot(kind='box',figsize=(20,10))
 
hist(clean_df, 'Dur (ms)', 'blue')
clean_df['Data_Volume_Gaming (Bytes)'] = clean_df['Gaming DL (Bytes)'] +  clean_df['Gaming UL (Bytes)']
clean_df['Data_Volume_Social (Bytes)'] = clean_df['Social Media DL (Bytes)'] +  clean_df['Social Media UL (Bytes)']
clean_df['Data_Volume_Google (Bytes)'] = clean_df['Google DL (Bytes)'] +  clean_df['Google UL (Bytes)']
clean_df['Data_Volume_Email (Bytes)'] = clean_df['Email DL (Bytes)'] +  clean_df['Email UL (Bytes)']
clean_df['Data_Volume_Youtube (Bytes)'] = clean_df['Youtube DL (Bytes)'] +  clean_df['Youtube UL (Bytes)']
clean_df['Data_Volume_Netflix (Bytes)'] = clean_df['Netflix DL (Bytes)'] +  clean_df['Netflix UL (Bytes)']  
clean_df['Data_Volume_Other (Bytes)'] = clean_df['Other DL (Bytes)'] +  clean_df['Other UL (Bytes)']
clean_df['Data_Volume_Total (Bytes)'] = clean_df['Total DL (Bytes)'] +  clean_df['Total UL (Bytes)']
agg = clean_df.groupby('MSISDN/Number')[['Data_Volume_Gaming (Bytes)','Data_Volume_Social (Bytes)','Data_Volume_Google (Bytes)'\
  ,'Data_Volume_Email (Bytes)','Data_Volume_Youtube (Bytes)','Data_Volume_Netflix (Bytes)'\
  ,'Data_Volume_Other (Bytes)','Data_Volume_Total (Bytes)','Dur (ms)']].aggregate('sum')
agg_df = pd.DataFrame(agg)
agg_df['count'] = clean_df['MSISDN/Number'].value_counts()
agg_df
hist(clean_df, 'Data_Volume_Gaming (Bytes)')
hist(clean_df, 'Data_Volume_Social (Bytes)')
hist(clean_df, 'Data_Volume_Google (Bytes)')
hist(clean_df, 'Data_Volume_Email (Bytes)')
hist(clean_df, 'Data_Volume_Youtube (Bytes)')
hist(clean_df, 'Data_Volume_Netflix (Bytes)')
hist(clean_df, 'Data_Volume_Other (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Gaming (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Social (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Google (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Email (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Netflix (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Other (Bytes)', 'Data_Volume_Total (Bytes)')
def heatmap(df:pd.DataFrame, title:str, cmap='Reds')->None:
  plt.figure(figsize=(13, 7))
  sns.heatmap(df, annot=True, cmap=cmap, vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=True )
  plt.title(title, size=18, fontweight='bold')
  plt.show()
df_corr = clean_df[[
  'Data_Volume_Gaming (Bytes)', 'Data_Volume_Social (Bytes)', 'Data_Volume_Google (Bytes)',
  'Data_Volume_Email (Bytes)', 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Netflix (Bytes)',
  'Data_Volume_Other (Bytes)']
].corr()
df_corr
heatmap(df_corr, "Correlation of Data Volume of Applications")
df_float = clean_df.select_dtypes(include=[float])
df_int = clean_df.select_dtypes(include=[int])
df = clean_df
number_array = df_float.columns.to_list() + df_int.columns.to_list()
number_array
features = number_array
 
x = df.loc[:, features].values
 
x = StandardScaler().fit_transform(x)
number = 20
number_array = range(0,number)
pca = PCA(n_components=number)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
  , columns = number_array)
principalDf
pca.explained_variance_ratio_.sum()
number = 30
number_array = range(0,number)
pca = PCA(n_components=number)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
  , columns = number_array)
principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
number = 40
number_array = range(0,number)
pca = PCA(n_components=number)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
  , columns = number_array)
principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
number = 2
number_array = range(0,number)
pca = PCA(n_components=number)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
  , columns = ["1","2"])
principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
df.columns
try:
  print('writing to the database')
  frame = clean_df.to_sql(
  "clean_df", con=engine, if_exists='replace')
  print('successful')
except Exception as e:
  print("Error writing to database: ", e)
import pandas as pd
import numpy as np
import psycopg2
import matplotlib.pyplot as plt
import seaborn as sns
from psycopg2 import sql
from sqlalchemy import create_engine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
connection_params = {
  "host": "localhost",
  "user": "postgres",
  "password": "post33
  "port": "5432"
}
db_name = 'telecom'
connection_params["database"] = db_name
engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
db_conn = engine.connect()
df = pd.read_sql("select * from \"xdr_data\"", db_conn);
df
df.isna().sum()
percent_missing = df.isna().sum() * 100 / len(df)
missing_percent = pd.DataFrame({'column_name': df.columns,'percent_missing': percent_missing})
missing_percent.sort_values('percent_missing', inplace=True)
missing_percent
columns_to_remove = missing_percent[missing_percent['percent_missing'] >= 30].index.tolist()
columns_to_remove
clean_df = df.drop(columns_to_remove,axis=1)
clean_df.isna().sum()
clean_df['Avg RTT DL (ms)'] = clean_df['Avg RTT DL (ms)'].fillna(method='ffill')
clean_df['Avg RTT UL (ms)'] = clean_df['Avg RTT UL (ms)'].fillna(method='ffill')
clean_df.isna().sum()
clean_df['Handset Type'] = clean_df['Handset Type'].fillna('undefined')
clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].fillna('undefined')
clean_df.isna().sum()
clean_df.dropna(inplace=True)
clean_df = clean_df.drop_duplicates()
clean_df.drop(['Dur. (ms).1'], axis=1, inplace=True)
clean_df.isna().sum()
clean_df.dtypes
 
clean_df['Bearer Id'] = clean_df['Bearer Id'].astype("int64")
clean_df['MSISDN/Number'] = clean_df['MSISDN/Number'].astype("int64")
clean_df
clean_df['Last Location Name'] = clean_df['Last Location Name'].astype("string")
clean_df['Handset Type'] = clean_df['Handset Type'].astype("string")
clean_df['Handset Manufacturer'] = clean_df['Handset Manufacturer'].astype("string")
clean_df.dtypes
clean_df.hist(bins=60,figsize=(20,10))
clean_df.columns
clean_df['Handset Type'].value_counts()
pd.DataFrame(clean_df['Handset Type'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
clean_df['Handset Manufacturer'].value_counts()
pd.DataFrame(clean_df['Handset Manufacturer'].value_counts()[:10]).plot(kind='bar',figsize=(20,10))
samsung_manu = clean_df[clean_df['Handset Manufacturer'] == 'Samsung']
pd.DataFrame(samsung_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
apple_manu = clean_df[clean_df['Handset Manufacturer'] == 'Apple']
pd.DataFrame(apple_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
huawei_manu = clean_df[clean_df['Handset Manufacturer'] == 'Huawei']
pd.DataFrame(huawei_manu['Handset Type'].value_counts()[:5]).plot(kind='bar',figsize=(20,10))
clean_df.columns
def fix_outliers(df: pd.DataFrame):
  for col in df.select_dtypes('float64').columns.tolist():
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower = Q1 - (IQR * 1.5)
  upper = Q3 + (IQR * 1.5)
  df[col] = np.where(df[col] > upper, upper, df[col])
  df[col] = np.where(df[col] < lower, lower, df[col])
  return df
 
def hist(df:pd.DataFrame, column:str, color:str='orange')->None:
  sns.displot(data=df, x=column, color=color, kde=True, height=6, aspect=2)
  plt.title(f'Distribution of {column}', size=20, fontweight='bold')
  plt.show()
def scatter(df: pd.DataFrame, x_col: str, y_col: str) -> None:
  plt.figure(figsize=(12, 7))
  sns.scatterplot(data = df, x=x_col, y=y_col)
  plt.title(f'{x_col} Vs. {y_col}\n', size=20)
  plt.xticks(fontsize=14)
  plt.yticks( fontsize=14)
  plt.show()
clean_df = fix_outliers(clean_df)
 
clean_df['Dur. (ms)'].plot(kind='box',figsize=(20,10))
 
hist(clean_df, 'Dur. (ms)', 'blue')
clean_df['Data_Volume_Gaming (Bytes)'] = clean_df['Gaming DL (Bytes)'] +  clean_df['Gaming UL (Bytes)']
clean_df['Data_Volume_Social (Bytes)'] = clean_df['Social Media DL (Bytes)'] +  clean_df['Social Media UL (Bytes)']
clean_df['Data_Volume_Google (Bytes)'] = clean_df['Google DL (Bytes)'] +  clean_df['Google UL (Bytes)']
clean_df['Data_Volume_Email (Bytes)'] = clean_df['Email DL (Bytes)'] +  clean_df['Email UL (Bytes)']
clean_df['Data_Volume_Youtube (Bytes)'] = clean_df['Youtube DL (Bytes)'] +  clean_df['Youtube UL (Bytes)']
clean_df['Data_Volume_Netflix (Bytes)'] = clean_df['Netflix DL (Bytes)'] +  clean_df['Netflix UL (Bytes)']  
clean_df['Data_Volume_Other (Bytes)'] = clean_df['Other DL (Bytes)'] +  clean_df['Other UL (Bytes)']
clean_df['Data_Volume_Total (Bytes)'] = clean_df['Total DL (Bytes)'] +  clean_df['Total UL (Bytes)']
agg = clean_df.groupby('MSISDN/Number')[['Data_Volume_Gaming (Bytes)','Data_Volume_Social (Bytes)','Data_Volume_Google (Bytes)'\
  ,'Data_Volume_Email (Bytes)','Data_Volume_Youtube (Bytes)','Data_Volume_Netflix (Bytes)'\
  ,'Data_Volume_Other (Bytes)','Data_Volume_Total (Bytes)','Dur. (ms)']].aggregate('sum')
agg_df = pd.DataFrame(agg)
agg_df['count'] = clean_df['MSISDN/Number'].value_counts()
agg_df
hist(clean_df, 'Data_Volume_Gaming (Bytes)')
hist(clean_df, 'Data_Volume_Social (Bytes)')
hist(clean_df, 'Data_Volume_Google (Bytes)')
hist(clean_df, 'Data_Volume_Email (Bytes)')
hist(clean_df, 'Data_Volume_Youtube (Bytes)')
hist(clean_df, 'Data_Volume_Netflix (Bytes)')
hist(clean_df, 'Data_Volume_Other (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Gaming (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Social (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Google (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Email (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Netflix (Bytes)', 'Data_Volume_Total (Bytes)')
scatter(clean_df.sample(10000), 'Data_Volume_Other (Bytes)', 'Data_Volume_Total (Bytes)')
def heatmap(df:pd.DataFrame, title:str, cmap='Reds')->None:
  plt.figure(figsize=(13, 7))
  sns.heatmap(df, annot=True, cmap=cmap, vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=True )
  plt.title(title, size=18, fontweight='bold')
  plt.show()
df_corr = clean_df[[
  'Data_Volume_Gaming (Bytes)', 'Data_Volume_Social (Bytes)', 'Data_Volume_Google (Bytes)',
  'Data_Volume_Email (Bytes)', 'Data_Volume_Youtube (Bytes)', 'Data_Volume_Netflix (Bytes)',
  'Data_Volume_Other (Bytes)']
].corr()
df_corr
heatmap(df_corr, "Correlation of Data Volume of Applications")
df_float = clean_df.select_dtypes(include=[float])
df_int = clean_df.select_dtypes(include=[int])
df = clean_df
number_array = df_float.columns.to_list() + df_int.columns.to_list()
number_array
features = number_array
 
x = df.loc[:, features].values
 
x = StandardScaler().fit_transform(x)
pca.explained_variance_ratio_.sum()
number = 20
number_array = range(0,number)
pca = PCA(n_components=number)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
  , columns = number_array)
principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
number = 40
number_array = range(0,number)
pca = PCA(n_components=number)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
  , columns = number_array)
principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
number = 2
number_array = range(0,number)
pca = PCA(n_components=number)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
  , columns = ["1","2"])
principalDf
pca.explained_variance_ratio_
pca.explained_variance_ratio_.sum()
import pandas as pd
import numpy as np
import psycopg2
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from psycopg2 import sql
from sqlalchemy import create_engine
 
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, normalize
import plotly.io as pio
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
connection_params = {
  "host": "localhost",
  "user": "postgres",
  "password": "post33
  "port": "5432"
}
db_name = 'telecom'
connection_params["database"] = db_name
engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
db_conn = engine.connect()
df = pd.read_sql("select * from \"clean_df\"", db_conn);
df.info()
user_experience_df = df[[
  'MSISDN/Number',
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)',
  'Handset Type']]
user_experience_df
 
user_experience_df['Avg RTT Total (ms)'] = user_experience_df['Avg RTT DL (ms)'] + user_experience_df['Avg RTT UL (ms)']
user_experience_df['Avg Bearer TP Total (kbps)'] = user_experience_df['Avg Bearer TP DL (kbps)'] + user_experience_df['Avg Bearer TP UL (kbps)']
user_experience_df['TCP Retrans. Vol Total (Bytes)'] = user_experience_df['TCP DL Retrans. Vol (Bytes)'] + user_experience_df['TCP UL Retrans. Vol (Bytes)']
user_experience_df_agg = user_experience_df.groupby(
  'MSISDN/Number').agg({
  'Avg RTT Total (ms)': 'sum',
  'Avg Bearer TP Total (kbps)': 'sum',
  'TCP Retrans. Vol Total (Bytes)': 'sum',
  'Handset Type': [lambda x: x.mode()[0]]})
user_experience_df_agg.head()
user_experience_df_agg_new = pd.DataFrame(columns=[
  "Avg RTT Total (ms)",
  "Avg Bearer TP Total (kbps)",
  "TCP Retrans. Vol Total (Bytes)",
  "Handset Type"])
 
user_experience_df_agg_new["Avg RTT Total (ms)"] = user_experience_df_agg["Avg RTT Total (ms)"]['sum']
user_experience_df_agg_new["Avg Bearer TP Total (kbps)"] = user_experience_df_agg["Avg Bearer TP Total (kbps)"]['sum']
user_experience_df_agg_new["TCP Retrans. Vol Total (Bytes)"] = user_experience_df_agg["TCP Retrans. Vol Total (Bytes)"]['sum']
user_experience_df_agg_new["Handset Type"] = user_experience_df_agg["Handset Type"]['<lambda>']
user_experience_df_agg_new.head()
telco_tcp = user_experience_df_agg_new.sort_values('TCP Retrans. Vol Total (Bytes)', ascending=False)
top_10 = telco_tcp.head(10)['TCP Retrans. Vol Total (Bytes)']
bottom_10 = telco_tcp.tail(10)['TCP Retrans. Vol Total (Bytes)']
most_10 = user_experience_df_agg_new['TCP Retrans. Vol Total (Bytes)'].value_counts().head(10)
def plotly_multi_hist(sr, rows, cols, title_text, subplot_titles):
  fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)
  for i in range(rows):
  for j in range(cols):
  x = ["-> " + str(i) for i in sr[i+j].index]
  fig.add_trace(go.Bar(x=x, y=sr[i+j].values ), row=i+1, col=j+1)
  fig.update_layout(showlegend=False, title_text=title_text)
  fig.show()
pio.renderers.default = "notebook"
plotly_multi_hist([top_10, bottom_10, most_10], 1, 3,
  "TCP values", ['Top 10', 'Bottom 10', 'Most 10'])
telco_rtt = user_experience_df_agg_new.sort_values('Avg RTT Total (ms)', ascending=False)
top_10 = telco_rtt.head(10)['Avg RTT Total (ms)']
bottom_10 = telco_rtt.tail(10)['Avg RTT Total (ms)']
most_10 = user_experience_df_agg_new['Avg RTT Total (ms)'].value_counts().head(10)
plotly_multi_hist([top_10, bottom_10, most_10], 1, 3,
  "RTT values", ['Top 10', 'Bottom 10', 'Most 10'])
telco_tp = user_experience_df_agg_new.sort_values('Avg Bearer TP Total (kbps)', ascending=False)
top_10 = telco_tp.head(10)['Avg Bearer TP Total (kbps)']
bottom_10 = telco_tp.tail(10)['Avg Bearer TP Total (kbps)']
most_10 = user_experience_df_agg_new['Avg Bearer TP Total (kbps)'].value_counts().head(10)
plotly_multi_hist([top_10, bottom_10, most_10], 1, 3,
  "TP values", ['Top 10', 'Bottom 10', 'Most 10'])
handset_type_df = user_experience_df_agg_new.groupby('Handset Type').mean()
handset_type_df.head()
handset_tp_df = handset_type_df.sort_values(
  'Avg Bearer TP Total (kbps)', ascending=False)
handset_tp_df.head()
handset_tcp_df = handset_tp_df.sort_values(
  'TCP Retrans. Vol Total (Bytes)', ascending=False)
handset_tcp_df.head()
user_experience_df_agg_new.boxplot()
def replace_outliers_with_fences(df,columns):
  for col in columns:
  Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)
  IQR = Q3 - Q1
  cut_off = IQR * 1.5
  lower, upper = Q1 - cut_off, Q3 + cut_off
  df[col] = np.where(df[col] > upper, upper, df[col])
  df[col] = np.where(df[col] < lower, lower, df[col])
  return df
user_experience_df_agg_new = user_experience_df_agg_new[["Avg RTT Total (ms)",
  "Avg Bearer TP Total (kbps)",
  "TCP Retrans. Vol Total (Bytes)"]]
replace_outliers_with_fences(user_experience_df_agg_new,["Avg RTT Total (ms)",
  "Avg Bearer TP Total (kbps)",
  "TCP Retrans. Vol Total (Bytes)"])
user_experience_df_agg_new.boxplot()
scaler_instance = StandardScaler()
scaled_data = scaler_instance.fit_transform(user_experience_df_agg_new)
scaled_data
normalized_data = normalize(scaled_data)
normalized_data
kmeans = KMeans(n_clusters=3, random_state=1).fit(normalized_data)
kmeans.labels_
user_experience_df_agg_new.insert(0, 'cluster', kmeans.labels_)
user_experience_df_agg_new
user_experience_df_agg_new['cluster'].value_counts()
fig = px.scatter(user_experience_df_agg_new, x='TCP Retrans. Vol Total (Bytes)', y='Avg Bearer TP Total (kbps)',
  color='cluster', size='Avg RTT Total (ms)')
fig.show()
cluster0 = user_experience_df_agg_new[user_experience_df_agg_new["cluster"]==0]
cluster0[["Avg RTT Total (ms)",
  "Avg Bearer TP Total (kbps)",
  "TCP Retrans. Vol Total (Bytes)"]].describe()
cluster1 = user_experience_df_agg_new[user_experience_df_agg_new["cluster"]==1]
cluster1[["Avg RTT Total (ms)",
  "Avg Bearer TP Total (kbps)",
  "TCP Retrans. Vol Total (Bytes)"]].describe()
cluster2 = user_experience_df_agg_new[user_experience_df_agg_new["cluster"]==2]
cluster2[["Avg RTT Total (ms)",
  "Avg Bearer TP Total (kbps)",
  "TCP Retrans. Vol Total (Bytes)"]].describe()
 
try:
  print('writing to the database')
  frame = user_experience_df_agg_new.to_sql(
  "user_experience", con=engine, if_exists='replace')
  print('successful')
except Exception as e:
  print("Error writing to database: ", e)
with open("../models/user_experience.pkl", "wb") as f:
  pickle.dump(kmeans, f)
import pandas as pd
import numpy as np
import psycopg2
import matplotlib.pyplot as plt
import pickle
import seaborn as sns
from psycopg2 import sql
from sqlalchemy import create_engine
 
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, normalize
import plotly.io as pio
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
connection_params = {
  "host": "localhost",
  "user": "postgres",
  "password": "post33
  "port": "5432"
}
db_name = 'telecom'
connection_params["database"] = db_name
engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
db_conn = engine.connect()
df = pd.read_sql("select * from \"clean_df\"", db_conn);
df.info()
user_engagement_df = df[['MSISDN/Number', 'Bearer Id', 'Dur (ms)', 'Data_Volume_Total (Bytes)']]
user_engagement_df
user_engagement_df = user_engagement_df.groupby(
  'MSISDN/Number').agg({'Bearer Id': 'count', 'Dur (ms)': 'sum', 'Data_Volume_Total (Bytes)': 'sum'})
user_engagement_df = user_engagement_df.rename(
  columns={'Bearer Id': 'xDR Sessions'})
user_engagement_df.head()
user_engagement_df.nlargest(10, 'xDR Sessions')
user_engagement_df.nlargest(10, 'Dur (ms)')
user_engagement_df.nlargest(10, 'Data_Volume_Total (Bytes)')
user_engagement_df.boxplot();
def replace_outliers_with_fences(df,columns):
  for col in columns:
  Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)
  IQR = Q3 - Q1
  cut_off = IQR * 1.5
  lower, upper = Q1 - cut_off, Q3 + cut_off
  df[col] = np.where(df[col] > upper, upper, df[col])
  df[col] = np.where(df[col] < lower, lower, df[col])
  return df
user_engagement_df = replace_outliers_with_fences(user_engagement_df,['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)'])
user_engagement_df.boxplot()
scaler_instance = StandardScaler()
scaled_data = scaler_instance.fit_transform(user_engagement_df)
scaled_data
normalized_data = normalize(scaled_data)
normalized_data
kmeans = KMeans(n_clusters=3, random_state=1).fit(normalized_data)
kmeans.labels_
user_engagement_df.insert(0, 'cluster', kmeans.labels_)
user_engagement_df
user_engagement_df['cluster'].value_counts()
pio.renderers.default = "notebook"
fig = px.scatter(user_engagement_df, x='Data_Volume_Total (Bytes)', y='Dur (ms)',
  color='cluster', size='xDR Sessions')
fig.show()
sns.pairplot(
  user_engagement_df[['cluster','xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']],
  hue = 'cluster', diag_kind = 'kde',
  plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},
  height=3
);
cluster0 = user_engagement_df[user_engagement_df["cluster"]==0]
cluster0[['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']].describe()
cluster1 = user_engagement_df[user_engagement_df["cluster"]==1]
cluster1[['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']].describe()
cluster2 = user_engagement_df[user_engagement_df["cluster"]==2]
cluster2[['xDR Sessions', 'Dur (ms)', 'Data_Volume_Total (Bytes)']].describe()
user_app_engagement_df = df[['MSISDN/Number', 'Data_Volume_Gaming (Bytes)', 'Data_Volume_Social (Bytes)',
  'Data_Volume_Google (Bytes)', 'Data_Volume_Email (Bytes)', 'Data_Volume_Youtube (Bytes)',
  'Data_Volume_Netflix (Bytes)', 'Data_Volume_Other (Bytes)']]
user_app_engagement_df = user_app_engagement_df.groupby(
  'MSISDN/Number').sum()
user_app_engagement_df.head()
gaming = user_app_engagement_df.nlargest(10, "Data_Volume_Gaming (Bytes)")['Data_Volume_Gaming (Bytes)']
social_media = user_app_engagement_df.nlargest(10, "Data_Volume_Social (Bytes)")['Data_Volume_Social (Bytes)']
google = user_app_engagement_df.nlargest(10, "Data_Volume_Google (Bytes)")['Data_Volume_Google (Bytes)']
email = user_app_engagement_df.nlargest(10, "Data_Volume_Email (Bytes)")['Data_Volume_Email (Bytes)']
youtube = user_app_engagement_df.nlargest(10, "Data_Volume_Youtube (Bytes)")['Data_Volume_Youtube (Bytes)']
social_media = user_app_engagement_df.nlargest(10, "Data_Volume_Netflix (Bytes)")['Data_Volume_Netflix (Bytes)']
other = user_app_engagement_df.nlargest(10, "Data_Volume_Other (Bytes)")['Data_Volume_Other (Bytes)']
from scipy.spatial.distance import cdist
def choose_kmeans(df: pd.DataFrame, num: int):
  distortions = []
  inertias = []
  K = range(1, num)
  for k in K:
  kmeans = KMeans(n_clusters=k, random_state=0).fit(df)
  distortions.append(sum(
  np.min(cdist(df, kmeans.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])
  inertias.append(kmeans.inertia_)
  return (distortions, inertias)
distortions, inertias =  choose_kmeans(normalized_data, 20)
fig = make_subplots(
  rows=1, cols=2, subplot_titles=("Distortion", "Inertia")
)
fig.add_trace(go.Scatter(x=np.array(range(1, 20)), y=distortions), row=1, col=1)
fig.add_trace(go.Scatter(x=np.array(range(1, 20)), y=inertias), row=1, col=2)
fig.update_layout(title_text="The Elbow Method", height=500)
fig.show()
kmeans = KMeans(n_clusters=6, random_state=6).fit(normalized_data)
kmeans.labels_
user_engagement_df["cluster"]= kmeans.labels_
user_engagement_df
fig = px.scatter(user_engagement_df, x='Data_Volume_Total (Bytes)', y='Dur (ms)',
  color='cluster', size='xDR Sessions')
fig.show()
try:
  print('writing to the database')
  frame = user_engagement_df.to_sql(
  "user_engagement", con=engine, if_exists='replace')
  print('successful')
except Exception as e:
  print("Error writing to database: ", e)
with open("../models/user_engagement.pkl", "wb") as f:
  pickle.dump(kmeans, f)
import os
from abc import ABC, abstractmethod
from sqlalchemy.engine.base import Engine
from typing import Generic, Optional, TypeVar
from dotenv import load_dotenv
RawConnectionT = TypeVar("RawConnectionT")
class ConnectionBase(ABC, Generic[RawConnectionT]):
  """The abstract base class that all connections must inherit from
  This base class provides connection authors with a way to set up   database parameters like database name, database password, port, and also   instance of database engine.
  """
  def __init__(self, **kwargs):
  """ create a BaseConnection
  Parameters
  ----------
  **kwargs: dict
  dictionary of parameters of any length to pass tho the connection class
  Returns
  -------
  None
  """
  self._kwargs = kwargs if kwargs else {
  'user': os.getenv('DB_USER'),
  'password': os.getenv('DB_PASSWORD'),
  'host': os.getenv('DB_HOST'),
  'port': os.getenv('DB_PORT'),
  'database': os.getenv('DB_NAME'),
  }
  self._raw_instance: Optional[Engine] = self._connect()
  @property
  def _instance(self) -> RawConnectionT:
  """Get an instance of the underlying connection, creating a new one if needed."""
  if self._raw_instance is None:
  self._raw_instance = self._connect()
  return self._raw_instance
  @abstractmethod
  def _connect(self) -> RawConnectionT:
  """Create an instance of an underlying connection object.
  This abstract method is the one method that we require subclasses of
  BaseConnection to provide an implementation for. It is called when first
  creating a connection and when reconnecting after a connection is reset.
  Returns
  -------
  RawConnectionT
  The underlying connection object.
  """
  raise NotImplementedError
  def __enter__(self):
  self._raw_instance = self._connect()
  return self._raw_instance
  def __exit__(self, exc_type, exc_value, traceback):
  if self._raw_instance is not None:
  self._raw_instance.dispose()
import os, sys
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
import logging
from sqlalchemy import create_engine
from connections.connection_base import ConnectionBase
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
class PostgresConnection(ConnectionBase):
  """A class representing a connection to a PostgreSQL database.
  This class inherits from ConnectionBase, providing a standardized way to manage
  the connection to the PostgreSQL database.
  Parameters
  ----------
  **kwargs : dict
  Keyword arguments containing the database connection parameters.
  Attributes
  ----------
  _kwargs : dict
  Dictionary containing the database connection parameters.
  _raw_instance : sqlalchemy.engine.base.Engine
  """
  def __init__(self, **kwargs):
  super().__init__(**kwargs)
  def _connect(self):
  """Create an instance of the underlying SQLAlchemy Engine for PostgreSQL.
  Returns
  -------
  sqlalchemy.engine.base.Engine
  The SQLAlchemy Engine instance representing the PostgreSQL database connection.
  Raises
  ------
  Exception
  If there is an error connecting to the database.
  """
  try:
  engine = create_engine(
  f"postgresql://{self._kwargs['user']}:{self._kwargs['password']}@{self._kwargs['host']}:{self._kwargs['port']}/{self._kwargs['database']}"
  )
  engine.connect()
  return engine
  except Exception as e:
  logger.error(f"Error connecting to the database: {e}")
  raise
import os, sys
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
df = rd.read_data()
df.shape
df.head()
df.info()
df.isnull().sum()
df.duplicated().sum()
df.describe()
for col in df.columns:
  print(df[col].value_counts())
import os, sys
import pandas as pd
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
import scripts.write_to_db as wd
import scripts.data_cleaning as sd  
df = rd.read_data()
df = sd.drop_high_missing_columns(df, 0.7)
df.columns
def remove_missing_values():
  columns_to_check = [
  'Start',   'Start ms',   'End',   'End ms',   'Dur. (ms)',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
  'Activity Duration DL (ms)',   'Activity Duration UL (ms)',   'Dur. (ms).1',
  'Total UL (Bytes)',   'Total DL (Bytes)'
  ]
  return sd.remove_rows_with_missing_values(df, columns_to_check)
 
cleaned_df = remove_missing_values()
df.shape
cleaned_df.isnull().sum()
def impute_columns():
  columns_to_impute = [
  "Avg RTT DL (ms)",   "Avg RTT UL (ms)",   "TCP DL Retrans. Vol (Bytes)",   "TCP UL Retrans. Vol (Bytes)",   "DL TP < 50 Kbps (%)",   "50 Kbps < DL TP < 250 Kbps (%)",   "250 Kbps < DL TP < 1 Mbps (%)",   "DL TP > 1 Mbps (%)",   "UL TP < 10 Kbps (%)",   "10 Kbps < UL TP < 50 Kbps (%)",   "50 Kbps < UL TP < 300 Kbps (%)",   "UL TP > 300 Kbps (%)",   "HTTP DL (Bytes)",   "HTTP UL (Bytes)",   "Nb of sec with 125000B < Vol DL",   "Nb of sec with 1250B < Vol UL < 6250B",   "Nb of sec with 31250B < Vol DL < 125000B",   "Nb of sec with 6250B < Vol DL < 31250B",   "Nb of sec with Vol DL < 6250B",   "Nb of sec with Vol UL < 1250B"   ]
  return sd.impute_numeric_missing(cleaned_df, columns_to_impute)
 
imputed_df = impute_columns()
imputed_df.isnull().sum()
imputed_df.shape
def replace_with_mode():
  columns_to_replace = [
  'Bearer Id',
  'IMSI',
  'MSISDN/Number',
  'IMEI',   'Last Location Name',
  'Handset Manufacturer',
  'Handset Type'   ]
  return sd.replace_column_with_mode(imputed_df, columns_to_replace)
 
cleaned_df = replace_with_mode()
cleaned_df.isnull().sum()
cleaned_df.shape
def handle_outliers():
  columns = [
  "Avg RTT DL (ms)",   "Avg RTT UL (ms)",   "TCP DL Retrans. Vol (Bytes)",   "TCP UL Retrans. Vol (Bytes)",   "DL TP < 50 Kbps (%)",   "50 Kbps < DL TP < 250 Kbps (%)",   "250 Kbps < DL TP < 1 Mbps (%)",   "DL TP > 1 Mbps (%)",   "UL TP < 10 Kbps (%)",   "10 Kbps < UL TP < 50 Kbps (%)",   "50 Kbps < UL TP < 300 Kbps (%)",   "UL TP > 300 Kbps (%)",   "HTTP DL (Bytes)",   "HTTP UL (Bytes)",   "Nb of sec with 125000B < Vol DL",   "Nb of sec with 1250B < Vol UL < 6250B",   "Nb of sec with 31250B < Vol DL < 125000B",   "Nb of sec with 6250B < Vol DL < 31250B",   "Nb of sec with Vol DL < 6250B",   "Nb of sec with Vol UL < 1250B"   ]
  return sd.handle_outliers(cleaned_df, columns)
 
processed_df = handle_outliers()
processed_df.head()
processed_df.shape
wd.write_data(processed_df, 'processed_data')
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
def drop_high_missing_columns(
  df: pd.DataFrame,   threshold=0.8
  ) -> pd.DataFrame:
  """Drop columns with missing values exceeding a specified threshold.
  Parameters
  ----------
  - df: DataFrame
  The dataframe of raw data set.
  - threshold: float, default 0.8
  The threshold for the percentage of missing values in a column.
  Returns
  -------
  - Dataframe object
  where columns specified in the columns_to_drop list,   which have missing values, dropped.
  """
  missing_percentage = df.isnull().mean()
  columns_to_drop = missing_percentage[missing_percentage > threshold].index
  df_cleaned = df.drop(columns=columns_to_drop)
  return df_cleaned
def impute_numeric_missing(
  df:pd.DataFrame,   columns_list:list[str],
  strategy:str='mean'
  ) -> pd.DataFrame:
  """Impute missing values for numerical columns.
  Parameters
  ----------
  - df: DataFrame
  - columns_list
  list of columnst to Impute the missing values
  - strategy: str, default 'mean'
  Imputation strategy, options: 'mean', 'zero'.
  Returns
  -------
  - Dataframe object
  with imputed missing columns from columns_list
  """
  numeric_columns = df[columns_list].select_dtypes(include='number').columns
  if strategy == 'zero':
  df_imputed = df.copy()   df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(0)
  else:
  imputation_values = df[numeric_columns].mean() if strategy == 'mean' else df[numeric_columns].median()
  df_imputed = df.copy()   df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(imputation_values)
  return df_imputed
def remove_rows_with_missing_values(
  df: pd.DataFrame,   columns_to_check: list[str]
  ):
  """
  Remove rows from a DataFrame where any of the specified columns have missing values.
  Parameters
  ----------
  df : pandas.DataFrame
  The input DataFrame.
  columns_to_check : list of str
  A list of column names to check for missing values. Rows will be dropped
  if any of these columns have missing values.
  Returns
  -------
  pandas.DataFrame
  A new DataFrame with rows removed where any of the specified columns have missing values.
  """
  cleaned_df = df.dropna(subset=columns_to_check, how='any')
  return cleaned_df
import pandas as pd
def replace_column_with_mode(
  df: pd.DataFrame,   column_names: list[str]
  ) -> pd.DataFrame:
  """Replace missing values in a column with its mode.
  Parameters
  ----------
  - df: DataFrame
  - column_list: list[str]
  Name of the list of columns to replace missing values.
  Returns
  -------
  - DataFrame object
  with missing values in the specified column replaced by its mode.
  """
  df_mode = df.copy()   for column_name in column_names:
  mode_value = df_mode[column_name].mode().iloc[0]
  df_mode[column_name] = df_mode[column_name].fillna(mode_value)
  return df_mode
def handle_outliers(
  df: pd.DataFrame,   columns: list[str],   method:str='mean'
  ) -> pd.DataFrame:
  """Handle outliers in specified columns using a specified method.
  Parameters
  ----------
  - df: DataFrame
  - columns: list
  List of columns to handle outliers.
  - method: str, default 'clip'
  Outlier handling method, options: 'clip', 'remove', 'mean'
  Returns
  -------
  - Dataframe object
  where outliers are handled
  """
  if method == 'clip':
  for col in columns:
  df[col] = np.clip(df[col], df[col].quantile(0.05), df[col].quantile(0.95))
  elif method == 'remove':
  for col in columns:
  q1 = df[col].quantile(0.25)
  q3 = df[col].quantile(0.75)
  iqr = q3 - q1
  df = df[(df[col] >= q1 - 1.5 * iqr) & (df[col] <= q3 + 1.5 * iqr)]
  elif method == 'mean':
  for col in columns:
  mean_val = df[col].mean()
  df[col] = np.where(
  (df[col] < df[col].quantile(0.05)) | (df[col] > df[col].quantile(0.95)),
  mean_val,
  df[col]
  )
  return df
import os, sys
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
def read_data(table_name='xdr_data'):
  """Read data from a PostgreSQL database table.
  Parameters
  ----------
  table_name : str, optional
  The name of the table from which to fetch data. Default is 'xdr_data'.
  Returns
  -------
  pandas.DataFrame
  A DataFrame containing the data retrieved from the specified table.
  Raises
  ------
  Exception
  If there is an error during the data retrieval process.
  Notes
  -----
  This function uses the `PostgresConnection` class to establish a connection
  to the PostgreSQL database. Ensure that the necessary environment variables
  (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
  this function.
  """
  try:
  with PostgresConnection() as postgres_conn:
  df = pd.read_sql_query(f"SELECT * FROM {table_name};", postgres_conn)
  logger.info('Data fetched succesfully')
  return df
  except Exception as e:
  logger.error(f"Error in the main block: {e}")
import os, sys
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
def write_data(
  df: pd.DataFrame,
  table_name : str
  ) -> None:
  """Write DataFrame to a PostgreSQL database table.
  Parameters
  ----------
  df : pandas.DataFrame
  The DataFrame containing the data to be written.
  table_name : str
  The name of the table where the data will be written.
  Returns
  -------
  None
  Raises
  ------
  Exception
  If there is an error during the data writing process.
  Notes
  -----
  This function uses the `PostgresConnection` class to establish a connection
  to the PostgreSQL database. Ensure that the necessary environment variables
  (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
  this function.
  """
  try:
  conn = PostgresConnection()
  df.to_sql(name=table_name, con=conn._instance, index=False, if_exists='replace')
  logger.info('Data written successfully')
  except Exception as e:
  logger.error(f"Error in the main block: {e}")
import os, sys
import pandas as pd
from pandasql import sqldf
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
import scripts.utils as util
df = rd.read_data(table_name='processed_data')
df.shape
pysqldf = lambda q: sqldf(q, globals())
df.columns
query = '''   SELECT DISTINCT   "Handset Type",   COUNT(*) as UsageCount
  FROM df
  GROUP BY "Handset Type"   ORDER BY UsageCount DESC
  limit 10
'''
 
result_df = pysqldf(query)
result_df
query = '''   SELECT DISTINCT   "Handset Manufacturer",   COUNT(*) as "Number of Users"
  FROM df
  GROUP BY "Handset Manufacturer"   ORDER BY "Number of Users" DESC
  limit 3
'''
 
result_df = pysqldf(query)
result_df
query = '''   SELECT "Handset Manufacturer", AVG("Dur. (ms)") AS "Avg Session Duration (ms)"
  FROM df
  WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')
  GROUP BY "Handset Manufacturer";
 
'''
result_df = pysqldf(query)
result_df
query = '''   SELECT
  "Handset Manufacturer",
  SUM("Total UL (Bytes)" + "Total DL (Bytes)") AS "Total Data Volume (Bytes)"
  FROM df
  WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')
  GROUP BY "Handset Manufacturer";
'''
result_df = sqldf(query)
result_df
query = '''   WITH RankedHandsets AS (
  SELECT
  "Handset Manufacturer",
  "Handset Type",
  RANK() OVER (PARTITION BY "Handset Manufacturer" ORDER BY COUNT(*) DESC) AS "Rank"
  FROM df
  WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')
  GROUP BY "Handset Manufacturer", "Handset Type"
  )
  SELECT
  "Handset Manufacturer",
  "Handset Type",
  "Rank"
  FROM RankedHandsets
  WHERE "Rank" <= 5;
'''
 
result_df = sqldf(query)
result_df
query = '''   SELECT "MSISDN/Number" AS UserIdentifer,
  COUNT(*) AS NumberOfXDRSessions
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY NumberOfXDRSessions DESc;
'''
 
result_df = pysqldf(query)
result_df
query = '''
  SELECT COUNT(*) AS num_users
  FROM (
  SELECT COUNT(*) AS NumberOfXDRSessions
  FROM df
  GROUP BY "MSISDN/Number"
  HAVING COUNT(*) >= 10
  ) AS subquery
'''
 
result_df = pysqldf(query)
result_df
query = '''   SELECT
  "MSISDN/Number" AS UserIdentifier,
  SUM("Dur. (ms)") / 1000 AS TotalSessionDurationInSeconds
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalSessionDurationInSeconds DESC;
 
'''
 
result_df = pysqldf(query)
result_df
query = '''   SELECT
  CAST(SUBSTRING(CAST("Start" AS TEXT), INSTR(CAST("Start" AS TEXT), ' ') + 1, INSTR(CAST("Start" AS TEXT), ':') - INSTR(CAST("Start" AS TEXT), ' ') - 1) AS INTEGER) AS HourOfDay,
  COUNT(*) AS NumberOfSessions
  FROM df
  WHERE "Start" IS NOT NULL
  GROUP BY HourOfDay
  ORDER BY NumberOfSessions DESC;
 
'''
 
result_df = pysqldf(query)  
result_df
query = '''   SELECT
  "MSISDN/Number" AS User,
  COUNT(*) AS SessionCount,
  SUM("Dur. (ms)") AS TotalSessionDuration
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalSessionDuration DESC;
 
'''
 
result_df = pysqldf(query)
result_df
result_df = util.get_total_download_for_each_app(df, "Total DL (Bytes)", "Total UL (Bytes)")
query = '''SELECT COUNT(*) AS num_users
  FROM result_df
  WHERE Total >= 100000000;
  '''
 
pysqldf(query)
util.get_total_download_for_each_app(df, "Social Media DL (Bytes)", "Social Media UL (Bytes)")
util.get_total_download_for_each_app(df, "YouTube DL (Bytes)", "YouTube UL (Bytes)")
util.get_total_download_for_each_app(df, "Netflix DL (Bytes)", "Netflix UL (Bytes)")
util.get_total_download_for_each_app(df, "Google DL (Bytes)", "Google UL (Bytes)")
util.get_total_download_for_each_app(df, "Email DL (Bytes)", "Email UL (Bytes)")
util.get_total_download_for_each_app(df, "Gaming DL (Bytes)", "Gaming UL (Bytes)")
util.get_total_download_for_each_app(df, "Other DL", "Other UL")
import os, sys
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandasql import sqldf
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
import scripts.data_cleaning as dc
import scripts.utils as util
df = rd.read_data(table_name='processed_data')
quantitative_columns = [
  "Avg RTT DL (ms)",   "Avg RTT UL (ms)",   "Avg Bearer TP DL (kbps)",
  "Avg Bearer TP UL (kbps)",   "TCP DL Retrans. Vol (Bytes)",   "TCP UL Retrans. Vol (Bytes)",   "DL TP < 50 Kbps (%)",   "50 Kbps < DL TP < 250 Kbps (%)",   "250 Kbps < DL TP < 1 Mbps (%)",   "DL TP > 1 Mbps (%)",   "UL TP < 10 Kbps (%)",   "10 Kbps < UL TP < 50 Kbps (%)",   "50 Kbps < UL TP < 300 Kbps (%)",   "UL TP > 300 Kbps (%)",
  "Activity Duration DL (ms)",
  "Activity Duration UL (ms)",   "HTTP DL (Bytes)",   "HTTP UL (Bytes)",   "Dur. (ms).1",   "Nb of sec with 125000B < Vol DL",   "Nb of sec with 1250B < Vol UL < 6250B",   "Nb of sec with 31250B < Vol DL < 125000B",   "Nb of sec with 6250B < Vol DL < 31250B",   "Nb of sec with Vol DL < 6250B",   "Nb of sec with Vol UL < 1250B",   "Social Media DL (Bytes)",
  "Social Media UL (Bytes)",
  "Youtube DL (Bytes)",
  "Youtube UL (Bytes)",
  "Netflix DL (Bytes)",
  "Netflix UL (Bytes)",
  "Google DL (Bytes)",
  "Google UL (Bytes)",
  "Email DL (Bytes)",
  "Email UL (Bytes)",
  "Gaming DL (Bytes)",
  "Gaming UL (Bytes)",
  "Other DL (Bytes)",
  "Other UL (Bytes)",
  "Total DL (Bytes)",
  "Total UL (Bytes)"
]
df = dc.handle_outliers(df, quantitative_columns)
df.shape
descriptive_stats = df[quantitative_columns].describe()
descriptive_stats
plt.hist(df['Avg RTT UL (ms)'], bins=20, color='blue', alpha=0.7)
plt.xlabel('Average RTT UL (ms)')
plt.ylabel('Frequency')
plt.title('Histogram of Avg RTT DL (ms)')
plt.show()
 
sns.boxplot(x=df['Avg Bearer TP DL (kbps)'])
plt.xlabel('Average Bearer TP DL (kbps)')
plt.title('Boxplot of Avg Bearer TP DL (kbps)')
plt.show()
plt.figure(figsize=(8, 5))
df['Handset Type'].value_counts().head(10).plot(kind='bar', color='green')
plt.xlabel('Handset Type')
plt.ylabel('Count')
plt.title('Distribution of Handset Types')
plt.show()
correlation_matrix = df[
  [
  "Social Media DL (Bytes)",
  "Social Media UL (Bytes)",
  "Youtube DL (Bytes)",
  "Youtube UL (Bytes)",
  "Netflix DL (Bytes)",
  "Netflix UL (Bytes)",
  "Google DL (Bytes)",
  "Google UL (Bytes)",
  "Email DL (Bytes)",
  "Email UL (Bytes)",
  "Gaming DL (Bytes)",
  "Gaming UL (Bytes)",
  "Other DL (Bytes)",
  "Other UL (Bytes)",
  "Total DL (Bytes)",
  "Total UL (Bytes)"
]].corr()
correlation_matrix
 
sns.scatterplot(x='Netflix DL (Bytes)', y='Total DL (Bytes)', data=df)
plt.title('Youtube download vs Total Download')
plt.show()
import os, sys
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandasql import sqldf
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
import scripts.data_cleaning as dc
import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = '''   SELECT
  "MSISDN/Number" AS MSISDN,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY SessionFrequency DESC
  LIMIT 10
'''
 
result_df = pysqldf(query)
result_df
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration
  FROM df
  GROUP BY "MSISDN/Number"   ORDER BY SessionDuration DESC
  LIMIT 10;
  '''  
result_df = pysqldf(query)
result_df
query = '''   SELECT   "MSISDN/Number",
  SUM("Total DL (Bytes)") AS TotalDownload,
  SUM("Total UL (Bytes)") AS TotalUpload,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalTraffic DESC
  LIMIT 10;
'''
 
pysqldf(query)
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  '''  
aggregated_df = pysqldf(query)
aggregated_df.tail()
scaler = MinMaxScaler()
columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]
customer_id = aggregated_df['MSISDN/Number']
transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])
normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)
df_normalized = pd.concat([customer_id, normalized_data], axis=1)
df_normalized
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')
 
ax.set_xlabel('SessionDuration')
ax.set_ylabel('SessionFrequency')
ax.set_zlabel('TotalTraffic')
 
plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')
plt.show()
processed_df = dc.handle_outliers(normalized_data, columns_to_normalize)
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(processed_df['SessionDuration'], processed_df['SessionFrequency'], processed_df['TotalTraffic'], c='blue', marker='o')
 
ax.set_xlabel('SessionDuration')
ax.set_ylabel('SessionFrequency')
ax.set_zlabel('TotalTraffic')
 
plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')
plt.show()
selected_columns = columns_to_normalize
X = processed_df[selected_columns]
k = 3
 
kmeans = KMeans(n_clusters=3, random_state=0)  
kmeans.fit(X)
 
processed_df['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = processed_df[processed_df['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['SessionDuration'],   cluster_data['SessionFrequency'],   cluster_data['TotalTraffic'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('SessionDuration')
ax.set_ylabel('SessionFrequency')
ax.set_zlabel('TotalTraffic')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')
plt.show()
import os, sys
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandasql import sqldf
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
import scripts.data_cleaning as dc
import scripts.write_to_db as wd
import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = '''   SELECT
  "MSISDN/Number" AS MSISDN,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY SessionFrequency DESC
  LIMIT 10
'''
 
result_df = pysqldf(query)
result_df
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration
  FROM df
  GROUP BY "MSISDN/Number"   ORDER BY SessionDuration DESC
  LIMIT 10;
  '''  
result_df = pysqldf(query)
result_df
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration
  FROM df
  GROUP BY "MSISDN/Number"   ORDER BY SessionDuration DESC;
  '''  
result_df = pysqldf(query)
result_df['SessionDuration'].median()
query = '''   SELECT   "MSISDN/Number",
  SUM("Total DL (Bytes)") AS TotalDownload,
  SUM("Total UL (Bytes)") AS TotalUpload,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalTraffic DESC
  LIMIT 10;
'''
 
pysqldf(query)
query = '''   SELECT   "MSISDN/Number",
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalTraffic DESC
'''
 
pysqldf(query)['TotalTraffic'].median()
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  '''  
aggregated_df = pysqldf(query)
aggregated_df.tail()
aggregated_df = dc.handle_outliers(aggregated_df, ["SessionDuration","SessionFrequency","TotalTraffic"])
scaler = MinMaxScaler()
columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]
customer_id = aggregated_df['MSISDN/Number']
transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])
normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)
df_normalized = pd.concat([customer_id, normalized_data], axis=1)
df_normalized
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')
 
ax.set_xlabel('SessionDuration')
ax.set_ylabel('SessionFrequency')cases = 707,443
 
Total deaths = 3,891
ax.set_zlabel('TotalTraffic')
 
plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')
plt.show()
selected_columns = columns_to_normalize
X = df_normalized[selected_columns]
k = 3
 
kmeans = KMeans(n_clusters=3, random_state=0, n_init=25)  
kmeans.fit(X)
 
df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['SessionDuration'],   cluster_data['SessionFrequency'],   cluster_data['TotalTraffic'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('SessionDuration')
ax.set_ylabel('SessionFrequency')
ax.set_zlabel('TotalTraffic')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')
plt.show()
grouped_df = (df_normalized
  .groupby('Cluster')[["SessionDuration", "SessionFrequency", "TotalTraffic"]]
  .agg(['min', 'max', 'mean', 'sum']))
 
grouped_df
wcss = []
X = df_normalized.iloc[:, [1, 3]]
 
for i in range(1, 11):
  kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)
 
plt.plot(range(1, 11), wcss)
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
selected_columns = columns_to_normalize
X = df_normalized[selected_columns]
k = 7
 
kmeans = KMeans(n_clusters=k, random_state=0, n_init=25)  
kmeans.fit(X)
 
df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green', 3:'black', 4:'yellow', 5:'brown', 6:'purple'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['SessionDuration'],   cluster_data['SessionFrequency'],   cluster_data['TotalTraffic'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('SessionDuration')
ax.set_ylabel('SessionFrequency')
ax.set_zlabel('TotalTraffic')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')
plt.show()
wd.write_data(df_normalized, 'user_engagement')
import os, sys
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandasql import sqldf
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
import scripts.write_to_db as wd
import scripts.data_cleaning as dc
import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
def helper(field1 : str, field2 : str) -> pd.DataFrame:
  query = f'''   SELECT
  "MSISDN/Number" AS CustomerID,
  AVG("{field1}") AS AvgDL,
  AVG("{field2}") AS AvgUL,
  (
  AVG("{field1}") +   AVG("{field2}")
  ) / 2 AS Avg
  FROM df
  GROUP BY "MSISDN/Number";
  '''
  return pysqldf(query)
avg_retransmission = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")
avg_retransmission
avg_rtt = helper("Avg RTT DL (ms)", "Avg RTT UL (ms)")
avg_rtt
query = '''   SELECT
  "MSISDN/Number" AS User_MSISDN,
  "Handset Type" AS HandsetType,
  COUNT(*) AS HandsetTypeCount
  FROM df
  GROUP BY "MSISDN/Number", "Handset Type";
 
'''
 
count_handset = pysqldf(query)  
count_handset
avg_throughput = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")
avg_throughput
def helper(field, order):
  query = f'''   SELECT   "{field}",
  COUNT("{field}") AS "Frequency"
  FROM df
  GROUP BY "{field}"
  ORDER BY "Frequency" {order}   LIMIT 10;
  '''
  return pysqldf(query)
top_tcp = helper("TCP DL Retrans. Vol (Bytes)", "DESC")
top_tcp
bottom_tcp = helper("TCP DL Retrans. Vol (Bytes)", "ASC")
bottom_tcp
top_rtt = helper("Avg RTT DL (ms)", "DESC")
top_rtt
bottom_rtt = helper("Avg RTT DL (ms)", "ASC")
bottom_rtt
top_throughput = helper("Avg Bearer TP DL (kbps)", "Desc")
top_throughput
bottom_throughput = helper("Avg Bearer TP DL (kbps)", "ASC")
bottom_throughput
def helper(field1, field2):
  query = f'''
  SELECT
  "Handset Type" AS HandsetType,
  AVG("{field1}" + "{field2}") / 2 AS Average
  FROM df
  GROUP BY "Handset Type"
  ORDER BY Average DESC;
  '''
  return pysqldf(query)
avg_throughput_per_handset = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")
avg_throughput_per_handset
average_tcp_per_handset = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")
average_tcp_per_handset
query = '''   SELECT   "MSISDN/Number" AS CustomerID,
  (AVG("TCP DL Retrans. Vol (Bytes)") + AVG("TCP UL Retrans. Vol (Bytes)")) / 2 AS AvgTCP,
  (AVG("Avg RTT DL (ms)") + AVG("Avg RTT UL (ms)")) / 2 AS AvgRTT,
  (AVG("Avg Bearer TP DL (kbps)") + AVG("Avg Bearer TP UL (kbps)")) / 2 AS AvgThroughput
  FROM df
  GROUP BY CustomerID
'''
 
agg_df = pysqldf(query)
agg_df.head()
agg_df = dc.handle_outliers(agg_df, ["AvgTCP", "AvgRTT","AvgThroughput"])
scaler = MinMaxScaler()
columns_to_normalize = ["AvgTCP", "AvgRTT","AvgThroughput"]
 
customer_id = agg_df['CustomerID']
transformed_data = scaler.fit_transform(agg_df[columns_to_normalize])
normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)
 
df_normalized = pd.concat([customer_id, normalized_data], axis=1)
df_normalized
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(df_normalized["AvgTCP"], df_normalized["AvgRTT"], df_normalized["AvgThroughput"], c='blue', marker='o')
 
ax.set_xlabel("AvgTCP")
ax.set_ylabel("AvgRTT")
ax.set_zlabel("AvgThroughput")
 
plt.title('3D Scatter Plot of AvgTCP, AvgRTT, and AvgThroughput')
plt.show()
wcss = []
X = df_normalized.iloc[:, [1, 3]]
 
for i in range(1, 11):
  kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)
 
plt.plot(range(1, 11), wcss)
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
selected_columns = columns_to_normalize
X = df_normalized[selected_columns]
k = 3
 
kmeans = KMeans(n_clusters=k, random_state=0, n_init=15)  
kmeans.fit(X)
 
df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['AvgTCP'],   cluster_data['AvgRTT'],   cluster_data['AvgThroughput'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('AvgTCP')
ax.set_ylabel('AvgRTT')
ax.set_zlabel('AvgThroughput')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')
plt.show()
grouped_df = (df_normalized
  .groupby('Cluster')[["AvgTCP", "AvgRTT", "AvgThroughput"]]
  .agg(['min', 'max', 'mean', 'sum']))
 
grouped_df
wd.write_data(df_normalized, 'user_experience')
import os, sys
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandasql import sqldf
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import euclidean_distances
from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd
import scripts.data_cleaning as dc
import scripts.write_to_db as wd
import scripts.utils as util
experience_df = rd.read_data(table_name='user_experience')
engagement_df = rd.read_data(table_name='user_engagement')
pysqldf = lambda q: sqldf(q, globals())
experience_df.head()
engagement_df.head()
merged_df = engagement_df.merge(experience_df, left_on='MSISDN/Number', right_on="CustomerID", how='inner')
merged_df.head()
experience_columns = ["AvgTCP","AvgRTT","AvgThroughput"]
engagement_columns = ["SessionDuration","SessionFrequency","TotalTraffic"]
k = 3
X_eng = merged_df[engagement_columns]
X_exp = merged_df[experience_columns]
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_eng)
merged_df['cluster'] = cluster_labels
query = '''   SELECT   cluster,
  AVG(("SessionDuration"+"SessionFrequency"+"TotalTraffic") / 3) AS AVG   FROM merged_df
  GROUP BY cluster;
'''
 
grouped = pysqldf(query)
less_engaged_cluster = grouped['AVG'].idxmin()
less_engaged_cluster
less_engaged_cluster_center = kmeans.cluster_centers_[less_engaged_cluster]
 
distances = euclidean_distances(X_eng, [less_engaged_cluster_center]).flatten()
merged_df['EngagementScore'] =  distances
merged_df.head()
query = '''   SELECT   cluster,
  AVG(("AvgTCP" + "AvgRTT" + "AvgThroughput") / 3) AS AVG   FROM merged_df
  GROUP BY cluster;
'''
 
grouped = pysqldf(query)
worst_experience_cluster = grouped['AVG'].idxmin()
worst_experience_cluster
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_exp)
 
worst_experience_cluster_cluster_center = kmeans.cluster_centers_[worst_experience_cluster]
 
distances = euclidean_distances(X_eng, [worst_experience_cluster_cluster_center]).flatten()
merged_df['ExperienceScore'] =  distances
merged_df.head()
merged_df['SatisfactionScore'] = (merged_df['EngagementScore'] + merged_df['ExperienceScore']) / 2
merged_df.head()
query = '''   SELECT   "MSISDN/Number",
  SatisfactionScore
  FROM merged_df
  ORDER BY SatisfactionScore DESC
  LIMIT 10
'''
 
pysqldf(query)
merged_df['SatisfactionScore'].describe()
bins = np.arange(0, 1.4142, 0.35355)
labels = ["Low Satisfaction", "Medium Satisfaction", "High Satisfaction"]
 
merged_df['SatisfactionGroup'] = pd.cut(merged_df['SatisfactionScore'], bins=bins, labels=labels, include_lowest=True)
 
group_counts = merged_df['SatisfactionGroup'].value_counts()
 
plt.pie(group_counts, labels=group_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Satisfaction Score Distribution')
 
plt.show()
bins
features = engagement_columns + experience_columns
X = merged_df[features]
y = merged_df.SatisfactionScore
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()  
model.fit(X_train, y_train)
 
y_pred  = model.predict(X_test)
selected_columns = ["EngagementScore", "SatisfactionScore"]
X = merged_df[selected_columns]
k = 2
 
kmeans = KMeans(n_clusters=k, random_state=0, n_init=15)  
kmeans.fit(X)
 
merged_df['cluster'] = kmeans.labels_
cluster_colors = {0: 'red', 1: 'blue'}
for cluster_label, color in cluster_colors.items():
  cluster_df = merged_df[merged_df["cluster"] == cluster_label]
  plt.scatter(cluster_df["EngagementScore"], cluster_df["ExperienceScore"], c=color)
 
plt.xlabel("EngagementScore")
plt.ylabel("ExperienceScore")
plt.title("'2D Scatter Plot'of EngagementScore vs ExperienceScore")
 
plt.show()
query = '''   SELECT   cluster,
  AVG("SatisfactionScore") As "Average Satisfaction"
  FROM merged_df   GROUP BY cluster;
'''
avg_satsifaction = pysqldf(query)
avg_satsifaction
query = '''   SELECT   cluster,
  AVG("ExperienceScore") AS "Average Experience"   FROM merged_df   GROUP BY cluster  
'''
 
avg_experience = pysqldf(query)
avg_experience
df = pd.DataFrame({
  "UserID": merged_df["CustomerID"],
  "SatisfactionScore": merged_df["SatisfactionScore"],
  "ExperienceScore": merged_df["ExperienceScore"],
  "EngagementScore": merged_df["EngagementScore"]
})
 
wd.write_data(df, "satisfaction_score")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import warnings
warnings.filterwarnings('ignore')
 
sys.path.append('../helpers/')
 
from data import Database
from utils import Helpers
db = Database(host="localhost", database="telecom", user="postgres", password="heisenberg")
df = db.read_table_to_dataframe('xdr_data')
df.head()
df.info()
df.describe()
def percent_missing(df):
  totalCells = np.product(df.shape)
  missingCount = df.isnull().sum()
  totalMissing = missingCount.sum()
  print("The dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")
percent_missing(df)
df.isna().sum()
percent_missing(df['Nb of sec with 6250B < Vol UL < 37500B'])
df['Handset Manufacturer'].unique()
df['Handset Type'].describe()
top_10_handsets = df['Handset Type'].value_counts().head(10)
print(top_10_handsets)
top_3_manufacturers = df['Handset Manufacturer'].value_counts().head(3)
print(top_3_manufacturers)
top_3_manufacturers_list = top_3_manufacturers.index.tolist()
top_5_handsets_per_manufacturer = df[df['Handset Manufacturer'].isin(top_3_manufacturers_list)]
 
top_5_handsets_per_manufacturer = top_5_handsets_per_manufacturer.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')
 
top_5_handsets_per_manufacturer = top_5_handsets_per_manufacturer.sort_values(by=['Handset Manufacturer', 'Count'], ascending=[True, False])
top_5_handsets_per_manufacturer = top_5_handsets_per_manufacturer.groupby('Handset Manufacturer').head(5)
top_5_handsets_per_manufacturer
user_data = df.groupby('MSISDN/Number')
user_data.head()
 
num_xdr = user_data.size().rename('Number of XDR Sessions')
session_duration = user_data['Dur. (ms)'].sum() / 1000
total_up_dl_per_user = user_data[['Total DL (Bytes)', 'Total UL (Bytes)']].sum()
 
app_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)',
  'Email DL (Bytes)', 'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)',
  'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)', 'Gaming UL (Bytes)',
  'Other DL (Bytes)', 'Other UL (Bytes)']
 
for data in app_columns:
  df[data] /= (1024 * 1024)
total_data = user_data[app_columns].sum()
 
user_behavior = pd.concat([num_xdr, session_duration, total_up_dl_per_user, total_data], axis=1) 
user_behavior
user_behavior.describe()
 
top_10_xdr = user_behavior['Number of XDR Sessions'].nlargest(10)
top_10_xdr
 
top_10_duration = user_behavior['Dur. (ms)'].nlargest(10)
top_10_duration_seconds = top_10_duration / 1000
top_10_duration_seconds
 
top_10_download = user_behavior['Total DL (Bytes)'].nlargest(10)
top_10_download /= (1024 * 1024)
top_10_download
missing_values = df.isnull().sum()
missing_values
data_types = df.dtypes
type_counts = data_types.value_counts()
type_counts
 
for column in df.columns:
  if df[column].dtype == 'float64' and df[column].isnull().any():
  mean_value = df[column].mean()
  df[column].fillna(mean_value, inplace=True)
df.isnull().sum()
description = df.describe(include='all')
description
numeric_columns = df.select_dtypes(include=['float64', 'int64'])
 
mean_values_all = numeric_columns.mean()
median_values = numeric_columns.median()
std_deviation = numeric_columns.std()
 
median_values
numeric_columns = df.select_dtypes(include=['float64', 'int64'])
dispersion_parameters = numeric_columns.agg(['std', 'var'])
dispersion_parameters
numeric_columns = df.select_dtypes(include=['float64', 'int64'])
numeric_columns.hist(bins=20, figsize=(12, 10))
plt.tight_layout()
plt.show()
plt.figure(figsize=(8,6))
plt.hist(df['Total UL (Bytes)'], bins= 20, color='skyblue')
plt.show()
plt.figure(figsize=(8,6))
plt.hist(df['Avg Bearer TP DL (kbps)'], bins= 20, color='skyblue')
plt.show()
plt.figure(figsize=(8,6))
plt.hist(df['Total UL (Bytes)'], bins= 20, color='skyblue')
plt.show()
plt.figure(figsize=(8,6))
plt.hist(df['Start ms'], bins= 20, color='skyblue')
plt.show()
 
threshold = 30
 
cols_with_outliers = []
 
for col in df.columns:
  if df[col].dtype in ['int64', 'float64']:
  mean = df[col].mean()
  std = df[col].std()
  outliers = df[(df[col] - mean).abs() > threshold * std]
  if len(outliers) > 0:
  cols_with_outliers.append(col)
print('Potential columns with outliers: ', cols_with_outliers)
print(len(cols_with_outliers))
 
import matplotlib.pyplot as plt
 
plt.figure(figsize=(8,6))
 
plt.hist(df['Activity Duration UL (ms)'], bins = 20, color='skyblue', edgecolor='black')
plt.xlabel('Activity Duration UL (ms)')
plt.ylabel('Number')
plt.show()
df['Activity Duration UL (ms)'].describe()
 
Q1 = df['Activity Duration UL (ms)'].quantile(0.25)
Q3 = df['Activity Duration UL (ms)'].quantile(0.75)
 
IQR = Q3 - Q1
upper_ts = Q3 + 1.5 * IQR
 
outliers_activity_duration = df[df['Activity Duration UL (ms)'] > upper_ts]
outliers_activity_duration
outliers_activity_duration['Activity Duration UL (ms)'].describe()
plt.figure(figsize=(8, 6))
plt.hist(outliers_activity_duration['Activity Duration UL (ms)'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Activity Duration UL (ms)')
plt.ylabel('Frequency')
plt.title('Distribution of Activity Duration UL (ms) - Outliers Subset')
plt.grid(True)
plt.show()
import pandas as pd
import psycopg2
from sqlalchemy import create_engine
 
def connect_to_database(connection_params):
  try:
  connection = psycopg2.connect(**connection_params)
  return connection
  except psycopg2.Error as e:
  print(f"Error: Unable to connect to the database. {e}")
  return None
 
class Database:
  def __init__(self, host, database, user, password):
  self.connection_params = {
  'dbname': 'telecom',
  'user': 'postgres',
  'password': 'heisenberg',
  'host': 'localhost',
  'port': '5432'
  }
  self.conn = connect_to_database(self.connection_params)
  def read_table_to_dataframe(self, table_name):
  if self.conn:
  query = f"SELECT * FROM {table_name};"
  df = pd.read_sql_query(query, self.conn)
  return df
  else:
  print("Error: No connection detected!")
  return None
  def write_dataframe_to_table(self, df, table_name, if_exists='replace'):
  engine = create_engine(
  f"postgresql://{self.connection_params['user']}:{self.connection_params['password']}@"
  f"{self.connection_params['host']}:{self.connection_params['port']}/{self.connection_params['dbname']}"
  )
  df.to_sql(table_name, engine, index=False, if_exists=if_exists)
  print(f"Dataframe successfully written to the '{table_name}' table.")
  def update_table_by_appending(self, df, table_name):
  self.write_dataframe_to_table(df, table_name, if_exists='append')
  def delete_table(self, table_name):
  if self.conn:
  cursor = self.conn.cursor()
  cursor.execute(f"DROP TABLE IF EXISTS {table_name};")
  self.conn.commit()
  cursor.close()
  print(f"Table '{table_name}' successfully deleted.")
  else:
  print("Error: No connection detected.")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import warnings
warnings.filterwarnings('ignore')
 
sys.path.append('../helpers/')
 
from data import Database
from utils import Helpers
db = Database(host="localhost", database="telecom", user="postgres", password="heisenberg")
df = db.read_table_to_dataframe('xdr_data')
df.head()
 
for column in df.columns:
  if df[column].dtype == 'float64' and df[column].isnull().any():
  mean_value = df[column].mean()
  df[column].fillna(mean_value, inplace=True)
df.isnull().sum()
correlation_matrix_UL = df[['Google UL (Bytes)', 'Social Media UL (Bytes)', 'Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)']].corr()
 
correlation_matrix_DL = df[['Google DL (Bytes)', 'Social Media DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)']].corr()
 
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_UL, annot=True, cmap=sns.cubehelix_palette(as_cmap=True),)
plt.title('Correlation Matrix of Applications and Total DL+UL')
plt.show()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix_DL, annot=True, cmap=sns.cubehelix_palette(as_cmap=True),)
plt.title('Correlation Matrix of Applications and Total DL+UL')
plt.show()
import pandas as pd
import matplotlib.pyplot as plt
 
upload_columns = ['Google UL (Bytes)', 'Social Media UL (Bytes)', 'Email UL (Bytes)',
  'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)']
 
def bytes_to_megabytes(value):
  try:
  return float(value) / (1024 * 1024)
  except (ValueError, TypeError):
  return pd.NA  
total_upload_data = df[upload_columns].applymap(bytes_to_megabytes).sum()
 
plt.figure(figsize=(8, 6))
total_upload_data.sort_values(ascending=False).plot(kind='bar', color='skyblue')
plt.xlabel('Applications')
plt.ylabel('Total Upload Data (MB)')
plt.title('Total Upload Data for Each Application (in MB)')
plt.xticks(rotation=45)
plt.show()
download_columns = ['Google DL (Bytes)', 'Social Media DL (Bytes)', 'Email DL (Bytes)',
  'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)']
 
def bytes_to_megabytes(value):
  try:
  return float(value) / (1024 * 1024)
  except (ValueError, TypeError):
  return pd.NA  
total_download_data = df[download_columns].applymap(bytes_to_megabytes).sum()
 
plt.figure(figsize=(8, 6))
total_download_data.sort_values(ascending=False).plot(kind='bar', color='skyblue')
plt.xlabel('Applications')
plt.ylabel('Total Upload Data (MB)')
plt.title('Total Upload Data for Each Application (in MB)')
plt.xticks(rotation=45)
plt.show()
 
df['Total_Session_Duration'] = df['Dur. (ms)'] + df['Dur. (ms).1']
 
df['Decile_Rank'] = pd.qcut(df['Total_Session_Duration'], q=10, labels=False)
 
df['Total_Data_DL_UL'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']
 
data_per_decile = df.groupby('Decile_Rank')['Total_Data_DL_UL'].sum()
 
data_per_decile
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
 
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
numerical_columns = numerical_columns.drop('MSISDN/Number', errors='ignore')  
numerical_columns = numerical_columns.drop('Bearer Id', errors='ignore')  
numerical_data = df[numerical_columns]
numerical_data = numerical_data.dropna()
 
scaler = StandardScaler()
numerical_data = scaler.fit_transform(numerical_data)
 
pca = PCA(n_components=2)
pca_result = pca.fit_transform(numerical_data)
pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])
final_df = pd.concat([df, pca_df], axis=1)
 
final_df
final_df.info()
df.info()
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
 
class Helpers:
  @staticmethod
  def normalize(df, columns):
  scaler = StandardScaler()
  df_normalized = df.copy()
  df_normalized[columns] = scaler.fit_transform(df_normalized[columns])
  return df_normalized
  @staticmethod
  def reduce(df):
  numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
  numerical_columns = numerical_columns.drop('MSISDN/Number', axis=1)
  numerical_data = df[numerical_columns]
  numerical_data = numerical_data.dropna()
  scaler = StandardScaler()
  numerical_data = scaler.fit_transform(numerical_data)
  pca = PCA(n_components=2)
  pca_result = pca.fit_transform(numerical_data)
  pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])
  final_df = pd.concat([df, pca_df], axis=1)
  return final_df
  @staticmethod
  def perform_kmeans(agg_data: pd.DataFrame, columns: list, n_clusters: int):
  cluster_data = agg_data[columns]
  kmeans = KMeans(n_clusters=n_clusters, random_state=42)
  agg_data['Cluster'] = kmeans.fit_predict(cluster_data)
  return agg_data[['MSISDN/Number', 'Cluster']]
  @staticmethod
  def plot_clusters(data, x_column, y_column, cluster_column):
  plt.figure(figsize=(8, 6))
  clusters = data[cluster_column].unique()
  for cluster in clusters:
  cluster_data = data[data[cluster_column] == cluster]
  plt.scatter(cluster_data[x_column], cluster_data[y_column], label=f'Cluster {cluster}')
  plt.xlabel(x_column)
  plt.ylabel(y_column)
  plt.title(f'Clusters based on {x_column} vs {y_column}')
  plt.legend()
  plt.show()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import warnings
warnings.filterwarnings('ignore')
 
sys.path.append('../helpers/')
 
from data import Database
from utils import Helpers
db = Database(host="localhost", database="telecom", user="postgres", password="heisenberg")
df = db.read_table_to_dataframe('xdr_data')
df.head()
 
for column in df.columns:
  if df[column].dtype == 'float64' and df[column].isnull().any():
  mean_value = df[column].mean()
  df[column].fillna(mean_value, inplace=True)
 
df.isnull().sum()
no_null_df = df
no_null_df.isnull().sum()
 
task_4_1_columns = [
  'MSISDN/Number',
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'Handset Type',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)'
]
 
task4_1_data = no_null_df[task_4_1_columns]
task4_1_data.isnull().sum()
 
tcp_retransmission = (
  task4_1_data.groupby('MSISDN/Number')
  .agg({'TCP DL Retrans. Vol (Bytes)': 'mean', 'TCP UL Retrans. Vol (Bytes)': 'mean'})
  .reset_index()
)
 
tcp_retransmission.columns = ['MSISDN/Number', 'Avg_TCP_DL_Retrans (MB)', 'Avg_TCP_UL_Retrans (MB)']
tcp_retransmission /= (1024 * 1024)
tcp_retransmission.head()
 
rtt_data = no_null_df[task_4_1_columns]
 
rtt_data = (
  task4_1_data.groupby('MSISDN/Number')
  .agg({'Avg RTT DL (ms)': 'mean', 'Avg RTT UL (ms)': 'mean'})
  .reset_index()
)
 
rtt_data.columns = ['MSISDN/Number', 'Avg_RTT_DL (ms)', 'Avg_RTT_UL (ms)']
rtt_data.head()
most_freq = task4_1_data['Handset Type'].mode()[0]
 
task4_1_data['Handset Type'].fillna(most_freq, inplace=True)
 
handset = task4_1_data.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode()[0]).reset_index()
handset.columns = ['MSISDN/Number', 'Most Used Handset Type']
handset.head()
import matplotlib.pyplot as plt
 
handset_counts = handset['Most Used Handset Type'].value_counts().nlargest(10)
 
plt.figure(figsize=(10, 6))
handset_counts.sort_values().plot(kind='barh', color='skyblue')  
plt.title('Most Used Handset Types')
plt.xlabel('Frequency')
plt.ylabel('Handset Types')
plt.tight_layout()
plt.show()
handset['Most Used Handset Type'].value_counts()
throughput_df = task4_1_data[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']]
throughput_df['Avg Throughput (kbps)'] = (throughput_df['Avg Bearer TP DL (kbps)'] + throughput_df['Avg Bearer TP UL (kbps)']) / 2
 
throughput_df
 
top_10_tcp = tcp_retransmission.nlargest(10, 'Avg_TCP_DL_Retrans (MB)')
top_10_tcp
 
bottom_10_tcp = tcp_retransmission.nsmallest(10, 'Avg_TCP_DL_Retrans (MB)')
bottom_10_tcp
top_10_rtt = rtt_data.nsmallest(10, 'Avg_RTT_DL (ms)')
top_10_rtt
bottom_10_rtt = rtt_data.nlargest(10, 'Avg_RTT_DL (ms)')
bottom_10_rtt
top_10_throughput = throughput_df.nlargest(10, 'Avg Throughput (kbps)')
top_10_throughput
bottom_10_throughput = throughput_df.nsmallest(10, 'Avg Throughput (kbps)')
bottom_10_throughput
avg_tp_per_handset = task4_1_data.groupby('Handset Type')[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean()
avg_tp_per_handset
 
avg_tp_per_handset.nlargest(10, 'Avg Bearer TP DL (kbps)')
avg_tp_per_handset.nsmallest(10, 'Avg Bearer TP DL (kbps)')
avg_tcp_ret_p_handset = task4_1_data.groupby('Handset Type')[['TCP DL Retrans. Vol (Bytes)']].mean()
avg_tcp_ret_p_handset /= (1024 * 1024)
avg_tcp_ret_p_handset.nlargest(10, 'TCP DL Retrans. Vol (Bytes)')
numeric_columns = [
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)'
]
 
numeric_data = no_null_df[['MSISDN/Number'] + numeric_columns]
 
agg_data = numeric_data.groupby('MSISDN/Number').agg('mean').reset_index()
 
agg_data.head()
columns_for_kmeans = [
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)'
]
 
result_clusters = Helpers.perform_kmeans(agg_data=agg_data, columns=columns_for_kmeans, n_clusters=3)
result_clusters.head(10)
result_clusters['Cluster'].unique()
x_column = 'Avg RTT DL (ms)'  
y_column = 'Avg Bearer TP DL (kbps)'  
cluster_column = 'Cluster'  
Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
x_column = 'TCP DL Retrans. Vol (Bytes)'  
y_column = 'Avg Bearer TP DL (kbps)'  
cluster_column = 'Cluster'  
Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
x_column = 'TCP DL Retrans. Vol (Bytes)'  
y_column = 'Avg RTT DL (ms)'  
cluster_column = 'Cluster'  
Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
x_column = 'TCP DL Retrans. Vol (Bytes)'  
y_column = 'Avg Bearer TP UL (kbps)'  
cluster_column = 'Cluster'  
Helpers.plot_clusters(agg_data, x_column, y_column, cluster_column)
from scipy.spatial import distance
 
centroids = result_clusters.groupby('Cluster').mean()
 
for idx, user_data in agg_data.iterrows():
  user_point = user_data[columns_for_kmeans]   distances = [distance.euclidean(user_point, centroid) for _, centroid in centroids.iterrows()]
  less_engaged_centroid = distances.index(min(distances))
  worst_experience_centroid = distances.index(max(distances))
  agg_data.at[idx, 'Engagement_Score'] = min(distances)
  agg_data.at[idx, 'Experience_Score'] = max(distances)
agg_data.info()
agg_data['Satisfaction_Score'] = (agg_data['Engagement_Score'] + agg_data['Experience_Score']) / 2
top_10_satisfied_customers = agg_data.nsmallest(10, 'Satisfaction_Score')[['MSISDN/Number', 'Satisfaction_Score']]
top_10_satisfied_customers
agg_data['Satisfaction_Score'] = (agg_data['Engagement_Score'] + agg_data['Experience_Score']) / 2
bottom_10_satisfied_customers = agg_data.nlargest(10, 'Satisfaction_Score')[['MSISDN/Number', 'Satisfaction_Score']]
bottom_10_satisfied_customers
add_data['Satisfaction Score] = 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
 
X = agg_data.drop(['Satisfaction_Score', 'MSISDN/Number'], axis=1)  
y = agg_data['Satisfaction_Score']  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
 
model = LinearRegression()
model.fit(X_train, y_train)
 
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
 
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared Score: {r2}")
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot(y_test, y_test, color='red')
plt.title('Actual vs. Predicted Values')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()
from sklearn.cluster import KMeans
import pandas as pd
 
scores = agg_data[['Engagement_Score', 'Experience_Score']]
 
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(scores)
 
agg_data['Score_Cluster'] = kmeans.labels_
 
plt.figure(figsize=(8, 6))
plt.scatter(agg_data['Engagement_Score'], agg_data['Experience_Score'], c=agg_data['Score_Cluster'], cmap='viridis', edgecolors='k')
plt.title('K-Means Clustering of Engagement & Experience Scores (k=2)')
plt.xlabel('Engagement Score')
plt.ylabel('Experience Score')
plt.colorbar(label='Cluster')
plt.show()
cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=['Engagement_Score', 'Experience_Score'])
cluster_centers
import pandas as pd
from sqlalchemy import create_engine
import numpy as np
from IPython.display import Image
import seaborn as sns
import matplotlib.pyplot as plt
path = '../Data'
df = pd.read_csv(path+"/teledata.csv")
df.head(5)
df.columns.tolist()
len(df.columns)
df.shape
print (f"There are {df.shape[0]} rows and {df.shape[1]} columns")
df.isnull().sum().sum()
 
df.describe()
def percent_missing(df):
  totalCells = np.product(df.shape)
  missingCount = df.isnull().sum()
  totalMissing = missingCount.sum()   print("The tele database contains", round(((totalMissing/totalCells) * 100),2), "%", 'missing values')
 
percent_missing(df)
df.isna().sum()
percent_missing(df['Nb of sec with 37500B < Vol UL'])
percent_missing(df['Nb of sec with 6250B < Vol UL < 37500B'])
percent_missing(df['Nb of sec with 1250B < Vol UL < 6250B'])
percent_missing(df['Nb of sec with 125000B < Vol DL'])
percent_missing(df['Nb of sec with 31250B < Vol DL < 125000B'])
percent_missing(df['Nb of sec with 6250B < Vol DL < 31250B'])
percent_missing(df['TCP DL Retrans. Vol (Bytes)'])
percent_missing(df['TCP UL Retrans. Vol (Bytes)'])
percent_missing(df['HTTP DL (Bytes)'])
percent_missing(df['HTTP UL (Bytes)'])
percent_missing(df['Bearer Id'])
 
df_clean = df.drop(['Nb of sec with 37500B < Vol UL',
  'Nb of sec with 6250B < Vol UL < 37500B','Nb of sec with 1250B < Vol UL < 6250B',
  'Nb of sec with 125000B < Vol DL',
  'Nb of sec with 31250B < Vol DL < 125000B',
  'Nb of sec with 6250B < Vol DL < 31250B',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)',
  'HTTP DL (Bytes)',
  'HTTP UL (Bytes)'], axis=1)
df_clean=df_clean.dropna(subset=['End'])
df_clean.shape
print (f"The shape of the dataframe after cleaning some data is {df_clean.shape}")
df_clean.isna().sum()
df_clean.sample(5)
df_clean['Start'] = pd.to_datetime(df['Start'])
df_clean['End'] = pd.to_datetime(df['End'])
df_clean.isna().sum()
df_clean.sample(5)
df_clean.head()
df_clean.skew(axis=0)
missing_columns = ["Avg RTT DL (ms)",
  "Avg RTT UL (ms)",
  "DL TP < 50 Kbps (%)",
  "50 Kbps < DL TP < 250 Kbps (%)",
  "250 Kbps < DL TP < 1 Mbps (%)",
  "DL TP > 1 Mbps (%)",
  "UL TP < 10 Kbps (%)",
  "10 Kbps < UL TP < 50 Kbps (%)",
  "50 Kbps < UL TP < 300 Kbps (%)",
  "UL TP > 300 Kbps (%)",
  "Nb of sec with Vol DL < 6250B",
  "Nb of sec with Vol UL < 1250B"]
def fix_missing_mean(df, col):
  df[col] = df[col].fillna(df[col].mean())
  return df[col]
 
def fix_missing_median(df, col):
  df[col] = df[col].fillna(df[col].median())   return df[col]
 
def fix_missing_mode(df, col):
  df[col] = df[col].fillna(df_clean[col].mode()[0])
  return df[col]
for i in missing_columns:
  df_clean[i] = fix_missing_median(df_clean, i)
df_clean["Last Location Name"] = fix_missing_mode(df_clean,"Last Location Name")
df_clean.isna().sum()
df_clean['Bearer Id'].hist()
df_clean=df_clean.dropna(subset=['Bearer Id'])
df_clean=df_clean.dropna(subset=['MSISDN/Number'])
df_clean=df_clean.dropna(subset=['IMSI'])
df_clean.isna().sum()
df['Handset Type'].value_counts()[0:10]
df['Handset Manufacturer'].value_counts()[0:3]
df_top = df_clean.query('`Handset Manufacturer` in ["Apple", "Samsung", "Huawei"]')
len(df_top)
df_top_apple = df_clean.loc[df['Handset Manufacturer']=='Apple']
df_top_samsung = df_clean.loc[df['Handset Manufacturer']=='Samsung']
df_top_huawei = df_clean.loc[df['Handset Manufacturer']=='Huawei']
 
df_top['Handset Type'].value_counts()[0:5]
df_top_apple['Handset Type'].value_counts()[0:5]
df_top_samsung['Handset Type'].value_counts()[0:5]
df_top_huawei['Handset Type'].value_counts()[0:5]
Users = df_clean.groupby(['IMSI'])
 
xDR_users = df_clean['IMSI'].value_counts()
session_duration = Users['Dur. (ms)'].sum()
Total_session_UL = Users['Total UL (Bytes)'].sum()
Total_session_DL = Users['Total DL (Bytes)'].sum()
Total_Download_Social = Users['Social Media DL (Bytes)'].sum() + Users['Social Media UL (Bytes)'].sum()
Total_Download_Google = Users['Google DL (Bytes)'].sum() + Users['Google UL (Bytes)'].sum()
Total_Download_Email = Users['Email DL (Bytes)'].sum() + Users['Email UL (Bytes)'].sum()
Total_Download_Youtube = Users['Youtube DL (Bytes)'].sum() + Users['Youtube UL (Bytes)'].sum()
Total_Download_Netflix = Users['Netflix DL (Bytes)'].sum() + Users['Netflix UL (Bytes)'].sum()
Total_Download_Gaming = Users['Gaming DL (Bytes)'].sum() + Users['Gaming UL (Bytes)'].sum()
Total_Download_Other = Users['Other DL (Bytes)'].sum() + Users['Other DL (Bytes)'].sum()
 
new = pd.concat([xDR_users,
  session_duration,
  Total_session_UL,
  Total_session_DL,
  Total_Download_Social,
  Total_Download_Google,
  Total_Download_Email,
  Total_Download_Youtube,
  Total_Download_Netflix,
  Total_Download_Gaming,
  Total_Download_Other
  ], axis="columns", sort=True)
 
new = new.rename(columns={0: 'Social media total volume (Bytes)'})
new = new.rename(columns={1: 'Google total volume (Bytes)'})
new = new.rename(columns={2: 'Email total volume (Bytes)'})
new = new.rename(columns={3: 'Youtube total volume (Bytes)'})
new = new.rename(columns={4: 'Netflix total volume (Bytes)'})
new = new.rename(columns={5: 'Gaming total volume (Bytes)'})
new = new.rename(columns={'Other DL (Bytes)': 'Other source total volume (Bytes)'})
new = new.rename(columns={'IMSI': 'number of sessions'})
df_norm = (new-new.min())/(new.max()-new.min())
df_norm.head(10)
new1 = new.copy()
 
def fix_outlier(df, column):
  df[column] = np.where(df[column] > df[column].quantile(0.95), df[column].median(),df[column])
  return df[column]
 
for i in df_norm.columns.tolist():
  df_norm[i]=fix_outlier(df_norm,i)
 
for i in new.columns.tolist():
  new[i]=fix_outlier(new,i)
len(new)
fig, ax=plt.subplots(1,2, figsize=(20, 6))
sns.histplot(df_norm['number of sessions'], ax=ax[0])
ax[0].set_title("numbers of sessions")
sns.histplot(df_norm['Dur. (ms)'], ax=ax[1])
ax[1].set_title("Dur. (ms)")
plt.ylim(0, 2500)
fig, ax=plt.subplots(1,2, figsize=(20, 6))
sns.histplot(df_norm['Total UL (Bytes)'], ax=ax[0])
ax[0].set_title("Total UL (Bytes)")
sns.histplot(df_norm['Total DL (Bytes)'], ax=ax[1])
ax[1].set_title("Total DL (Bytes)")
fig, ax=plt.subplots(1,2, figsize=(20, 6))
sns.histplot(new['Netflix total volume (Bytes)'], ax=ax[0])
ax[0].set_title("Netflix total volume (Bytes)")
sns.histplot(new['Gaming total volume (Bytes)'], ax=ax[1])
ax[1].set_title("Gaming total volume (Bytes)")
plt.ylim(0, 2500)
fig, ax=plt.subplots(1, figsize=(9, 6))
sns.histplot(new['Other source total volume (Bytes)'])
ax.set_title("Other source total volume")
plt.ylim(0, 2500)
new['Total_DL_UL'] = new['Total DL (Bytes)'] + new['Total UL (Bytes)']
a4_dims = (20, 7)
fig, axs = plt.subplots(ncols=6,figsize=a4_dims)
sns.scatterplot(x="Social media total volume (Bytes)", y="Total_DL_UL",ax=axs[0], data=new)
sns.scatterplot(x="Gaming total volume (Bytes)", y="Total_DL_UL", ax=axs[1],data=new)
sns.scatterplot(x="Email total volume (Bytes)", y="Total_DL_UL",ax=axs[2], data=new)
sns.scatterplot(x="Youtube total volume (Bytes)", y="Total_DL_UL",ax=axs[3], data=new)
sns.scatterplot(x="Google total volume (Bytes)", y="Total_DL_UL",ax=axs[4], data=new)
sns.scatterplot(x="Other source total volume (Bytes)", y="Total_DL_UL",ax=axs[5], data=new)
plt.show()
 
new['decile'] = pd.qcut(new['Dur. (ms)'], 5, labels =False, )
new.head(10)
data_per_decile = new.groupby(['decile']).agg({'Total_DL_UL':['sum']})
data_per_decile.columns = ["_".join(x) for x in data_per_decile.columns.ravel()]
 
data_per_decile.head(10)
df_new=df_norm[['Social media total volume (Bytes)',
  'Google total volume (Bytes)',
  'Email total volume (Bytes)',
  'Youtube total volume (Bytes)',
  'Netflix total volume (Bytes)',
  'Gaming total volume (Bytes)',
  'Other source total volume (Bytes)']]
 
corrMatrix = df_new.corr()
f, ax = plt.subplots(figsize=(13, 9))
sns.heatmap(corrMatrix,annot=True,fmt='.3f',annot_kws={"size":12})
plt.show()
new.index.name = 'customer ID'
new.reset_index(inplace=True)
 
new.head()
 
new.sort_values(by='number of sessions', ascending=False)[['customer ID','number of sessions']]
new.sort_values(by='Dur. (ms)', ascending=False)[['customer ID','Dur. (ms)']].head(10)
new.sort_values(by='Total_DL_UL', ascending=False)[['customer ID','Total_DL_UL']].head(10)
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
df_model = new[['customer ID','Dur. (ms)', 'Total_DL_UL','number of sessions']]
df_scaled = df_model.copy()
df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']] = (df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']]-df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']].min())/(df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']].max()-df_scaled[['Dur. (ms)', 'Total_DL_UL','number of sessions']].min())
df_scaled.head(10)
 
kmeans = KMeans(n_clusters=3, random_state=0)
df_scaled['cluster'] = kmeans.fit_predict(df_scaled[['number of sessions', 'Dur. (ms)','Total_DL_UL']])
 
df_eng = df_scaled.copy()
df_scaled.head()
centroids = kmeans.cluster_centers_
cen_x = [i[0] for i in centroids]  
cen_y = [i[1] for i in centroids]
 
df_scaled['cen_x'] = df_scaled['cluster'].map({0:cen_x[0], 1:cen_x[1], 2:cen_x[2]})
 
df_scaled['cen_y'] = df_scaled['cluster'].map({0:cen_y[0], 1:cen_y[1], 2:cen_y[2]})
df_scaled.groupby(['cluster']).describe().transpose()
 
new.sort_values(by='Social media total volume (Bytes)', ascending=False)[['customer ID','Social media total volume (Bytes)']].head(10)
new.sort_values(by='Netflix total volume (Bytes)', ascending=False)[['customer ID','Netflix total volume (Bytes)']].head(10)
 
new.sort_values(by='Google total volume (Bytes)', ascending=False)[['customer ID','Google total volume (Bytes)']].head(10)
new.sort_values(by='Youtube total volume (Bytes)', ascending=False)[['customer ID','Youtube total volume (Bytes)']].head(10)
new.sort_values(by='Email total volume (Bytes)',   ascending=False)[['customer ID','Email total volume (Bytes)']].head(10)
new.sort_values(by='Gaming total volume (Bytes)', ascending=False)[['customer ID','Gaming total volume (Bytes)']].head(10)
new.sort_values(by='Other source total volume (Bytes)', ascending=False)[['customer ID','Other source total volume (Bytes)']].head(10)
labels = ['Social media', 'Google', 'Email','Youtube','Netflix','Gaming']
sizes = [new['Social media total volume (Bytes)'].sum(),
  new['Google total volume (Bytes)'].sum(),
  new['Email total volume (Bytes)'].sum(),
  new['Youtube total volume (Bytes)'].sum(),
  new['Netflix total volume (Bytes)'].sum(),
  new['Gaming total volume (Bytes)'].sum(),
  ]
 
fig1, ax1 = plt.subplots(figsize = (10, 10))
colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple']
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, colors=colors)
 
ax1.axis('equal')
plt.title("Total traffics per application")
plt.show()
 
labels=['Gaming','Youtube','Netflix']
 
sizes.sort()
top3 = sizes[3:]
ys=top3
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
plt.title("Top 3 used application", color='brown')
ax.bar(labels,ys)
plt.show()
%reload_ext autoreload
%autoreload 2
import missingno as msno  
import matplotlib.pyplot as plt  
import pandas as pd
from sqlalchemy import create_engine
import sys  
import os  
import numpy as np  
import seaborn as sns  
import sys, os
import sklearn
from sklearn.decomposition import PCA
import logging
from scipy.stats.mstats import winsorize
 
current_directory = os.getcwd()
parent_directory = os.path.abspath(os.path.join(current_directory, '..'))
 
if parent_directory not in sys.path:
  sys.path.insert(0, parent_directory)
 
from src.utils import percent_missing, format_float, find_agg, missing_values_table,convert_bytes_to_megabytes,fix_missing_ffill,fix_missing_bfill
 
database_name = 'tcom'
table_name= 'xdr_data'
 
connection_params = { "host": "localhost", "user": "postgres", "password": "1234",
  "port": "5432", "database": database_name}
 
engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
 
sql_query = 'SELECT * FROM xdr_data'
 
df = pd.read_sql(sql_query, con= engine)
df.head()
df.columns
 
df.info()
df.describe().T
df.shape
df.isnull().sum()
columns = []
counts=[]
i=0
for key, item in df.isnull().sum().items():
  if item != 0:
  columns.append(key)
  counts.append(item)
  i+=1
print('the dataset has {} columns with missing values'.format(i))
pd.DataFrame({'column name':columns,'counts':counts})
msno.bar(df)
msno.matrix(df)
msno.heatmap(df)
msno.dendrogram(df)
totalCells, missingCount, totalMissing = percent_missing(df)
print("The Telcom dataset contains", round(
  ((totalMissing/totalCells) * 100), 2), "%", "missing values.")
mis_val_table_ren_columns = missing_values_table(df)
mis_val_table_ren_columns
class MissingInformation:
  def __init__(self,df:pd.DataFrame):
  self.df = df
  logging.basicConfig(filename='../logfile.log', filemode='a',
  encoding='utf-8', level=logging.DEBUG)
  def missing_values_table(self,df:pd.DataFrame)->pd.DataFrame:
  mis_val = df.isnull().sum()
  mis_val_percent = 100 * df.isnull().sum() / len(df)
  mis_val_dtype = df.dtypes
  mis_val_table = pd.concat(
  [mis_val, mis_val_percent, mis_val_dtype], axis=1)
  mis_val_table_ren_columns = mis_val_table.rename(
  columns={0: 'Missing Values', 1: '% of Total Values', 2: 'Dtype'})
  mis_val_table_ren_columns = mis_val_table_ren_columns[
  mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(
  '% of Total Values', ascending=False).round(1)
  logging.info("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
  "There are " + str(mis_val_table_ren_columns.shape[0]) +
  " columns that have missing values.")
  return mis_val_table_ren_columns
  def percent_missing(self,df:pd.DataFrame):
  totalCells = np.product(df.shape)
  missingCount = df.isnull().sum()
  totalMissing = missingCount.sum()
  return totalCells, missingCount, totalMissing
class DataFrameInformation:
  def __init__(self,data:pd.DataFrame):
  self.data = data
  logging.basicConfig(filename='../logfile.log', filemode='a',
  encoding='utf-8', level=logging.DEBUG)
  def get_skewness(self,data:pd.DataFrame):
  skewness = data.skew(axis=0, skipna=True)
  df_skewness = pd.DataFrame(skewness)
  df_skewness = df_skewness.rename(
  columns={0: 'skewness'})
  return df_skewness
  def get_skewness_missing_count(self,data:pd.DataFrame):
  df_skewness = self.get_skewness(data)
  minfo = MissingInformation(data)
  mis_val_table_ren_columns = minfo.missing_values_table(data)
  df1 = pd.concat([df_skewness, mis_val_table_ren_columns], axis=1)
  df1['Dtype'] = df1['Dtype'].fillna('float64')
  df1['% of Total Values'] = df1['% of Total Values'].fillna(0.0)
  df1['Missing Values'] = df1['Missing Values'].fillna(0)
  df1 = df1.sort_values(by='Missing Values', ascending=False)
  return df1
  def get_column_with_string(self,df: pd.DataFrame, text):
  return [col for col in df.columns if re.findall(text, col) != []]
  def get_dataframe_information(self,df: pd.DataFrame):
  columns = []
  counts = []
  i = 0
  for key, item in df.isnull().sum().items():
  if item != 0:
  columns.append(key)
  counts.append(item)
  i += 1
  logging.info(
  'the dataset contain {} columns with missing values'.format(i))
  return pd.DataFrame({'column name': columns, 'counts': counts})
 
numeric_columns = df.select_dtypes(include=np.number)
skewness = numeric_columns.skew(axis=0, skipna=True)
 
df_skewness = pd.DataFrame({'skewness': skewness})
df_skewness
df_skewness.plot(kind='bar')
d_f = pd.concat([df_skewness, mis_val_table_ren_columns], axis=1)
d_f['Dtype'] = d_f['Dtype'].fillna('float64')
d_f['% of Total Values'] = d_f['% of Total Values'].fillna(0.0)
d_f['Missing Values'] = d_f['Missing Values'].fillna(0)
d_f.sort_values(by='Missing Values', ascending=False)
 
df['Nb of sec with Vol UL < 1250B'].plot(kind='kde')
i = 0
for key, item in df.isnull().sum().items():
  if item==0:
  i+=1
  print(key)
print('the dataset contain {} columns with no missing values'.format(i))
columns = []
counts=[]
i=0
for key, item in df.isnull().sum().items():
  if item != 0:
  columns.append(key)
  counts.append(item)
  i+=1
print('the dataset contain {} columns with missing values'.format(i))
pd.DataFrame({'column name':columns,'counts':counts})
from scipy.stats.mstats import winsorize
class CleanData:
  def __init__(self,df:pd.DataFrame):
  self.df = df
  logging.basicConfig(filename='../logfile.log', filemode='a',
  encoding='utf-8', level=logging.DEBUG)
  def convert_dtype(self, df: pd.DataFrame, columns, dtype):
  for col in columns:
  df[col] = df[col].astype(dtype=dtype)
  return df
  def format_float(self,value):
  return f'{value:,.2f}'
  def convert_bytes_to_megabytes(self, df:pd.DataFrame, columns):
  megabyte = 1*10e+5
  for col in columns:
  df[col] = df[col] / megabyte
  return df
  def convert_ms_to_sec(self, df:pd.DataFrame, columns):   s = 10e+3
  for col in columns:
  df[col] = df[col] / s
  return df   def fix_missing_ffill(self, df: pd.DataFrame,col):
  df[col] = df[col].fillna(method='ffill')
  return df[col]
  def fix_missing_bfill(self, df: pd.DataFrame, col):
  df[col] = df[col].fillna(method='bfill')
  return df[col]
  def drop_column(self, df: pd.DataFrame, columns) -> pd.DataFrame:
  for col in columns:
  df = df.drop([col], axis=1)
  return df
  def drop_missing_count_greaterthan_20p(self,data:pd.DataFrame):
  data_info = DataFrameInformation(data)
  df = data_info.get_skewness_missing_count(data)
  not_fill = df[(df['% of Total Values'] >= 20.0)].index.tolist()
  df_clean = self.drop_column(data, not_fill)
  return df_clean
  def fill_mode(self, df: pd.DataFrame, columns) -> pd.DataFrame:
  for col in columns:
  df[col] = df[col].fillna(df[col].mode()[0])
  return df
  def fix_outlier(self,df:pd.DataFrame, columns):
  for column in columns:
  df[column] = np.where(df[column] > df[column].quantile(0.95), df[column].median(), df[column])
  return df
  def handle_outliers(self, df: pd.DataFrame,lower,upper):
  selected_columns = df.select_dtypes(include='float64').columns
  for col in selected_columns:
  df[col] = winsorize(df[col], (lower, upper))
  return df
df_cleaned = df.copy()
 
data_cleaner = CleanData(df_cleaned)
 
df_cleaned['Handset Manufacturer'] = df_cleaned['Handset Manufacturer'].fillna(
  df_cleaned['Handset Manufacturer'].mode()[0])
df_cleaned['Handset Type'] = df_cleaned['Handset Type'].fillna(
  df_cleaned['Handset Type'].mode()[0])
df_cleaned['Last Location Name'] = df_cleaned['Last Location Name'].fillna(
  df_cleaned['Last Location Name'].mode()[0])
df_cleaned['End'] = fix_missing_ffill(df_cleaned, 'End')
df_cleaned['Start'] = fix_missing_ffill(df_cleaned, 'Start')
 
drop_column = mis_val_table_ren_columns[mis_val_table_ren_columns['% of Total Values']>15].index
print(drop_column.tolist())
 
df_cleaned= df.drop(drop_column.tolist(),axis=1)
df_cleaned.head()
df_cleaned['Nb of sec with Vol DL < 6250B'] = fix_missing_bfill(
  df_cleaned, 'Nb of sec with Vol DL < 6250B')
 
missing_percentage = (df.isnull().sum() / len(df)) * 100
 
fill_mode = missing_percentage[(missing_percentage < 20.0) & (missing_percentage >= 0.4)].index.tolist()
not_fill_mode = ['IMEI', 'IMSI', 'MSISDN/Number']
 
fill_mode_columns = [x for x in fill_mode if x not in not_fill_mode]
 
columns_to_fill = [col for col in fill_mode_columns if col in df_cleaned.columns]
 
df_cleaned[columns_to_fill] = df_cleaned[columns_to_fill].fillna(df_cleaned[columns_to_fill].mode().iloc[0])
 
print(df_cleaned[columns_to_fill].isnull().sum())  def cap_outliers(series):
  q1 = series.quantile(0.25)
  q3 = series.quantile(0.75)
  iqr = q3 - q1
  lower_bound = q1 - 1.5 * iqr
  upper_bound = q3 + 1.5 * iqr
  return series.clip(lower=lower_bound, upper=upper_bound)
missing_info = MissingInformation(df_cleaned)
mis_val_table_after_clean = missing_info.missing_values_table(df_cleaned)
mis_val_table_after_clean
 
df_cleaned.dropna(inplace=True)
mis_val_table_after_clean = missing_info.missing_values_table(df_cleaned)
mis_val_table_after_clean
 
df_cleaned.info()
df_cleaned.shape
 
handset_counts = df_cleaned['Handset Type'].value_counts()
 
sorted_handsets = handset_counts.sort_values(ascending=False)
 
top_10_handsets = sorted_handsets.head(10)
 
plt.figure(figsize=(10,6))
top_10_handsets.plot(kind='bar')
plt.title('Top 10 handsets used by customers')
plt.xlabel('handset type')
plt.ylabel('frequency')
plt.xticks(rotation=45)
plt.show()
 
manu_counts = df_cleaned['Handset Manufacturer'].value_counts()
 
top_3_manufacturers = manu_counts.head(3)
 
plt.figure(figsize=(8,6))
top_3_manufacturers.plot(kind='bar')
plt.title('Top 3 Handset Manufacturers')
plt.xlabel('Manufacturer')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.show()
 
manufacturer_counts = df_cleaned['Handset Manufacturer'].value_counts()
 
top_3_manufacturers = manufacturer_counts.head(3).index
 
filtered_df = df_cleaned[df_cleaned['Handset Manufacturer'].isin(top_3_manufacturers)]
 
handset_counts = filtered_df.groupby(['Handset Manufacturer','Handset Type']).size()
 
top_5_handsets = handset_counts.groupby('Handset Manufacturer').nlargest(5)
 
top_5_handsets.unstack().plot(kind='bar', figsize=(10,6))
plt.title('top 5 handsets per manufacturer')
plt.xlabel('manufacturer')
plt.ylabel('freqency')
plt.xticks(rotation=0)
plt.legend(title = 'handset model')
plt.show()
handset_man= df_cleaned[df_cleaned['Handset Manufacturer'].isin(['Apple','Sumsung','Huawei'])]
handset_man.groupby('Handset Manufacturer')['Handset Type'].value_counts()[:10].plot.bar(
  figsize=(12, 10), fontsize=15)
 
aggregated_data = df_cleaned.groupby('MSISDN/Number').agg({
  'Dur. (ms)': 'count',   'Dur. (ms)': 'sum',   'Total UL (Bytes)': 'sum',
  'Total DL (Bytes)': 'sum',
  'Social Media DL (Bytes)': 'sum',
  'Social Media UL (Bytes)': 'sum',
  'Google DL (Bytes)': 'sum',
  'Google UL (Bytes)': 'sum',
  'Email DL (Bytes)': 'sum',
  'Email UL (Bytes)': 'sum',
  'Youtube DL (Bytes)': 'sum',
  'Youtube UL (Bytes)': 'sum',
  'Netflix DL (Bytes)': 'sum',
  'Netflix UL (Bytes)': 'sum',
  'Gaming DL (Bytes)': 'sum',
  'Gaming UL (Bytes)': 'sum',
  'Other DL (Bytes)': 'sum',
  'Other UL (Bytes)': 'sum'
}).reset_index()
aggregated_data.head()  
dispersion_params = df_cleaned[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)',   'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',
  'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].std()
dispersion_params
df_cleaned[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)',   'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',
  'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].hist(figsize=(10, 10))
plt.tight_layout()
plt.show()
app_vs_total_data = df_cleaned[['Total UL (Bytes)', 'Total DL (Bytes)', 'Social Media DL (Bytes)',   'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',
  'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']].corr()
app_vs_total_data
 
df_cleaned['Total_Session_duration'] = df_cleaned['Dur. (ms)'].groupby(df_cleaned['MSISDN/Number']).transform('sum')
df_cleaned['Decile_Class'] = pd.qcut(df_cleaned['Total_Session_duration'], q=5, labels=False)
 
total_data_per_decile = df_cleaned.groupby('Decile_Class')[['Total UL (Bytes)', 'Total DL (Bytes)']].sum()
total_data_per_decile
correlation_matrix = df_cleaned[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',   'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',   'Other DL (Bytes)']].corr()
correlation_matrix
 
pca_data = df_cleaned[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',   'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',   'Other DL (Bytes)']]
 
pca_data_standardized = (pca_data - pca_data.mean()) / pca_data.std()
 
pca = PCA(n_components=2)
pca.fit(pca_data_standardized)
 
components = pca.components_
explained_variance = pca.explained_variance_ratio_
 
print("Principal Components:")
print(components)
print("Explained Variance Ratio:")
print(explained_variance)
import streamlit as st
st.title('User analytics in Telecommunication industry \n this is my dashboard to explain eda on telecommunication')
st.header('This is a header')
st.write("Welcome to my Streamlit Dashboard!")
%reload_ext autoreload
%autoreload 2
import missingno as msno  
import matplotlib.pyplot as plt  
import pandas as pd
from sqlalchemy import create_engine
import sys  
import os  
import numpy as np  
import seaborn as sns  
import sys, os
import sklearn
from sklearn.decomposition import PCA
import logging
from scipy.stats.mstats import winsorize
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
 
df_cleaned = pd.read_csv('/home/hp/10academy/10telecom/useranalyst.csv')
 
aggregated_data = df_cleaned.groupby('MSISDN/Number').agg({
  'Dur. (ms)': 'count',   'Dur. (ms)': 'sum',   'Total UL (Bytes)': 'sum',
  'Total DL (Bytes)': 'sum',
 
}).reset_index()
aggregated_data.head()  
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
 
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
 
customer_engagement = df_cleaned.groupby('MSISDN/Number').agg({
  'Dur. (ms)': ['count', 'sum'],   'Total UL (Bytes)': 'sum',
  'Total DL (Bytes)': 'sum',
})
 
customer_engagement.columns = ['SessionCount', 'TotalDuration', 'Total UL (Bytes)', 'Total DL (Bytes)']
 
print(customer_engagement.head(10))
 
normalized_engagement = (customer_engagement - customer_engagement.min()) / (customer_engagement.max() - customer_engagement.min())
 
top_10_engaged_customers = {
  'Dur. (ms) Count': normalized_engagement.nlargest(10, 'SessionCount').index,
  'Dur. (ms) Sum': normalized_engagement.nlargest(10, 'TotalDuration').index,
  'Total UL (Bytes)': normalized_engagement.nlargest(10, 'Total UL (Bytes)').index,
  'Total DL (Bytes)': normalized_engagement.nlargest(10, 'Total DL (Bytes)').index
}
top_10_sessioncount_engaged_customers = normalized_engagement.nlargest(10, 'SessionCount').index
 
print(top_10_sessioncount_engaged_customers)
 
print("Top 10 Customers per Engagement Metric:")
print(top_10_engaged_customers)
 
session_frequency = df_cleaned['MSISDN/Number'].value_counts()
session_frequency
average_session_duration = df_cleaned.groupby('MSISDN/Number')['Dur. (ms)'].mean()
average_session_duration
df_cleaned['Total_Traffic'] = df_cleaned['Total UL (Bytes)'] + df_cleaned['Total DL (Bytes)']
total_traffic_per_user = df_cleaned.groupby('MSISDN/Number')['Total_Traffic'].sum()
total_traffic_per_user
 
engagement_metrics = pd.DataFrame({
  'Session_Frequency': session_frequency,
  'Average_Session_Duration': average_session_duration,
  'Total_Traffic': total_traffic_per_user
}).reset_index()
 
engagement_metrics.head()
 
top_10_session_frequency = engagement_metrics.nlargest(10, 'Session_Frequency')
top_10_session_frequency
top_10_avg_duration = engagement_metrics.nlargest(10, 'Average_Session_Duration')
top_10_avg_duration
top_10_total_traffic = engagement_metrics.nlargest(10, 'Total_Traffic')
top_10_total_traffic
scaler = MinMaxScaler()
normalized_metrics = scaler.fit_transform(engagement_metrics.drop('MSISDN/Number', axis=1))
normalized_metrics
kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)
engagement_metrics['Cluster'] = kmeans.fit_predict(normalized_metrics)
 
cluster_stats = engagement_metrics.groupby('Cluster').agg({
  'Session_Frequency': ['min', 'max', 'mean', 'sum'],
  'Average_Session_Duration': ['min', 'max', 'mean', 'sum'],
  'Total_Traffic': ['min', 'max', 'mean', 'sum']
}).reset_index()
cluster_stats
app_traffic = df_cleaned.groupby('MSISDN/Number')[['Social Media DL (Bytes)', 'Google DL (Bytes)',   'Email DL (Bytes)', 'Youtube DL (Bytes)',
  'Netflix DL (Bytes)', 'Gaming DL (Bytes)',   'Other DL (Bytes)']].sum()
app_traffic
app_traffic['Total_App_Traffic'] = app_traffic.sum(axis=1)
top_10_social_media = app_traffic.nlargest(10, 'Social Media DL (Bytes)')
top_10_social_media
top_10_google = app_traffic.nlargest(10, 'Google DL (Bytes)')
top_10_google
top_10_youtube = app_traffic.nlargest(10, 'Youtube DL (Bytes)')
top_10_youtube  
total_app_traffic = app_traffic.drop('Total_App_Traffic', axis=1).sum()
 
top_3_apps = total_app_traffic.nlargest(3)
top_3_apps.plot(kind='bar', xlabel='Applications', ylabel='Total Traffic', title='Top 3 Most Used Applications')
plt.show()
inertia_values = []
for k in range(1, 11):
  kmeans = KMeans(n_clusters=k, random_state=42)
  kmeans.fit(normalized_metrics)
  inertia_values.append(kmeans.inertia_)
 
plt.plot(range(1, 11), inertia_values, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()
import sys
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
sys.path.append('script')
sys.path.append('analysis')
from script import dbconn
from analysis import data_preprocessing
from analysis  import user_overview_analysis
from analysis import user_engagement_analysis
df_telecom = data_preprocessing.load_data_from_postgres()
df_telecom.head(5)
df_telecom.isnull().sum()
df1 = data_preprocessing.load_data_from_sqlalchemy()
df1.info()
df_clean = data_preprocessing.clear_data(df1)
df_clean.head()
df_clean.info()
final_data = data_preprocessing.read_data()
final_data.head(10)
final_data.describe()
user_overview_analysis.history_graph(df_clean)
df_clean.isnull().sum()
final_data.describe()
user_overview_analysis.top_handsets_identifier(final_data)
user_overview_analysis.top_manfacturers(final_data)
user_overview_analysis.handsets_per_manufacturers(final_data)
user_overview_analysis.user_aggregation(final_data)
display(user_overview_analysis.total_user_aggregation(final_data).head(10))
data_description = final_data.dtypes.reset_index()
data_description.columns = ['Variable', 'Data Type']
 
data_description
user_overview_analysis.dispersion_parameters(final_data).head(20)
user_overview_analysis.histogram_of_session_duration(final_data)
user_overview_analysis.bivariate_analysis(final_data)
user_overview_analysis.correlation_matrix(final_data)
user_overview_analysis.principal_variance(final_data)
user_engagement_analysis.user_engagement_metrics(final_data)
user_engagement_analysis.normalized_engagment_metrics(final_data)
user_engagement_analysis.app_traffic(final_data)
user_engagement_analysis.top_apps_per_user_engagement(final_data)
user_engagement_analysis.KElbowVisualizer_on_normalized_metrics(final_data)
import pandas.io.sql as sqlio
import psycopg2
from psycopg2 import sql
from sqlalchemy import create_engine
import pandas as pd
 
def db_connection_sqlalchemy():
  engine = create_engine('postgresql+psycopg2://postgres:1234@localhost/telecom')
  return engine
def db_read_table_sqlalchemy(engine, table_name):
  query = f'SELECT * FROM {table_name}'
  df= pd.read_sql_query(query, engine)
  return df
 
def db_connection_psycopg():
  pgconn = psycopg2.connect(dbname="telecom",user="postgres",password="1234",host="localhost",port="5432")
  return pgconn
 
def db_read_table_psycopg(pgconn, table_name):
  sql = f'SELECT * FROM {table_name}'
  df = sqlio.read_sql_query(sql, pgconn)
  return df
def db_write_table_psycopg(pgconn, tablename, df):
  pass
def db_delete_table_pyscopg():
  cursor = pgconn.cursor()
  drop_table_query = sql.SQL("DROP TABLE IF EXISTS {} CASCADE").format(sql.Identifier(table_name))
  cursor.execute(drop_table_query)
  pgconn.commit()
  print(f"Table `{table_name}` has been successfully deleted.")
  if cursor:
  cursor.close()
def marketing_recommendation(df,self):
  interpretation = "Based on the user overview analysis, the marketing team should focus on:"
  top_handsets_list = top_handsets(df).index.tolist()
  interpretation += f"\n- Promoting the top handsets: {', '.join(top_handsets_list)}"
  top_manufacturers_list = top_manufacturers(df).index.tolist()
  interpretation += f"\n- Collaborating with the top manufacturers: {', '.join(top_manufacturers_list)}"
  for manufacturer in top_manufacturers_list:
  top_handsets_list = top_handsets_per_manufacturer(df, manufacturer).index.tolist()
  interpretation += f"\n- Highlighting top handsets for {manufacturer}: {', '.join(top_handsets_list)}"
  return interpretation
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import joblib
import pymysql
def assign_engagement_experience_scores(df, clustering_results, features_for_clustering):
  scaler = StandardScaler()
  user_data_for_clustering = df[features_for_clustering].copy()
  user_data_for_clustering_scaled = scaler.fit_transform(user_data_for_clustering)
  less_engaged_cluster_center = clustering_results.groupby('Cluster')[features_for_clustering].mean().iloc[0]
  engagement_scores = np.linalg.norm(user_data_for_clustering_scaled - less_engaged_cluster_center, axis=1)
  worst_experience_cluster_center = clustering_results.groupby('Cluster')[features_for_clustering].mean().idxmax()
  experience_scores = np.linalg.norm(user_data_for_clustering_scaled - worst_experience_cluster_center, axis=1)
  df['Engagement Score'] = engagement_scores
  df['Experience Score'] = experience_scores
  return df
def calculate_satisfaction_score(df):
  df['Satisfaction Score'] = (df['Engagement Score'] + df['Experience Score']) / 2
  return df
def top_satisfied_customers(df, n=10):
  top_satisfied = df.nlargest(n, 'Satisfaction Score')
  return top_satisfied
def build_regression_model(df, target_column='Satisfaction Score'):
  features = ['Engagement Score', 'Experience Score']
  X = df[features]
  y = df[target_column]
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  model = LinearRegression()
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  mse = mean_squared_error(y_test, y_pred)
  return model, mse
def kmeans_clustering_2(df, features, n_clusters=2):
  scaler = StandardScaler()
  data_for_clustering = df[features].copy()
  data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)
  kmeans = KMeans(n_clusters=n_clusters, random_state=42)
  df['Cluster 2'] = kmeans.fit_predict(data_for_clustering_scaled)
  cluster_summary = df.groupby('Cluster 2')[features].mean()
  return df, cluster_summary
def aggregate_scores_per_cluster(df, cluster_column='Cluster 2'):
  agg_per_cluster = df.groupby(cluster_column)['Satisfaction Score', 'Experience Score'].mean()
  return agg_per_cluster
def export_to_mysql(df, table_name='satisfaction_scores', host='localhost', user='root', password='password', database='telecom_analysis'):
  connection = pymysql.connect(host=host, user=user, password=password, database=database)
  cursor = connection.cursor()
  create_table_query = f"""
  CREATE TABLE IF NOT EXISTS {table_name} (
  `MSISDN/Number` INT PRIMARY KEY,
  `Engagement Score` FLOAT,
  `Experience Score` FLOAT,
  `Satisfaction Score` FLOAT,
  `Cluster 2` INT
  );
  """
  cursor.execute(create_table_query)
  insert_query = f"""
  INSERT INTO {table_name} (`MSISDN/Number`, `Engagement Score`, `Experience Score`, `Satisfaction Score`, `Cluster 2`)
  VALUES (%s, %s, %s, %s, %s);
  """
  for _, row in df.iterrows():
  cursor.execute(insert_query, (row['MSISDN/Number'], row['Engagement Score'], row['Experience Score'], row['Satisfaction Score'], row['Cluster 2']))
  connection.commit()
  connection.close()
if __name__ == "__main__":
  features_for_clustering = ['TCP Retransmission', 'RTT', 'Throughput']
  df = assign_engagement_experience_scores(df, clustering_results, features_for_clustering)
  print(df[['MSISDN/Number', 'Engagement Score', 'Experience Score']].head())
  df = calculate_satisfaction_score(df)
  top_satisfied = top_satisfied_customers(df, n=10)
  print("\nTask 5.2 - Top Satisfied Customers:")
  print(top_satisfied[['MSISDN/Number', 'Satisfaction Score']])
  regression_model, mse = build_regression_model(df)
  print("\nTask 5.3 - Regression Model Evaluation:")
  print(f"Mean Squared Error: {mse}")
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
def clean_data(df):
  df_cleaned = df.copy()
  df_cleaned['TCP Retransmission'] = df_cleaned['TCP Retransmission'].fillna(df_cleaned['TCP Retransmission'].mean())
  df_cleaned['RTT'] = df_cleaned['RTT'].fillna(df_cleaned['RTT'].mean())
  df_cleaned['Handset Type'] = df_cleaned['Handset Type'].fillna(df_cleaned['Handset Type'].mode()[0])
  df_cleaned['Throughput'] = df_cleaned['Throughput'].fillna(df_cleaned['Throughput'].mean())
  return df_cleaned
def aggregate_user_experience(df):
  user_experience_agg = df.groupby('MSISDN/Number').agg({
  'TCP Retransmission': 'mean',
  'RTT': 'mean',
  'Handset Type': 'first',   'Throughput': 'mean'
  }).reset_index()
  return user_experience_agg
def top_bottom_frequent_values(df, column, n=10):
  top_values = df[column].nlargest(n)
  bottom_values = df[column].nsmallest(n)
  frequent_values = df[column].value_counts().nlargest(n)
  return top_values, bottom_values, frequent_values
def compute_and_report_distribution(df, column_group, column_value, title):
  plt.figure(figsize=(12, 6))
  sns.boxplot(x=column_group, y=column_value, data=df)
  plt.title(title)
  plt.show()
def kmeans_clustering(df, features, n_clusters=3):
  scaler = StandardScaler()
  data_for_clustering = df[features].copy()
  data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)
  kmeans = KMeans(n_clusters=n_clusters, random_state=42)
  df['Cluster'] = kmeans.fit_predict(data_for_clustering_scaled)
  cluster_summary = df.groupby('Cluster')[features].mean()
  return df, cluster_summary
import numpy as np
import pandas as pd
from scipy import stats
class PreProcess:
  def __init__(self, data):
  self.data = data
  def remove_duplicate(self):
  return self.data.drop_duplicates()
  def remove_missing(self):
  '''
  This function removes all columns   with missing values of above 30%
  '''
  self.data = self.data.dropna(thresh=len(self.data)*0.7, axis=1)
  return self.data
  def convert_to_timestamp(self, column_name):
  '''
  This function converts the date column to timestamp
  '''
  self.data[column_name] = pd.to_datetime(self.data[column_name])
  return self.data
  def impute_mean_numeric(self):
  '''
  This function imputes mean values to missing values in numeric columns
  '''
  numeric_columns = self.data.select_dtypes(include=['float64', 'number']).columns
  self.data.loc[:, numeric_columns] = self.data.loc[:, numeric_columns].apply(lambda x: x.fillna(x.mean()))
  return self.data
  def impute_mean_datetime(self):
  '''
  This function imputes mean values to missing values in datetime columns
  '''
  datetime_columns = self.data.select_dtypes(include=['datetime64']).columns
  for col in datetime_columns:
  mean_timestamp = self.data[col].mean().timestamp()
  self.data.loc[:, col] = self.data[col].fillna(pd.to_datetime(mean_timestamp, unit='s'))
  return self.data
  def impute_mode_categorical(self):
  '''
  This function imputes mode values to missing values in categorical columns
  '''
  categorical_columns = self.data.select_dtypes(include=['object']).columns
  for col in categorical_columns:
  self.data.loc[:, col] = self.data[col].fillna(self.data[col].mode()[0])
  return self.data
  def remove_outliers(self):
  '''
  This function removes outliers from the dataset for numeric fields
  '''
  numeric_columns = self.data.select_dtypes(include=['float64', 'number']).columns
  z_scores = np.abs(stats.zscore(self.data[numeric_columns]))
  filtered_data = self.data[(z_scores < 3).all(axis=1)]
  self.data = filtered_data
  return filtered_data
%load_ext autoreload
%autoreload 2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import warnings
warnings.filterwarnings('ignore')
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0,rpath)
from src.preprocessing import PreProcess
from src.overview import Overview
from src.outlier import Outlier
from db_connection.database import Database
conn = Database(host='localhost',port=5432,user='basilel',dbname='telecom',password='Basi')
conn.connect()
query = "SELECT * FROM xdr_data;"
df_data = pd.read_sql(query, conn.connection)
df_data.to_csv('../data/uncleanData.csv', index=False)
df = df_data.copy()
df.head()
df.shape
 
print(f" There are {df.shape[0]} rows and {df.shape[1]} columns")
df.describe()
df.columns.tolist()
df.isna().sum()
df.info()
duplicated_entries = df[df.duplicated()]
duplicated_entries.shape
df.hist(bins=80, figsize=(30,25))
overview = Overview(df)
percent_missing = overview.percent_missing(df)
df['MSISDN/Number'].plot(kind='density')
print('This distribution has skew', df['MSISDN/Number'].skew())
print('This distribution has kurtosis', df['MSISDN/Number'].kurt())
preprocess = PreProcess(df)
df = preprocess.clean_feature_name(df)
df.columns
df.dtypes
%load_ext autoreload
%autoreload 2
df = preprocess.drop_duplicates(df)
df_c, df_before_filling, missing_cols = preprocess.drop_variables(df)
print(missing_cols)
cols, df_single, num_cols = preprocess.fill_numerical_variables(df)
print(len(missing_cols))
df_cols, df_single, cat_cols = preprocess.fill_categorical_variables(df, cols, num_cols, df_single)
df_single.columns[df_single.isnull().mean() > 0] 
df_single.info()
df_single.isna().sum().nlargest(10)
df_single.head()
df_single.columns
df.columns
 
df.to_csv('../data/cleaned_data2.csv', index=False)
import os
import sys
import pandas as pd  
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0,rpath)
from src import utils
from src.plots import Plot
from src.preprocessing import PreProcess
from src.overview import Overview
df = pd.read_csv('../data/cleaned_data_outliers.csv')
plots = Plot()
preprocess = PreProcess(df)
overview = Overview(df)
handset_counts = df['Handset Type'].value_counts().head(10)
plots.plot_bar(x=handset_counts.values, y=handset_counts.index, xlabel='Number of Users', title='Top 10 Handsets by Users',ylabel='Handset Type')
manufacturer_counts = df['Handset Manufacturer'].value_counts().head(3)
plots.plot_pie(manufacturer_counts, title='Top 3 Handset Manufacturers by Users')
top_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index
 
df_top_manufacturers = df[df['Handset Manufacturer'].isin(top_manufacturers)]
 
top_handsets_per_manufacturer = (
  df_top_manufacturers
  .groupby('Handset Manufacturer')['Handset Type']
  .value_counts()
  .groupby(level=0, group_keys=False)
  .nlargest(5)
  .reset_index(name='Count')
)
 
plots.plot_bar(x=top_handsets_per_manufacturer['Count'], y=top_handsets_per_manufacturer['Handset Type'], xlabel='Number of Users', title='Top 5 Handsets per Top 3 Handset Manufacturers',ylabel='Handset Type',palette='viridis')
 
xDR_sess_agg = df['MSISDN/Number'].value_counts()
print(xDR_sess_agg.nlargest(10))
df.groupby('MSISDN/Number')['Dur. (ms)'].sum().nlargest(10)
df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().nlargest(10, 'Total DL (Bytes)')
df["social_media"] = df["Social Media DL (Bytes)"] + df['Social Media UL (Bytes)']
df["google"] = df["Google DL (Bytes)"] + df["Google UL (Bytes)"]
df['email'] = df["Email DL (Bytes)"] + df["Email UL (Bytes)"]
df['youtube'] = df["Youtube DL (Bytes)"] + df["Youtube UL (Bytes)"]
df['netflix'] = df["Netflix DL (Bytes)"] + df["Netflix UL (Bytes)"]
df["gaming"] = df["Gaming DL (Bytes)"] + df["Gaming UL (Bytes)"]
df['other'] = df["Other DL (Bytes)"]+df["Other UL (Bytes)"]
df['total_data'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']
 
quantitative_vars = df.select_dtypes(include=['float64', 'int64'])
 
dispersion_parameters = pd.DataFrame({
  'Mean': quantitative_vars.mean(),
  'Std Deviation': quantitative_vars.std(),
  'Min': quantitative_vars.min(),
  '25th Percentile (Q1)': quantitative_vars.quantile(0.25),
  'Median (Q2)': quantitative_vars.median(),
  '75th Percentile (Q3)': quantitative_vars.quantile(0.75),
  'Max': quantitative_vars.max(),
  'IQR': quantitative_vars.quantile(0.75) - quantitative_vars.quantile(0.25)
})
 
print(dispersion_parameters)
correlation_coefficients = quantitative_vars.corr(method='pearson')
correlation_coefficients
plots.plot_hist(df=df, column='Dur. (ms)', title='Distribution of Session Durations')
plots.plot_hist(df=df, column='Bearer Id', title='Distribution of Total Data')
plots.plot_hist(df=df, column='total_data',title='Distribution of Total Data')
plots.plot_hist(df=df, column='social_media', title='Distribution of Social Media Data')
plots.plot_hist(df=df, column='google', title='Distribution of Google Data')
plots.plot_hist(df=df, column='email', title='Distribution of Email Data')
plots.plot_hist(df=df, column='youtube', title='Distribution of Youtube Data')
plots.plot_hist(df=df, column='gaming', title='Distribution of Gaming Data')
plots.plot_scatter(df, df['google'], df['total_data'], 'Total Data Vs. Google', None, None)
plots.plot_scatter(df,  df['google'], df['Dur. (ms)'], 'Content Duration Vs. Google', None, None)
plots.plot_scatter(df,  df['youtube'], df['Dur. (ms)'], 'Content Duration Vs. YouTube', None, None)
plots.plot_scatter(df,  df['netflix'], df['Dur. (ms)'], 'Content Duration Vs. Netflix', None, None)
plots.plot_scatter(df, df['netflix'], df['total_data'], 'Total Data Vs. Netflix', None, None)
plots.plot_scatter(df, df['gaming'], df['Dur. (ms)'], 'Total Duration Vs. Gaming', None, None)
sns.relplot(data=df, x="Handset Manufacturer", y="google", hue=None, kind="line")
sns.relplot(x="total_data", y="google", hue="Handset Manufacturer", data=df);
preprocess = PreProcess(df)
df2 = df.copy()
df3 = pd.read_csv('../data/cleaned_data2.csv')
feature_to_use = df3[['msisdn/number', 'social_media', 'google', 'email', 'youtube', 'netflix',
  'gaming',  'other', 'total_data', 'handset_manufacturer']]
feature_to_use.head()
total_count_app = pd.DataFrame()
google_total = feature_to_use.sum()[1]
email_total = feature_to_use.sum()[2]
youtube_total = feature_to_use.sum()[3]
netflix_total = feature_to_use.sum()[4]
gaming_total = feature_to_use.sum()[5]
other_total = feature_to_use.sum()[6]
total_count_app['app'] = ['google', 'email', 'youtube', 'netflix', 'gaming', 'other']
total_count_app['total'] = [google_total, email_total, youtube_total, netflix_total, gaming_total,  other_total]
total_count_app.head(6)
total_count_app.head()
total_count_app
plots.plot_bar(x=total_count_app['app'], y=total_count_app['total'], title="Total data usage per app", xlabel="Application", ylabel="Total data volume")
df.columns
var_tr = df[['MSISDN/Number', 'Dur. (ms)', 'total_data']]
var_tr_agg = var_tr.groupby('MSISDN/Number').agg({'Dur. (ms)':'sum', 'total_data': 'sum'})
var_tr_agg.shape
var_tr_agg_decile = overview.get_decile(var_tr_agg, 'Dur. (ms)', 5, ['First Decile', 'Second Decile', 'Third Decile', 'Fourth Decile', 'Fifth Decile'])
var_tr_agg_decile.head()
plots.plot_bar(x=var_tr_agg_decile['deciles'], y=var_tr_agg_decile['total_data'] , title="Total data usage per decile", xlabel="Decile", ylabel="Total data volume")
corr_analysis = df3[['msisdn/number','social_media', 'google',
  'email', 'youtube', 'netflix', 'gaming', 'other']]
corr_analysis_agg = corr_analysis.groupby('msisdn/number').agg({'social_media':'sum', 'google':'sum', 'email':'sum', 'youtube':'sum', 'netflix':'sum', 'gaming':'sum', 'other':'sum'})
plots.plot_heatmap(corr_analysis_agg, "Correlation between apps")
corr_analysis_agg.corr()
num_cols = df.select_dtypes(include=np.number).columns
cat_cols = list(set(df.columns) - set(num_cols))
df[num_cols].columns
num_final = [col for col in num_cols if col not in ['msisdn/number','bearer_id', 'start_ms', 'end_ms', 'imsi', 'imei']]
len(num_final)
len(df.columns)
def clean_dataset(df):
  assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
  df.dropna(inplace=True)
  indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)
  return df[indices_to_keep].astype(np.float64)
df2 = df.copy()
df2 = clean_dataset(df2[num_final])
df2.shape
from sklearn.preprocessing import StandardScaler
 
scaler = StandardScaler()
scaler.fit(df2)
df_scaled = scaler.transform(df2)
df_scaled.shape
from sklearn.decomposition import PCA
pca_44 = PCA(n_components=44, random_state=42)
pca_44.fit(df_scaled)
x_pca_44 = pca_44.transform(df_scaled)
x_pca_44.shape 
sum(pca_44.explained_variance_ratio_)
np.cumsum(pca_44.explained_variance_ratio_ * 100)
plt.plot(np.cumsum(pca_44.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
print("Variance explained by first Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[0]))
print("Variance explained by  2 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[1]))
print("Variance explained by  3 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[2]))
print("Variance explained by  10 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[9]))
pca_65 = PCA(n_components=0.65, random_state=42)
pca_65.fit(df_scaled)
df_pca_95 = pca_65.transform(df_scaled)
df_pca_95.shape
import os
import sys
import numpy as np
import pandas as pd  
import matplotlib.pyplot as plt
import seaborn as sns
from kneed import KneeLocator
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0,rpath)
from src.plots import Plot
df = pd.read_csv('../data/cleaned_data_outliers.csv')
df.head(5)
column_name = 'msisdn/number'
value_to_match = 33663706799.0
mask = df[column_name] == value_to_match
df = df[~mask]
df.columns
df["social_media"] = df["social_media_dl_(bytes)"] + df['social_media_ul_(bytes)']
df["google"] = df["google_dl_(bytes)"] + df["google_ul_(bytes)"]
df['email'] = df["email_dl_(bytes)"] + df["email_ul_(bytes)"]
df['youtube'] = df["youtube_dl_(bytes)"] + df["youtube_ul_(bytes)"]
df['netflix'] = df["netflix_dl_(bytes)"] + df["netflix_ul_(bytes)"]
df["gaming"] = df["gaming_dl_(bytes)"] + df["gaming_ul_(bytes)"]
df['other'] = df["other_dl_(bytes)"]+df["other_ul_(bytes)"]
df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
df = df.rename( columns={'bearer_id': 'sessions'})
data_aggregate = df.groupby('msisdn/number').agg({'sessions': 'count', 'dur._(ms)': 'sum', 'total_data': 'sum'})
data_aggregate.head()
sessions = data_aggregate.nlargest(10, "sessions")['sessions']
duration = data_aggregate.nlargest(10, "dur._(ms)")['dur._(ms)']
total_data = data_aggregate.nlargest(10, "total_data")['total_data']
sesssions_df = pd.DataFrame()
sesssions_df['User_ID'] = sessions.index
sesssions_df['Sessions'] = sessions.values
durations_df = pd.DataFrame()
durations_df['User_ID'] = duration.index
durations_df['duration'] = duration.values
total_data_df = pd.DataFrame()
total_data_df['User_ID'] = total_data.index
total_data_df['total_data'] = total_data.values
durations_df.head()
f, axes = plt.subplots(1, 3, figsize=(25, 5))
ax1 = sns.barplot(data=sesssions_df, x='User_ID', orient='v', y='Sessions', ax=axes[0])
ax2 = sns.barplot(data=durations_df, x='User_ID',orient='v', y='duration', ax=axes[1])
ax3 = sns.barplot(data=total_data_df, x='User_ID',orient='v', y='total_data', ax=axes[2])
ax1.set_xticklabels(ax1.xaxis.get_majorticklabels(), rotation=45)
ax2.set_xticklabels(ax2.xaxis.get_majorticklabels(), rotation=45)
ax3.set_xticklabels(ax3.xaxis.get_majorticklabels(), rotation=45)
plt.plot()
from sklearn.preprocessing import Normalizer
from sklearn.cluster import KMeans
normalizer = Normalizer()
columns = ['sessions','dur._(ms)','total_data']
selected_columns = data_aggregate[columns]
normalized_data = normalizer.fit_transform(selected_columns)
normalized_user_data = pd.DataFrame(normalized_data, columns=columns)
normalized_user_data.head(5)
normalized_user_data.describe()
plt.figure(figsize=(15,6))
plt.subplot(1,2,1)
sns.violinplot(y=data_aggregate["dur._(ms)"])
 
plt.subplot(1,2,2)
sns.violinplot(y=data_aggregate["sessions"])
 
plt.show()
kmeans = KMeans(init="random",n_clusters=3,n_init=10,max_iter=300,random_state=42)
label = kmeans.fit_predict(normalized_user_data)
centroids = kmeans.cluster_centers_
lables_unique = np.unique(label)
 
plt.figure(figsize=(10,5))
plt.title('User K-Means Classification with 3 Groups (Standardized Data)')
for i in lables_unique:
  plt.scatter(normalized_user_data[label == i].iloc[:,0] , normalized_user_data[label == i].iloc[:,1], marker='o', label = i)
plt.scatter(centroids[:,0] , centroids[:,1],centroids[:,2], marker='x', color = 'black')
plt.legend()
plt.show()
normalized_user_data.head()
clustered_Df = pd.DataFrame()
clustered_Df['data_index'] = normalized_user_data.index.values
clustered_Df['cluster'] = kmeans.labels_
clustered_Df.cluster.value_counts()
u_eng = data_aggregate.copy()
u_eng.insert(0, 'cluster', kmeans.labels_)
u_eng.head(5)
cluster1 = u_eng[u_eng["cluster"]==0]
cluster1.describe()
cluster2 = u_eng[u_eng["cluster"] == 1]
cluster2.describe()
cluster3 = u_eng[u_eng["cluster"] == 2]
cluster3.describe()
cluster3.sessions.count()
data = [cluster1.sessions.count(), cluster2.sessions.count(), cluster3.sessions.count()]
keys = ['Cluster 1', 'Cluster 2', 'Cluster 3']
 
plot = Plot()
plot.plot_pie(data=data, label=keys, title="Cluster Distribution Count")
user_app_usage = df.groupby(
  'msisdn/number').agg({ 'social_media': 'sum', 'gaming': 'sum', 'youtube': 'sum', 'netflix': 'sum', 'google': 'sum', 'email': 'sum', 'other': 'sum'})
user_app_usage.reset_index(inplace=True)
 
user_app_usage = user_app_usage.drop('msisdn/number', axis=1)
user_app_usage
social_media = user_app_usage.nlargest(10, "social_media")['social_media']
gaming = user_app_usage.nlargest(10, "gaming")['gaming']
youtube = user_app_usage.nlargest(10, "youtube")['youtube']
netflix = user_app_usage.nlargest(10, "netflix")['netflix']
google = user_app_usage.nlargest(10, "google")['google']
email = user_app_usage.nlargest(10, "email")['email']
other = user_app_usage.nlargest(10, "other")['other']
social_media_df = pd.DataFrame()
social_media_df['User_ID'] = social_media.index
social_media_df['social_media'] = social_media.values
gaming_df = pd.DataFrame()
gaming_df['User_ID'] = gaming.index
gaming_df['gaming'] = gaming.values
youtube_df = pd.DataFrame()
youtube_df['User_ID'] = youtube.index
youtube_df['youtube'] = youtube.values
 
netflix_df = pd.DataFrame()
netflix_df['User_ID'] = netflix.index
netflix_df['netflix'] = netflix.values
google_df = pd.DataFrame()
google_df['User_ID'] = google.index
google_df['google'] = google.values
email_df = pd.DataFrame()
email_df['User_ID'] = email.index
email_df['email'] = email.values
others_df = pd.DataFrame()
others_df['User_ID'] = other.index
others_df['other'] = other.values
f, axes = plt.subplots(2, 4, figsize=(25, 15))
ax1 = sns.barplot(data=social_media_df, x='User_ID', orient='v', y='social_media', ax=axes[0,0], palette='Blues_d')
ax2 = sns.barplot(data=gaming_df, x='User_ID',orient='v', y='gaming', ax=axes[0,1], palette='Blues_d')
ax3 = sns.barplot(data=youtube_df, x='User_ID',orient='v', y='youtube', ax=axes[0,2], palette='Blues_d')
ax4 = sns.barplot(data=netflix_df, x='User_ID',orient='v', y='netflix', ax=axes[0,3], palette='Blues_d')
ax5 = sns.barplot(data=google_df, x='User_ID',orient='v', y='google', ax=axes[1,0], palette='Blues_d')
ax6 = sns.barplot(data=email_df, x='User_ID',orient='v', y='email', ax=axes[1,1], palette='Blues_d')
ax7 = sns.barplot(data=others_df, x='User_ID',orient='v', y='other', ax=axes[1,2], palette='Blues_d')
ax1.set_xticklabels(ax1.xaxis.get_majorticklabels(), rotation=45)
ax2.set_xticklabels(ax2.xaxis.get_majorticklabels(), rotation=45)
ax3.set_xticklabels(ax3.xaxis.get_majorticklabels(), rotation=45)
ax4.set_xticklabels(ax4.xaxis.get_majorticklabels(), rotation=45)
ax5.set_xticklabels(ax5.xaxis.get_majorticklabels(), rotation=45)
ax6.set_xticklabels(ax6.xaxis.get_majorticklabels(), rotation=45)
ax7.set_xticklabels(ax7.xaxis.get_majorticklabels(), rotation=45)
plt.plot()
top_used_applications = user_app_usage.sum()
top_used_applications.values
top_3_used = top_used_applications.nlargest(3)
top_3_used
plot.plot_bar(top_3_used, ["Netflix", "Email", "Gaming"], top_3_used.values, "Top 3 Used Applications", "Applications", "Usage Count")
inertias = []
for i in range(1,16):
  kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
  kmeans.fit(normalized_user_data)
  inertias.append(kmeans.inertia_)
plt.figure(figsize = (10,8))
plt.plot(range(1, 16), inertias, marker = 'o', linestyle = '--')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertias')
plt.title('K-means Clustering')
plt.show()
kl = KneeLocator(range(0, 15), inertias, curve="convex", direction="decreasing")
kl.elbow
u_eng.shape
u_eng.to_csv('../data/user_eng.csv')
user_app_usage.to_csv('../data/normalized_eng.csv')
import joblib
joblib.dump(kmeans,'../models/user_engagement.pkl')
%load_ext autoreload
%autoreload 2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0,rpath)
from src.preprocessing import PreProcess
from src.overview import Overview
from src.outlier import Outlier
from src.plots import Plot
df = pd.read_csv('../data/cleaned_data2.csv')
outlier = Outlier(df)
df.head()
df.columns
pl = Plot()
pl.plot_box(df, 'dur._(ms)', 'Total Duration of the xDR (in ms)')
pl.plot_box(df, 'avg_rtt_dl_(ms)', 'Average Round Trip Time measurement Downlink direction (msecond)')
pl.plot_box(df, 'tcp_ul_retrans._vol_(bytes)', 'TCP volume of Uplink packets detected as retransmitted (bytes)')
display(outlier.outlier_overview(df, 'activity_duration_ul_(ms)'))
display(outlier.outlier_overview(df, 'tcp_ul_retrans._vol_(bytes)'))
%load_ext autoreload
%autoreload 2
df.dtypes
num_cols = df.select_dtypes(include=np.number).columns
dlist = ['bearer_id', 'start_ms', 'end_ms', 'imsi', 'msisdn/number', 'imei']
num_cols = [el for el in num_cols if el not in dlist]
for col in num_cols:
  sample_outliers = outlier.calculate_num_outliers_zscore(df[col])
  print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
df = outlier.handle_outliers(df, num_cols)
for col in num_cols:
  sample_outliers = outlier.calculate_num_outliers_zscore(df[col])
  print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
outlier_columns = []
for col in num_cols:
  sample_outliers = outlier.calculate_num_outliers_zscore(df[col])
  if len(sample_outliers) > 0:
  outlier_columns.append(col)
len(outlier_columns)
df_cop = df.copy()
from sklearn.preprocessing import MinMaxScaler
 
minmax_scaler = MinMaxScaler()
 
scaled_data = df.copy()
 
scaled_data.sample(5)
df_cop[outlier_columns].shape 
scaled_data.shape
scaled_data = minmax_scaler.fit_transform(scaled_data[outlier_columns])
scaled_data.shape
outlier_columns = []
for col in num_cols:
  sample_outliers = outlier.calculate_num_outliers_zscore(df_cop[col])
  if len(sample_outliers) > 0:
  outlier_columns.append(col)
len(outlier_columns)
for col in num_cols:
  sample_outliers = outlier.calculate_num_outliers_zscore(df_cop[col])
  print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
df_cop[num_cols].boxplot()
pl.plot_box(df_cop, 'dur._(ms)', 'Total Duration of the xDR (in ms) after outlier handling')
df_cop.describe()
 
df_cop.to_csv('../data/cleaned_data_outliers.csv', index=False)
import sys
import numpy as np
import pandas as pd
 
class Outlier:
  def __init__(self, df: pd.DataFrame):
  """Initialize the PreProcess class.
  Args:
  df (pd.DataFrame): dataframe to be preprocessed
  """
  try:
  self.df = df
  except Exception:
  sys.exit(1)
  def handle_outliers(self, df: pd.DataFrame, cols):
  """Handle outliers in the dataset.
  Args:
  df (pd.DataFrame): a dataframe to be preprocessed
  Returns:
  pd.DataFrame: the dataframe
  """
  for col in cols:
  df[col] = [np.log(x) for x in df[col]]
  return df
  def calculate_num_outliers_zscore(self, col):
  """Return the number of outliers for each numerical col.
  Args:
  col (pd.DataFrame): a dataframe to be analyzed
  """
  outliers = []
  thres = 3
  mean = np.mean(col)
  std = np.std(col)
  for i in col:
  z_score = (i-mean)/std
  if (np.abs(z_score) > thres):
  outliers.append(i)
  return outliers   def calculate_num_outliers_iqr(self, df, cols):
  """Return the number of outliers for each col.
  Args:
  df (pd.DataFrame): a dataframe to be analyzed
  cols (list): list of columns to analyze
  """
  outliersTot = {}
  for col in cols:
  outliers = []
  df_sorted = df.sort_values(col)[col]
  q1 = np.percentile(df_sorted, 25)
  q3 = np.percentile(df_sorted, 75)
  IQR = q3 - q1
  lwr_bound = q1 - (1.5 * IQR)
  upr_bound = q3 + (1.5 * IQR)
  for i in df_sorted:
  if (i < lwr_bound or i > upr_bound):
  outliers.append(i)
  outliersTot[col] = outliers
  return outliersTot
  def outlier_overview(self, df, col):
  """Get outlier overview.
  Args:
  df (pd.DataFrame): a dataframe to be analyzed
  """
  upper_limit = df[col].mean() + 3 * df['total_ul_(bytes)'].std()
  lower_limit = df[col].mean() - 3 * df['total_ul_(bytes)'].std()
  return df[~((df[col] < upper_limit) & (df[col] > lower_limit))]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sys
 
class Plot:
  def __init__(self) -> None:
  pass
  def plot_bar(self, x, y, xlabel,ylabel,title,palette=None) -> None:
  plt.figure(figsize=(12, 6))
  sns.barplot(x=x, y=y, palette="viridis")
  plt.title(title)
  plt.xlabel(xlabel)
  plt.ylabel(ylabel)
  plt.show()
  def plot_count(self, df: pd.DataFrame, column: str) -> None:
  """Plot the count of the column.
  Args:
  df (pd.DataFrame): Dataframe to be plotted.
  column (str): column to be plotted.
  """
  plt.figure(figsize=(12, 7))
  sns.countplot(data=df, x=column)
  plt.title(f'Distribution of {column}', size=20, fontweight='bold')
  plt.show()
  def plot_heatmap(self, df: pd.DataFrame, title: str, cbar=False) -> None:
  """Plot Heat map of the dataset.
  Args:
  df (pd.DataFrame): Dataframe to be plotted.
  title (str): title of chart.
  """
  plt.figure(figsize=(12, 7))
  sns.heatmap(df, annot=True, cmap='viridis', vmin=0,
  vmax=1, fmt='.2f', linewidths=.7, cbar=cbar)
  plt.title(title, size=18, fontweight='bold')
  plt.show()
  def plot_box(self, df: pd.DataFrame, x_col: str, title: str) -> None:
  """Plot box chart of the column.
  Args:
  df (pd.DataFrame): Dataframe to be plotted.
  x_col (str): column to be plotted.
  title (str): title of chart.
  """
  plt.figure(figsize=(12, 7))
  sns.boxplot(data=df, x=x_col)
  plt.title(title, size=20)
  plt.xticks(rotation=75, fontsize=14)
  plt.show()
  def plot_box_multi(self, df: pd.DataFrame, x_col: str, y_col: str, title: str) -> None:
  """Plot the box chart for multiple column.
  Args:
  df (pd.DataFrame): Dataframe to be plotted.
  column (str): column to be plotted.
  """
  plt.figure(figsize=(12, 7))
  sns.boxplot(data=df, x=x_col, y=y_col)
  plt.title(title, size=20)
  plt.xticks(rotation=75, fontsize=14)
  plt.yticks(fontsize=14)
  plt.show()
  def plot_scatter(self, df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:
  """Plot Scatter chart of the data.
  Args:
  df (pd.DataFrame): Dataframe to be plotted.
  column (str): column to be plotted.
  """
  plt.figure(figsize=(12, 7))
  sns.scatterplot(data=df, x=x_col, y=y_col, hue=hue, style=style)
  plt.title(title, size=20)
  plt.xticks(fontsize=14)
  plt.yticks(fontsize=14)
  plt.show()
  def plot_pie(self, data, title, label) -> None:
  """Plot pie chart of the data.
  Args:
  data (list): Data to be plotted.
  labels (list): labels of the data.
  colors (list): colors of the data.
  """
  plt.style.context('seaborn-pastel')
  plt.figure(figsize=(8, 8))
  plt.pie(x=data, labels=label, autopct='%1.1f%%', startangle=140)
  plt.title(title)
  plt.show()
  def plot_hist(self, df: pd.DataFrame, column: str, title: str) -> None:
  """Plot histogram of the data.
  Args:
  df (pd.DataFrame): Dataframe to be plotted.
  column (str): column to be plotted.
  """
  plt.figure(figsize=(12, 7))
  sns.histplot(data=df, x=column, kde=True)
  plt.title(title, size=20)
  plt.xticks(fontsize=14)
  plt.yticks(fontsize=14)
  plt.show()
import os
import sys
import numpy as np
import pandas as pd  
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0,rpath)
from src.plots import Plot
from src.preprocessing import PreProcess
df = pd.read_csv('../data/cleaned_data_outliers.csv')
column_name = 'msisdn/number'
value_to_match = 33663706799.0
mask = df[column_name] == value_to_match
df = df[~mask]
df.head()
df.columns
df['avg_tcp_retrans'] = df['tcp_dl_retrans._vol_(bytes)'] + df['tcp_ul_retrans._vol_(bytes)']
df['avg_rrt'] = df['avg_rtt_dl_(ms)'] + df['avg_rtt_dl_(ms)']
df['avg_bearer_tp'] = df['avg_bearer_tp_dl_(kbps)'] + df['avg_bearer_tp_ul_(kbps)']
df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
preprocess = PreProcess(df)
agg_data = df.groupby('msisdn/number').agg({'avg_tcp_retrans':sum,'avg_rrt':sum,'avg_bearer_tp':sum, 'total_data':sum})
preprocess.replace_neg_inf_with_mean(agg_data)
sns.set_style('whitegrid')
sns.lmplot(x='avg_tcp_retrans',y='total_data',data=agg_data,palette='coolwarm',aspect=1)
plot = Plot()
plot.plot_scatter(agg_data,x_col='avg_rrt',y_col='total_data',title='Total Data vs Round Trip Time',hue=None,style=None)
plot.plot_scatter(agg_data,x_col='avg_bearer_tp',y_col='total_data',title='Total Data vs Average Bearer Throughput',hue=None,style=None)
column_name = 'avg_rrt'
 
top_values = agg_data[column_name].nlargest(10)
 
bottom_values = agg_data[column_name].nsmallest(10)
 
most_frequent_values = agg_data[column_name].value_counts().head(10)
 
rrt_result_table = pd.DataFrame({
  'Top Values': top_values.values,   'Bottom Values': bottom_values.values,
  'Most Frequent Values': most_frequent_values.index.values,   'Frequency': most_frequent_values.values
})
rrt_result_table
plot.plot_pie(rrt_result_table['Frequency'],title='Frequent Round Trip Time',label=rrt_result_table['Most Frequent Values'])
column_name = 'avg_rrt'
 
top_values = agg_data[column_name].nlargest(10)
 
bottom_values = agg_data[column_name].nsmallest(10)
 
most_frequent_values = agg_data[column_name].value_counts().head(10)
 
result_table = pd.DataFrame({
  'Top Values': top_values.values,   'Bottom Values': bottom_values.values,
  'Most Frequent Values': most_frequent_values.index.values,   'Frequency': most_frequent_values.values
})
result_table
column_name = 'avg_bearer_tp'
 
top_values = agg_data[column_name].nlargest(10)
 
bottom_values = agg_data[column_name].nsmallest(10)
 
most_frequent_values = agg_data[column_name].value_counts().head(10)
 
result_table = pd.DataFrame({
  'Top Values': top_values.values,   'Bottom Values': bottom_values.values,
  'Most Frequent Values': most_frequent_values.index.values,   'Frequency': most_frequent_values.values
})
result_table
manufacturer_agg = df.groupby('handset_type').agg({'avg_tcp_retrans':'mean','avg_rrt':'mean','avg_bearer_tp':'mean', 'total_data':sum})
manufacturer_agg.reset_index(inplace=True)
top_agg = manufacturer_agg.nlargest(10,columns='avg_bearer_tp')
plot.plot_bar(x=top_agg['handset_type'],y=top_agg['avg_bearer_tp'],xlabel='ReTransmission',ylabel='Manufacturer',title='Title')
top_agg = manufacturer_agg.nlargest(10,columns='avg_bearer_tp')
plot.plot_bar(x=top_agg['handset_type'],y=top_agg['avg_bearer_tp'],xlabel='Retransmission',ylabel='Manufacturer',title='Title')
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
scaler = StandardScaler()
scaled_features = scaler.fit_transform(agg_data[['avg_tcp_retrans','avg_rrt','avg_bearer_tp']])
kmeans = KMeans(n_clusters=3, random_state=42)
agg_data['Cluster'] = kmeans.fit_predict(scaled_features)
agg_data
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
centroid_df = pd.DataFrame(cluster_centers, columns=['avg_tcp_retrans','avg_rrt','avg_bearer_tp'])
print("Cluster Centers:")
print(centroid_df)
cluster_counts = agg_data['Cluster'].value_counts()
print("\nCluster Counts:")
print(cluster_counts)
sns.set(style="whitegrid")
 
plt.figure(figsize=(10, 6))
 
plt.scatter(agg_data.loc[agg_data['Cluster'] == 0, 'avg_tcp_retrans'],
  agg_data.loc[agg_data['Cluster'] == 0, 'avg_rrt'],
  label='Cluster 0', alpha=0.8, s=50)
 
plt.scatter(agg_data.loc[agg_data['Cluster'] == 1, 'avg_tcp_retrans'],
  agg_data.loc[agg_data['Cluster'] == 1, 'avg_rrt'],
  label='Cluster 1', alpha=0.8, s=50)
 
plt.scatter(agg_data.loc[agg_data['Cluster'] == 2, 'avg_tcp_retrans'],
  agg_data.loc[agg_data['Cluster'] == 2, 'avg_rrt'],
  label='Cluster 2', alpha=0.8, s=50)
 
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='X', s=200, color='red', label='Centroids')
 
plt.xlabel('Avg_TCP_Retransmission')
plt.ylabel('Avg_RTT')
plt.title('K-Means Clustering of Telecom Users')
 
plt.legend()
 
plt.show()
import joblib
joblib.dump(kmeans, '../models/kmeans_model.pkl')
import joblib
import os
import sys
import numpy as np
import pandas as pd  
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0,rpath)
from src.plots import Plot
from src.preprocessing import PreProcess
from db_connection.database import Database
kmeans = joblib.load('../models/kmeans_model.pkl')
kmeans
df = pd.read_csv('../data/cleaned_data_outliers.csv')
df['avg_tcp_retrans'] = df['tcp_dl_retrans._vol_(bytes)'] + df['tcp_ul_retrans._vol_(bytes)']
df['avg_rrt'] = df['avg_rtt_dl_(ms)'] + df['avg_rtt_dl_(ms)']
df['avg_bearer_tp'] = df['avg_bearer_tp_dl_(kbps)'] + df['avg_bearer_tp_ul_(kbps)']
df["social_media"] = df["social_media_dl_(bytes)"] + df['social_media_ul_(bytes)']
df["google"] = df["google_dl_(bytes)"] + df["google_ul_(bytes)"]
df['email'] = df["email_dl_(bytes)"] + df["email_ul_(bytes)"]
df['youtube'] = df["youtube_dl_(bytes)"] + df["youtube_ul_(bytes)"]
df['netflix'] = df["netflix_dl_(bytes)"] + df["netflix_ul_(bytes)"]
df["gaming"] = df["gaming_dl_(bytes)"] + df["gaming_ul_(bytes)"]
df['other'] = df["other_dl_(bytes)"]+df["other_ul_(bytes)"]
df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
column_name = 'msisdn/number'
value_to_match = 33663706799.0
mask = df[column_name] == value_to_match
df = df[~mask]
preprocess = PreProcess(df)
agg_data = df.groupby('msisdn/number').agg({'avg_tcp_retrans':sum,'avg_rrt':sum,'avg_bearer_tp':sum})
preprocess.replace_neg_inf_with_mean(agg_data)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_features = scaler.fit_transform(agg_data[['avg_tcp_retrans','avg_rrt','avg_bearer_tp']])
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
centroid_df = pd.DataFrame(cluster_centers, columns=['avg_tcp_retrans','avg_rrt','avg_bearer_tp'])
print("Cluster Centers:")
print(centroid_df)
from sklearn.metrics.pairwise import euclidean_distances
 
distances = euclidean_distances(scaled_features, cluster_centers)
 
less_engaged_cluster_index = 0
 
engagement_scores = distances[:, less_engaged_cluster_index]
 
agg_data['Experience_Score'] = engagement_scores
 
sns.set(style="whitegrid")
 
plt.figure(figsize=(10, 6))
sns.histplot(agg_data['Experience_Score'], bins=30, kde=True, color='skyblue')
 
plt.xlabel('Engagement Score')
plt.ylabel('Frequency')
plt.title('Distribution of Experience Scores')
 
plt.show()
user_engagement_kmeans = joblib.load('../models/user_engagement.pkl')
df = df.rename( columns={'bearer_id': 'sessions'})
data_aggregate = df.groupby('msisdn/number').agg({'sessions': 'count', 'dur._(ms)': 'sum', 'total_data': 'sum'})
data_aggregate.nlargest(5,'sessions')
scaler = StandardScaler()
scaled_features = scaler.fit_transform(data_aggregate[['sessions','dur._(ms)','total_data']])
cluster_centers = scaler.inverse_transform(user_engagement_kmeans.cluster_centers_)
centroid_df = pd.DataFrame(cluster_centers, columns=['sessions','dur._(ms)','total_data'])
from sklearn.metrics.pairwise import euclidean_distances
 
distances = euclidean_distances(scaled_features, cluster_centers)
 
less_engaged_cluster_index = 0
 
engagement_scores = distances[:, less_engaged_cluster_index]
 
data_aggregate['Engagement_Score'] = engagement_scores
 
sns.set(style="whitegrid")
 
plt.figure(figsize=(10, 6))
sns.histplot(data_aggregate['Engagement_Score'], bins=30, kde=True, color='skyblue')
 
plt.xlabel('Engagement Score')
plt.ylabel('Frequency')
plt.title('Distribution of Engagement Scores')
 
plt.show()
agg_data['Engagement_Score'] = data_aggregate['Engagement_Score'] 
agg_data['Satisfaction_Score'] = agg_data['Experience_Score'] + agg_data['Engagement_Score']
satified_customer = agg_data.nlargest(10,'Satisfaction_Score')
plt.figure(figsize=(10, 6))
sns.barplot(x=satified_customer.index, y="Satisfaction_Score", data=satified_customer, palette="Blues_d")
plt.title('Top 10 Satisfied Customers')
plt.xlabel('Customer ID')
plt.ylabel('Satisfaction Score')
plt.xticks(rotation=45)
plt.show()
agg_data
from sklearn.cluster import KMeans
 
kmeans = KMeans(n_clusters=2, random_state=42)
 
predictions = kmeans.fit_predict(agg_data[['Experience_Score','Engagement_Score']])
print(predictions)
 
agg_data['Cluster'] = predictions
 
agg_data.groupby('Cluster').mean()
 
plt.scatter(agg_data['Experience_Score'], agg_data['Engagement_Score'], c=agg_data['Cluster'], cmap='rainbow')
plt.show()
cluster_data = agg_data.groupby('Cluster').mean()
 
plt.figure(figsize=(10, 6))
sns.barplot(x=cluster_data.index, y="Satisfaction_Score", data=cluster_data, palette="Blues_d")
plt.title('Average Satisfaction Score per Cluster')
plt.xlabel('Cluster')
plt.ylabel('Satisfaction Score')
plt.xticks(rotation=45)
plt.show()
import pandas as pd
import sys, os
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
from utils import DataCleaner, DatabaseEngine
db_engine = DatabaseEngine()
engine = db_engine.create()
df = pd.read_sql_table("xdr_data", con=engine)
df.head()
 
df.isnull().sum().sum()
 
cleaner = DataCleaner(df)
cleaned_df = cleaner.clean()
 
cleaned_df.to_sql("clean_xdr_data", con=engine, if_exists="replace", index=False)
clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
 
clean_df.isnull().sum().sum()
filtered_handsets = clean_df[clean_df["Handset Type"] != "undefined"]
top_10_handsets = filtered_handsets["Handset Type"].value_counts().head(10)
pd.DataFrame(top_10_handsets)
top_3_manufacturers = clean_df["Handset Manufacturer"].value_counts().head(3)
pd.DataFrame(top_3_manufacturers)
top_3_manufacturers = ['Apple', 'Samsung', 'Huawei']
 
results = []
 
for manufacturer in top_3_manufacturers:
  top_5_handsets = clean_df[clean_df["Handset Manufacturer"] == manufacturer]["Handset Type"].value_counts().head(5)
  temp_df = pd.DataFrame({'Manufacturer': manufacturer, 'Handset Type': top_5_handsets.index, 'Count': top_5_handsets.values})
  results.append(temp_df)
 
results_df = pd.concat(results, ignore_index=True)
 
pd.DataFrame(results_df)
from urllib.parse import quote_plus
from pathlib import Path
from sqlalchemy import create_engine
from dotenv import dotenv_values
 
class DatabaseEngine:
  """
  A class used to create a SQLAlchemy engine object for the PostgreSQL database.
  ...
  Attributes
  ----------
  env_path : Path
  path to the .env file containing database configuration
  Methods
  -------
  create():
  Creates and returns a SQLAlchemy engine object for the PostgreSQL database.
  """
  def __init__(self):
  """
  Constructs all the necessary attributes for the DatabaseEngine object.
  """
  self.env_path = Path(__file__).parent / ".env"
  def create(self):
  """
  Creates and returns a SQLAlchemy engine object for the PostgreSQL database.
  Returns:
  engine: A SQLAlchemy engine object connected to the PostgreSQL database.
  Raises:
  Exception: If there is a problem connecting to the database.
  """
  env_values = dotenv_values(dotenv_path=self.env_path)
  db_password = env_values["POSTGRES_PASSWORD"]
  encoded_password = quote_plus(db_password)
  database = env_values["POSTGRES_DB"]
  user = env_values["POSTGRES_USER"]
  server = env_values["POSTGRES_SERVER"]
  engine = create_engine(
  f"postgresql://{user}:{encoded_password}@{server}/{database}"
  )
  try:
  with engine.connect() as connection_str:
  print(f'Successfully connected to the PostgreSQL "{database}" database')
  except Exception as ex:
  print(f"Sorry failed to connect: {ex}")
  return engine
class DataCleaner:
  """
  A class used to clean data in a DataFrame.
  ...
  Attributes
  ----------
  df : DataFrame
  a pandas DataFrame to be cleaned
  columns_to_exclude : list
  a list of column names to be excluded from certain operations
  Methods
  -------
  drop_null_rows():
  Drops rows that have a null value in any of the columns that have exactly one null value.
  fill_categorical():
  Fills missing categorical values with the mode of each column.
  fill_numerical():
  Fills missing numerical values with the mean of each column, excluding certain specified columns.
  fill_unknown():
  Fills missing values in certain specified columns with the string "Unknown".
  clean():
  Performs all the cleaning steps and returns the cleaned DataFrame.
  """
  def __init__(self, df):
  """
  Constructs all the necessary attributes for the DataCleaner object.
  Parameters
  ----------
  df : DataFrame
  a pandas DataFrame to be cleaned
  """
  self.df = df.copy()
  self.columns_to_exclude = ["Bearer Id", "IMSI", "MSISDN/Number", "IMEI"]
  def drop_null_rows(self):
  """
  Drops rows that have a null value in any of the columns that have exactly one null value.
  Returns
  -------
  self : object
  Returns self to allow chaining.
  """
  columns_with_one_null = self.df.columns[self.df.isnull().sum() == 1]
  self.df = self.df.dropna(subset=columns_with_one_null)
  return self
  def fill_categorical(self):
  """
  Fills missing categorical values with the mode of each column.
  Returns
  -------
  self : object
  Returns self to allow chaining.
  """
  categorical_columns = self.df.select_dtypes(include="object").columns
  self.df[categorical_columns] = self.df[categorical_columns].fillna(
  self.df[categorical_columns].mode().iloc[0]
  )
  return self
  def fill_numerical(self):
  """
  Fills missing numerical values with the mean of each column, excluding certain specified columns.
  Returns
  -------
  self : object
  Returns self to allow chaining.
  """
  numerical_columns = self.df.select_dtypes(include=["float64"]).columns
  numerical_columns = numerical_columns.drop(self.columns_to_exclude)
  self.df[numerical_columns] = self.df[numerical_columns].fillna(
  self.df[numerical_columns].mean()
  )
  return self
  def fill_unknown(self):
  """
  Fills missing values in certain specified columns with the string "Unknown".
  Returns
  -------
  self : object
  Returns self to allow chaining.
  """
  self.df[self.columns_to_exclude] = self.df[self.columns_to_exclude].fillna(
  "Unknown"
  )
  return self
  def clean(self):
  """
  Performs all the cleaning steps and returns the cleaned DataFrame.
  Returns
  -------
  DataFrame
  The cleaned DataFrame.
  """
  self.drop_null_rows().fill_categorical().fill_numerical().fill_unknown()
  return self.df
from .data_cleaning import DataCleaner
from .db_connection import DatabaseEngine
import pandas as pd
import sys, os
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
from utils import DatabaseEngine
db_engine = DatabaseEngine()
engine = db_engine.create()
clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
def process_user_info(clean_df):
  num_sessions = clean_df.groupby('MSISDN/Number')['Bearer Id'].count()
  session_duration = clean_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()
  total_DL = clean_df.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()
  total_UL = clean_df.groupby('MSISDN/Number')['Total UL (Bytes)'].sum()
  total_data_vol = clean_df.groupby('MSISDN/Number')[['Social Media DL (Bytes)', 'Social Media UL (Bytes)',
  'Google DL (Bytes)', 'Google UL (Bytes)',
  'Email DL (Bytes)', 'Email UL (Bytes)',
  'Youtube DL (Bytes)', 'Youtube UL (Bytes)',
  'Netflix DL (Bytes)', 'Netflix UL (Bytes)',
  'Gaming DL (Bytes)', 'Gaming UL (Bytes)',
  'Other DL (Bytes)', 'Other UL (Bytes)']].sum()
  user_info = pd.concat([num_sessions, session_duration, total_DL, total_UL, total_data_vol], axis=1)
  user_info = user_info.rename(columns={
  'Bearer Id': 'Number of Sessions',
  'Dur. (ms)': 'Total Session Duration',
  'Total DL (Bytes)': 'Total Download Data',
  'Total UL (Bytes)': 'Total Upload Data',
  'Social Media DL (Bytes)': 'Social Media Download Data',
  'Social Media UL (Bytes)': 'Social Media Upload Data',
  'Google DL (Bytes)': 'Google Download Data',
  'Google UL (Bytes)': 'Google Upload Data',
  'Email DL (Bytes)': 'Email Download Data',
  'Email UL (Bytes)': 'Email Upload Data',
  'Youtube DL (Bytes)': 'Youtube Download Data',
  'Youtube UL (Bytes)': 'Youtube Upload Data',
  'Netflix DL (Bytes)': 'Netflix Download Data',
  'Netflix UL (Bytes)': 'Netflix Upload Data',
  'Gaming DL (Bytes)': 'Gaming Download Data',
  'Gaming UL (Bytes)': 'Gaming Upload Data',
  'Other DL (Bytes)': 'Other Download Data',
  'Other UL (Bytes)': 'Other Upload Data'
  })
  return user_info
 
filtered_df = clean_df[clean_df['MSISDN/Number'] != 'Unknown']
user_info = process_user_info(filtered_df)
pd.DataFrame(user_info)
import sys, os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
from utils import DatabaseEngine
db_engine = DatabaseEngine()
engine = db_engine.create()
clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
data_df = pd.read_csv('../data/user_info.csv')
df = pd.concat([clean_df, data_df], axis=1)
df.info()
df.describe()
exclude_columns = ['Bearer Id', 'IMSI', 'MSISDN/Number', 'IMEI']
column_list = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col not in exclude_columns]
 
for column in column_list:
  print(f"--- {column} ---")
  print(f"Range: {df[column].max()} - {df[column].min()}")
  print(f"Variance: {df[column].var()}")
  print(f"Standard Deviation: {df[column].std()}")
  print(f"IQR: {df[column].quantile(0.75) - df[column].quantile(0.25)}")
plt.hist(df['Total Download Data'], bins=20)
plt.xlabel('Total Download Data')
plt.ylabel('Frequency')
plt.title('Total Download Data')
plt.show()
plt.hist(df['Total Upload Data'], bins=20)
plt.xlabel('Total Uploaded Data')
plt.ylabel('Frequency')
plt.title('Total Uploaded Data')
plt.show()
labels = ['DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)']
sizes = [df[label].mean() for label in labels]
 
plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Traffic Distribution')
plt.axis('equal')
plt.show()
q1 = df['Avg RTT DL (ms)'].quantile(0.25)
q3 = df['Avg RTT DL (ms)'].quantile(0.75)
iqr = q3 - q1
 
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
 
df['Avg RTT DL (ms)'] = df['Avg RTT DL (ms)'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))
 
sns.boxplot(y=df['Avg RTT DL (ms)'])
plt.ylabel('Avg RTT DL (ms)')
plt.title('Average Round Trip Time for Download')
plt.show()
 
q1 = df['Avg RTT UL (ms)'].quantile(0.25)
q3 = df['Avg RTT UL (ms)'].quantile(0.75)
iqr = q3 - q1
 
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
 
df['Avg RTT UL (ms)'] = df['Avg RTT UL (ms)'].apply(lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x))
 
sns.boxplot(y=df['Avg RTT UL (ms)'])
plt.ylabel('Avg RTT UL (ms)')
plt.title('Average Round Trip Time for Upload')
plt.show()
activity_duration = df['Activity Duration DL (ms)']
avg_bearer_throughput = df['Avg Bearer TP DL (kbps)']
 
plt.figure(figsize=(10, 6))
plt.scatter(activity_duration, avg_bearer_throughput, color='blue', alpha=0.5)
plt.xlabel('Activity Duration DL (ms)')
plt.ylabel('Avg Bearer TP DL (kbps)')
plt.title('Average Bearer Throughput for Downlink vs. Activity Duration')
plt.tight_layout()
plt.show()
 
user_info = {
  'Total DL (Bytes)': 'Total Download Data',
  'Social Media DL (Bytes)': 'Social Media Download Data',
  'Google DL (Bytes)': 'Google Download Data',
  'Email DL (Bytes)': 'Email Download Data',
  'Youtube DL (Bytes)': 'Youtube Download Data',
  'Netflix DL (Bytes)': 'Netflix Download Data',
  'Gaming DL (Bytes)': 'Gaming Download Data',
  'Other DL (Bytes)': 'Other Download Data',
  'Total UL (Bytes)': 'Total Upload Data',
  'Social Media UL (Bytes)': 'Social Media Upload Data',
  'Google UL (Bytes)': 'Google Upload Data',
  'Email UL (Bytes)': 'Email Upload Data',
  'Youtube UL (Bytes)': 'Youtube Upload Data',
  'Netflix UL (Bytes)': 'Netflix Upload Data',
  'Gaming UL (Bytes)': 'Gaming Upload Data',
  'Other UL (Bytes)': 'Other Upload Data'
}
 
download_columns = [
  'Total DL (Bytes)',
  'Social Media DL (Bytes)',
  'Google DL (Bytes)',
  'Email DL (Bytes)',
  'Youtube DL (Bytes)',
  'Netflix DL (Bytes)',
  'Gaming DL (Bytes)',
  'Other DL (Bytes)'
]
 
upload_columns = [
  'Total UL (Bytes)',
  'Social Media UL (Bytes)',
  'Google UL (Bytes)',
  'Email UL (Bytes)',
  'Youtube UL (Bytes)',
  'Netflix UL (Bytes)',
  'Gaming UL (Bytes)',
  'Other UL (Bytes)'
]
 
download_data = df[download_columns].sum()
upload_data = df[upload_columns].sum()
 
plt.figure(figsize=(10, 6))
plt.bar(download_data.index.map(user_info), download_data.values, color='orange')
plt.title('Total Download Data for Different Categories')
plt.xlabel('Categories')
plt.ylabel('Download Data')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
 
plt.figure(figsize=(10, 6))
plt.bar(upload_data.index.map(user_info), upload_data.values, color='green')
plt.title('Total Upload Data for Different Categories')
plt.xlabel('Categories')
plt.ylabel('Upload Data')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
df['Decile Class'] = pd.qcut(df['Total Session Duration'], 10, labels=False)
 
df_top_five = df[df['Decile Class'] >= 5].copy()
 
df_top_five.loc[:, 'Total Data'] = df_top_five['Total Download Data'] + df_top_five['Total Upload Data']
total_data_per_decile = df_top_five.groupby('Decile Class')['Total Data'].sum()
pd.DataFrame(total_data_per_decile)
data_for_correlation = df[['Total DL (Bytes)', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',
  'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)', 'Total UL (Bytes)', 'Social Media UL (Bytes)',   'Google UL (Bytes)', 'Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']]
 
correlation_matrix = data_for_correlation.corr()
 
col_info = {
  'Total DL (Bytes)': 'Total Download Data',
  'Social Media DL (Bytes)': 'Social Media Download Data',
  'Google DL (Bytes)': 'Google Download Data',
  'Email DL (Bytes)': 'Email Download Data',
  'Youtube DL (Bytes)': 'Youtube Download Data',
  'Netflix DL (Bytes)': 'Netflix Download Data',
  'Gaming DL (Bytes)': 'Gaming Download Data',
  'Other DL (Bytes)': 'Other Download Data',
  'Total UL (Bytes)': 'Total Upload Data',
  'Social Media UL (Bytes)': 'Social Media Upload Data',
  'Google UL (Bytes)': 'Google Upload Data',
  'Email UL (Bytes)': 'Email Upload Data',
  'Youtube UL (Bytes)': 'Youtube Upload Data',
  'Netflix UL (Bytes)': 'Netflix Upload Data',
  'Gaming UL (Bytes)': 'Gaming Upload Data',
  'Other UL (Bytes)': 'Other Upload Data'
}
 
correlation_matrix = correlation_matrix.rename(columns=col_info, index=col_info)
correlation_matrix
plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', linewidths=.05)
plt.title('Correlation Matrix')
plt.show()
import sys, os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
from utils import DatabaseEngine
db_engine = DatabaseEngine()
engine = db_engine.create()
clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
data_df = pd.read_csv('../data/user_info.csv')
data_df.columns = ['MSISDN/Number_data' if col=='MSISDN/Number' else col for col in data_df.columns]
df = pd.concat([clean_df, data_df], axis=1)
df['Total Traffic'] = df['Total Download Data'] + df['Total Upload Data']
engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']
 
for metric in engagement_metrics:
  top_10_customers = df.nlargest(10, metric)[['MSISDN/Number', metric]]
  print(f"Top 10 customers for {metric}:")
  print(top_10_customers.to_string(index=False))
  print("\n")
def replace_with_mean(df, column):
  mean = df[column].mean()
  std = df[column].std()
  is_outlier = (df[column] - mean).abs() > 3*std
  df.loc[is_outlier, column] = mean
  return df
data_df['Total Traffic'] = data_df['Total Download Data'] + data_df['Total Upload Data']
 
engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']
for metric in engagement_metrics:
  data_df = replace_with_mean(data_df, metric)
 
scaler = StandardScaler()
normalized_data = scaler.fit_transform(data_df[engagement_metrics])
 
kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)
clusters = kmeans.fit_predict(normalized_data)
data_df['Engagement Cluster'] = clusters
data_df.head()
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
 
scatter = ax.scatter(data_df['Number of Sessions'],   data_df['Total Session Duration'],   data_df['Total Traffic'],   c=clusters,   cmap='viridis')
 
ax.set_title('Customer Engagement Clusters')
ax.set_xlabel('Number of Sessions')
ax.set_ylabel('Total Session Duration')
ax.set_zlabel('Total Traffic')
 
cbar = plt.colorbar(scatter)
cbar.set_label('Cluster')
 
plt.show()
cluster_metrics = data_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])
pd.DataFrame(cluster_metrics)
cluster_metrics['Total Traffic'].plot(kind='bar', subplots=True, layout=(1,4), sharey=True, figsize=(20,5), title='Total Traffic Metrics by Cluster')
plt.yscale('log')
plt.show()
applications = {
  'Google': ['Google Download Data', 'Google Upload Data'],
  'Email': ['Email Download Data', 'Email Upload Data'],
  'Youtube': ['Youtube Download Data', 'Youtube Upload Data'],
  'Netflix': ['Netflix Download Data', 'Netflix Upload Data'],
 
}
 
for app, (dl, ul) in applications.items():
  data_df[app + ' Total Data'] = data_df[dl] + data_df[ul]
 
user_total_traffic = data_df.groupby('MSISDN/Number_data')[[app + ' Total Data' for app in applications]].sum()
 
for app in applications:
  top_users = user_total_traffic[app + ' Total Data'].nlargest(10)
  user_total_traffic.index = user_total_traffic.index.astype(int)
  print(f"Top 10 users for {app}:")
  print(pd.DataFrame(top_users).to_string())
  print("\n")
total_traffic_per_app = data_df[[app + ' Total Data' for app in applications]].sum()
top_3_apps = total_traffic_per_app.nlargest(3)
 
plt.figure(figsize=(10, 6))
plt.barh(top_3_apps.index, top_3_apps.values, color='skyblue')
plt.xlabel('Total Traffic')
plt.ylabel('Application')
plt.title('Top 3 Most Used Applications')
plt.gca().invert_yaxis()
plt.show()
engagement_metrics = data_df[['Number of Sessions', 'Total Session Duration', 'Total Traffic']]
scaler = StandardScaler()
engagement_metrics_scaled = scaler.fit_transform(engagement_metrics)
 
sse = []
for k in range(1, 11):
  kmeans = KMeans(n_clusters=k, n_init=10, random_state=1)
  kmeans.fit(engagement_metrics_scaled)
  sse.append(kmeans.inertia_)
 
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), sse, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
plt.title('Elbow Method to Determine Optimal k')
plt.show()
 
kmeans = KMeans(n_clusters=2, n_init=10, random_state=1)
clusters = kmeans.fit_predict(engagement_metrics_scaled)
data_df['Elbow_Cluster'] = clusters
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
 
for i in range(clusters.max()+1):
  cluster_data = data_df[data_df['Elbow_Cluster'] == i]
  ax.scatter(cluster_data['Number of Sessions'],   cluster_data['Total Session Duration'],   cluster_data['Total Traffic'],   label=f'Cluster {i}')
 
ax.set_xlabel('Number of Sessions')
ax.set_ylabel('Total Session Duration')
ax.set_zlabel('Total Traffic')
plt.legend()
plt.title('User Engagement Clusters')
plt.show()
import pandas as pd
import sys, os
import matplotlib.pyplot as plt
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
from utils import DatabaseEngine
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from mpl_toolkits.mplot3d import Axes3D
db_engine = DatabaseEngine()
engine = db_engine.create()
clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
def process_user_info(clean_df):
  avg_tcp_retrans = clean_df.groupby('MSISDN/Number')[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].mean()
  avg_rtt = clean_df.groupby('MSISDN/Number')[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean()
  handset_type = clean_df.groupby('MSISDN/Number')['Handset Type'].agg(pd.Series.mode)
  avg_throughput = clean_df.groupby('MSISDN/Number')[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean()
  user_info = pd.concat([avg_tcp_retrans, avg_rtt, handset_type, avg_throughput], axis=1)
  return user_info
 
filtered_df = clean_df[clean_df['MSISDN/Number'] != 'Unknown']
user_info = process_user_info(filtered_df)
pd.DataFrame(user_info)
top10_tcp = user_info[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].sum(axis=1).nlargest(10)
bottom10_tcp = user_info[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].sum(axis=1).nsmallest(10)
most_frequent_tcp = user_info[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].sum(axis=1).value_counts().nlargest(10)
pd.DataFrame(top10_tcp, columns=['Total TCP Retransmission Volume (Bytes)'])
pd.DataFrame(bottom10_tcp, columns=['Total TCP Retransmission Volume (Bytes)'])
pd.DataFrame(most_frequent_tcp).rename_axis('Total TCP Retransmission Volume (Bytes)').rename(columns={0: 'Count'})
top10_rtt = user_info[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean(axis=1).nlargest(10)
bottom10_rtt = user_info[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean(axis=1).nsmallest(10)
most_frequent_rtt = user_info[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].mean(axis=1).value_counts().nlargest(10)
pd.DataFrame(top10_rtt, columns=['Total Average RTT (ms)'])
pd.DataFrame(bottom10_rtt, columns=['Total Average RTT (ms)'])
pd.DataFrame(most_frequent_rtt).rename_axis('Total Average RTT (ms)').rename(columns={0: 'Count'})
top10_throughput = user_info[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean(axis=1).nlargest(10)
bottom10_throughput = user_info[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean(axis=1).nsmallest(10)
most_frequent_throughput = user_info[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean(axis=1).value_counts().nlargest(10)
pd.DataFrame(top10_throughput, columns=['Total Average Throughput (kbps)'])
pd.DataFrame(bottom10_throughput, columns=['Total Average Throughput (kbps)'])
pd.DataFrame(most_frequent_throughput).rename_axis('Total Average Throughput (kbps)').rename(columns={0: 'Count'})
filtered_df = clean_df[clean_df['Handset Type'] != 'undefined']
 
avg_throughput_per_handset = filtered_df.groupby('Handset Type')[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].mean().mean(axis=1)
 
pd.DataFrame(avg_throughput_per_handset, columns = ['Average Throughput (kbps)'])
filtered_df = clean_df[clean_df['Handset Type'] != 'undefined']
avg_tcp_retrans_per_handset = filtered_df.groupby('Handset Type')[['TCP UL Retrans. Vol (Bytes)', 'TCP DL Retrans. Vol (Bytes)']].mean().mean(axis=1)
pd.DataFrame(avg_tcp_retrans_per_handset, columns = ['Average TCP Retransmission Volume (Bytes)'])
def replace_with_mean(df, column):
  mean = df[column].mean()
  std = df[column].std()
  is_outlier = (df[column] - mean).abs() > 3*std
  df.loc[is_outlier, column] = mean
  return df
clean_df['Total TCP'] = clean_df['TCP UL Retrans. Vol (Bytes)'] + clean_df['TCP DL Retrans. Vol (Bytes)']
clean_df['Total RTT'] = clean_df['Avg RTT DL (ms)'] + clean_df['Avg RTT UL (ms)']
clean_df['Total Throughput'] = clean_df['Avg Bearer TP DL (kbps)'] + clean_df['Avg Bearer TP UL (kbps)']
 
engagement_metrics = ['Total TCP', 'Total RTT', 'Total Throughput']
for metric in engagement_metrics:
  clean_df = replace_with_mean(clean_df, metric)
 
scaler = StandardScaler()
normalized_data = scaler.fit_transform(clean_df[engagement_metrics])
 
kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)
clusters = kmeans.fit_predict(normalized_data)
clean_df['Engagement Cluster'] = clusters
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
 
scatter = ax.scatter(clean_df['Total TCP'],   clean_df['Total RTT'],   clean_df['Total Throughput'],   c=clusters,   cmap='viridis')
 
ax.set_title('Customer Experience Clusters')
ax.set_xlabel('Total TCP')
ax.set_ylabel('Total RTT')
ax.set_zlabel('Total Throughput')
 
cbar = plt.colorbar(scatter)
cbar.set_label('Cluster')
 
plt.show()
import pandas as pd
import sys, os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from scipy.stats.mstats import winsorize
 
rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
from utils import DatabaseEngine
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from mpl_toolkits.mplot3d import Axes3D
from scipy.spatial import distance
import warnings
 
warnings.filterwarnings('ignore', message="Warning: 'partition' will ignore the 'mask' of the MaskedArray.")
db_engine = DatabaseEngine()
engine = db_engine.create()
clean_df = pd.read_sql_table("clean_xdr_data", con=engine)
data_df = pd.read_csv('../data/user_info.csv')
df = pd.concat([clean_df, data_df], axis=1)
df['Total Traffic'] = df['Total Download Data'] + df['Total Upload Data']
engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']
def replace_with_mean(df, column):
  mean = df[column].mean()
  std = df[column].std()
  is_outlier = (df[column] - mean).abs() > 3*std
  df.loc[is_outlier, column] = mean
  return df
data_df['Total Traffic'] = data_df['Total Download Data'] + data_df['Total Upload Data']
 
engagement_metrics = ['Number of Sessions', 'Total Session Duration', 'Total Traffic']
for metric in engagement_metrics:
  data_df = replace_with_mean(data_df, metric)
 
scaler = StandardScaler()
normalized_data = scaler.fit_transform(data_df[engagement_metrics])
 
kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)
clusters = kmeans.fit_predict(normalized_data)
 
data_df['Engagement Cluster'] = clusters
avg_engagement = data_df.groupby('Engagement Cluster')[engagement_metrics].mean()
 
less_engaged_cluster = avg_engagement.mean(axis=1).idxmin()
 
centroid = kmeans.cluster_centers_[less_engaged_cluster]
 
data_df['Engagement Score'] = euclidean_distances(normalized_data, centroid.reshape(1, -1)).flatten()
data_df['Engagement Score'].describe()
clean_df['Total TCP'] = clean_df['TCP UL Retrans. Vol (Bytes)'] + clean_df['TCP DL Retrans. Vol (Bytes)']
clean_df['Total RTT'] = clean_df['Avg RTT DL (ms)'] + clean_df['Avg RTT UL (ms)']
clean_df['Total Throughput'] = clean_df['Avg Bearer TP DL (kbps)'] + clean_df['Avg Bearer TP UL (kbps)']
 
exp_engagement_metrics = ['Total TCP', 'Total RTT', 'Total Throughput']
for metric in exp_engagement_metrics:
  clean_df = replace_with_mean(clean_df, metric)
 
scaler = StandardScaler()
normalized_data = scaler.fit_transform(clean_df[exp_engagement_metrics])
 
kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)
exp_clusters = kmeans.fit_predict(normalized_data)
 
clean_df['Experiance Cluster'] = exp_clusters
exp_avg_engagement = clean_df.groupby('Experiance Cluster')[exp_engagement_metrics].mean()
 
exp_less_engaged_cluster = exp_avg_engagement.mean(axis=1).idxmin()
 
exp_centroid = kmeans.cluster_centers_[exp_less_engaged_cluster]
 
clean_df['Experiance Score'] = euclidean_distances(normalized_data, centroid.reshape(1, -1)).flatten()
clean_df['Experiance Score'].describe()
data_df['MSISDN/Number'] = data_df['MSISDN/Number'].astype(str)
clean_df['MSISDN/Number'] = clean_df['MSISDN/Number'].astype(str)
 
merged_df = pd.merge(data_df, clean_df, on='MSISDN/Number')
 
merged_df['Satisfaction Score'] = (merged_df['Experiance Score'] + merged_df['Engagement Score']) / 2
 
top_10_satisfied_users = merged_df.sort_values('Satisfaction Score', ascending=False).head(10)
merged_df['Satisfaction Score'].describe()
top_10_satisfied_customers = merged_df.sort_values('Satisfaction Score', ascending=False).head(10)
 
top_10_satisfied_customers = top_10_satisfied_customers[['MSISDN/Number', 'Satisfaction Score']]
 
print(top_10_satisfied_customers.to_string(index=False))
percentiles = merged_df['Satisfaction Score'].describe(percentiles=[0.25, 0.75])
 
bins = [percentiles['min'], percentiles['25%'], percentiles['75%'], percentiles['max']]
 
labels = ['low satisfaction', 'moderate satisfaction', 'high satisfaction']
 
merged_df['Satisfaction Category'] = pd.cut(merged_df['Satisfaction Score'], bins=bins, labels=labels, include_lowest=True)
 
category_counts = merged_df['Satisfaction Category'].value_counts()
print(category_counts)
category_percentages = category_counts / category_counts.sum() * 100
 
plt.figure(figsize=(10, 6))
plt.pie(category_percentages, labels=category_percentages.index, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Satisfaction Score Categories')
plt.show()
X = merged_df[['Engagement Score', 'Experiance Score']]
y = merged_df['Satisfaction Score']
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
 
model = LinearRegression()
 
model.fit(X_train, y_train)
 
y_pred = model.predict(X_test)
new_data = {'Engagement Score': [1.5], 'Experiance Score': [1.2]}
new_df = pd.DataFrame(new_data)
 
new_pred = model.predict(new_df)
 
print(f'The predicted satisfaction score for the new customer is: {new_pred[0]}')
X = merged_df[['Engagement Score', 'Experiance Score']]
kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)
kmeans.fit(X)
merged_df['Cluster'] = kmeans.labels_
average_scores = merged_df.groupby('Cluster')[['Satisfaction Score', 'Experiance Score']].mean()
 
print(average_scores)
merged_df.to_sql('user_satisfaction', engine, if_exists='replace', index=False)
query = "SELECT * FROM user_satisfaction LIMIT 5;"
query_df = pd.read_sql_query(query, engine)
query_df
import pandas as pd
import numpy as np
from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler
 
class Cleaner:
  def __init__(self):
  pass
  def drop_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
  """
  drop columns
  """
  return df.drop(columns=columns)
  def drop_nan(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  drop rows with nan values
  """
  return df.dropna()
  def drop_nan_column(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
  """
  drop rows with nan values
  """
  return df.dropna(subset=[col])   def drop_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  drop duplicate rows
  """
  return df.drop_duplicates()
  def convert_to_datetime(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
  """
  convert column to datetime
  """
  df[col] = df[col].apply(pd.to_datetime)
  return df
  def convert_to_string(self, df: pd.DataFrame, col = list) -> pd.DataFrame:
  """
  convert columns to string
  """
  df[col] = df[col].astype(str)
  return df
  def remove_whitespace_column(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  remove whitespace from columns
  """
  return df.columns.str.replace(' ', '_').str.lower()
  def percent_missing(self, df: pd.DataFrame) -> float:
  """
  calculate the percentage of missing values from dataframe
  """
  totalCells = np.product(df.shape)
  missingCount = df.isnull().sum()
  totalMising = missingCount.sum()
  return round(totalMising / totalCells * 100, 2)
  def percent_missing_column(self, df: pd.DataFrame, col:str) -> float:
  """
  calculate the percentage of missing values for the specified column
  """
  try:
  col_len = len(df[col])
  except KeyError:
  print(f"{col} not found")
  missing_count = df[col].isnull().sum()
  return round(missing_count / col_len * 100, 2)
  def get_numerical_columns(self, df: pd.DataFrame) -> list:
  """
  get numerical columns
  """
  return df.select_dtypes(include=['number']).columns.to_list()
  def get_categorical_columns(self, df: pd.DataFrame) -> list:   """
  get categorical columns
  """
  return  df.select_dtypes(include=['object','datetime64[ns]']).columns.to_list()
  def fill_missing_values_categorical(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
  """
  fill missing values with specified method
  """
  categorical_columns = df.select_dtypes(include=['object','datetime64[ns]']).columns
  if method == "ffill":
  for col in categorical_columns:
  df[col] = df[col].fillna(method='ffill')
  return df
  elif method == "bfill":
  for col in categorical_columns:
  df[col] = df[col].fillna(method='bfill')
  return df
  elif method == "mode":
  for col in categorical_columns:
  df[col] = df[col].fillna(df[col].mode()[0])
  return df
  else:
  print("Method unknown")
  return df
  def fill_missing_values_numeric(self, df: pd.DataFrame, method: str,columns: list =None) -> pd.DataFrame:
  """
  fill missing values with specified method
  """
  if(columns==None):
  numeric_columns = self.get_numerical_columns(df)
  else:
  numeric_columns=columns
  if method == "mean":
  for col in numeric_columns:
  df[col].fillna(df[col].mean(), inplace=True)
  elif method == "median":
  for col in numeric_columns:
  df[col].fillna(df[col].median(), inplace=True)
  else:
  print("Method unknown")
  return df
  def normalizer(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  normalize numerical columns
  """
  norm = Normalizer()
  return pd.DataFrame(norm.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
  def min_max_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  scale numerical columns
  """
  minmax_scaler = MinMaxScaler()
  return pd.DataFrame(minmax_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
  def standard_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  scale numerical columns
  """
  standard_scaler = StandardScaler()
  return pd.DataFrame(standard_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
  def handle_outliers(self, df:pd.DataFrame, col:str, method:str ='IQR') -> pd.DataFrame:
  """
  Handle Outliers of a specified column using Turkey's IQR method
  """
  df = df.copy()
  q1 = df[col].quantile(0.25)
  q3 = df[col].quantile(0.75)
  lower_bound = q1 - ((1.5) * (q3 - q1))
  upper_bound = q3 + ((1.5) * (q3 - q1))
  if method == 'mode':
  df[col] = np.where(df[col] < lower_bound, df[col].mode()[0], df[col])
  df[col] = np.where(df[col] > upper_bound, df[col].mode()[0], df[col])
  elif method == 'median':
  df[col] = np.where(df[col] < lower_bound, df[col].median, df[col])
  df[col] = np.where(df[col] > upper_bound, df[col].median, df[col])
  else:
  df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
  df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
  return df
  def find_agg(self,df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
  new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
  sort_values(by=col_name, ascending=order)[:top]
  return new_df
  def convert_bytes_to_megabytes(self,df, bytes_data):
  """
  This function takes the dataframe and the column which has the bytes values
  returns the megabytesof that value
  Args:
  -----
  df: dataframe
  bytes_data: column with bytes values
  Returns:
  --------
  A series
  """
  megabyte = 1*10e+5
  df[bytes_data] = df[bytes_data] / megabyte
  return df[bytes_data]
%reload_ext autoreload
%autoreload 2
import pandas as pd
import sys
sys.path.append('../script')
from data_extraction import pgdatabase
from utils import Cleaner
clean = Cleaner()
from plots import plots
plt = plots()
db = pgdatabase()
conn = db.connection()
 
data = db.get_dataframe_sql(conn, 'xdr_data')
data.shape
data.head()
data.info()
data.describe()
data.isnull().sum()
clean.missing_values_table(data)
 
data['TCP UL Retrans. Vol (Bytes)'].describe()
data['TCP DL Retrans. Vol (Bytes)'].describe()
data['HTTP UL (Bytes)'].describe()
data['HTTP DL (Bytes)'].describe()
percent = clean.percent_missing(data)
print(f'The data has {percent}% missing values')
col = ['Start ms', 'End ms']
data1 = clean.convert_to_datetime(data, col)
data.skew()
 
col = ['Nb of sec with 37500B < Vol UL',
  'Nb of sec with 6250B < Vol UL < 37500B',
  'Nb of sec with 1250B < Vol UL < 6250B',
  'Nb of sec with 125000B < Vol DL',
  'Nb of sec with 31250B < Vol DL < 125000B',
  'Nb of sec with 6250B < Vol DL < 31250B']
 
data2 = clean.impute_zero(data1, col)
 
col1 = ['TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)',
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'HTTP DL (Bytes)',
  'HTTP UL (Bytes)']
 
data2 = clean.fill_missing_values_numeric(data1, 'mean', col1)
 
data2['Handset Type'] = data2['Handset Type'].fillna('unknown')
data2['Handset Manufacturer'] = data2['Handset Manufacturer'].fillna('unknown')
categorical_columns = data.select_dtypes(include=['object','datetime64[ns]']).columns
 
for col in categorical_columns:
  data2[col] = data2[col].fillna(data2[col].mode()[0])
cleand_data = clean.fill_missing_values_numeric(data2, 'median')
clean.missing_values_table(cleand_data)
plt.plot_box(cleand_data, 'Activity Duration UL (ms)', 'Active Duration UL Outliers')
plt.plot_box(cleand_data, 'Activity Duration DL (ms)', 'Active Duration DL Outliers')
numeric_columns = clean.get_numerical_columns(cleand_data)
data5 = data[numeric_columns]
indices = clean.detect_outliers(data5,6)
print(len(indices))
telecom_data = clean.handle_outliers(data5, indices, 'mean')
%reload_ext autoreload
%autoreload 2
from urllib.parse import quote_plus
from sqlalchemy import create_engine
 
password = 'kerod53@'
 
encoded_password = quote_plus(password)
 
db_string = f'postgresql://postgres:{encoded_password}@localhost:5432/Telecom'
 
engine = create_engine(db_string)
table_name = 'telecom_data'
 
cleand_data.to_sql(table_name, engine, index=False, if_exists='replace')
import pandas as pd
import numpy as np
from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler
 
class Cleaner:
  def __init__(self):
  pass
  def drop_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
  """
  drop columns
  """
  return df.drop(columns=columns)
  def drop_nan(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  drop rows with nan values
  """
  return df.dropna()
  def drop_nan_column(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
  """
  drop rows with nan values
  """
  return df.dropna(subset=[col])   def drop_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  drop duplicate rows
  """
  return df.drop_duplicates()
  def convert_to_datetime(self, df: pd.DataFrame, col: list) -> pd.DataFrame:
  """
  convert column to datetime
  """
  df[col] = df[col].apply(pd.to_datetime)
  return df
  def convert_to_string(self, df: pd.DataFrame, col = list) -> pd.DataFrame:
  """
  convert columns to string
  """
  df[col] = df[col].astype(str)
  return df
  def remove_whitespace_column(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  remove whitespace from columns
  """
  return df.columns.str.replace(' ', '_').str.lower()
  def percent_missing(self, df: pd.DataFrame) -> float:
  """
  calculate the percentage of missing values from dataframe
  """
  totalCells = np.product(df.shape)
  missingCount = df.isnull().sum()
  totalMising = missingCount.sum()
  return round(totalMising / totalCells * 100, 2)
  def get_numerical_columns(self, df: pd.DataFrame) -> list:
  """
  get numerical columns
  """
  return df.select_dtypes(include=['float64']).columns.to_list()
  def get_categorical_columns(self, df: pd.DataFrame) -> list:   """
  get categorical columns
  """
  return  df.select_dtypes(include=['object','datetime64[ns]']).columns.to_list()
  def impute_zero(self, df: pd.DataFrame, column: list) -> pd.DataFrame:
  """
  imputes 0 inplace of NaN for a given columon(s)
  """
  df[column] = df[column].fillna(0)
  return df   def fill_missing_values_categorical(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
  """
  fill missing values with specified method
  """
  categorical_columns = df.select_dtypes(include=['object','datetime64[ns]']).columns
  if method == "ffill":
  for col in categorical_columns:
  df[col] = df[col].fillna(method='ffill')
  return df
  elif method == "bfill":
  for col in categorical_columns:
  df[col] = df[col].fillna(method='bfill')
  return df
  elif method == "mode":
  for col in categorical_columns:
  df[col] = df[col].fillna(df[col].mode()[0])
  return df
  else:
  print("Method unknown")
  return df
  def fill_missing_values_numeric(self, df: pd.DataFrame, method: str,columns: list = None) -> pd.DataFrame:
  """
  fill missing values with specified method
  """
  if(columns==None):
  numeric_columns = df.select_dtypes(include=['float64','int64']).columns
  else:
  numeric_columns=columns
  if method == "mean":
  for col in numeric_columns:
  df[col].fillna(df[col].mean(), inplace=True)
  elif method == "median":
  for col in numeric_columns:
  df[col].fillna(df[col].median(), inplace=True)
  else:
  print("Method unknown")
  return df
  def normalizer(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  normalize numerical columns
  """
  norm = Normalizer()
  return pd.DataFrame(norm.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
  def min_max_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  scale numerical columns
  """
  minmax_scaler = MinMaxScaler()
  return pd.DataFrame(minmax_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
  def standard_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
  """
  scale numerical columns
  """
  standard_scaler = StandardScaler()
  return pd.DataFrame(standard_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
  def detect_outliers(self, df:pd.DataFrame, threshold: int) -> list:
  """
  detect the indices of outliers using Z-method   """
  z_scores = df.apply(lambda x: np.abs((x - x.mean()) / x.std()))
  tr = threshold
  outliers = np.where(z_scores > tr)
  outlier_indices = [(df.index[i], df.columns[j]) for i, j in zip(*outliers)]
  return outlier_indices
  def handle_outliers(self, df:pd.DataFrame, indices:list, method:str) -> pd.DataFrame:
  """
  Handle Outliers of a specified column using the Z method
  """
  if method == 'mean':
  for idx, col_name in indices:
  column_mean = df[col_name].mean()
  df.iloc[idx, df.columns.get_loc(col_name)] = column_mean
  elif method == 'mode':
  for idx, col_name in indices:
  column_mode = df[col_name].mode()
  df.iloc[idx, df.columns.get_loc(col_name)] = column_mode
  elif method == 'median':
  for idx, col_name in indices:
  column_median = df[col_name].median()
  df.loc[idx, col_name] = column_median
  else:
  print("Method unknown")
  return df
  def find_agg(self,df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
  new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
  sort_values(by=col_name, ascending=order)[:top]
  return new_df
  def convert_bytes_to_megabytes(self,df, bytes_data):
  """
  This function takes the dataframe and the column which has the bytes values
  returns the megabytesof that value
  Args:
  -----
  df: dataframe
  bytes_data: column with bytes values
  Returns:
  --------
  A series
  """
  megabyte = 1*10e+5
  df[bytes_data] = df[bytes_data] / megabyte
  return df[bytes_data]
  def missing_values_table(self,df):
  mis_val = df.isnull().sum()
  mis_val_percent = 100 * df.isnull().sum() / len(df)
  mis_val_dtype = df.dtypes
  mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)
  mis_val_table_ren_columns = mis_val_table.rename(
  columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})
  mis_val_table_ren_columns = mis_val_table_ren_columns[
  mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
  '% of Total Values', ascending=False).round(1)
  print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"   "There are " + str(mis_val_table_ren_columns.shape[0]) +
  " columns that have missing values.")
  return mis_val_table_ren_columns
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
sys.path.append('../script')
from utils import Cleaner
from plots import *
from data_extraction import *
 
clean = Cleaner()
%reload_ext autoreload
%autoreload 2
conn = connection()
data = table_to_sql(conn, "telecom_data")
data.head()
data.info()
top_10 = data["Handset Type"].value_counts()[:10]
plt.figure(figsize= (10,5))
top_10.plot(kind="bar",title="top 10 handset types")
plt.show()
top_3 = data["Handset Manufacturer"].value_counts()[:3]
plt.figure(figsize=(10,5))
top_3.plot(kind="bar",title="top 3 handset manufacturers")
plt.show()
handsets_per_manufacturers=data[data["Handset Manufacturer"].str.contains("Apple|Samsung|Huawei") ][["Handset Manufacturer","Handset Type"]]
plt.figure(figsize=(10,5))
top_five_apple_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Apple')["Handset Type"].value_counts()[:5]
handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Apple')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Apple")
plt.show()
plt.figure(figsize=(10,5))
top_five_samsung_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Samsung')["Handset Type"].value_counts()[:5]
handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Samsung')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Samsung")
plt.show()
plt.figure(figsize=(10,5))
top_five_huawei_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Huawei')["Handset Type"].value_counts()[:5]
handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Huawei')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Huawei")
plt.show()
session=data.groupby(["MSISDN/Number"]).agg({"Bearer Id":"count"})
session = session.rename(columns={"Bearer Id":"XDR_sessions"})
session = session.sort_values(by=["XDR_sessions"],ascending=False)
 
session.head(10)
duration = data.groupby(["MSISDN/Number"]).agg({"Dur. (ms)":"sum"})
duration.sort_values(by=["Dur. (ms)"],ascending=False,inplace=True)
duration=duration.rename(columns={"Dur. (ms)":"total_duration(ms) "})
duration.head(10)
total_data = data.groupby(["MSISDN/Number"]).agg({"Total UL (Bytes)":"sum","Total DL (Bytes)":"sum"})
total_data["total_data(bytes)"]=total_data["Total UL (Bytes)"]+total_data["Total DL (Bytes)"]
total_data.sort_values(by=["total_data(bytes)"],ascending=False,inplace=True)
total_data.head(10)
 
data['social_media'] = data['Social Media DL (Bytes)'] + data['Social Media UL (Bytes)']
data['google'] = data['Google DL (Bytes)'] + data['Google UL (Bytes)']
data['email'] = data['Email DL (Bytes)'] + data['Email UL (Bytes)']
data['youtube'] = data['Youtube DL (Bytes)'] + data['Youtube UL (Bytes)']
data['netflix'] = data['Netflix DL (Bytes)'] + data['Netflix UL (Bytes)']
data['gaming'] = data['Gaming DL (Bytes)'] + data['Gaming UL (Bytes)']
data['other'] = data['Other DL (Bytes)'] + data['Other UL (Bytes)']
data['total_data'] = data['Total UL (Bytes)'] + data['Total DL (Bytes)']
app_total_data = data[['MSISDN/Number','social_media','google', 'email', 'youtube','netflix', 'gaming','other']].copy()
app_total_data.groupby('MSISDN/Number').sum().sample(10)
 
variables = data[['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'total_data', 'Total DL (Bytes)', 'social_media', 'google', 'email', 'youtube', 'netflix', 'gaming', 'other']].copy()
variables.groupby('MSISDN/Number').sum().sample(10)
 
non_graphical_univariate = variables.drop('MSISDN/Number', axis=1).describe()
non_graphical_univariate.transpose()
plot_hist(variables,'email','cornflowerblue')
plot_hist(variables,'gaming','cornflowerblue')
plot_hist(variables,'google','cornflowerblue')
plot_hist(variables,'netflix','cornflowerblue')
plot_hist(variables,'other','cornflowerblue')
plot_hist(variables,'social_media','cornflowerblue')
plot_hist(variables,'total_data','cornflowerblue')
plot_hist(variables,'youtube','cornflowerblue')
plot_hist(variables,'Total DL (Bytes)','cornflowerblue')
plot_hist(variables,'Total UL (Bytes)','cornflowerblue')
plot_hist(variables,'Dur. (ms)','cornflowerblue')
 
agg_data = variables.groupby('MSISDN/Number').sum()
agg_data.head(10)
plot_scatter(agg_data, 'social_media', 'total_data', 'social media data vs total data',  None,  None)
import seaborn as sns
plt.figure(figsize=(15,12))
plt.subplot(2,3,1,title="social media data vs total_data data")
sns.scatterplot(data=agg_data,x="social_media",y="total_data",hue=None,style=None)
 
plt.subplot(2,3,2,title="email data vs total_data data")
sns.scatterplot(data=agg_data,x="email",y="total_data",hue=None,style=None)
 
plt.subplot(2,3,3,title="gaming data vs total_data data")
sns.scatterplot(data=agg_data,x="gaming",y="total_data",hue=None,style=None)
 
plt.subplot(2,3,4,title="google data vs total_data data")
sns.scatterplot(data=agg_data,x="google",y="total_data",hue=None)
 
plt.subplot(2,3,5,title="netflix data vs total_data data")
sns.scatterplot(data=agg_data,x="netflix",y="total_data",hue="netflix")
 
plt.subplot(2,3,6,title="youtube data vs total_data data")
sns.scatterplot(data=variables,x="youtube",y="total_data",hue="youtube")
 
plt.show()
 
from sklearn.preprocessing import MinMaxScaler
 
scaled_explore_feature_df = variables[['MSISDN/Number', 'total_data', 'Dur. (ms)']]
 
scaled_explore_feature_df['Dur. (ms)'] = variables['Dur. (ms)'] /1000
 
scaled_explore_feature_df = scaled_explore_feature_df.rename(columns={'Dur. (ms)': 'duration'})
 
scaled_explore_feature_df_agg = scaled_explore_feature_df.groupby('MSISDN/Number').agg({'duration':'sum', 'total_data': 'sum'})
 
deciles = pd.qcut(scaled_explore_feature_df_agg['duration'], 5, labels=["1st_decile", "2nd_decile",
  "3rd_decile", "4th_decile",
  "5th_decile"])
 
explore_feature_df_with_decile = scaled_explore_feature_df_agg.copy()
 
explore_feature_df_with_decile['decile'] = deciles
 
explore_feature_df_with_decile_agg = explore_feature_df_with_decile.groupby('decile').agg({'total_data': 'sum',
  'duration': 'sum'})
explore_feature_df_with_decile_agg
feature2 = variables.drop(['MSISDN/Number', 'Dur. (ms)'], axis=1)
feature2.corr(method='pearson')
plt.figure(figsize=(12,5))
sns.heatmap(feature2.corr(),cmap="YlGnBu")
plt.title("Application Data Correlation")
plt.show()
data_reduction = variables.drop(['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)'], axis=1)
data_reduction.head()
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
 
scaler = StandardScaler()
scaled_explore_feature_df = scaler.fit_transform(data_reduction)
 
pca = PCA(n_components=2)
pca.fit(scaled_explore_feature_df)
pca_df = pd.DataFrame(pca.transform(scaled_explore_feature_df))
 
plt.figure(figsize=(8, 6))
plt.scatter(pca_df[0], pca_df[1])
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('PCA')
plt.show()
pca_df.head()
%reload_ext autoreload
%autoreload 2
import pandas as pd
from sqlalchemy import create_engine
import psycopg2
 
def connection():
  """
  connects to my pg database
  """
  conn = psycopg2.connect(dbname = 'Telecom',
  user = 'postgres',
  password = 'kerod53@',
  host = 'localhost',
  port = '5432')
  return conn
def table_to_sql(conn, table_name:str) -> pd.DataFrame:
  query = f'SELECT * FROM public.{table_name}'
  data = pd.read_sql_query(query, conn)
  return data
my_parameters = {'dbname': 'Telecom',
  'user': 'postgres',
  'password':'kerod53@',
  'host':'localhost',
  'port': '5432'}
def connection2(self, parameters: dict = my_parameters):
  """
  Connects to the PostgreSQL database. for a given paramters:
  connection_params is a dictionary that define the following:
  {
  'dbname': 'your_database_name',
  'user': 'your_username',
  'password': 'your_password',
  'host': 'your_host',
  'port': 'your_port'
  }
  """
  try:
  conn = psycopg2.connect(**parameters)
  return conn
  except psycopg2.Error as e:
  print(f"Error: Unable to connect to the database. {e}")
  return None
def get_dataframe_sql(self, conn, table_name) -> pd.DataFrame:
  query = f'SELECT * FROM public.{table_name}'
  data = pd.read_sql_query(query, conn)
  return data
def create_engine(self, connection_params: dict = my_parameters):
  """
  creates engine using sqlalchemy for a given paramters:
  """
  engine = create_engine("postgresql://postgres:kerod53@@localhost:5432/Telecom")
  return engine
def write_dataframe_to_table(self, df: pd.DataFrame, table_name: str,engine)->None:
  """
  Writes a pandas dataframe to a new table in the PostgreSQL database.
  """
  df.to_sql(table_name, engine, index=False, if_exists='replace')
  print(f"Dataframe successfully written to the '{table_name}' table.")
def update_table_by_appending(df, table_name, connection_params = my_parameters):
  """
  Appends a pandas dataframe to an existing PostgreSQL table.
  """
  engine = create_engine(f"postgresql://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['dbname']}")
  df.to_sql(table_name, engine, index=False, if_exists='append')
  print(f"Dataframe successfully appended to the '{table_name}' table.")
def delete_table(table_name, connection_params = my_parameters):
  """
  Deletes a table from the PostgreSQL database.
  """
  connection = connect_to_database(connection_params)
  if connection:
  cursor = connection.cursor()
  cursor.execute(f"DROP TABLE IF EXISTS {table_name};")
  connection.commit()
  connection.close()
  print(f"Table '{table_name}' successfully deleted.")
  else:
  print("Error: Unable to connect to the database.")
def connect(self):
  conn = psycopg2.connect(dbname = 'Telecom',
  user = 'postgres',
  password = 'kerod53@',
  host = 'localhost',
  port = '5432')
  return conn
def get_data_sql(self, conn, table_name):
  query = f'SELECT * FROM public.{table_name}'
  data = pd.read_sql_query(query, conn)
  return data
import imp
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
def plot_hist(df:pd.DataFrame, column:str, color:str)->None:
  plt.figure(figsize=(12, 7))
  sns.histplot(data=df, x=column, color=color, kde=True)
  plt.title(f'Distribution of {column}', size=20, fontweight='bold')
  plt.show()
def plot_count(self,df:pd.DataFrame, column:str) -> None:
  plt.figure(figsize=(12, 7))
  sns.countplot(data=df, x=column)
  plt.title(f'Distribution of {column}', size=20, fontweight='bold')
  plt.show()
 
def plot_bar(self,df:pd.DataFrame, x_col:str, y_col:str, title:str, xlabel:str, ylabel:str)->None:
  plt.figure(figsize=(12, 7))
  sns.barplot(data = df, x=x_col, y=y_col)
  plt.title(title, size=20)
  plt.xticks(rotation=75, fontsize=14)
  plt.yticks( fontsize=14)
  plt.xlabel(xlabel, fontsize=16)
  plt.ylabel(ylabel, fontsize=16)
  plt.show()
def plot_heatmap(self,df:pd.DataFrame, title:str, cbar=False)->None:
  plt.figure(figsize=(12, 7))
  sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )
  plt.title(title, size=18, fontweight='bold')
  plt.show()
def plot_box(self,df:pd.DataFrame, x_col:str, title:str) -> None:
  plt.figure(figsize=(12, 7))
  sns.boxplot(data = df, x=x_col)
  plt.title(title, size=20)
  plt.xticks(rotation=75, fontsize=14)
  plt.show()
def plot_box_multi(self,df:pd.DataFrame, x_col:str, y_col:str, title:str) -> None:
  plt.figure(figsize=(12, 7))
  sns.boxplot(data = df, x=x_col, y=y_col)
  plt.title(title, size=20)
  plt.xticks(rotation=75, fontsize=14)
  plt.yticks( fontsize=14)
  plt.show()
def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:
  plt.figure(figsize=(12, 7))
  sns.scatterplot(data = df, x=x_col, y=y_col, hue=hue, style=style)
  plt.title(title, size=20)
  plt.xticks(fontsize=14)
  plt.yticks( fontsize=14)
  plt.show()
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import Normalizer
import sys
sys.path.append('../script')
from plots import *
from data_extraction import *
conn = connection()
data = table_to_sql(conn, 'telecom_data')
data.head()
experiance = data[['Bearer Id', 'MSISDN/Number', 'Avg RTT DL (ms)','Avg RTT UL (ms)','TCP DL Retrans. Vol (Bytes)','TCP UL Retrans. Vol (Bytes)','Avg Bearer TP DL (kbps)','Avg Bearer TP UL (kbps)', 'Handset Type']]
 
experiance.head()
experiance['total_RTT'] = experiance['Avg RTT DL (ms)'] + experiance['Avg RTT UL (ms)']
experiance['total_TCP'] = experiance['TCP DL Retrans. Vol (Bytes)'] + experiance['TCP UL Retrans. Vol (Bytes)']
experiance['total_TP'] = experiance['Avg Bearer TP DL (kbps)'] + experiance['Avg Bearer TP UL (kbps)']
 
experiance.head()
 
agg_experiance = experiance.groupby('MSISDN/Number').agg({'total_RTT': 'mean', 'total_TCP': 'mean', 'total_TP': 'mean', 'Handset Type': 'first'})
agg_experiance.rename(columns={'total_RTT': 'avg_RTT', 'total_TCP': 'avg_TCP', 'total_TP': 'avg_TP'}, inplace=True)
agg_experiance.head()
top_10_RTT = agg_experiance.sort_values(by='avg_RTT', ascending=False)
print('The top 10 values of RTT per user are: \n', top_10_RTT['avg_RTT'].head(10))
print('---------------------------------------------------')
print('The bottom to values of RTT per user are: \n',top_10_RTT['avg_RTT'].tail(10))
print('---------------------------------------------------')
freq = agg_experiance['avg_RTT'].value_counts()
print('The most frequent values of RTT are: \n', freq.head(10))
top_10_TP = agg_experiance.sort_values(by='avg_TP', ascending=False)
print('The top 10 values of TP per user are: \n', top_10_TP['avg_TP'].head(10))
print('---------------------------------------------------')
print('The bottom to values of TP per user are: \n',top_10_TP['avg_TP'].tail(10))
print('---------------------------------------------------')
freq = agg_experiance['avg_TP'].value_counts()
print('The most frequent values of TP are: \n', freq.head(10))
top_10_tcp = agg_experiance.sort_values(by='avg_TCP', ascending=False)
print('The top 10 values of TCP per user are: \n', top_10_tcp['avg_TCP'].head(10))
print('---------------------------------------------------')
print('The bottom to values of TCP per user are: \n',top_10_tcp['avg_TCP'].tail(10))
print('---------------------------------------------------')
freq = agg_experiance['avg_TCP'].value_counts()
print('The most frequent values of TCP are: \n', freq.head(10))
freq = agg_experiance['Handset Type'].value_counts()
print('The most frequent handsets are: \n', freq.head(10))
avg_TP_by_handset_type=experiance.groupby('Handset Type').agg({'total_TP':'mean'})
avg_TP_by_handset_type.rename(columns={'total_TP':'avg_tp'},inplace=True)
avg_TP_by_handset_type.head()
avg_TCP_by_handset_type=experiance.groupby('Handset Type').agg({'total_TCP':'mean'})
avg_TCP_by_handset_type.rename(columns={'total_TCP':'avg_tcp'},inplace=True)
avg_TCP_by_handset_type.head()
experiance_cluster = experiance.groupby('MSISDN/Number').agg({'total_RTT': 'mean', 'total_TCP': 'mean', 'total_TP': 'mean'})
experiance_cluster.rename(columns={'total_RTT': 'avg_RTT', 'total_TCP': 'avg_TCP', 'total_TP': 'avg_TP'}, inplace=True)
experiance_cluster.head()
experiance_cluster.reset_index(inplace=True)
experiance_cluster.drop(columns=['MSISDN/Number'], inplace=False)
from sklearn.preprocessing import Normalizer
 
norm = Normalizer()
normilzed_data = norm.fit_transform(experiance_cluster)
normilized_data = pd.DataFrame(normilzed_data, columns=experiance_cluster.columns)
normilized_data.head()
kmeans = KMeans(n_clusters=3)
kmeans.fit(normilized_data)
labels = kmeans.labels_
agg_experiance['clusters'] = labels
 
agg_experiance.head()
agg_experiance[agg_experiance['clusters']==0].describe().transpose()
agg_experiance[agg_experiance['clusters']==1].describe().transpose()
agg_experiance[agg_experiance['clusters']==2].describe().transpose()
from urllib.parse import quote_plus
from sqlalchemy import create_engine
 
password = 'kerod53@'
 
encoded_password = quote_plus(password)
 
db_string = f'postgresql://postgres:{encoded_password}@localhost:5432/Telecom'
 
engine = create_engine(db_string)
table_name = 'user_experiance'
 
agg_experiance.to_sql(table_name, engine, index=False, if_exists='replace')
