import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

def set_seaborn_style():
    """
    Set a custom Seaborn style.
    """
    sns.set(style="whitegrid")

def plot_histogram_kde(data, title, x_label, y_label, color='skyblue'):
    """
    Plot a histogram with KDE.

    Parameters:
    - data: Series or array-like
        Data to be plotted.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - y_label: str
        Label for the y-axis.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(10, 6))
    sns.histplot(data, kde=True, color=color, edgecolor='black')
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.ylabel(y_label, fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def plot_boxplot(data, title, x_label, color='lightcoral'):
    """
    Plot a boxplot.

    Parameters:
    - data: Series or array-like
        Data to be plotted.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=data, color=color)
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def plot_time_series(data, time_column, title, x_label, y_label, color='skyblue'):
    """
    Plot a time series using Matplotlib.

    Parameters:
    - data: DataFrame
        Data containing a time series.
    - time_column: str
        Column representing the time.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - y_label: str
        Label for the y-axis.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(12, 6))
    sns.lineplot(data=data.resample('D').size(), color=color, marker='o')
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.ylabel(y_label, fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def plot_countplot(data, x_column, title, x_label, rotation=45, color='skyblue'):
    """
    Plot a countplot using Seaborn.

    Parameters:
    - data: DataFrame
        Data to be plotted.
    - x_column: str
        Column for the x-axis.
    - title: str
        Plot title.
    - x_label: str
        Label for the x-axis.
    - rotation: int, optional
        Rotation angle for x-axis labels.
    - color: str, optional
        Color for the plot.

    Returns:
    - None
    """
    plt.figure(figsize=(10, 6))
    sns.countplot(x=data[x_column], color=color)
    plt.title(title, fontsize=16)
    plt.xlabel(x_label, fontsize=14)
    plt.xticks(rotation=rotation, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

def create_table(table_data):
    """
    Create a table using Plotly.

    Parameters:
    - table_data: DataFrame
        Data for the table.

    Returns:
    - None
    """
    table = go.Figure(data=[go.Table(
        header=dict(values=list(table_data.columns),
                    fill_color='lightblue',
                    align='center',
                    font=dict(color='black', size=14)),
        cells=dict(values=[table_data['Handset Manufacturer'], table_data['Handset Type'], table_data['count']],
                   fill=dict(color=['white', 'lightcyan', 'lightcyan']),
                   align='center',
                   font=dict(color='black', size=12)))
    ])

    table.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=0, b=0))

    table.show()
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as pl

from sqlalchemy import create_engine, text

from scipy.stats import zscore

import psycopg2

import os

import plotly.graph_objects as go

import plotly.express as px
os.chdir('..')
from db.connection import DatabaseConnection

from db.sql_preprocessor import DBFilter

from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot
db_connection = DatabaseConnection()
db_connection.connect()
query = "SELECT * FROM xdr_data"

df = db_connection.execute_query(query)
df.head()
df.columns
missing_values = df.isnull().sum()

print(missing_values)
duplicates = df.duplicated()

print("Number of duplicate rows:", duplicates.sum())
top_handsets = df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')

top_handsets = top_handsets.sort_values(by='count', ascending=False).head(10)
table_data = top_handsets[['Handset Manufacturer', 'Handset Type', 'count']]




table = go.Figure(data=[go.Table(

    header=dict(values=list(table_data.columns),

                fill_color='lightblue',

                align='center',

                font=dict(color='black', size=14)),

    cells=dict(values=[table_data['Handset Manufacturer'], table_data['Handset Type'], table_data['count']],

               fill=dict(color=['lightcyan', 'lightcyan', 'lightcyan']),

               align='center',

               font=dict(color='black', size=12)))

])




table.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=0, b=0))




table.show()
top_handsets = df.query("`Handset Manufacturer` != 'undefined' and `Handset Type` != 'undefined'")[['Handset Manufacturer', 'Handset Type']]
top_handsets = top_handsets.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='count')

top_handsets = top_handsets.sort_values(by='count', ascending=False).head(10)
top_handsets[['Handset Manufacturer', 'Handset Type', 'count']]

fig = px.bar(top_handsets, x='Handset Type', y='count', color='Handset Manufacturer',

             labels={'count': 'Count', 'Handset Type': 'Handset Type'},

             title='Top Handsets by Manufacturer and Type',

             template='plotly_white',  
             color_discrete_sequence=px.colors.qualitative.Set1)  



fig.update_layout(

    xaxis=dict(title='Handset Type'),

    yaxis=dict(title='Count'),

    legend=dict(title='Manufacturer'),

    barmode='group',

    showlegend=True

)




fig.show()
top_manufacturers = df[df['Handset Manufacturer'] != 'undefined']
top_manufacturers = top_manufacturers['Handset Manufacturer'].value_counts().head(3).reset_index(name='count')
top_manufacturers.columns = ['Handset Manufacturer', 'count']

top_manufacturers
len(top_manufacturers)

total_rows = len(df)

top_manufacturers['percentage'] = (top_manufacturers['count'] / total_rows) * 100




print(top_manufacturers)

total_handsets = df['Handset Type'].count()




print("Total number of handsets:", total_handsets)

top_manufacturers = df[df['Handset Manufacturer'] != 'undefined']
top_manufacturers = top_manufacturers['Handset Manufacturer'].value_counts().head(3).index
filtered_df = df[df['Handset Manufacturer'].isin(top_manufacturers)]
top_handsets_per_manufacturer = (

    filtered_df.groupby(['Handset Manufacturer', 'Handset Type'])

    .size()

    .reset_index(name='count')

    .sort_values(by=['Handset Manufacturer', 'count'], ascending=[True, False])

    .groupby('Handset Manufacturer')

    .head(5)

)
top_handsets_per_manufacturer
import pandas as pd
import numpy as np
import sqlite3

class DBFilter:
    def __init__(self, dataframe):
        self.df = dataframe

    def filter_numeric_columns(self, threshold=0):
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        filtered_df = self.df[numeric_columns].apply(lambda x: x[x > threshold])

        return filtered_df
    
    def load_data_from_db(self, db_path, sql_query):
        connection = sqlite3.connect(db_path)
        df = pd.read_sql_query(sql_query, connection)
        connection.close()
        return df

    def get_unique_values(self, column):
        unique_values = self.df[column].unique()
        return unique_values
    
    def most_repeated_value(self, column):
        return self.df[column].mode().values[0]

    def calculate_average(self, column):
        return self.df[column].mean()

    def close_connection(self):
                pass
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv
import pandas as pd

class DatabaseConnection:
    def __init__(self):
                load_dotenv()

                self.username = os.getenv("DB_USERNAME")
        self.password = os.getenv("DB_PASSWORD")
        self.host = os.getenv("DB_HOST")
        self.port = os.getenv("DB_PORT")
        self.database = os.getenv("DB_DATABASE")

                if None in (self.username, self.password, self.host, self.port, self.database):
            raise ValueError("One or more database credentials are missing.")

        self.connection_url = f"postgresql+psycopg2://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}"
        self.engine = None
        self.connection = None

    def connect(self):
        try:
            self.engine = create_engine(self.connection_url)
            self.connection = self.engine.connect()
            print("Connected to the database.")
        except Exception as e:
            print(f"Error connecting to the database: {str(e)}")

    def execute_query(self, query):
        try:
            df = pd.read_sql_query(query, self.connection)
            return df
        except Exception as e:
            print(f"Error executing query: {str(e)}")
            
    def execute_update_query(self, query):
        try:
            self.connection.execute(query)
            print("Query executed successfully.")
        except Exception as e:
            print(f"Error executing query: {str(e)}")

    def close_connection(self):
        try:
            self.connection.close()
            print("Connection closed.")
        except Exception as e:
            print(f"Error closing connection: {str(e)}")
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from sqlalchemy import create_engine, text

from scipy.stats import zscore

import psycopg2

import os

import plotly.express as px

import random

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

import tabulate

import plotly.graph_objects as go

from scipy.spatial import distance

from sklearn.metrics import pairwise_distances_argmin_min

from functools import reduce

from tabulate import tabulate

from scipy.stats.mstats import winsorize
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.linear_model import Ridge

from sklearn.model_selection import cross_val_score

from sklearn.linear_model import RidgeCV

from sklearn.ensemble import RandomForestRegressor

from sklearn.ensemble import GradientBoostingRegressor
os.chdir('..')
from db.connection import DatabaseConnection

from db.sql_preprocessor import DBFilter

from src.utils import missing_values_table, find_agg, convert_bytes_to_megabytes, fix_outlier, remove_outliers

from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot, create_table
db_connection = DatabaseConnection()

set_seaborn_style()
db_connection.connect()
query = "SELECT * FROM xdr_data"

df = db_connection.execute_query(query)
df.columns

features = ['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']

df['Start'] = pd.to_datetime(df['Start'])

df['End'] = pd.to_datetime(df['End'])




df['Session Duration (ms)'] = (df['End'] - df['Start']).dt.total_seconds() * 1000




df['Total Traffic (Bytes)'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']




user_engagement = df.groupby('MSISDN/Number').agg({

    'Start': 'count',                           
    'Session Duration (ms)': 'mean',           
    'Total Traffic (Bytes)': 'sum'             
})




user_engagement = user_engagement.rename(columns={

    'Start': 'Sessions Frequency',

    'Session Duration (ms)': 'Average Session Duration (ms)',

    'Total Traffic (Bytes)': 'Total Traffic (Bytes)'

})




print(user_engagement)

scaler = StandardScaler()

normalized_engagement = scaler.fit_transform(user_engagement)




kmeans = KMeans(n_clusters=3, random_state=42)

user_engagement['Cluster'] = kmeans.fit_predict(normalized_engagement)




print("Customers Classified into 3 Groups of Engagement:")

print(user_engagement)

kmeans = KMeans(n_clusters=3, random_state=42)

user_engagement['Cluster'] = kmeans.fit_predict(normalized_engagement)




less_engaged_cluster = user_engagement.groupby('Cluster')['Sessions Frequency'].mean().idxmin()




distances = pairwise_distances_argmin_min(normalized_engagement, kmeans.cluster_centers_)[1]




user_engagement.reset_index(inplace=True)




user_engagement['Engagement Score'] = distances if less_engaged_cluster == 0 else -distances

print("Customers with Engagement Scores:")

print(user_engagement[['MSISDN/Number', 'Engagement Score']])

top_10_engaged_customers = user_engagement['Engagement Score'].sort_values(ascending=False).head(10)




top_10_df = pd.DataFrame({'Engagement Score': top_10_engaged_customers.values})




top_10_df.reset_index(inplace=True, drop=True)




fig = px.bar(top_10_df, x=top_10_df.index + 1, y='Engagement Score', title='Top 10 Engaged Customers')




fig.update_layout(xaxis_title='Rank', yaxis_title='Engagement Score')




fig.show()
top_10_engaged_customers

grouped_data_throughput = df.groupby('MSISDN/Number').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




grouped_data_throughput['Avg Bearer TP DL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP DL (kbps)'].mean(), inplace=True)

grouped_data_throughput['Avg Bearer TP UL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP UL (kbps)'].mean(), inplace=True)




print(grouped_data_throughput)
grouped_data_rtt = df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean().reset_index()




grouped_data_rtt['Avg RTT DL (ms)'].fillna(grouped_data_rtt['Avg RTT DL (ms)'].mean(), inplace=True)




mean_value_rtt = grouped_data_rtt['Avg RTT DL (ms)'].mean()

std_dev_rtt = grouped_data_rtt['Avg RTT DL (ms)'].std()




outlier_threshold_rtt = 3




grouped_data_rtt['Avg RTT DL (ms)'] = grouped_data_rtt['Avg RTT DL (ms)'].apply(

    lambda x: mean_value_rtt if abs(x - mean_value_rtt) > outlier_threshold_rtt * std_dev_rtt else x

)




print(grouped_data_rtt)
grouped_data_retrans = df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].fillna(grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].mean(), inplace=True)




mean_value = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].mean()

std_dev = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].std()




outlier_threshold = 3




grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'] = grouped_data_retrans['TCP DL Retrans. Vol (Bytes)'].apply(

    lambda x: mean_value if abs(x - mean_value) > outlier_threshold * std_dev else x

)




print(grouped_data_retrans)

grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()




grouped_data_tcp = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




merged_data = pd.merge(grouped_data_handset, grouped_data_tcp, on='Handset Type')




print(merged_data)

consolidated_data = pd.merge(grouped_data_handset, grouped_data_retrans, on='MSISDN/Number')

consolidated_data = pd.merge(consolidated_data, grouped_data_rtt, on='MSISDN/Number')

consolidated_data = pd.merge(consolidated_data, grouped_data_throughput, on='MSISDN/Number')

consolidated_data = pd.merge(consolidated_data, user_engagement, on='MSISDN/Number')



print(consolidated_data.dtypes)


features_for_clustering = ['TCP DL Retrans. Vol (Bytes)', 'Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']




consolidated_data.dropna(subset=features_for_clustering, inplace=True)




scaler = StandardScaler()

consolidated_data_scaled = scaler.fit_transform(consolidated_data[features_for_clustering])




n_clusters = 3

kmeans = KMeans(n_clusters=n_clusters, random_state=42)

consolidated_data['cluster'] = kmeans.fit_predict(consolidated_data_scaled)




consolidated_data['euclidean_distance'] = consolidated_data.apply(

    lambda row: distance.euclidean(row[features_for_clustering], kmeans.cluster_centers_[row['cluster']]),

    axis=1

)




consolidated_data['experience_score'] = 1 / (1 + consolidated_data['euclidean_distance'])




print(consolidated_data[['MSISDN/Number', 'experience_score']])


top_10_experience_customers = consolidated_data.sort_values(by='experience_score', ascending=False).head(10)




top_10_experience_df = pd.DataFrame({'experience_score': top_10_experience_customers['experience_score'].values})




top_10_experience_df.reset_index(inplace=True, drop=True)




fig = px.bar(top_10_experience_df, x=top_10_experience_df.index + 1, y='experience_score', title='Top 10 Customers by Experience Score')




fig.update_layout(xaxis_title='Rank', yaxis_title='Experience Score')




fig.show()
top_10_experience_customers = consolidated_data.sort_values(by='experience_score', ascending=False).head(10)




columns_to_include = [

    'MSISDN/Number',

    'experience_score',

]




result_df = top_10_experience_customers[columns_to_include]




print(result_df)
top_10_experience_scores

columns_to_handle_outliers = ['Engagement Score', 'experience_score']




def handle_outliers_iqr(consolidated_data, columns):

    for column in columns:

        Q1 = consolidated_data[column].quantile(0.25)

        Q3 = consolidated_data[column].quantile(0.75)

        IQR = Q3 - Q1

        lower_limit = Q1 - 1.5 * IQR

        upper_limit = Q3 + 1.5 * IQR

        consolidated_data[column] = consolidated_data[column].apply(lambda x: max(lower_limit, min(x, upper_limit)))

    return consolidated_data




consolidated_data = handle_outliers_iqr(consolidated_data, columns_to_handle_outliers)

consolidated_data['satisfaction_score'] = (user_engagement['Engagement Score'] + consolidated_data['experience_score']) / 2




top_satisfied_customers = consolidated_data.sort_values(by='satisfaction_score', ascending=False)




top_10_satisfied_customers = top_satisfied_customers.head(10)

print("Top 10 Satisfied Customers:")

print(top_10_satisfied_customers[['MSISDN/Number', 'satisfaction_score']])

consolidated_data

sorted_data = consolidated_data.sort_values(by='satisfaction_score', ascending=False)




top_10 = sorted_data.head(10)




fig = go.Figure(data=[go.Table(

    header=dict(values=list(top_10.columns),

                fill_color='paleturquoise',

                align='left'),

    cells=dict(values=[top_10[col] for col in top_10.columns],

               fill_color='lavender',

               align='left'))

])




fig.show()

fig1 = px.scatter(consolidated_data, x='satisfaction_score', y='Engagement Score',

                  title='Satisfaction Score vs Engagement Score',

                  labels={'satisfaction_score': 'Satisfaction Score',

                          'engagement_score': 'Engagement Score'})




fig2 = px.scatter(consolidated_data, x='satisfaction_score', y='experience_score',

                  title='Satisfaction Score vs Experience Score',

                  labels={'satisfaction_score': 'Satisfaction Score',

                          'experience_score': 'Experience Score'})




fig3 = px.scatter(consolidated_data, x='Engagement Score', y='experience_score',

                  title='Engagement Score vs Experience Score',

                  labels={'engagement_score': 'Engagement Score',

                          'experience_score': 'Experience Score'})




fig1.show()

fig2.show()

fig3.show()
consolidated_data

max_satisfaction_score = consolidated_data['satisfaction_score'].max()




low_satisfaction_threshold = 0.1







































0 * max_satisfaction_score

moderate_satisfaction_threshold = 0.25 * max_satisfaction_score

high_satisfaction_threshold = 0.50 * max_satisfaction_score




consolidated_data['Satisfaction Level'] = pd.cut(consolidated_data['satisfaction_score'],

                                  bins=[-float('inf'), low_satisfaction_threshold, moderate_satisfaction_threshold, high_satisfaction_threshold, float('inf')],

                                  labels=['Low Satisfied', 'Moderately Satisfied', 'Satisfied', 'Highly Satisfied'])




satisfaction_counts = consolidated_data['Satisfaction Level'].value_counts()




satisfaction_percentage = (satisfaction_counts / len(df)) * 100




fig = px.pie(satisfaction_percentage, names=satisfaction_percentage.index, values=satisfaction_percentage.values,

             title='Percentage Distribution of Satisfaction Levels',

             labels={'index': 'Satisfaction Level', 'values': 'Percentage of Individuals'})




fig.show()

columns_to_handle_outliers = ['Engagement Score', 'experience_score', 'satisfaction_score']




def handle_outliers_iqr(consolidated_data, columns):

    for column in columns:

        Q1 = consolidated_data[column].quantile(0.25)

        Q3 = consolidated_data[column].quantile(0.75)

        IQR = Q3 - Q1

        lower_limit = Q1 - 1.5 * IQR

        upper_limit = Q3 + 1.5 * IQR

        consolidated_data[column] = consolidated_data[column].apply(lambda x: max(lower_limit, min(x, upper_limit)))

    return consolidated_data




consolidated_data = handle_outliers_iqr(consolidated_data, columns_to_handle_outliers)

consolidated_data.to_csv('consolidated_data.csv', index=False)

top_satisfied_customers = consolidated_data.sort_values(by='satisfaction_score', ascending=False)




top_10_satisfied_customers = top_satisfied_customers.head(10)

print("Top 10 Satisfied Customers:")

print(top_10_satisfied_customers[['MSISDN/Number', 'satisfaction_score']])

max_satisfaction_score = consolidated_data['satisfaction_score'].max()




low_satisfaction_threshold = 0.25 * max_satisfaction_score

moderate_satisfaction_threshold = 0.50 * max_satisfaction_score

high_satisfaction_threshold = 0.75 * max_satisfaction_score




consolidated_data['Satisfaction Level'] = pd.cut(consolidated_data['satisfaction_score'],

                                  bins=[-float('inf'), low_satisfaction_threshold, moderate_satisfaction_threshold, high_satisfaction_threshold, float('inf')],

                                  labels=['Low Satisfied', 'Moderately Satisfied', 'Satisfied', 'Highly Satisfied'])




satisfaction_counts = consolidated_data['Satisfaction Level'].value_counts()




satisfaction_percentage = (satisfaction_counts / len(df)) * 100




fig = px.pie(satisfaction_percentage, names=satisfaction_percentage.index, values=satisfaction_percentage.values,

             title='Percentage Distribution of Satisfaction Levels',

             labels={'index': 'Satisfaction Level', 'values': 'Percentage of Individuals'})




fig.show()

regression_features = ['Engagement Score', 'experience_score']




regression_data = consolidated_data.dropna(subset=regression_features)




X_train, X_test, y_train, y_test = train_test_split(

    regression_data[regression_features],

    regression_data['satisfaction_score'],

    test_size=0.2, random_state=42

)




scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)




model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)




model.fit(X_train_scaled, y_train)




y_pred = model.predict(X_test_scaled)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)




cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)



consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])



print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])

print(f'Cross-validated R-squared scores: {cv_scores}')

print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')

plt.scatter(y_test, y_pred)

plt.xlabel('Actual Satisfaction Score')

plt.ylabel('Predicted Satisfaction Score')

plt.title('Actual vs. Predicted Satisfaction Score')

plt.show()

scores = cross_val_score(model, X_train_scaled, y_train, cv=5)

print(f'Cross-validated R-squared scores: {scores}')

regression_features = ['Engagement Score', 'experience_score']




regression_data = consolidated_data.dropna(subset=regression_features)




X_train, X_test, y_train, y_test = train_test_split(

    regression_data[regression_features],

    regression_data['satisfaction_score'],

    test_size=0.2, random_state=42

)




model = RandomForestRegressor(n_estimators=100, random_state=42)




model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])



print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])




cv_scores = cross_val_score(model, X_train, y_train, cv=5)

print(f'Cross-validated R-squared scores: {cv_scores}')

print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')

plt.scatter(y_test, y_pred)

plt.xlabel('Actual Satisfaction Score')

plt.ylabel('Predicted Satisfaction Score')

plt.title('Actual vs. Predicted Satisfaction Score')

plt.show()

regression_features = ['Engagement Score', 'experience_score']




regression_data = consolidated_data.dropna(subset=regression_features)




X_train, X_test, y_train, y_test = train_test_split(

    regression_data[regression_features],

    regression_data['satisfaction_score'],

    test_size=0.2, random_state=42

)




model = GradientBoostingRegressor(n_estimators=100, random_state=42)




model.fit(X_train, y_train)




y_pred = model.predict(X_test)




mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)



consolidated_data['predicted_satisfaction_score'] = model.predict(consolidated_data[regression_features])



print(consolidated_data[['MSISDN/Number', 'satisfaction_score', 'predicted_satisfaction_score']])




cv_scores = cross_val_score(model, X_train, y_train, cv=5)

print(f'Cross-validated R-squared scores: {cv_scores}')

print(f'Mean Squared Error: {mse}')

print(f'R-squared: {r2}')

clustering_features = ['Engagement Score', 'experience_score']




clustering_data = consolidated_data.dropna(subset=clustering_features)




scaler = StandardScaler()

clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])




kmeans = KMeans(n_clusters=2, random_state=42)

clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)




print("Results of K-means Clustering (k=2):")

print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])

clustering_features = ['Engagement Score', 'experience_score']




clustering_data = consolidated_data.dropna(subset=clustering_features)




scaler = StandardScaler()

clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])




kmeans = KMeans(n_clusters=3, random_state=42)

clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)




print("Results of K-means Clustering (k=2):")

print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])

clustering_features = ['Engagement Score', 'experience_score']




clustering_data = consolidated_data.dropna(subset=clustering_features)




scaler = StandardScaler()

clustering_data_scaled = scaler.fit_transform(clustering_data[clustering_features])




kmeans = KMeans(n_clusters=4, random_state=42)

clustering_data['cluster'] = kmeans.fit_predict(clustering_data_scaled)




print("Results of K-means Clustering (k=2):")

print(clustering_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'cluster']])

cluster_aggregation = clustering_data.groupby('cluster').agg({

    'satisfaction_score': 'mean',

    'experience_score': 'mean'

}).reset_index()




print("Average Scores per Cluster:")

print(cluster_aggregation)

create_table_query = """

CREATE TABLE user_scores (

    user_id VARCHAR(255),

    engagement_score FLOAT,

    experience_score FLOAT,

    satisfaction_score FLOAT

);

"""

db_connection.execute_query(create_table_query)
user_scores_df = consolidated_data[['MSISDN/Number', 'Engagement Score', 'experience_score', 'satisfaction_score']]

user_scores_df.to_sql('user_scores', con=db_connection.engine, index=False, if_exists='append')




db_connection.close_connection()
import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from sqlalchemy import create_engine, text

from scipy.stats import zscore

import psycopg2

import os

import plotly.express as px

import random

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

import tabulate

import plotly.graph_objects as go
os.chdir('..')
from db.connection import DatabaseConnection

from db.sql_preprocessor import DBFilter

from src.utils import missing_values_table, find_agg, convert_bytes_to_megabytes, fix_outlier, remove_outliers

from src.plot_utils import set_seaborn_style, plot_histogram_kde, plot_boxplot, plot_time_series, plot_countplot, create_table
db_connection = DatabaseConnection()

set_seaborn_style()
db_connection.connect()
query = "SELECT * FROM xdr_data"

df = db_connection.execute_query(query)

missing_values_df = missing_values_table(df)

print("Missing Values in df:")

print(missing_values_df)
grouped_data = df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




grouped_data['TCP DL Retrans. Vol (Bytes)'].fillna(grouped_data['TCP DL Retrans. Vol (Bytes)'].mean(), inplace=True)




mean_value = grouped_data['TCP DL Retrans. Vol (Bytes)'].mean()

std_dev = grouped_data['TCP DL Retrans. Vol (Bytes)'].std()




outlier_threshold = 3




grouped_data['TCP DL Retrans. Vol (Bytes)'] = grouped_data['TCP DL Retrans. Vol (Bytes)'].apply(

    lambda x: mean_value if abs(x - mean_value) > outlier_threshold * std_dev else x

)




print(grouped_data)
 grouped_data_rtt = df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean().reset_index()




grouped_data_rtt['Avg RTT DL (ms)'].fillna(grouped_data_rtt['Avg RTT DL (ms)'].mean(), inplace=True)




mean_value_rtt = grouped_data_rtt['Avg RTT DL (ms)'].mean()

std_dev_rtt = grouped_data_rtt['Avg RTT DL (ms)'].std()




outlier_threshold_rtt = 3




grouped_data_rtt['Avg RTT DL (ms)'] = grouped_data_rtt['Avg RTT DL (ms)'].apply(

    lambda x: mean_value_rtt if abs(x - mean_value_rtt) > outlier_threshold_rtt * std_dev_rtt else x

)




print(grouped_data_rtt)
grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()




grouped_data_handset['Handset Type'].fillna(grouped_data_handset['Handset Type'].mode().iat[0], inplace=True)




print(grouped_data_handset)

grouped_data_throughput = df.groupby('MSISDN/Number').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




grouped_data_throughput['Avg Bearer TP DL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP DL (kbps)'].mean(), inplace=True)

grouped_data_throughput['Avg Bearer TP UL (kbps)'].fillna(grouped_data_throughput['Avg Bearer TP UL (kbps)'].mean(), inplace=True)




print(grouped_data_throughput)

top_10_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].nlargest(10)




bottom_10_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)




most_frequent_tcp_values = grouped_data['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)




print("Top 10 TCP values:")

print(top_10_tcp_values)



print("\nBottom 10 TCP values:")

print(bottom_10_tcp_values)



print("\nMost frequent TCP values:")

print(most_frequent_tcp_values)





top_10_data = pd.DataFrame({'Top 10 TCP Values': top_10_tcp_values.values}, index=top_10_tcp_values.index)

bottom_10_data = pd.DataFrame({'Bottom 10 TCP Values': bottom_10_tcp_values.values}, index=bottom_10_tcp_values.index)

most_frequent_data = pd.DataFrame({'Frequency': most_frequent_tcp_values.values}, index=most_frequent_tcp_values.index)


most_frequent_tcp_values = most_frequent_tcp_values[~(most_frequent_tcp_values.index == "16853393.739320666")]


fig, axes = plt.subplots(3, 1, figsize=(12, 15))






bottom_10_data.plot(kind='bar', ax=axes[1], legend=False)

axes[1].set_ylabel('TCP DL Retrans. Vol (Bytes)')

axes[1].set_title('Bottom 10 TCP Values')






plt.show()

bottom_10_data.plot(kind='bar', ax=axes[1], legend=False)

axes[1].set_ylabel('TCP DL Retrans. Vol (Bytes)')

axes[1].set_title('Bottom 10 TCP Values')




most_frequent_data.plot(kind='bar', ax=axes[2], legend=False)

axes[2].set_xlabel('Index')

axes[2].set_ylabel('Frequency')

axes[2].set_title('Most Frequent TCP Values')




plt.show()
data

top_10_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].nlargest(10)




bottom_10_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].nsmallest(10)




most_frequent_rtt_values = grouped_data_rtt['Avg RTT DL (ms)'].value_counts().head(10)




print("Top 10 RTT values:")

print(top_10_rtt_values)



print("\nBottom 10 RTT values:")

print(bottom_10_rtt_values)



print("\nMost frequent RTT values:")

print(most_frequent_rtt_values)

top_10_rtt_data = pd.DataFrame({'Top 10 RTT Values': top_10_rtt_values.values}, index=top_10_rtt_values.index)

bottom_10_rtt_data = pd.DataFrame({'Bottom 10 RTT Values': bottom_10_rtt_values.values}, index=bottom_10_rtt_values.index)




fig, axes = plt.subplots(2, 1, figsize=(12, 10))






bottom_10_rtt_data.plot(kind='bar', ax=axes[1], legend=False, color='skyblue')

axes[1].set_xlabel('Index')

axes[1].set_ylabel('Avg RTT DL (ms)')

axes[1].set_title('Bottom 10 RTT Values')




plt.show()

top_10_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].nlargest(10)




bottom_10_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].nsmallest(10)




most_frequent_throughput_dl_values = grouped_data_throughput['Avg Bearer TP DL (kbps)'].value_counts().head(10)




top_10_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].nlargest(10)




bottom_10_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].nsmallest(10)




most_frequent_throughput_ul_values = grouped_data_throughput['Avg Bearer TP UL (kbps)'].value_counts().head(10)




print("Top 10 Throughput values (DL):")

print(top_10_throughput_dl_values)



print("\nBottom 10 Throughput values (DL):")

print(bottom_10_throughput_dl_values)



print("\nMost frequent Throughput values (DL):")

print(most_frequent_throughput_dl_values)



print("\nTop 10 Throughput values (UL):")

print(top_10_throughput_ul_values)



print("\nBottom 10 Throughput values (UL):")

print(bottom_10_throughput_ul_values)



print("\nMost frequent Throughput values (UL):")

print(most_frequent_throughput_ul_values)

top_10_throughput_dl_data = pd.DataFrame({'Top 10 Throughput DL Values': top_10_throughput_dl_values.values}, index=top_10_throughput_dl_values.index)

bottom_10_throughput_dl_data = pd.DataFrame({'Bottom 10 Throughput DL Values': bottom_10_throughput_dl_values.values}, index=bottom_10_throughput_dl_values.index)




fig, axes = plt.subplots(2, 1, figsize=(12, 10))




bottom_10_throughput_dl_data.plot(kind='bar', ax=axes[1], legend=False, color='purple')

axes[1].set_xlabel('Index')

axes[1].set_ylabel('Avg Bearer TP DL (kbps)')

axes[1].set_title('Bottom 10 Throughput DL Values')




plt.show()

grouped_throughput = df.groupby('Handset Type').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




print(grouped_throughput)

grouped_throughput_handset = pd.merge(grouped_data_handset, grouped_data_throughput, on='MSISDN/Number')




print(grouped_throughput_handset)



grouped_throughput_handset = grouped_throughput_handset.groupby('Handset Type').agg({

    'Avg Bearer TP DL (kbps)': 'mean',

    'Avg Bearer TP UL (kbps)': 'mean'

}).reset_index()




print(grouped_throughput_handset)

merged_data = pd.merge(grouped_data_handset, grouped_data, on='MSISDN/Number')




print(merged_data)




grouped_data_handset = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None).reset_index()




grouped_data_tcp = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean().reset_index()




merged_data = pd.merge(grouped_data_handset, grouped_data_tcp, on='Handset Type')




print(merged_data)

top_10_mean_tcp_values = merged_data.nlargest(10, 'TCP DL Retrans. Vol (Bytes)')




plt.figure(figsize=(12, 6))




plt.bar(top_10_mean_tcp_values['Handset Type'], top_10_mean_tcp_values['TCP DL Retrans. Vol (Bytes)'], color='blue')




plt.xlabel('Handset Type')

plt.ylabel('Mean TCP DL Retrans. Vol (Bytes)')

plt.title('Top 10 Mean TCP DL Retrans. Vol (Bytes) for Each Handset Type')




plt.xticks(rotation=90)




plt.show()
top_10_mean_tcp_values
import os
from abc import ABC, abstractmethod
from sqlalchemy.engine.base import Engine
from typing import Generic, Optional, TypeVar
from dotenv import load_dotenv

RawConnectionT = TypeVar("RawConnectionT")

class ConnectionBase(ABC, Generic[RawConnectionT]):
    """The abstract base class that all connections must inherit from

        This base class provides connection authors with a way to set up 
        database parameters like database name, database password, port, and also 
        instance of database engine.
    """
    def __init__(self, **kwargs):
        """ create a BaseConnection

        Parameters
        ----------
        **kwargs: dict
            dictionary of parameters of any length to pass tho the connection class

        Returns
        -------
        None

        """
        self._kwargs = kwargs if kwargs else {
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD'),
            'host': os.getenv('DB_HOST'),
            'port': os.getenv('DB_PORT'),
            'database': os.getenv('DB_NAME'),
        }
        self._raw_instance: Optional[Engine] = self._connect()

    @property
    def _instance(self) -> RawConnectionT:
        """Get an instance of the underlying connection, creating a new one if needed."""
        if self._raw_instance is None:
            self._raw_instance = self._connect()

        return self._raw_instance

    @abstractmethod
    def _connect(self) -> RawConnectionT:
        """Create an instance of an underlying connection object.

        This abstract method is the one method that we require subclasses of
        BaseConnection to provide an implementation for. It is called when first
        creating a connection and when reconnecting after a connection is reset.

        Returns
        -------
        RawConnectionT
            The underlying connection object.
        """
        raise NotImplementedError

    def __enter__(self):
        self._raw_instance = self._connect()
        return self._raw_instance

    def __exit__(self, exc_type, exc_value, traceback):
        if self._raw_instance is not None:
            self._raw_instance.dispose()
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

import logging
from sqlalchemy import create_engine
from connections.connection_base import ConnectionBase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PostgresConnection(ConnectionBase):
    """A class representing a connection to a PostgreSQL database.

    This class inherits from ConnectionBase, providing a standardized way to manage
    the connection to the PostgreSQL database.

    Parameters
    ----------
    **kwargs : dict
        Keyword arguments containing the database connection parameters.

    Attributes
    ----------
    _kwargs : dict
        Dictionary containing the database connection parameters.
    _raw_instance : sqlalchemy.engine.base.Engine
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def _connect(self):
        """Create an instance of the underlying SQLAlchemy Engine for PostgreSQL.

        Returns
        -------
        sqlalchemy.engine.base.Engine
            The SQLAlchemy Engine instance representing the PostgreSQL database connection.

        Raises
        ------
        Exception
            If there is an error connecting to the database.
        """
        try:
            engine = create_engine(
                f"postgresql://{self._kwargs['user']}:{self._kwargs['password']}@{self._kwargs['host']}:{self._kwargs['port']}/{self._kwargs['database']}"
            )

            engine.connect()

            return engine

        except Exception as e:
            logger.error(f"Error connecting to the database: {e}")
            raise
import os, sys



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

df = rd.read_data()
df.shape
df.head()
df.info()
df.isnull().sum()
df.duplicated().sum()
df.describe()
for col in df.columns:

    print(df[col].value_counts())
import os, sys

import pandas as pd

rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.write_to_db as wd

import scripts.data_cleaning as sd 

df = rd.read_data()
df = sd.drop_high_missing_columns(df, 0.7)

df.columns

def remove_missing_values():

    columns_to_check = [

    'Start', 

    'Start ms', 

    'End', 

    'End ms', 

    'Dur. (ms)',

    'Avg Bearer TP DL (kbps)',

    'Avg Bearer TP UL (kbps)',

    'Activity Duration DL (ms)',                        

    'Activity Duration UL (ms)',                        

    'Dur. (ms).1',

    'Total UL (Bytes)',                                 

    'Total DL (Bytes)'

    ]



    return sd.remove_rows_with_missing_values(df, columns_to_check)



cleaned_df = remove_missing_values()
df.shape
cleaned_df.isnull().sum()

def impute_columns():

    columns_to_impute = [

        "Avg RTT DL (ms)",                             

        "Avg RTT UL (ms)",                              

        "TCP DL Retrans. Vol (Bytes)",                  

        "TCP UL Retrans. Vol (Bytes)",                  

        "DL TP < 50 Kbps (%)",                            

        "50 Kbps < DL TP < 250 Kbps (%)",              

        "250 Kbps < DL TP < 1 Mbps (%)",                  

        "DL TP > 1 Mbps (%)",                             

        "UL TP < 10 Kbps (%)",                           

        "10 Kbps < UL TP < 50 Kbps (%)",                 

        "50 Kbps < UL TP < 300 Kbps (%)",                 

        "UL TP > 300 Kbps (%)",                           

        "HTTP DL (Bytes)",                              

        "HTTP UL (Bytes)",                             

        "Nb of sec with 125000B < Vol DL",             

        "Nb of sec with 1250B < Vol UL < 6250B",        

        "Nb of sec with 31250B < Vol DL < 125000B",                   

        "Nb of sec with 6250B < Vol DL < 31250B",           

        "Nb of sec with Vol DL < 6250B",                  

        "Nb of sec with Vol UL < 1250B"                 

    ]



    return sd.impute_numeric_missing(cleaned_df, columns_to_impute)





imputed_df = impute_columns()
imputed_df.isnull().sum()
imputed_df.shape
def replace_with_mode():

    columns_to_replace = [

        'Bearer Id',

        'IMSI',

        'MSISDN/Number',

        'IMEI',        

        'Last Location Name',

        'Handset Manufacturer',

        'Handset Type' 

    ]



    return sd.replace_column_with_mode(imputed_df, columns_to_replace)



cleaned_df = replace_with_mode()
cleaned_df.isnull().sum()
cleaned_df.shape
def handle_outliers():

    columns = [

        "Avg RTT DL (ms)",                             

        "Avg RTT UL (ms)",                              

        "TCP DL Retrans. Vol (Bytes)",                  

        "TCP UL Retrans. Vol (Bytes)",                  

        "DL TP < 50 Kbps (%)",                            

        "50 Kbps < DL TP < 250 Kbps (%)",              

        "250 Kbps < DL TP < 1 Mbps (%)",                  

        "DL TP > 1 Mbps (%)",                             

        "UL TP < 10 Kbps (%)",                           

        "10 Kbps < UL TP < 50 Kbps (%)",                 

        "50 Kbps < UL TP < 300 Kbps (%)",                 

        "UL TP > 300 Kbps (%)",                           

        "HTTP DL (Bytes)",                              

        "HTTP UL (Bytes)",                             

        "Nb of sec with 125000B < Vol DL",             

        "Nb of sec with 1250B < Vol UL < 6250B",        

        "Nb of sec with 31250B < Vol DL < 125000B",                   

        "Nb of sec with 6250B < Vol DL < 31250B",          

        "Nb of sec with Vol DL < 6250B",                  

        "Nb of sec with Vol UL < 1250B"                 

    ]



    return sd.handle_outliers(cleaned_df, columns)



processed_df = handle_outliers()

processed_df.head()
processed_df.shape
wd.write_data(processed_df, 'processed_data')
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

def drop_high_missing_columns(
    df: pd.DataFrame, 
    threshold=0.8
    ) -> pd.DataFrame:
    """Drop columns with missing values exceeding a specified threshold.

    Parameters
    ----------
    - df: DataFrame
        The dataframe of raw data set.
    - threshold: float, default 0.8
        The threshold for the percentage of missing values in a column.
        
    Returns
    -------
    - Dataframe object
        where columns specified in the columns_to_drop list, 
        which have missing values, dropped.
        
    """
    missing_percentage = df.isnull().mean()
    columns_to_drop = missing_percentage[missing_percentage > threshold].index
    df_cleaned = df.drop(columns=columns_to_drop)
    return df_cleaned

def impute_numeric_missing(
    df:pd.DataFrame, 
    columns_list:list[str],
    strategy:str='mean'
    ) -> pd.DataFrame:
    """Impute missing values for numerical columns.

    Parameters
    ----------
    - df: DataFrame
    - columns_list
        list of columnst to Impute the missing values
    - strategy: str, default 'mean'
        Imputation strategy, options: 'mean', 'zero'.

    Returns
    -------
    - Dataframe object
        with imputed missing columns from columns_list
    """
    numeric_columns = df[columns_list].select_dtypes(include='number').columns
    
    if strategy == 'zero':
        df_imputed = df.copy()  
        df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(0)
    else:
        imputation_values = df[numeric_columns].mean() if strategy == 'mean' else df[numeric_columns].median()
        df_imputed = df.copy() 
        df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(imputation_values)

    return df_imputed

def remove_rows_with_missing_values(
    df: pd.DataFrame, 
    columns_to_check: list[str]
    ):
    """
    Remove rows from a DataFrame where any of the specified columns have missing values.

    Parameters
    ----------
    df : pandas.DataFrame
        The input DataFrame.
    columns_to_check : list of str
        A list of column names to check for missing values. Rows will be dropped
        if any of these columns have missing values.

    Returns
    -------
    pandas.DataFrame
        A new DataFrame with rows removed where any of the specified columns have missing values.

    """
    cleaned_df = df.dropna(subset=columns_to_check, how='any')
    return cleaned_df

import pandas as pd

def replace_column_with_mode(
    df: pd.DataFrame, 
    column_names: list[str]
    ) -> pd.DataFrame:
    """Replace missing values in a column with its mode.

    Parameters
    ----------
    - df: DataFrame
    - column_list: list[str]
        Name of the list of columns to replace missing values.

    Returns
    -------
    - DataFrame object
        with missing values in the specified column replaced by its mode.
    """
    df_mode = df.copy()  
    for column_name in column_names:
        mode_value = df_mode[column_name].mode().iloc[0]
        df_mode[column_name] = df_mode[column_name].fillna(mode_value)

    return df_mode

def handle_outliers(
    df: pd.DataFrame, 
    columns: list[str], 
    method:str='mean'
    ) -> pd.DataFrame:
    """Handle outliers in specified columns using a specified method.

    Parameters
    ----------
    - df: DataFrame
    - columns: list
        List of columns to handle outliers.
    - method: str, default 'clip'
        Outlier handling method, options: 'clip', 'remove', 'mean'

    Returns
    -------
    - Dataframe object
        where outliers are handled
    """
    if method == 'clip':
        for col in columns:
            df[col] = np.clip(df[col], df[col].quantile(0.05), df[col].quantile(0.95))
    elif method == 'remove':
        for col in columns:
            q1 = df[col].quantile(0.25)
            q3 = df[col].quantile(0.75)
            iqr = q3 - q1
            df = df[(df[col] >= q1 - 1.5 * iqr) & (df[col] <= q3 + 1.5 * iqr)]
    elif method == 'mean':
        for col in columns:
            mean_val = df[col].mean()
            df[col] = np.where(
                (df[col] < df[col].quantile(0.05)) | (df[col] > df[col].quantile(0.95)),
                mean_val,
                df[col]
            )
    return df
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def read_data(table_name='xdr_data'):
    """Read data from a PostgreSQL database table.

    Parameters
    ----------
    table_name : str, optional
        The name of the table from which to fetch data. Default is 'xdr_data'.

    Returns
    -------
    pandas.DataFrame
        A DataFrame containing the data retrieved from the specified table.

    Raises
    ------
    Exception
        If there is an error during the data retrieval process.

    Notes
    -----
    This function uses the `PostgresConnection` class to establish a connection
    to the PostgreSQL database. Ensure that the necessary environment variables
    (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
    this function.
    """
    try:
        with PostgresConnection() as postgres_conn:
            df = pd.read_sql_query(f"SELECT * FROM {table_name};", postgres_conn)
            logger.info('Data fetched succesfully')
            return df

    except Exception as e:
        logger.error(f"Error in the main block: {e}")
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
    sys.path.insert(0, rpath)

import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def write_data(
        df: pd.DataFrame,
        table_name : str
        ) -> None:
        """Write DataFrame to a PostgreSQL database table.

        Parameters
        ----------
        df : pandas.DataFrame
            The DataFrame containing the data to be written.
        table_name : str
            The name of the table where the data will be written.

        Returns
        -------
        None

        Raises
        ------
        Exception
            If there is an error during the data writing process.

        Notes
        -----
        This function uses the `PostgresConnection` class to establish a connection
        to the PostgreSQL database. Ensure that the necessary environment variables
        (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
        this function.
        """
        try:
            conn = PostgresConnection()

            df.to_sql(name=table_name, con=conn._instance, index=False, if_exists='replace')

            logger.info('Data written successfully')
        
        except Exception as e:
            logger.error(f"Error in the main block: {e}")
import os, sys

import pandas as pd

from pandasql import sqldf



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
df.shape
pysqldf = lambda q: sqldf(q, globals())
df.columns
query = ''' 

    SELECT DISTINCT 

        "Handset Type", 

        COUNT(*) as UsageCount

    FROM df

    GROUP BY "Handset Type" 

    ORDER BY UsageCount DESC

    limit 10

'''



result_df = pysqldf(query)

result_df

query = ''' 

    SELECT DISTINCT 

        "Handset Manufacturer", 

        COUNT(*) as "Number of Users"

    FROM df

    GROUP BY "Handset Manufacturer" 

    ORDER BY "Number of Users" DESC

    limit 3

'''



result_df = pysqldf(query)

result_df
query = ''' 

    SELECT "Handset Manufacturer", AVG("Dur. (ms)") AS "Avg Session Duration (ms)"

    FROM df

    WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')

    GROUP BY "Handset Manufacturer";



'''

result_df = pysqldf(query)

result_df

query = ''' 

   SELECT

    "Handset Manufacturer",

    SUM("Total UL (Bytes)" + "Total DL (Bytes)") AS "Total Data Volume (Bytes)"

    FROM df

    WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')

    GROUP BY "Handset Manufacturer";

'''

result_df = sqldf(query)

result_df
query = ''' 

    WITH RankedHandsets AS (

    SELECT

        "Handset Manufacturer",

        "Handset Type",

        RANK() OVER (PARTITION BY "Handset Manufacturer" ORDER BY COUNT(*) DESC) AS "Rank"

    FROM df

    WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')

    GROUP BY "Handset Manufacturer", "Handset Type"

    )

    SELECT

        "Handset Manufacturer",

        "Handset Type",

        "Rank"

    FROM RankedHandsets

    WHERE "Rank" <= 5;

'''



result_df = sqldf(query)

result_df
query = ''' 

    SELECT "MSISDN/Number" AS UserIdentifer,

    COUNT(*) AS NumberOfXDRSessions

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY NumberOfXDRSessions DESc;

'''



result_df = pysqldf(query)

result_df
query = '''

    SELECT COUNT(*) AS num_users

    FROM (

        SELECT COUNT(*) AS NumberOfXDRSessions

        FROM df

        GROUP BY "MSISDN/Number"

        HAVING COUNT(*) >= 10

    ) AS subquery

'''



result_df = pysqldf(query)

result_df
query = ''' 

    SELECT

        "MSISDN/Number" AS UserIdentifier,

        SUM("Dur. (ms)") / 1000 AS TotalSessionDurationInSeconds

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalSessionDurationInSeconds DESC;



'''



result_df = pysqldf(query)

result_df
query = ''' 

    SELECT

        CAST(SUBSTRING(CAST("Start" AS TEXT), INSTR(CAST("Start" AS TEXT), ' ') + 1, INSTR(CAST("Start" AS TEXT), ':') - INSTR(CAST("Start" AS TEXT), ' ') - 1) AS INTEGER) AS HourOfDay,

        COUNT(*) AS NumberOfSessions

    FROM df

    WHERE "Start" IS NOT NULL

    GROUP BY HourOfDay

    ORDER BY NumberOfSessions DESC;



'''



result_df = pysqldf(query) 

result_df
query = ''' 

    SELECT

        "MSISDN/Number" AS User,

        COUNT(*) AS SessionCount,

        SUM("Dur. (ms)") AS TotalSessionDuration

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalSessionDuration DESC;



'''



result_df = pysqldf(query)

result_df
result_df = util.get_total_download_for_each_app(df, "Total DL (Bytes)", "Total UL (Bytes)")
query = '''SELECT COUNT(*) AS num_users

    FROM result_df

    WHERE Total >= 100000000;

    '''



pysqldf(query)

util.get_total_download_for_each_app(df, "Social Media DL (Bytes)", "Social Media UL (Bytes)")
util.get_total_download_for_each_app(df, "YouTube DL (Bytes)", "YouTube UL (Bytes)")
util.get_total_download_for_each_app(df, "Netflix DL (Bytes)", "Netflix UL (Bytes)")
util.get_total_download_for_each_app(df, "Google DL (Bytes)", "Google UL (Bytes)")
util.get_total_download_for_each_app(df, "Email DL (Bytes)", "Email UL (Bytes)")
util.get_total_download_for_each_app(df, "Gaming DL (Bytes)", "Gaming UL (Bytes)")
util.get_total_download_for_each_app(df, "Other DL", "Other UL")
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
quantitative_columns = [

        "Avg RTT DL (ms)",                             

        "Avg RTT UL (ms)",   

        "Avg Bearer TP DL (kbps)",

        "Avg Bearer TP UL (kbps)",                          

        "TCP DL Retrans. Vol (Bytes)",                  

        "TCP UL Retrans. Vol (Bytes)",                  

        "DL TP < 50 Kbps (%)",                            

        "50 Kbps < DL TP < 250 Kbps (%)",              

        "250 Kbps < DL TP < 1 Mbps (%)",                  

        "DL TP > 1 Mbps (%)",                             

        "UL TP < 10 Kbps (%)",                           

        "10 Kbps < UL TP < 50 Kbps (%)",                 

        "50 Kbps < UL TP < 300 Kbps (%)",                 

        "UL TP > 300 Kbps (%)",

        "Activity Duration DL (ms)",

        "Activity Duration UL (ms)",                           

        "HTTP DL (Bytes)",                              

        "HTTP UL (Bytes)", 

        "Dur. (ms).1",                            

        "Nb of sec with 125000B < Vol DL",  

        "Nb of sec with 1250B < Vol UL < 6250B",        

        "Nb of sec with 31250B < Vol DL < 125000B",                   

        "Nb of sec with 6250B < Vol DL < 31250B",       

        "Nb of sec with Vol DL < 6250B",                  

        "Nb of sec with Vol UL < 1250B", 

        "Social Media DL (Bytes)",

        "Social Media UL (Bytes)",

        "Youtube DL (Bytes)",

        "Youtube UL (Bytes)",

        "Netflix DL (Bytes)",

        "Netflix UL (Bytes)",

        "Google DL (Bytes)",

        "Google UL (Bytes)",

        "Email DL (Bytes)",

        "Email UL (Bytes)",

        "Gaming DL (Bytes)",

        "Gaming UL (Bytes)",

        "Other DL (Bytes)",

        "Other UL (Bytes)",

        "Total DL (Bytes)",

        "Total UL (Bytes)"

]

df = dc.handle_outliers(df, quantitative_columns)
df.shape
descriptive_stats = df[quantitative_columns].describe()

descriptive_stats
plt.hist(df['Avg RTT UL (ms)'], bins=20, color='blue', alpha=0.7)

plt.xlabel('Average RTT UL (ms)')

plt.ylabel('Frequency')

plt.title('Histogram of Avg RTT DL (ms)')

plt.show()










sns.boxplot(x=df['Avg Bearer TP DL (kbps)'])

plt.xlabel('Average Bearer TP DL (kbps)')

plt.title('Boxplot of Avg Bearer TP DL (kbps)')

plt.show()

plt.figure(figsize=(8, 5))

df['Handset Type'].value_counts().head(10).plot(kind='bar', color='green')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.title('Distribution of Handset Types')

plt.show()

correlation_matrix = df[

    [

        "Social Media DL (Bytes)",

        "Social Media UL (Bytes)",

        "Youtube DL (Bytes)",

        "Youtube UL (Bytes)",

        "Netflix DL (Bytes)",

        "Netflix UL (Bytes)",

        "Google DL (Bytes)",

        "Google UL (Bytes)",

        "Email DL (Bytes)",

        "Email UL (Bytes)",

        "Gaming DL (Bytes)",

        "Gaming UL (Bytes)",

        "Other DL (Bytes)",

        "Other UL (Bytes)",

        "Total DL (Bytes)",

        "Total UL (Bytes)"

]].corr()

correlation_matrix


sns.scatterplot(x='Netflix DL (Bytes)', y='Total DL (Bytes)', data=df)

plt.title('Youtube download vs Total Download')

plt.show()
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = ''' 

        SELECT

            "MSISDN/Number" AS MSISDN,

            COUNT(DISTINCT "Bearer Id") AS SessionFrequency

        FROM df

        GROUP BY "MSISDN/Number"

        ORDER BY SessionFrequency DESC

        LIMIT 10

'''



result_df = pysqldf(query)

result_df
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration

    FROM df

    GROUP BY "MSISDN/Number" 

    ORDER BY SessionDuration DESC

    LIMIT 10;

    ''' 



result_df = pysqldf(query)

result_df

query = ''' 

    SELECT 

        "MSISDN/Number",

        SUM("Total DL (Bytes)") AS TotalDownload,

        SUM("Total UL (Bytes)") AS TotalUpload,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalTraffic DESC

    LIMIT 10;

'''



pysqldf(query)
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration,

        COUNT(DISTINCT "Bearer Id") AS SessionFrequency,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ''' 



aggregated_df = pysqldf(query)

aggregated_df.tail()
scaler = MinMaxScaler()

columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]

customer_id = aggregated_df['MSISDN/Number']

transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)

df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
processed_df = dc.handle_outliers(normalized_data, columns_to_normalize)
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(processed_df['SessionDuration'], processed_df['SessionFrequency'], processed_df['TotalTraffic'], c='blue', marker='o')



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
selected_columns = columns_to_normalize

X = processed_df[selected_columns]

k = 3



kmeans = KMeans(n_clusters=3, random_state=0) 

kmeans.fit(X)



processed_df['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green'}



for cluster_label, color in cluster_colors.items():

    cluster_data = processed_df[processed_df['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['SessionDuration'], 

        cluster_data['SessionFrequency'], 

        cluster_data['TotalTraffic'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.write_to_db as wd

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = ''' 

        SELECT

            "MSISDN/Number" AS MSISDN,

            COUNT(DISTINCT "Bearer Id") AS SessionFrequency

        FROM df

        GROUP BY "MSISDN/Number"

        ORDER BY SessionFrequency DESC

        LIMIT 10

'''



result_df = pysqldf(query)

result_df
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration

    FROM df

    GROUP BY "MSISDN/Number" 

    ORDER BY SessionDuration DESC

    LIMIT 10;

    ''' 



result_df = pysqldf(query)

result_df

query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration

    FROM df

    GROUP BY "MSISDN/Number" 

    ORDER BY SessionDuration DESC;

    ''' 



result_df = pysqldf(query)

result_df['SessionDuration'].median()
query = ''' 

    SELECT 

        "MSISDN/Number",

        SUM("Total DL (Bytes)") AS TotalDownload,

        SUM("Total UL (Bytes)") AS TotalUpload,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalTraffic DESC

    LIMIT 10;

'''



pysqldf(query)
query = ''' 

    SELECT 

        "MSISDN/Number",

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ORDER BY TotalTraffic DESC

'''



pysqldf(query)['TotalTraffic'].median()
query = '''

    SELECT 

        "MSISDN/Number",

        SUM("Dur. (ms)") AS SessionDuration,

        COUNT(DISTINCT "Bearer Id") AS SessionFrequency,

        (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic

    FROM df

    GROUP BY "MSISDN/Number"

    ''' 



aggregated_df = pysqldf(query)

aggregated_df.tail()
aggregated_df = dc.handle_outliers(aggregated_df, ["SessionDuration","SessionFrequency","TotalTraffic"])
scaler = MinMaxScaler()

columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]

customer_id = aggregated_df['MSISDN/Number']

transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)

df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')cases = 707,443



Total deaths = 3,891

ax.set_zlabel('TotalTraffic')



plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 3



kmeans = KMeans(n_clusters=3, random_state=0, n_init=25) 

kmeans.fit(X)



df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green'}



for cluster_label, color in cluster_colors.items():

    cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['SessionDuration'], 

        cluster_data['SessionFrequency'], 

        cluster_data['TotalTraffic'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
grouped_df = (df_normalized

                .groupby('Cluster')[["SessionDuration", "SessionFrequency", "TotalTraffic"]]

                .agg(['min', 'max', 'mean', 'sum']))



grouped_df

wcss = []

X = df_normalized.iloc[:, [1, 3]]



for i in range(1, 11):

    kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)

    kmeans.fit(X)

    wcss.append(kmeans.inertia_)



plt.plot(range(1, 11), wcss)

plt.xlabel('Number of clusters')

plt.ylabel('WCSS')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 7



kmeans = KMeans(n_clusters=k, random_state=0, n_init=25) 

kmeans.fit(X)



df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green', 3:'black', 4:'yellow', 5:'brown', 6:'purple'}



for cluster_label, color in cluster_colors.items():

    cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['SessionDuration'], 

        cluster_data['SessionFrequency'], 

        cluster_data['TotalTraffic'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
wd.write_data(df_normalized, 'user_engagement')
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.write_to_db as wd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
def helper(field1 : str, field2 : str) -> pd.DataFrame:

    query = f''' 

        SELECT

            "MSISDN/Number" AS CustomerID,

            AVG("{field1}") AS AvgDL,

            AVG("{field2}") AS AvgUL,

            (

                AVG("{field1}") + 

                AVG("{field2}")

            ) / 2 AS Avg

        FROM df

        GROUP BY "MSISDN/Number";

    '''

    return pysqldf(query)
avg_retransmission = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")

avg_retransmission
avg_rtt = helper("Avg RTT DL (ms)", "Avg RTT UL (ms)")

avg_rtt
query = ''' 

    SELECT

        "MSISDN/Number" AS User_MSISDN,

        "Handset Type" AS HandsetType,

        COUNT(*) AS HandsetTypeCount

    FROM df

    GROUP BY "MSISDN/Number", "Handset Type";



'''



count_handset = pysqldf(query) 

count_handset

avg_throughput = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")

avg_throughput
def helper(field, order):

    query = f''' 

        SELECT 

            "{field}",

            COUNT("{field}") AS "Frequency"

        FROM df

        GROUP BY "{field}"

        ORDER BY "Frequency" {order} 

        LIMIT 10;

    '''



    return pysqldf(query)
top_tcp = helper("TCP DL Retrans. Vol (Bytes)", "DESC")

top_tcp
bottom_tcp = helper("TCP DL Retrans. Vol (Bytes)", "ASC")

bottom_tcp
top_rtt = helper("Avg RTT DL (ms)", "DESC")

top_rtt
bottom_rtt = helper("Avg RTT DL (ms)", "ASC")

bottom_rtt
top_throughput = helper("Avg Bearer TP DL (kbps)", "Desc")

top_throughput
bottom_throughput = helper("Avg Bearer TP DL (kbps)", "ASC")

bottom_throughput
def helper(field1, field2):

    query = f'''

        SELECT

            "Handset Type" AS HandsetType,

            AVG("{field1}" + "{field2}") / 2 AS Average

        FROM df

        GROUP BY "Handset Type"

        ORDER BY Average DESC;

    '''

    return pysqldf(query)
avg_throughput_per_handset = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")

avg_throughput_per_handset
average_tcp_per_handset = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")

average_tcp_per_handset
query = ''' 

    SELECT 

        "MSISDN/Number" AS CustomerID,

        (AVG("TCP DL Retrans. Vol (Bytes)") + AVG("TCP UL Retrans. Vol (Bytes)")) / 2 AS AvgTCP,

        (AVG("Avg RTT DL (ms)") + AVG("Avg RTT UL (ms)")) / 2 AS AvgRTT,

        (AVG("Avg Bearer TP DL (kbps)") + AVG("Avg Bearer TP UL (kbps)")) / 2 AS AvgThroughput

    FROM df

    GROUP BY CustomerID

'''



agg_df = pysqldf(query)

agg_df.head()
agg_df = dc.handle_outliers(agg_df, ["AvgTCP", "AvgRTT","AvgThroughput"])
scaler = MinMaxScaler()

columns_to_normalize = ["AvgTCP", "AvgRTT","AvgThroughput"]



customer_id = agg_df['CustomerID']

transformed_data = scaler.fit_transform(agg_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)



df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



ax.scatter(df_normalized["AvgTCP"], df_normalized["AvgRTT"], df_normalized["AvgThroughput"], c='blue', marker='o')



ax.set_xlabel("AvgTCP")

ax.set_ylabel("AvgRTT")

ax.set_zlabel("AvgThroughput")



plt.title('3D Scatter Plot of AvgTCP, AvgRTT, and AvgThroughput')

plt.show()
wcss = []

X = df_normalized.iloc[:, [1, 3]]



for i in range(1, 11):

    kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)

    kmeans.fit(X)

    wcss.append(kmeans.inertia_)



plt.plot(range(1, 11), wcss)

plt.xlabel('Number of clusters')

plt.ylabel('WCSS')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 3



kmeans = KMeans(n_clusters=k, random_state=0, n_init=15) 

kmeans.fit(X)



df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')



cluster_colors = {0: 'red', 1: 'blue', 2:'green'}





for cluster_label, color in cluster_colors.items():

    cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]

    ax.scatter(

        cluster_data['AvgTCP'], 

        cluster_data['AvgRTT'], 

        cluster_data['AvgThroughput'], 

        label=f'Cluster {cluster_label}',

        color=color

        )



ax.set_xlabel('AvgTCP')

ax.set_ylabel('AvgRTT')

ax.set_zlabel('AvgThroughput')



plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
grouped_df = (df_normalized

                .groupby('Cluster')[["AvgTCP", "AvgRTT", "AvgThroughput"]]

                .agg(['min', 'max', 'mean', 'sum']))



grouped_df
wd.write_data(df_normalized, 'user_experience')
import os, sys

import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.linear_model import LinearRegression

from sklearn.cluster import KMeans

from sklearn.model_selection import train_test_split

from sklearn.metrics import euclidean_distances

from mpl_toolkits.mplot3d import Axes3D



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.write_to_db as wd

import scripts.utils as util
experience_df = rd.read_data(table_name='user_experience')

engagement_df = rd.read_data(table_name='user_engagement')
pysqldf = lambda q: sqldf(q, globals())
experience_df.head()
engagement_df.head()
merged_df = engagement_df.merge(experience_df, left_on='MSISDN/Number', right_on="CustomerID", how='inner')

merged_df.head()
experience_columns = ["AvgTCP","AvgRTT","AvgThroughput"]

engagement_columns = ["SessionDuration","SessionFrequency","TotalTraffic"]
k = 3

X_eng = merged_df[engagement_columns]

X_exp = merged_df[experience_columns]
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)

cluster_labels = kmeans.fit_predict(X_eng)

merged_df['cluster'] = cluster_labels
query = ''' 

    SELECT 

        cluster,

        AVG(("SessionDuration"+"SessionFrequency"+"TotalTraffic") / 3) AS AVG 

    FROM merged_df

    GROUP BY cluster;

'''



grouped = pysqldf(query)

less_engaged_cluster = grouped['AVG'].idxmin()

less_engaged_cluster
less_engaged_cluster_center = kmeans.cluster_centers_[less_engaged_cluster]



distances = euclidean_distances(X_eng, [less_engaged_cluster_center]).flatten()

merged_df['EngagementScore'] =  distances

merged_df.head()
query = ''' 

    SELECT 

        cluster,

        AVG(("AvgTCP" + "AvgRTT" + "AvgThroughput") / 3) AS AVG 

    FROM merged_df

    GROUP BY cluster;

'''



grouped = pysqldf(query)

worst_experience_cluster = grouped['AVG'].idxmin()

worst_experience_cluster
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)

cluster_labels = kmeans.fit_predict(X_exp)



worst_experience_cluster_cluster_center = kmeans.cluster_centers_[worst_experience_cluster]



distances = euclidean_distances(X_eng, [worst_experience_cluster_cluster_center]).flatten()

merged_df['ExperienceScore'] =  distances

merged_df.head()
merged_df['SatisfactionScore'] = (merged_df['EngagementScore'] + merged_df['ExperienceScore']) / 2

merged_df.head()
query = ''' 

    SELECT 

        "MSISDN/Number",

        SatisfactionScore

    FROM merged_df

    ORDER BY SatisfactionScore DESC

    LIMIT 10

'''



pysqldf(query)
merged_df['SatisfactionScore'].describe()

bins = np.arange(0, 1.4142, 0.35355)

labels = ["Low Satisfaction", "Medium Satisfaction", "High Satisfaction"]



merged_df['SatisfactionGroup'] = pd.cut(merged_df['SatisfactionScore'], bins=bins, labels=labels, include_lowest=True)



group_counts = merged_df['SatisfactionGroup'].value_counts()



plt.pie(group_counts, labels=group_counts.index, autopct='%1.1f%%', startangle=90)

plt.title('Satisfaction Score Distribution')



plt.show()
bins
features = engagement_columns + experience_columns

X = merged_df[features]

y = merged_df.SatisfactionScore



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression() 

model.fit(X_train, y_train)



y_pred  = model.predict(X_test)

selected_columns = ["EngagementScore", "SatisfactionScore"]

X = merged_df[selected_columns]

k = 2



kmeans = KMeans(n_clusters=k, random_state=0, n_init=15) 

kmeans.fit(X)



merged_df['cluster'] = kmeans.labels_
cluster_colors = {0: 'red', 1: 'blue'}

for cluster_label, color in cluster_colors.items():

    cluster_df = merged_df[merged_df["cluster"] == cluster_label]

    plt.scatter(cluster_df["EngagementScore"], cluster_df["ExperienceScore"], c=color)





plt.xlabel("EngagementScore")

plt.ylabel("ExperienceScore")

plt.title("'2D Scatter Plot'of EngagementScore vs ExperienceScore")



plt.show()

query = ''' 

    SELECT 

        cluster,

        AVG("SatisfactionScore") As "Average Satisfaction"

    FROM merged_df 

    GROUP BY cluster;

'''

avg_satsifaction = pysqldf(query)

avg_satsifaction
query = ''' 

    SELECT 

        cluster,

        AVG("ExperienceScore") AS "Average Experience" 

    FROM merged_df 

    GROUP BY cluster 

'''



avg_experience = pysqldf(query)

avg_experience
df = pd.DataFrame({

    "UserID": merged_df["CustomerID"],

    "SatisfactionScore": merged_df["SatisfactionScore"],

    "ExperienceScore": merged_df["ExperienceScore"],

    "EngagementScore": merged_df["EngagementScore"]

})



wd.write_data(df, "satisfaction_score")
from doctest import debug_script
from json import load
from pydoc import describe
from quopri import decodestring
import pandas as pd
import psycopg2

def loadData():
    conn = psycopg2.connect(
        host="localhost",
        port="5432",
        database="tellco",
        user="postgres",
        password="root"
    )
        cursor = conn.cursor('my_cursor_name', withhold=True)
    cursor.execute("SELECT * FROM xdr_data")
            batch_size = 1000
    rows = cursor.fetchmany(batch_size)
    
    data = [] 

    while rows:
                data += rows
                rows = cursor.fetchmany(batch_size)
        if not rows:
            break
        if len(data) >= 50000:
            columns = [desc[0] for desc in cursor.description]
            df = pd.DataFrame(data, columns=columns)
            return df

    columns = [desc[0] for desc in cursor.description]
    cursor.close()
    conn.close()  
    df = pd.DataFrame(data, columns=columns) 
    print(len(data))
    
    return (df)
import numpy as np
import pandas as pd
from scipy import stats

class PreProcess:
    def __init__(self, data):
        self.data = data

    def remove_duplicate(self):
        return self.data.drop_duplicates()

    def remove_missing(self):
        '''
        This function removes all columns 
        with missing values of above 30%
        '''
        self.data = self.data.dropna(thresh=len(self.data)*0.7, axis=1)
        return self.data

    def convert_to_timestamp(self, column_name):
        '''
        This function converts the date column to timestamp
        '''
        self.data[column_name] = pd.to_datetime(self.data[column_name])
        return self.data
    
    def impute_mean_numeric(self):
        '''
        This function imputes mean values to missing values in numeric columns
        '''
        numeric_columns = self.data.select_dtypes(include=['float64', 'number']).columns
        self.data.loc[:, numeric_columns] = self.data.loc[:, numeric_columns].apply(lambda x: x.fillna(x.mean()))
        return self.data

    
    def impute_mean_datetime(self):
        '''
        This function imputes mean values to missing values in datetime columns
        '''
        datetime_columns = self.data.select_dtypes(include=['datetime64']).columns

        for col in datetime_columns:
            mean_timestamp = self.data[col].mean().timestamp()
            self.data.loc[:, col] = self.data[col].fillna(pd.to_datetime(mean_timestamp, unit='s'))

        return self.data
    
    def impute_mode_categorical(self):
        '''
        This function imputes mode values to missing values in categorical columns
        '''
        categorical_columns = self.data.select_dtypes(include=['object']).columns

        for col in categorical_columns:
            self.data.loc[:, col] = self.data[col].fillna(self.data[col].mode()[0])

        return self.data

    def remove_outliers(self):
        '''
        This function removes outliers from the dataset for numeric fields
        '''
        numeric_columns = self.data.select_dtypes(include=['float64', 'number']).columns
        z_scores = np.abs(stats.zscore(self.data[numeric_columns]))
        filtered_data = self.data[(z_scores < 3).all(axis=1)]
        self.data = filtered_data
        return filtered_data
%load_ext autoreload

%autoreload 2

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import os

import sys
import warnings

warnings.filterwarnings('ignore')

rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.preprocessing import PreProcess

from src.overview import Overview

from src.outlier import Outlier

from db_connection.database import Database
conn = Database(host='localhost',port=5432,user='basilel',dbname='telecom',password='Basi')

conn.connect()
query = "SELECT * FROM xdr_data;"

df_data = pd.read_sql(query, conn.connection)

df_data.to_csv('../data/uncleanData.csv', index=False)
df = df_data.copy()
df.head()
df.shape


print(f" There are {df.shape[0]} rows and {df.shape[1]} columns")
df.describe()

df.columns.tolist()

df.isna().sum()
df.info()
duplicated_entries = df[df.duplicated()]

duplicated_entries.shape
df.hist(bins=80, figsize=(30,25))
overview = Overview(df)
percent_missing = overview.percent_missing(df)
df['MSISDN/Number'].plot(kind='density')

print('This distribution has skew', df['MSISDN/Number'].skew())

print('This distribution has kurtosis', df['MSISDN/Number'].kurt())
preprocess = PreProcess(df)
df = preprocess.clean_feature_name(df)
df.columns
df.dtypes
%load_ext autoreload

%autoreload 2

df = preprocess.drop_duplicates(df)
df_c, df_before_filling, missing_cols = preprocess.drop_variables(df)

print(missing_cols)

cols, df_single, num_cols = preprocess.fill_numerical_variables(df)
print(len(missing_cols))

df_cols, df_single, cat_cols = preprocess.fill_categorical_variables(df, cols, num_cols, df_single)

df_single.columns[df_single.isnull().mean() > 0] 
df_single.info()
df_single.isna().sum().nlargest(10)
df_single.head()
df_single.columns
df.columns

















df.to_csv('../data/cleaned_data2.csv', index=False)
import os

import sys

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

import numpy as np
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src import utils

from src.plots import Plot

from src.preprocessing import PreProcess

from src.overview import Overview
df = pd.read_csv('../data/cleaned_data_outliers.csv')
plots = Plot()

preprocess = PreProcess(df)

overview = Overview(df)

handset_counts = df['Handset Type'].value_counts().head(10)

plots.plot_bar(x=handset_counts.values, y=handset_counts.index, xlabel='Number of Users', title='Top 10 Handsets by Users',ylabel='Handset Type')
manufacturer_counts = df['Handset Manufacturer'].value_counts().head(3)

plots.plot_pie(manufacturer_counts, title='Top 3 Handset Manufacturers by Users')
top_manufacturers = df['Handset Manufacturer'].value_counts().head(3).index




df_top_manufacturers = df[df['Handset Manufacturer'].isin(top_manufacturers)]




top_handsets_per_manufacturer = (

    df_top_manufacturers

    .groupby('Handset Manufacturer')['Handset Type']

    .value_counts()

    .groupby(level=0, group_keys=False)

    .nlargest(5)

    .reset_index(name='Count')

)



plots.plot_bar(x=top_handsets_per_manufacturer['Count'], y=top_handsets_per_manufacturer['Handset Type'], xlabel='Number of Users', title='Top 5 Handsets per Top 3 Handset Manufacturers',ylabel='Handset Type',palette='viridis')


xDR_sess_agg = df['MSISDN/Number'].value_counts()

print(xDR_sess_agg.nlargest(10))
df.groupby('MSISDN/Number')['Dur. (ms)'].sum().nlargest(10)

df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().nlargest(10, 'Total DL (Bytes)')
df["social_media"] = df["Social Media DL (Bytes)"] + df['Social Media UL (Bytes)']

df["google"] = df["Google DL (Bytes)"] + df["Google UL (Bytes)"]

df['email'] = df["Email DL (Bytes)"] + df["Email UL (Bytes)"]

df['youtube'] = df["Youtube DL (Bytes)"] + df["Youtube UL (Bytes)"]

df['netflix'] = df["Netflix DL (Bytes)"] + df["Netflix UL (Bytes)"]

df["gaming"] = df["Gaming DL (Bytes)"] + df["Gaming UL (Bytes)"]

df['other'] = df["Other DL (Bytes)"]+df["Other UL (Bytes)"]

df['total_data'] = df['Total DL (Bytes)'] + df['Total UL (Bytes)']


quantitative_vars = df.select_dtypes(include=['float64', 'int64'])




dispersion_parameters = pd.DataFrame({

    'Mean': quantitative_vars.mean(),

    'Std Deviation': quantitative_vars.std(),

    'Min': quantitative_vars.min(),

    '25th Percentile (Q1)': quantitative_vars.quantile(0.25),

    'Median (Q2)': quantitative_vars.median(),

    '75th Percentile (Q3)': quantitative_vars.quantile(0.75),

    'Max': quantitative_vars.max(),

    'IQR': quantitative_vars.quantile(0.75) - quantitative_vars.quantile(0.25)

})




print(dispersion_parameters)

correlation_coefficients = quantitative_vars.corr(method='pearson')

correlation_coefficients
plots.plot_hist(df=df, column='Dur. (ms)', title='Distribution of Session Durations')
plots.plot_hist(df=df, column='Bearer Id', title='Distribution of Total Data')
plots.plot_hist(df=df, column='total_data',title='Distribution of Total Data')
plots.plot_hist(df=df, column='social_media', title='Distribution of Social Media Data')
plots.plot_hist(df=df, column='google', title='Distribution of Google Data')
plots.plot_hist(df=df, column='email', title='Distribution of Email Data')
plots.plot_hist(df=df, column='youtube', title='Distribution of Youtube Data')
plots.plot_hist(df=df, column='gaming', title='Distribution of Gaming Data')

plots.plot_scatter(df, df['google'], df['total_data'], 'Total Data Vs. Google', None, None)

plots.plot_scatter(df,  df['google'], df['Dur. (ms)'], 'Content Duration Vs. Google', None, None)

plots.plot_scatter(df,  df['youtube'], df['Dur. (ms)'], 'Content Duration Vs. YouTube', None, None)

plots.plot_scatter(df,  df['netflix'], df['Dur. (ms)'], 'Content Duration Vs. Netflix', None, None)

plots.plot_scatter(df, df['netflix'], df['total_data'], 'Total Data Vs. Netflix', None, None)

plots.plot_scatter(df, df['gaming'], df['Dur. (ms)'], 'Total Duration Vs. Gaming', None, None)
sns.relplot(data=df, x="Handset Manufacturer", y="google", hue=None, kind="line")
sns.relplot(x="total_data", y="google", hue="Handset Manufacturer", data=df);
preprocess = PreProcess(df)
df2 = df.copy()

df3 = pd.read_csv('../data/cleaned_data2.csv')
feature_to_use = df3[['msisdn/number', 'social_media', 'google', 'email', 'youtube', 'netflix',

                                 'gaming',  'other', 'total_data', 'handset_manufacturer']]
feature_to_use.head()
total_count_app = pd.DataFrame()

google_total = feature_to_use.sum()[1]

email_total = feature_to_use.sum()[2]

youtube_total = feature_to_use.sum()[3]

netflix_total = feature_to_use.sum()[4]

gaming_total = feature_to_use.sum()[5]

other_total = feature_to_use.sum()[6]
total_count_app['app'] = ['google', 'email', 'youtube', 'netflix', 'gaming', 'other']

total_count_app['total'] = [google_total, email_total, youtube_total, netflix_total, gaming_total,  other_total]
total_count_app.head(6)
total_count_app.head()
total_count_app
plots.plot_bar(x=total_count_app['app'], y=total_count_app['total'], title="Total data usage per app", xlabel="Application", ylabel="Total data volume")

df.columns
var_tr = df[['MSISDN/Number', 'Dur. (ms)', 'total_data']]
var_tr_agg = var_tr.groupby('MSISDN/Number').agg({'Dur. (ms)':'sum', 'total_data': 'sum'})
var_tr_agg.shape
var_tr_agg_decile = overview.get_decile(var_tr_agg, 'Dur. (ms)', 5, ['First Decile', 'Second Decile', 'Third Decile', 'Fourth Decile', 'Fifth Decile'])
var_tr_agg_decile.head()
plots.plot_bar(x=var_tr_agg_decile['deciles'], y=var_tr_agg_decile['total_data'] , title="Total data usage per decile", xlabel="Decile", ylabel="Total data volume")
corr_analysis = df3[['msisdn/number','social_media', 'google',

                    'email', 'youtube', 'netflix', 'gaming', 'other']]

corr_analysis_agg = corr_analysis.groupby('msisdn/number').agg({'social_media':'sum', 'google':'sum', 'email':'sum', 'youtube':'sum', 'netflix':'sum', 'gaming':'sum', 'other':'sum'})

plots.plot_heatmap(corr_analysis_agg, "Correlation between apps")

corr_analysis_agg.corr()

num_cols = df.select_dtypes(include=np.number).columns

cat_cols = list(set(df.columns) - set(num_cols))
df[num_cols].columns
num_final = [col for col in num_cols if col not in ['msisdn/number','bearer_id', 'start_ms', 'end_ms', 'imsi', 'imei']]
len(num_final)
len(df.columns)
def clean_dataset(df):

    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"

    df.dropna(inplace=True)

    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)

    return df[indices_to_keep].astype(np.float64)
df2 = df.copy()
df2 = clean_dataset(df2[num_final])
df2.shape
from sklearn.preprocessing import StandardScaler



scaler = StandardScaler()

scaler.fit(df2)

df_scaled = scaler.transform(df2)
df_scaled.shape
from sklearn.decomposition import PCA

pca_44 = PCA(n_components=44, random_state=42)

pca_44.fit(df_scaled)
x_pca_44 = pca_44.transform(df_scaled)
x_pca_44.shape 
sum(pca_44.explained_variance_ratio_)
np.cumsum(pca_44.explained_variance_ratio_ * 100)
plt.plot(np.cumsum(pca_44.explained_variance_ratio_))

plt.xlabel('Number of Components')

plt.ylabel('Cumulative Explained Variance')
print("Variance explained by first Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[0]))

print("Variance explained by  2 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[1]))

print("Variance explained by  3 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[2]))

print("Variance explained by  10 Principal components: {}".format( np.cumsum(pca_44.explained_variance_ratio_ * 100)[9]))

pca_65 = PCA(n_components=0.65, random_state=42)

pca_65.fit(df_scaled)

df_pca_95 = pca_65.transform(df_scaled)
df_pca_95.shape
import os

import sys

import numpy as np

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

from kneed import KneeLocator
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.plots import Plot
df = pd.read_csv('../data/cleaned_data_outliers.csv')
df.head(5)

column_name = 'msisdn/number'

value_to_match = 33663706799.0

mask = df[column_name] == value_to_match

df = df[~mask]
df.columns
df["social_media"] = df["social_media_dl_(bytes)"] + df['social_media_ul_(bytes)']

df["google"] = df["google_dl_(bytes)"] + df["google_ul_(bytes)"]

df['email'] = df["email_dl_(bytes)"] + df["email_ul_(bytes)"]

df['youtube'] = df["youtube_dl_(bytes)"] + df["youtube_ul_(bytes)"]

df['netflix'] = df["netflix_dl_(bytes)"] + df["netflix_ul_(bytes)"]

df["gaming"] = df["gaming_dl_(bytes)"] + df["gaming_ul_(bytes)"]

df['other'] = df["other_dl_(bytes)"]+df["other_ul_(bytes)"]

df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
df = df.rename( columns={'bearer_id': 'sessions'})
data_aggregate = df.groupby('msisdn/number').agg({'sessions': 'count', 'dur._(ms)': 'sum', 'total_data': 'sum'})
data_aggregate.head()

sessions = data_aggregate.nlargest(10, "sessions")['sessions']

duration = data_aggregate.nlargest(10, "dur._(ms)")['dur._(ms)']

total_data = data_aggregate.nlargest(10, "total_data")['total_data']
sesssions_df = pd.DataFrame()

sesssions_df['User_ID'] = sessions.index

sesssions_df['Sessions'] = sessions.values

durations_df = pd.DataFrame()

durations_df['User_ID'] = duration.index

durations_df['duration'] = duration.values

total_data_df = pd.DataFrame()

total_data_df['User_ID'] = total_data.index

total_data_df['total_data'] = total_data.values
durations_df.head()
f, axes = plt.subplots(1, 3, figsize=(25, 5))

ax1 = sns.barplot(data=sesssions_df, x='User_ID', orient='v', y='Sessions', ax=axes[0])

ax2 = sns.barplot(data=durations_df, x='User_ID',orient='v', y='duration', ax=axes[1])

ax3 = sns.barplot(data=total_data_df, x='User_ID',orient='v', y='total_data', ax=axes[2])

ax1.set_xticklabels(ax1.xaxis.get_majorticklabels(), rotation=45)

ax2.set_xticklabels(ax2.xaxis.get_majorticklabels(), rotation=45)

ax3.set_xticklabels(ax3.xaxis.get_majorticklabels(), rotation=45)

plt.plot()

from sklearn.preprocessing import Normalizer

from sklearn.cluster import KMeans
normalizer = Normalizer()
columns = ['sessions','dur._(ms)','total_data']

selected_columns = data_aggregate[columns]

normalized_data = normalizer.fit_transform(selected_columns)
normalized_user_data = pd.DataFrame(normalized_data, columns=columns)

normalized_user_data.head(5)
normalized_user_data.describe()
plt.figure(figsize=(15,6))

plt.subplot(1,2,1)

sns.violinplot(y=data_aggregate["dur._(ms)"])


plt.subplot(1,2,2)

sns.violinplot(y=data_aggregate["sessions"])


plt.show()

kmeans = KMeans(init="random",n_clusters=3,n_init=10,max_iter=300,random_state=42)

label = kmeans.fit_predict(normalized_user_data)

centroids = kmeans.cluster_centers_
lables_unique = np.unique(label)


plt.figure(figsize=(10,5))

plt.title('User K-Means Classification with 3 Groups (Standardized Data)')

for i in lables_unique:

    plt.scatter(normalized_user_data[label == i].iloc[:,0] , normalized_user_data[label == i].iloc[:,1], marker='o', label = i)

plt.scatter(centroids[:,0] , centroids[:,1],centroids[:,2], marker='x', color = 'black')

plt.legend()

plt.show()
normalized_user_data.head()
clustered_Df = pd.DataFrame()

clustered_Df['data_index'] = normalized_user_data.index.values

clustered_Df['cluster'] = kmeans.labels_
clustered_Df.cluster.value_counts()
u_eng = data_aggregate.copy()
u_eng.insert(0, 'cluster', kmeans.labels_)

u_eng.head(5)
cluster1 = u_eng[u_eng["cluster"]==0]

cluster1.describe()
cluster2 = u_eng[u_eng["cluster"] == 1]

cluster2.describe()
cluster3 = u_eng[u_eng["cluster"] == 2]

cluster3.describe()
cluster3.sessions.count()
data = [cluster1.sessions.count(), cluster2.sessions.count(), cluster3.sessions.count()]

keys = ['Cluster 1', 'Cluster 2', 'Cluster 3']


plot = Plot()

plot.plot_pie(data=data, label=keys, title="Cluster Distribution Count")
user_app_usage = df.groupby(

    'msisdn/number').agg({ 'social_media': 'sum', 'gaming': 'sum', 'youtube': 'sum', 'netflix': 'sum', 'google': 'sum', 'email': 'sum', 'other': 'sum'})

user_app_usage.reset_index(inplace=True)


user_app_usage = user_app_usage.drop('msisdn/number', axis=1)

user_app_usage
social_media = user_app_usage.nlargest(10, "social_media")['social_media']

gaming = user_app_usage.nlargest(10, "gaming")['gaming']

youtube = user_app_usage.nlargest(10, "youtube")['youtube']

netflix = user_app_usage.nlargest(10, "netflix")['netflix']

google = user_app_usage.nlargest(10, "google")['google']

email = user_app_usage.nlargest(10, "email")['email']

other = user_app_usage.nlargest(10, "other")['other']

social_media_df = pd.DataFrame()

social_media_df['User_ID'] = social_media.index

social_media_df['social_media'] = social_media.values

gaming_df = pd.DataFrame()

gaming_df['User_ID'] = gaming.index

gaming_df['gaming'] = gaming.values

youtube_df = pd.DataFrame()

youtube_df['User_ID'] = youtube.index

youtube_df['youtube'] = youtube.values



netflix_df = pd.DataFrame()

netflix_df['User_ID'] = netflix.index

netflix_df['netflix'] = netflix.values

google_df = pd.DataFrame()

google_df['User_ID'] = google.index

google_df['google'] = google.values

email_df = pd.DataFrame()

email_df['User_ID'] = email.index

email_df['email'] = email.values

others_df = pd.DataFrame()

others_df['User_ID'] = other.index

others_df['other'] = other.values
f, axes = plt.subplots(2, 4, figsize=(25, 15))

ax1 = sns.barplot(data=social_media_df, x='User_ID', orient='v', y='social_media', ax=axes[0,0], palette='Blues_d')

ax2 = sns.barplot(data=gaming_df, x='User_ID',orient='v', y='gaming', ax=axes[0,1], palette='Blues_d')

ax3 = sns.barplot(data=youtube_df, x='User_ID',orient='v', y='youtube', ax=axes[0,2], palette='Blues_d')

ax4 = sns.barplot(data=netflix_df, x='User_ID',orient='v', y='netflix', ax=axes[0,3], palette='Blues_d')

ax5 = sns.barplot(data=google_df, x='User_ID',orient='v', y='google', ax=axes[1,0], palette='Blues_d')

ax6 = sns.barplot(data=email_df, x='User_ID',orient='v', y='email', ax=axes[1,1], palette='Blues_d')

ax7 = sns.barplot(data=others_df, x='User_ID',orient='v', y='other', ax=axes[1,2], palette='Blues_d')

ax1.set_xticklabels(ax1.xaxis.get_majorticklabels(), rotation=45)

ax2.set_xticklabels(ax2.xaxis.get_majorticklabels(), rotation=45)

ax3.set_xticklabels(ax3.xaxis.get_majorticklabels(), rotation=45)

ax4.set_xticklabels(ax4.xaxis.get_majorticklabels(), rotation=45)

ax5.set_xticklabels(ax5.xaxis.get_majorticklabels(), rotation=45)

ax6.set_xticklabels(ax6.xaxis.get_majorticklabels(), rotation=45)

ax7.set_xticklabels(ax7.xaxis.get_majorticklabels(), rotation=45)

plt.plot()

top_used_applications = user_app_usage.sum()
top_used_applications.values
top_3_used = top_used_applications.nlargest(3)
top_3_used
plot.plot_bar(top_3_used, ["Netflix", "Email", "Gaming"], top_3_used.values, "Top 3 Used Applications", "Applications", "Usage Count")
inertias = []

for i in range(1,16):

    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)

    kmeans.fit(normalized_user_data)

    inertias.append(kmeans.inertia_)

plt.figure(figsize = (10,8))

plt.plot(range(1, 16), inertias, marker = 'o', linestyle = '--')

plt.xlabel('Number of Clusters')

plt.ylabel('Inertias')

plt.title('K-means Clustering')

plt.show()
kl = KneeLocator(range(0, 15), inertias, curve="convex", direction="decreasing")

kl.elbow
u_eng.shape
u_eng.to_csv('../data/user_eng.csv')
user_app_usage.to_csv('../data/normalized_eng.csv')
import joblib

joblib.dump(kmeans,'../models/user_engagement.pkl')
%load_ext autoreload

%autoreload 2

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import os

import sys
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.preprocessing import PreProcess

from src.overview import Overview

from src.outlier import Outlier

from src.plots import Plot
df = pd.read_csv('../data/cleaned_data2.csv')
outlier = Outlier(df)
df.head()
df.columns
pl = Plot()
pl.plot_box(df, 'dur._(ms)', 'Total Duration of the xDR (in ms)')
pl.plot_box(df, 'avg_rtt_dl_(ms)', 'Average Round Trip Time measurement Downlink direction (msecond)')
pl.plot_box(df, 'tcp_ul_retrans._vol_(bytes)', 'TCP volume of Uplink packets detected as retransmitted (bytes)')
display(outlier.outlier_overview(df, 'activity_duration_ul_(ms)'))
display(outlier.outlier_overview(df, 'tcp_ul_retrans._vol_(bytes)'))
%load_ext autoreload

%autoreload 2

df.dtypes
num_cols = df.select_dtypes(include=np.number).columns
dlist = ['bearer_id', 'start_ms', 'end_ms', 'imsi', 'msisdn/number', 'imei']
num_cols = [el for el in num_cols if el not in dlist]

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df[col])

    print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
df = outlier.handle_outliers(df, num_cols)

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df[col])

    print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
outlier_columns = []

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df[col])

    if len(sample_outliers) > 0:

        outlier_columns.append(col)
len(outlier_columns)
df_cop = df.copy()
from sklearn.preprocessing import MinMaxScaler



minmax_scaler = MinMaxScaler()




scaled_data = df.copy()



scaled_data.sample(5)
df_cop[outlier_columns].shape 
scaled_data.shape
scaled_data = minmax_scaler.fit_transform(scaled_data[outlier_columns])
scaled_data.shape
outlier_columns = []

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df_cop[col])

    if len(sample_outliers) > 0:

        outlier_columns.append(col)
len(outlier_columns)

for col in num_cols:

    sample_outliers = outlier.calculate_num_outliers_zscore(df_cop[col])

    print(f"Outliers from Z-scores method of {col}", len(sample_outliers))
df_cop[num_cols].boxplot()
pl.plot_box(df_cop, 'dur._(ms)', 'Total Duration of the xDR (in ms) after outlier handling')
df_cop.describe()


df_cop.to_csv('../data/cleaned_data_outliers.csv', index=False)
import sys

import numpy as np
import pandas as pd


class Outlier:
    def __init__(self, df: pd.DataFrame):
        """Initialize the PreProcess class.

        Args:
            df (pd.DataFrame): dataframe to be preprocessed
        """
        try:
            self.df = df
        except Exception:
            sys.exit(1)

    
    def handle_outliers(self, df: pd.DataFrame, cols):
        """Handle outliers in the dataset.

        Args:
            df (pd.DataFrame): a dataframe to be preprocessed

        Returns:
            pd.DataFrame: the dataframe
        """
                        for col in cols:
                        df[col] = [np.log(x) for x in df[col]]
        return df

    def calculate_num_outliers_zscore(self, col):
        """Return the number of outliers for each numerical col.

        Args:
            col (pd.DataFrame): a dataframe to be analyzed
        """
                outliers = []

        thres = 3
        mean = np.mean(col)
        std = np.std(col)
                for i in col:
            z_score = (i-mean)/std
            if (np.abs(z_score) > thres):
                outliers.append(i)
        return outliers  
                        
    def calculate_num_outliers_iqr(self, df, cols):
        """Return the number of outliers for each col.

        Args:
            df (pd.DataFrame): a dataframe to be analyzed
            cols (list): list of columns to analyze
        """
        outliersTot = {}

        for col in cols:
            outliers = []
            df_sorted = df.sort_values(col)[col]
            q1 = np.percentile(df_sorted, 25)
            q3 = np.percentile(df_sorted, 75)

            IQR = q3 - q1
            lwr_bound = q1 - (1.5 * IQR)
            upr_bound = q3 + (1.5 * IQR)

            for i in df_sorted:
                if (i < lwr_bound or i > upr_bound):
                    outliers.append(i)

            outliersTot[col] = outliers

        return outliersTot


    def outlier_overview(self, df, col):
        """Get outlier overview.

        Args:
            df (pd.DataFrame): a dataframe to be analyzed
        """

                upper_limit = df[col].mean() + 3 * df['total_ul_(bytes)'].std()
        lower_limit = df[col].mean() - 3 * df['total_ul_(bytes)'].std()

                return df[~((df[col] < upper_limit) & (df[col] > lower_limit))]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sys


class Plot:
    def __init__(self) -> None:
        pass

    def plot_bar(self, x, y, xlabel,ylabel,title,palette=None) -> None:
        plt.figure(figsize=(12, 6))
        sns.barplot(x=x, y=y, palette="viridis")

        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)

        plt.show()

    def plot_count(self, df: pd.DataFrame, column: str) -> None:
        """Plot the count of the column.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.countplot(data=df, x=column)
        plt.title(f'Distribution of {column}', size=20, fontweight='bold')
        plt.show()


    def plot_heatmap(self, df: pd.DataFrame, title: str, cbar=False) -> None:
        """Plot Heat map of the dataset.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            title (str): title of chart.
        """
                plt.figure(figsize=(12, 7))
        sns.heatmap(df, annot=True, cmap='viridis', vmin=0,
                    vmax=1, fmt='.2f', linewidths=.7, cbar=cbar)
        plt.title(title, size=18, fontweight='bold')
        plt.show()

    def plot_box(self, df: pd.DataFrame, x_col: str, title: str) -> None:
        """Plot box chart of the column.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            x_col (str): column to be plotted.
            title (str): title of chart.
        """
        plt.figure(figsize=(12, 7))
        sns.boxplot(data=df, x=x_col)
        plt.title(title, size=20)
        plt.xticks(rotation=75, fontsize=14)
        plt.show()

    def plot_box_multi(self, df: pd.DataFrame, x_col: str, y_col: str, title: str) -> None:
        """Plot the box chart for multiple column.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.boxplot(data=df, x=x_col, y=y_col)
        plt.title(title, size=20)
        plt.xticks(rotation=75, fontsize=14)
        plt.yticks(fontsize=14)
        plt.show()

    def plot_scatter(self, df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:
        """Plot Scatter chart of the data.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.scatterplot(data=df, x=x_col, y=y_col, hue=hue, style=style)
        plt.title(title, size=20)
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        plt.show()

    def plot_pie(self, data, title, label) -> None:
        """Plot pie chart of the data.

        Args:
            data (list): Data to be plotted.
            labels (list): labels of the data.
            colors (list): colors of the data.
        """
        plt.style.context('seaborn-pastel')
        plt.figure(figsize=(8, 8))
        plt.pie(x=data, labels=label, autopct='%1.1f%%', startangle=140)

        plt.title(title)
        plt.show()

    def plot_hist(self, df: pd.DataFrame, column: str, title: str) -> None:
        """Plot histogram of the data.

        Args:
            df (pd.DataFrame): Dataframe to be plotted.
            column (str): column to be plotted.
        """
        plt.figure(figsize=(12, 7))
        sns.histplot(data=df, x=column, kde=True)
        plt.title(title, size=20)
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        plt.show()
import os

import sys

import numpy as np

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

%matplotlib inline
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.plots import Plot

from src.preprocessing import PreProcess
df = pd.read_csv('../data/cleaned_data_outliers.csv')
column_name = 'msisdn/number'

value_to_match = 33663706799.0

mask = df[column_name] == value_to_match

df = df[~mask]
df.head()
df.columns
df['avg_tcp_retrans'] = df['tcp_dl_retrans._vol_(bytes)'] + df['tcp_ul_retrans._vol_(bytes)']

df['avg_rrt'] = df['avg_rtt_dl_(ms)'] + df['avg_rtt_dl_(ms)']

df['avg_bearer_tp'] = df['avg_bearer_tp_dl_(kbps)'] + df['avg_bearer_tp_ul_(kbps)']

df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
preprocess = PreProcess(df)
agg_data = df.groupby('msisdn/number').agg({'avg_tcp_retrans':sum,'avg_rrt':sum,'avg_bearer_tp':sum, 'total_data':sum})

preprocess.replace_neg_inf_with_mean(agg_data)
sns.set_style('whitegrid')

sns.lmplot(x='avg_tcp_retrans',y='total_data',data=agg_data,palette='coolwarm',aspect=1)
plot = Plot()
plot.plot_scatter(agg_data,x_col='avg_rrt',y_col='total_data',title='Total Data vs Round Trip Time',hue=None,style=None)
plot.plot_scatter(agg_data,x_col='avg_bearer_tp',y_col='total_data',title='Total Data vs Average Bearer Throughput',hue=None,style=None)
column_name = 'avg_rrt'




top_values = agg_data[column_name].nlargest(10)




bottom_values = agg_data[column_name].nsmallest(10)




most_frequent_values = agg_data[column_name].value_counts().head(10)



rrt_result_table = pd.DataFrame({

    'Top Values': top_values.values,  
    'Bottom Values': bottom_values.values,

    'Most Frequent Values': most_frequent_values.index.values,  
    'Frequency': most_frequent_values.values

})

rrt_result_table

plot.plot_pie(rrt_result_table['Frequency'],title='Frequent Round Trip Time',label=rrt_result_table['Most Frequent Values'])

column_name = 'avg_rrt'




top_values = agg_data[column_name].nlargest(10)




bottom_values = agg_data[column_name].nsmallest(10)




most_frequent_values = agg_data[column_name].value_counts().head(10)



result_table = pd.DataFrame({

    'Top Values': top_values.values,  
    'Bottom Values': bottom_values.values,

    'Most Frequent Values': most_frequent_values.index.values,  
    'Frequency': most_frequent_values.values

})

result_table

column_name = 'avg_bearer_tp'




top_values = agg_data[column_name].nlargest(10)




bottom_values = agg_data[column_name].nsmallest(10)




most_frequent_values = agg_data[column_name].value_counts().head(10)



result_table = pd.DataFrame({

    'Top Values': top_values.values,  
    'Bottom Values': bottom_values.values,

    'Most Frequent Values': most_frequent_values.index.values,  
    'Frequency': most_frequent_values.values

})

result_table
manufacturer_agg = df.groupby('handset_type').agg({'avg_tcp_retrans':'mean','avg_rrt':'mean','avg_bearer_tp':'mean', 'total_data':sum})
manufacturer_agg.reset_index(inplace=True)

top_agg = manufacturer_agg.nlargest(10,columns='avg_bearer_tp')

plot.plot_bar(x=top_agg['handset_type'],y=top_agg['avg_bearer_tp'],xlabel='ReTransmission',ylabel='Manufacturer',title='Title')
top_agg = manufacturer_agg.nlargest(10,columns='avg_bearer_tp')

plot.plot_bar(x=top_agg['handset_type'],y=top_agg['avg_bearer_tp'],xlabel='Retransmission',ylabel='Manufacturer',title='Title')
from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans
scaler = StandardScaler()

scaled_features = scaler.fit_transform(agg_data[['avg_tcp_retrans','avg_rrt','avg_bearer_tp']])
kmeans = KMeans(n_clusters=3, random_state=42)

agg_data['Cluster'] = kmeans.fit_predict(scaled_features)
agg_data
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)

centroid_df = pd.DataFrame(cluster_centers, columns=['avg_tcp_retrans','avg_rrt','avg_bearer_tp'])
print("Cluster Centers:")

print(centroid_df)
cluster_counts = agg_data['Cluster'].value_counts()

print("\nCluster Counts:")

print(cluster_counts)
sns.set(style="whitegrid")




plt.figure(figsize=(10, 6))




plt.scatter(agg_data.loc[agg_data['Cluster'] == 0, 'avg_tcp_retrans'],

            agg_data.loc[agg_data['Cluster'] == 0, 'avg_rrt'],

            label='Cluster 0', alpha=0.8, s=50)




plt.scatter(agg_data.loc[agg_data['Cluster'] == 1, 'avg_tcp_retrans'],

            agg_data.loc[agg_data['Cluster'] == 1, 'avg_rrt'],

            label='Cluster 1', alpha=0.8, s=50)




plt.scatter(agg_data.loc[agg_data['Cluster'] == 2, 'avg_tcp_retrans'],

            agg_data.loc[agg_data['Cluster'] == 2, 'avg_rrt'],

            label='Cluster 2', alpha=0.8, s=50)




plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='X', s=200, color='red', label='Centroids')




plt.xlabel('Avg_TCP_Retransmission')

plt.ylabel('Avg_RTT')

plt.title('K-Means Clustering of Telecom Users')




plt.legend()




plt.show()

import joblib
joblib.dump(kmeans, '../models/kmeans_model.pkl')
import joblib
import os

import sys

import numpy as np

import pandas as pd 

import matplotlib.pyplot as plt

import seaborn as sns

%matplotlib inline
rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0,rpath)

from src.plots import Plot

from src.preprocessing import PreProcess

from db_connection.database import Database
kmeans = joblib.load('../models/kmeans_model.pkl')

kmeans
df = pd.read_csv('../data/cleaned_data_outliers.csv')

df['avg_tcp_retrans'] = df['tcp_dl_retrans._vol_(bytes)'] + df['tcp_ul_retrans._vol_(bytes)']

df['avg_rrt'] = df['avg_rtt_dl_(ms)'] + df['avg_rtt_dl_(ms)']

df['avg_bearer_tp'] = df['avg_bearer_tp_dl_(kbps)'] + df['avg_bearer_tp_ul_(kbps)']

df["social_media"] = df["social_media_dl_(bytes)"] + df['social_media_ul_(bytes)']

df["google"] = df["google_dl_(bytes)"] + df["google_ul_(bytes)"]

df['email'] = df["email_dl_(bytes)"] + df["email_ul_(bytes)"]

df['youtube'] = df["youtube_dl_(bytes)"] + df["youtube_ul_(bytes)"]

df['netflix'] = df["netflix_dl_(bytes)"] + df["netflix_ul_(bytes)"]

df["gaming"] = df["gaming_dl_(bytes)"] + df["gaming_ul_(bytes)"]

df['other'] = df["other_dl_(bytes)"]+df["other_ul_(bytes)"]

df['total_data'] = df['total_dl_(bytes)'] + df['total_ul_(bytes)']
column_name = 'msisdn/number'

value_to_match = 33663706799.0

mask = df[column_name] == value_to_match

df = df[~mask]
preprocess = PreProcess(df)
agg_data = df.groupby('msisdn/number').agg({'avg_tcp_retrans':sum,'avg_rrt':sum,'avg_bearer_tp':sum})

preprocess.replace_neg_inf_with_mean(agg_data)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_features = scaler.fit_transform(agg_data[['avg_tcp_retrans','avg_rrt','avg_bearer_tp']])
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)

centroid_df = pd.DataFrame(cluster_centers, columns=['avg_tcp_retrans','avg_rrt','avg_bearer_tp'])
print("Cluster Centers:")

print(centroid_df)
from sklearn.metrics.pairwise import euclidean_distances




distances = euclidean_distances(scaled_features, cluster_centers)




less_engaged_cluster_index = 0




engagement_scores = distances[:, less_engaged_cluster_index]




agg_data['Experience_Score'] = engagement_scores


sns.set(style="whitegrid")




plt.figure(figsize=(10, 6))

sns.histplot(agg_data['Experience_Score'], bins=30, kde=True, color='skyblue')




plt.xlabel('Engagement Score')

plt.ylabel('Frequency')

plt.title('Distribution of Experience Scores')




plt.show()

user_engagement_kmeans = joblib.load('../models/user_engagement.pkl')
df = df.rename( columns={'bearer_id': 'sessions'})
data_aggregate = df.groupby('msisdn/number').agg({'sessions': 'count', 'dur._(ms)': 'sum', 'total_data': 'sum'})

data_aggregate.nlargest(5,'sessions')
scaler = StandardScaler()

scaled_features = scaler.fit_transform(data_aggregate[['sessions','dur._(ms)','total_data']])

cluster_centers = scaler.inverse_transform(user_engagement_kmeans.cluster_centers_)

centroid_df = pd.DataFrame(cluster_centers, columns=['sessions','dur._(ms)','total_data'])
from sklearn.metrics.pairwise import euclidean_distances




distances = euclidean_distances(scaled_features, cluster_centers)




less_engaged_cluster_index = 0




engagement_scores = distances[:, less_engaged_cluster_index]




data_aggregate['Engagement_Score'] = engagement_scores


sns.set(style="whitegrid")




plt.figure(figsize=(10, 6))

sns.histplot(data_aggregate['Engagement_Score'], bins=30, kde=True, color='skyblue')




plt.xlabel('Engagement Score')

plt.ylabel('Frequency')

plt.title('Distribution of Engagement Scores')




plt.show()
agg_data['Engagement_Score'] = data_aggregate['Engagement_Score'] 
agg_data['Satisfaction_Score'] = agg_data['Experience_Score'] + agg_data['Engagement_Score']
satified_customer = agg_data.nlargest(10,'Satisfaction_Score')

plt.figure(figsize=(10, 6))

sns.barplot(x=satified_customer.index, y="Satisfaction_Score", data=satified_customer, palette="Blues_d")

plt.title('Top 10 Satisfied Customers')

plt.xlabel('Customer ID')

plt.ylabel('Satisfaction Score')

plt.xticks(rotation=45)

plt.show()
agg_data

from sklearn.cluster import KMeans




kmeans = KMeans(n_clusters=2, random_state=42)




predictions = kmeans.fit_predict(agg_data[['Experience_Score','Engagement_Score']])

print(predictions)




agg_data['Cluster'] = predictions




agg_data.groupby('Cluster').mean()




plt.scatter(agg_data['Experience_Score'], agg_data['Engagement_Score'], c=agg_data['Cluster'], cmap='rainbow')

plt.show()

cluster_data = agg_data.groupby('Cluster').mean()


plt.figure(figsize=(10, 6))

sns.barplot(x=cluster_data.index, y="Satisfaction_Score", data=cluster_data, palette="Blues_d")

plt.title('Average Satisfaction Score per Cluster')

plt.xlabel('Cluster')

plt.ylabel('Satisfaction Score')

plt.xticks(rotation=45)

plt.show()
import pandas as pd
import numpy as np

class DataWrangler:
    def __init__(self, df):
        self.df = df

    def calculate_null_percentage(self):
        """
        Calculate the percentage of null values in each column of the DataFrame.

        Returns:
        - pd.Series: Percentage of null values for each column.
        """
        total_cells = self.df.size
        total_null_cells = self.df.isnull().sum().sum()
        null_percentage = (total_null_cells / total_cells) * 100
        return null_percentage

    def handle_duplicate_data(self):
        """
        Handle duplicate data in the DataFrame.

        Returns:
        - pd.DataFrame: DataFrame with duplicate data handled.
        """
        self.df.drop_duplicates(inplace=True)
        return self.df

    def replace_outliers_with_mean(self, columns):
        for column in columns:
                        z_scores = np.abs((self.df[column] - self.df[column].mean()) / self.df[column].std())
        
                        self.df[column] = np.where(z_scores > 3, self.df[column].mean(), self.df[column])

    def aggregate_data(self, group_by_columns, aggregation_functions):
        """
        Aggregate data in the DataFrame.

        Parameters:
        - group_by_columns (list): List of columns to group by.
        - aggregation_functions (dict): Dictionary of {column: aggregation function}.

        Returns:
        - pd.DataFrame: Aggregated DataFrame.
        """
        self.df = (
            self.df.groupby(group_by_columns).agg(aggregation_functions).reset_index()
        )
        return self.df

    def handle_categorical_data(self, encoding_method="one-hot", columns=None):
        """
        Handle categorical data in the DataFrame.

        Parameters:
        - encoding_method (str): Method for encoding categorical data ('one-hot', 'label', etc.).
        - columns (list): List of columns to encode.

        Returns:
        - pd.DataFrame: DataFrame with categorical data handled.
        """
        if columns is None:
            columns = self.df.select_dtypes(include="object").columns

        if encoding_method == "one-hot":
            self.df = pd.get_dummies(self.df, columns=columns, drop_first=True)
        elif encoding_method == "label":
                        pass
                return self.df

    def calculate_skewness(self):
        """
        Calculate skewness for numeric columns in a DataFrame.

        Returns:
        - pd.Series: Skewness values for each numeric column.
        """
        numeric_columns = self.df.select_dtypes(include=["float", "int"])
        skewness_values = numeric_columns.apply(lambda x: x.skew()).round(1)
        return skewness_values

    def get_numeric_columns(self):
        """
        Extract numeric columns from a DataFrame.

        Parameters:
        - df (pd.DataFrame): Input DataFrame.

        Returns:
        - pd.DataFrame: DataFrame containing only numeric columns.
        """
        numeric_columns = self.df.select_dtypes(include=["float", "int"])
        return numeric_columns

    def get_object_columns(self):
        """
        Extract object (string) columns from the DataFrame.

        Returns:
        - pd.Index: Index containing object column names.
        """
        object_columns = self.df.select_dtypes(include=["object"]).columns
        return object_columns

    def repl_numeric_columns(self):
        """
        Impute missing values in numeric columns based on skewness.

        Returns:
        - pd.DataFrame: DataFrame with missing values imputed.
        """

        for column_name in self.df:
            column_skew = self.df[column_name].skew().round()
            fill_value = self.df[column_name].median()

            self.df[column_name].fillna(fill_value, inplace=True)

        return self.df

    def calculate_categorical_mode(self, column_name):
        """
        Calculate the mode of a categorical column in a DataFrame.

        Parameters:
        - column_name (str): Name of the categorical column.

        Returns:
        - pd.Series: Mode(s) of the specified column.
        """
        category_mode = self.df[column_name].mode()
        return category_mode
import pandas as pd

import numpy as np

import psycopg2

import seaborn as sb

from sqlalchemy  import create_engine,MetaData, Table

database_name = 'tellcom'

table_name= 'xdr_data'



connection_params = { 

                    "host": "localhost",

                     "user": "postgres",

                     "password": "postgres",

                     "port": "5432",

                     "database": database_name

                   }



engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")




sql_query = 'SELECT * FROM xdr_data'



df = pd.read_sql(sql_query, con= engine)

df.head()

df.shape

sum(df.duplicated())


import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.wrangling import DataWrangler
data_wrangler = DataWrangler(df)
data_wrangler.calculate_null_percentage().round(2)

df.info()

numerical_df = data_wrangler.get_numeric_columns()

numerical_df.head()

numerical_df.info()
numerical_wrangler = DataWrangler(numerical_df)
skewnes_value = numerical_wrangler.calculate_skewness()
skewnes_value




df_numerical = numerical_wrangler.repl_numeric_columns()
df_numerical.head()
col_name = ['Dur. (ms)', 'Total DL (Bytes)', 'Total UL (Bytes)',

                         'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']
numerical_wrangler.replace_outliers_with_mean(col_name)
object_column_names = data_wrangler.get_object_columns()
categorical_df = data_wrangler.df[object_column_names]
categorical_df.head()
df_clean = pd.concat([df_numerical, categorical_df], axis=1)
df_clean.info()
df_clean.dropna(inplace=True)
df_clean.shape
df_clean.head()
df_clean.to_csv('../data/clean_data.csv',index=False)

!jupyter nbconvert <Part_II_Filename>.ipynb --to slides --post serve --no-input --no-prompt
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt
df_clean = pd.read_csv('../data/clean_data.csv')
df_clean.head()

df_clean.describe()


import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.visualizer import Plotter

handset_data = df_clean[['MSISDN/Number', 'Handset Type']]

handset_counts = handset_data['Handset Type'].value_counts().reset_index()

handset_counts.columns = ['Handset Type', 'Count']

top_ten_handsets = handset_counts.head(10)

top_ten_handsets
plotter_instance = Plotter(top_ten_handsets)
!jupyter nbconvert user_overview_analysis.ipynb --to slides --post serve --no-input --no-prompt

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Type', data=top_ten_handsets, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.title('Top Ten Handsets Used by Customers')

plt.show();

handsetM_data = df_clean['Handset Manufacturer']

handsetM_counts = handsetM_data.value_counts().reset_index()

handsetM_counts.columns = ['Handset Manufacturer', 'Count']

top_three_handsets_manf = handsetM_counts.head(3)

top_three_handsets_manf

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Manufacturer', data=top_three_handsets_manf, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Manufacturer')

plt.title('Top Three Handset Manufacturer')

plt.show();

handset_ocurrance_counts = df_clean.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')

top_manufacturers = handset_ocurrance_counts.groupby('Handset Manufacturer')['Count'].sum().nlargest(3).index

top_manufacturer_data = handset_ocurrance_counts[handset_ocurrance_counts['Handset Manufacturer'].isin(top_manufacturers)]

top_5_handsets_per_manufacturer = (

    top_manufacturer_data.groupby('Handset Manufacturer')

    .apply(lambda group: group.nlargest(5, 'Count'))

    .reset_index(drop=True)

)




top_5_handsets_per_manufacturer

sns.set(style="whitegrid")




plt.figure(figsize=(12, 8))

sns.barplot(x='Handset Manufacturer', y='Count', hue='Handset Type', data=top_5_handsets_per_manufacturer)

plt.title('Handset Types Count by Manufacturer')

plt.xlabel('Handset Manufacturer')

plt.ylabel('Count')

plt.xticks(rotation=45, ha='right')  
plt.legend(title='Handset Type', bbox_to_anchor=(1, 1))  
plt.show()












print(df_clean.columns.tolist())

columns_to_aggregate = ['Dur. (ms)', 'Total DL (Bytes)', 'Total UL (Bytes)',

                         'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']

user_aggregated_data = df_clean.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',  
    'Dur. (ms)': 'sum',  
    'Total DL (Bytes)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    'Social Media DL (Bytes)': 'sum', 'Social Media UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum', 'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum', 'Netflix UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum', 'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum', 'Email UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum', 'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum', 'Other UL (Bytes)': 'sum'

})






user_aggregated_data
user_aggregated_data['Session duration (s)'] = user_aggregated_data['Dur. (ms)'] / 1000

user_aggregated_data.head()

user_aggregated_data['Social Media Total (Bytes)'] = (

                                                        user_aggregated_data['Social Media DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Social Media UL (Bytes)']

                                                )

user_aggregated_data['Youtube Total (Bytes)'] = (

                                                        user_aggregated_data['Youtube DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Youtube UL (Bytes)']

                                                )

user_aggregated_data['Netflix Total (Bytes)'] = (

                                                        user_aggregated_data['Netflix DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Netflix UL (Bytes)']

                                                )

user_aggregated_data['Google Total (Bytes)'] = (

                                                        user_aggregated_data['Google DL (Bytes)'] 

                                                        + 

                                                        user_aggregated_data['Google UL (Bytes)']

                                                )

user_aggregated_data['Email Total (Bytes)'] = (

                                                        user_aggregated_data['Email DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Email UL (Bytes)']

                                                )

user_aggregated_data['Gaming Total (Bytes)'] = (

                                                        user_aggregated_data['Gaming DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Gaming UL (Bytes)']

                                                )

user_aggregated_data['Other Total (Bytes)'] = (

                                                        user_aggregated_data['Other DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Other UL (Bytes)']

                                                        )

user_aggregated_data.head()
user_aggregated_data.describe()









plt.hist(user_aggregated_data['Session duration (s)'], bins=30, edgecolor='black')

plt.title('Histogram of Session duration')

plt.xlabel('Session duration (ms)')

plt.ylabel('Frequency')

plt.show()


fig, ax = plt.subplots()


ax.boxplot(user_aggregated_data['Session duration (s)'])

ax.set_yscale('log')  
ax.set_title('Box Plot of Session duration')

ax.set_ylabel('Session duration (seccond)')

plt.show()
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

class Plotter:
    def __init__(self, data):
        self.data = data

    def plot_horizontal_bar_chart(self, x_values, y_values, title="Horizontal Bar Chart", xlabel="X Axis", ylabel="Y Axis"):
        plt.figure(figsize=(10, 6))
        sns.barplot(x=x_values, y=y_values, data=self.df, palette='viridis')
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.title(title)
        plt.show()

    def plot_heatmap(self, title="Heatmap"):
        plt.figure(figsize=(8, 6))
        sns.heatmap(self.data, annot=True, cmap="YlGnBu", fmt=".2f", linewidths=0.5)
        plt.title(title)
        plt.show()

    def plot_scatter_plot(self, x_values, y_values, title="Scatter Plot", xlabel="X Axis", ylabel="Y Axis"):
        plt.figure(figsize=(8, 6))
        plt.scatter(x_values, y_values)
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.show()
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt
df_clean = pd.read_csv('../data/clean_data.csv')
df_clean.head()

df_clean.describe()


import sys, os



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.visualizer import Plotter

handset_data = df_clean[['MSISDN/Number', 'Handset Type']]

handset_counts = handset_data['Handset Type'].value_counts().reset_index()

handset_counts.columns = ['Handset Type', 'Count']

top_ten_handsets = handset_counts.head(10)

top_ten_handsets
plotter_instance = Plotter(top_ten_handsets)

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Type', data=top_ten_handsets, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Type')

plt.title('Top Ten Handsets Used by Customers')

plt.show();

handsetM_data = df_clean['Handset Manufacturer']

handsetM_counts = handsetM_data.value_counts().reset_index()

handsetM_counts.columns = ['Handset Manufacturer', 'Count']

top_three_handsets_manf = handsetM_counts.head(3)

top_three_handsets_manf

plt.figure(figsize=(10, 6))

sns.barplot(x='Count', y='Handset Manufacturer', data=top_three_handsets_manf, palette='viridis')

plt.xlabel('Count')

plt.ylabel('Handset Manufacturer')

plt.title('Top Three Handset Manufacturer')

plt.show();

handset_ocurrance_counts = df_clean.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')

top_manufacturers = handset_ocurrance_counts.groupby('Handset Manufacturer')['Count'].sum().nlargest(3).index

top_manufacturer_data = handset_ocurrance_counts[handset_ocurrance_counts['Handset Manufacturer'].isin(top_manufacturers)]

top_5_handsets_per_manufacturer = (

    top_manufacturer_data.groupby('Handset Manufacturer')

    .apply(lambda group: group.nlargest(5, 'Count'))

    .reset_index(drop=True)

)




top_5_handsets_per_manufacturer

sns.set(style="whitegrid")




plt.figure(figsize=(12, 8))

sns.barplot(x='Handset Manufacturer', y='Count', hue='Handset Type', data=top_5_handsets_per_manufacturer)

plt.title('Handset Types Count by Manufacturer')

plt.xlabel('Handset Manufacturer')

plt.ylabel('Count')

plt.xticks(rotation=45, ha='right')  
plt.legend(title='Handset Type', bbox_to_anchor=(1, 1))  
plt.show()












print(df_clean.columns.tolist())

columns_to_aggregate = ['Dur. (ms)', 'Total DL (Bytes)', 'Total UL (Bytes)',

                         'Social Media DL (Bytes)', 'Social Media UL (Bytes)',

                         'Youtube DL (Bytes)', 'Youtube UL (Bytes)',

                         'Netflix DL (Bytes)', 'Netflix UL (Bytes)',

                         'Google DL (Bytes)', 'Google UL (Bytes)',

                         'Email DL (Bytes)', 'Email UL (Bytes)',

                         'Gaming DL (Bytes)', 'Gaming UL (Bytes)',

                         'Other DL (Bytes)', 'Other UL (Bytes)']

user_aggregated_data = df_clean.groupby('MSISDN/Number').agg({

    'Bearer Id': 'count',  
    'Dur. (ms)': 'sum',  
    'Total DL (Bytes)': 'sum',  
    'Total UL (Bytes)': 'sum',  
    'Social Media DL (Bytes)': 'sum', 'Social Media UL (Bytes)': 'sum',

    'Youtube DL (Bytes)': 'sum', 'Youtube UL (Bytes)': 'sum',

    'Netflix DL (Bytes)': 'sum', 'Netflix UL (Bytes)': 'sum',

    'Google DL (Bytes)': 'sum', 'Google UL (Bytes)': 'sum',

    'Email DL (Bytes)': 'sum', 'Email UL (Bytes)': 'sum',

    'Gaming DL (Bytes)': 'sum', 'Gaming UL (Bytes)': 'sum',

    'Other DL (Bytes)': 'sum', 'Other UL (Bytes)': 'sum'

})






user_aggregated_data
user_aggregated_data['Session duration (s)'] = user_aggregated_data['Dur. (ms)'] / 1000

user_aggregated_data.head()

user_aggregated_data['Social Media Total (Bytes)'] = (

                                                        user_aggregated_data['Social Media DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Social Media UL (Bytes)']

                                                )

user_aggregated_data['Youtube Total (Bytes)'] = (

                                                        user_aggregated_data['Youtube DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Youtube UL (Bytes)']

                                                )

user_aggregated_data['Netflix Total (Bytes)'] = (

                                                        user_aggregated_data['Netflix DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Netflix UL (Bytes)']

                                                )

user_aggregated_data['Google Total (Bytes)'] = (

                                                        user_aggregated_data['Google DL (Bytes)'] 

                                                        + 

                                                        user_aggregated_data['Google UL (Bytes)']

                                                )

user_aggregated_data['Email Total (Bytes)'] = (

                                                        user_aggregated_data['Email DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Email UL (Bytes)']

                                                )

user_aggregated_data['Gaming Total (Bytes)'] = (

                                                        user_aggregated_data['Gaming DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Gaming UL (Bytes)']

                                                )

user_aggregated_data['Other Total (Bytes)'] = (

                                                        user_aggregated_data['Other DL (Bytes)']

                                                        + 

                                                        user_aggregated_data['Other UL (Bytes)']

                                                        )

user_aggregated_data.head()
user_aggregated_data.describe()









plt.hist(user_aggregated_data['Session duration (s)'], bins=30, edgecolor='black')

plt.title('Histogram of Session duration')

plt.xlabel('Session duration (ms)')

plt.ylabel('Frequency')

plt.show()


fig, ax = plt.subplots()


ax.boxplot(user_aggregated_data['Session duration (s)'])

ax.set_yscale('log')  
ax.set_title('Box Plot of Session duration')

ax.set_ylabel('Session duration (seccond)')

plt.show()
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from IPython.display import Image

from sklearn.preprocessing import StandardScaler,MinMaxScaler

from sklearn.cluster import KMeans

import plotly.io as pio

from plotly.subplots import make_subplots

import plotly.graph_objects as go

from mpl_toolkits.mplot3d import Axes3D
df = pd.read_csv('../data/clean_data.csv')
df.head()
df.info()
df.columns.tolist()
tcp_retrans_cols = ['MSISDN/Number', 'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']

avg_tcp_retrans = df[tcp_retrans_cols].groupby('MSISDN/Number').mean()
avg_tcp_retrans
avg_tcp_retrans.info()
rtt_cols = ['MSISDN/Number', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)']

avg_rtt = df[rtt_cols].groupby('MSISDN/Number').mean()
avg_rtt
handset_mode = df.groupby('MSISDN/Number')['Handset Type'].agg(lambda x: x.mode().iat[0] if not x.mode().empty else None)
handset_mode
throughput_cols = ['MSISDN/Number', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']

avg_throughput = df[throughput_cols].groupby('MSISDN/Number').mean()
avg_throughput

result_df = pd.concat([avg_tcp_retrans, avg_rtt, handset_mode, avg_throughput], axis=1)
result_df.head()

tcp_values = df[['TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']].stack().reset_index(level=1, drop=True)
top_tcp_values = tcp_values.nlargest(10)

print("Top TCP Values:")

print(top_tcp_values)
bottom_tcp_values = tcp_values.nsmallest(10)
print("\nBottom TCP Values:")

print(bottom_tcp_values)
most_frequent_tcp_values = tcp_values.value_counts().nlargest(10)
print("\nMost Frequent TCP Values:")

print(most_frequent_tcp_values)
rtt_values = df[['Avg RTT DL (ms)', 'Avg RTT UL (ms)']].stack().reset_index(level=1, drop=True)
top_rtt_values = rtt_values.nlargest(10)
print("\nTop RTT Values:")

print(top_rtt_values)
bottom_rtt_values = rtt_values.nsmallest(10)
print("\nBottom RTT Values:")

print(bottom_rtt_values)
most_frequent_rtt_values = rtt_values.value_counts().nlargest(10)
print("\nMost Frequent RTT Values:")

print(most_frequent_rtt_values)

throughput_values = df[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)']].stack().reset_index(level=1, drop=True)
top_throughput_values = throughput_values.nlargest(10)
print("\nTop Throughput Values:")

print(top_throughput_values)
bottom_throughput_values = throughput_values.nsmallest(10)
print("\nBottom Throughput Values:")

print(bottom_throughput_values)
most_frequent_throughput_values = throughput_values.value_counts().nlargest(10)
print("\nMost Frequent Throughput Values:")

print(most_frequent_throughput_values)

avg_throughput = df.groupby('Handset Type')['Avg Bearer TP DL (kbps)'].mean()




print("Average Throughput per Handset Type:")

print(avg_throughput)




avg_tcp_retransmission = df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean()




print("Average TCP Retransmission per Handset Type:")

print(avg_tcp_retransmission)



experience_metrics = df[['Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'TCP DL Retrans. Vol (Bytes)']]
experience_metrics.info()
experience_metrics.head()

scaler = StandardScaler()

scaled_data = scaler.fit_transform(experience_metrics)

scaled_data

kmeans = KMeans(n_clusters=3, random_state=10)

df['Cluster'] = kmeans.fit_predict(scaled_data)
df['Cluster'].unique()
df1 = df[df.Cluster==0]

df2 = df[df.Cluster==1]

df3 = df[df.Cluster==2]
plt.scatter(df1['TCP DL Retrans. Vol (Bytes)'], df1['Avg Bearer TP DL (kbps)'], color='green', label='Avg Bearer TP DL (kbps)')

plt.scatter(df2['TCP DL Retrans. Vol (Bytes)'], df2['Avg Bearer TP DL (kbps)'], color='red', label='Avg Bearer TP DL (kbps)')

plt.scatter(df3['TCP DL Retrans. Vol (Bytes)'], df3['Avg Bearer TP DL (kbps)'], color='black', label='Avg Bearer TP DL (kbps)')



plt.xlabel('TCP DL Retransmission Volume (Bytes)')

plt.ylabel('Average Bearer Throughput DL (kbps)')

plt.legend()

plt.show();












experience_metrics.head()
Sessions Frequencyexperience_metrics.info()
experience_metrics.to_csv('../data/user_experience_metrics.csv',index=False)
import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from IPython.display import Image

from sklearn.preprocessing import StandardScaler,MinMaxScaler

from sklearn.cluster import KMeans

import plotly.io as pio

from plotly.subplots import make_subplots

import plotly.graph_objects as go

from mpl_toolkits.mplot3d import Axes3D
df = pd.read_csv('../data/clean_data.csv')
df.head()
df.columns.tolist()
df.describe()

sessions_frequency = df.groupby('MSISDN/Number')['Bearer Id'].count()

sessions_frequency

session_duration = df.groupby('MSISDN/Number')['Dur. (ms)'].sum()

session_duration

df['Total Traffic (Bytes)'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']


total_traffic_per_user = df.groupby('MSISDN/Number')['Total Traffic (Bytes)'].sum()


print(total_traffic_per_user)













engagement_metrics = pd.DataFrame({

    'Sessions Frequency': sessions_frequency,

    'session_duration': session_duration,

    'Total Traffic (Bytes)': total_traffic_per_user

})




top_10_sessions_frequency = engagement_metrics['Sessions Frequency'].nlargest(10)

top_10_session_duration = engagement_metrics['session_duration'].nlargest(10)

top_10_total_traffic = engagement_metrics['Total Traffic (Bytes)'].nlargest(10)
top_10_sessions_frequency

def mult_hist(sr, rows, cols, title_text, subplot_titles, interactive=False):

    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)

    for i in range(rows):

        for j in range(cols):

            x = ["-> " + str(i) for i in sr[i+j].index]

            fig.add_trace(go.Bar(x=x, y=sr[i+j].values), row=i+1, col=j+1)

    fig.update_layout(showlegend=False, title_text=title_text)

    if(interactive):

        fig.show()

    else:

        return Image(pio.to_image(fig, format='png', width=1200))


mult_hist([top_10_session_duration, top_10_sessions_frequency,

           top_10_total_traffic], 1,3, "User metrix",

          ['Sessions Duration', 'Frequency Duration','Total Traffic']

         )

engagement_metrics.describe()
engagement_metrics['Total Traffic (Bytes)']

engagement_metrics['Sessions Frequency'] = (engagement_metrics['Sessions Frequency'] - engagement_metrics['Sessions Frequency'].min()) / (engagement_metrics['Sessions Frequency'].max() - engagement_metrics['Sessions Frequency'].min())

engagement_metrics['session_duration'] = (engagement_metrics['session_duration'] - engagement_metrics['session_duration'].min()) / (engagement_metrics['session_duration'].max() - engagement_metrics['session_duration'].min())

engagement_metrics['session_duration'] = (engagement_metrics['session_duration'] - engagement_metrics['session_duration'].min()) / (engagement_metrics['session_duration'].max() - engagement_metrics['session_duration'].min())

engagement_metrics['Total Traffic (Bytes)']['Total Traffic (Bytes)'] = (engagement_metrics['Total Traffic (Bytes)'] - engagement_metrics['Total Traffic (Bytes)'].min()) / (engagement_metrics['Total Traffic (Bytes)'].max() - engagement_metrics['Total Traffic (Bytes)'].min())


Km = KMeans(n_clusters=3, n_init=10)

Km
model_predicted =Km.fit_predict(engagement_metrics[['Sessions Frequency','session_duration','Total Traffic (Bytes)']])

model_predicted
engagement_metrics['clusters'] = model_predicted
engagement_metrics.head()
engagement_metrics['clusters'].unique()
df1 = engagement_metrics[engagement_metrics.clusters==0]

df2 = engagement_metrics[engagement_metrics.clusters==1]

df3 = engagement_metrics[engagement_metrics.clusters==2]
df1.head()
import pandas as pd
import numpy as np
from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler


class Cleaner:
    def __init__(self):
        pass

    def drop_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        """
        drop columns
        """
        return df.drop(columns=columns)
    
    def drop_nan(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna()
    
    def drop_nan_column(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna(subset=[col])  
    
    def drop_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop duplicate rows
        """
        return df.drop_duplicates()
    
    def convert_to_datetime(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
        """
        convert column to datetime
        """
        df[col] = df[col].apply(pd.to_datetime)
        return df
    
    def convert_to_string(self, df: pd.DataFrame, col = list) -> pd.DataFrame:
        """
        convert columns to string
        """
        df[col] = df[col].astype(str)
        return df
    
    def remove_whitespace_column(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        remove whitespace from columns
        """
        return df.columns.str.replace(' ', '_').str.lower()
       
    def percent_missing(self, df: pd.DataFrame) -> float:
        """
        calculate the percentage of missing values from dataframe
        """
        totalCells = np.product(df.shape)
        missingCount = df.isnull().sum()
        totalMising = missingCount.sum()
        
        return round(totalMising / totalCells * 100, 2)
    
    def percent_missing_column(self, df: pd.DataFrame, col:str) -> float:
        """
        calculate the percentage of missing values for the specified column
        """
        try:
            col_len = len(df[col])
        except KeyError:
            print(f"{col} not found")
        missing_count = df[col].isnull().sum()
        
        return round(missing_count / col_len * 100, 2)
    
    def get_numerical_columns(self, df: pd.DataFrame) -> list:
        """
        get numerical columns
        """
        return df.select_dtypes(include=['number']).columns.to_list()
    
    def get_categorical_columns(self, df: pd.DataFrame) -> list:    
        """
        get categorical columns
        """
        return  df.select_dtypes(include=['object','datetime64[ns]']).columns.to_list()
    
    
    
    def fill_missing_values_categorical(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        
        categorical_columns = df.select_dtypes(include=['object','datetime64[ns]']).columns
        
        if method == "ffill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='ffill')
                
            return df
        
        elif method == "bfill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='bfill')
                
            return df
        
        elif method == "mode":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(df[col].mode()[0])
                
            return df
        else:
            print("Method unknown")
            return df
    
    def fill_missing_values_numeric(self, df: pd.DataFrame, method: str,columns: list =None) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        if(columns==None):
            numeric_columns = self.get_numerical_columns(df)
        else:
            numeric_columns=columns
        
        if method == "mean":
            for col in numeric_columns:
                df[col].fillna(df[col].mean(), inplace=True)
                
        elif method == "median":
            for col in numeric_columns:
                df[col].fillna(df[col].median(), inplace=True)
        else:
            print("Method unknown")
        
        return df
    
    def normalizer(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        normalize numerical columns
        """
        norm = Normalizer()
        return pd.DataFrame(norm.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def min_max_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        minmax_scaler = MinMaxScaler()
        return pd.DataFrame(minmax_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def standard_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        standard_scaler = StandardScaler()
        return pd.DataFrame(standard_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def handle_outliers(self, df:pd.DataFrame, col:str, method:str ='IQR') -> pd.DataFrame:
        """
        Handle Outliers of a specified column using Turkey's IQR method
        """
        df = df.copy()
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        
        lower_bound = q1 - ((1.5) * (q3 - q1))
        upper_bound = q3 + ((1.5) * (q3 - q1))
        if method == 'mode':
            df[col] = np.where(df[col] < lower_bound, df[col].mode()[0], df[col])
            df[col] = np.where(df[col] > upper_bound, df[col].mode()[0], df[col])
        
        elif method == 'median':
            df[col] = np.where(df[col] < lower_bound, df[col].median, df[col])
            df[col] = np.where(df[col] > upper_bound, df[col].median, df[col])
        else:
            df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
            df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
        
        return df
    
    def find_agg(self,df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
        
        new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
                            sort_values(by=col_name, ascending=order)[:top]
        
        return new_df

    def convert_bytes_to_megabytes(self,df, bytes_data):

        """
            This function takes the dataframe and the column which has the bytes values
            returns the megabytesof that value
            
            Args:
            -----
            df: dataframe
            bytes_data: column with bytes values
            
            Returns:
            --------
            A series
        """
        
        megabyte = 1*10e+5
        df[bytes_data] = df[bytes_data] / megabyte
        
        return df[bytes_data]
%reload_ext autoreload

%autoreload 2
import pandas as pd

import sys

sys.path.append('../script')

from data_extraction import pgdatabase

from utils import Cleaner

clean = Cleaner()

from plots import plots

plt = plots()

db = pgdatabase()

conn = db.connection()


data = db.get_dataframe_sql(conn, 'xdr_data')
data.shape

data.head()

data.info()
data.describe()
data.isnull().sum()

clean.missing_values_table(data)


data['TCP UL Retrans. Vol (Bytes)'].describe()
data['TCP DL Retrans. Vol (Bytes)'].describe()
data['HTTP UL (Bytes)'].describe()
data['HTTP DL (Bytes)'].describe()
percent = clean.percent_missing(data)

print(f'The data has {percent}% missing values')

col = ['Start ms', 'End ms']

data1 = clean.convert_to_datetime(data, col)

data.skew()















col = ['Nb of sec with 37500B < Vol UL',

    'Nb of sec with 6250B < Vol UL < 37500B',

    'Nb of sec with 1250B < Vol UL < 6250B',

    'Nb of sec with 125000B < Vol DL',

    'Nb of sec with 31250B < Vol DL < 125000B',

    'Nb of sec with 6250B < Vol DL < 31250B']



data2 = clean.impute_zero(data1, col)



col1 = ['TCP DL Retrans. Vol (Bytes)',

        'TCP UL Retrans. Vol (Bytes)',

        'Avg RTT DL (ms)',

        'Avg RTT UL (ms)',

        'HTTP DL (Bytes)',

        'HTTP UL (Bytes)']



data2 = clean.fill_missing_values_numeric(data1, 'mean', col1)


data2['Handset Type'] = data2['Handset Type'].fillna('unknown')

data2['Handset Manufacturer'] = data2['Handset Manufacturer'].fillna('unknown')
categorical_columns = data.select_dtypes(include=['object','datetime64[ns]']).columns



for col in categorical_columns:

                data2[col] = data2[col].fillna(data2[col].mode()[0])

cleand_data = clean.fill_missing_values_numeric(data2, 'median')
clean.missing_values_table(cleand_data)

plt.plot_box(cleand_data, 'Activity Duration UL (ms)', 'Active Duration UL Outliers')

plt.plot_box(cleand_data, 'Activity Duration DL (ms)', 'Active Duration DL Outliers')
numeric_columns = clean.get_numerical_columns(cleand_data)

data5 = data[numeric_columns]
indices = clean.detect_outliers(data5,6)

print(len(indices))
telecom_data = clean.handle_outliers(data5, indices, 'mean')
%reload_ext autoreload

%autoreload 2
from urllib.parse import quote_plus

from sqlalchemy import create_engine



password = 'kerod53@'




encoded_password = quote_plus(password)




db_string = f'postgresql://postgres:{encoded_password}@localhost:5432/Telecom'




engine = create_engine(db_string)
table_name = 'telecom_data'

 

cleand_data.to_sql(table_name, engine, index=False, if_exists='replace')
import pandas as pd
import numpy as np
from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler


class Cleaner:
    def __init__(self):
        pass

    def drop_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        """
        drop columns
        """
        return df.drop(columns=columns)
    
    def drop_nan(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna()
    
    def drop_nan_column(self, df: pd.DataFrame, col:str) -> pd.DataFrame:
        """
        drop rows with nan values
        """
        return df.dropna(subset=[col])  
    
    def drop_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        drop duplicate rows
        """
        return df.drop_duplicates()
    
    def convert_to_datetime(self, df: pd.DataFrame, col: list) -> pd.DataFrame:
        """
        convert column to datetime
        """
        df[col] = df[col].apply(pd.to_datetime)
        return df
    
    def convert_to_string(self, df: pd.DataFrame, col = list) -> pd.DataFrame:
        """
        convert columns to string
        """
        df[col] = df[col].astype(str)
        return df
    
    def remove_whitespace_column(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        remove whitespace from columns
        """
        return df.columns.str.replace(' ', '_').str.lower()
       
    def percent_missing(self, df: pd.DataFrame) -> float:
        """
        calculate the percentage of missing values from dataframe
        """
        totalCells = np.product(df.shape)
        missingCount = df.isnull().sum()
        totalMising = missingCount.sum()
        
        return round(totalMising / totalCells * 100, 2)
     
    def get_numerical_columns(self, df: pd.DataFrame) -> list:
        """
        get numerical columns
        """
        return df.select_dtypes(include=['float64']).columns.to_list()
    
    def get_categorical_columns(self, df: pd.DataFrame) -> list:    
        """
        get categorical columns
        """
        return  df.select_dtypes(include=['object','datetime64[ns]']).columns.to_list()
    
    def impute_zero(self, df: pd.DataFrame, column: list) -> pd.DataFrame:
        """
        imputes 0 inplace of NaN for a given columon(s)
        """
        df[column] = df[column].fillna(0)
        
        return df 

    def fill_missing_values_categorical(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        
        categorical_columns = df.select_dtypes(include=['object','datetime64[ns]']).columns
        
        if method == "ffill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='ffill')
                
            return df
        
        elif method == "bfill":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(method='bfill')
                
            return df
        
        elif method == "mode":
            
            for col in categorical_columns:
                df[col] = df[col].fillna(df[col].mode()[0])
                
            return df
        else:
            print("Method unknown")
            return df
    
    def fill_missing_values_numeric(self, df: pd.DataFrame, method: str,columns: list = None) -> pd.DataFrame:
        """
        fill missing values with specified method
        """
        if(columns==None):
            numeric_columns = df.select_dtypes(include=['float64','int64']).columns
        else:
            numeric_columns=columns
        
        if method == "mean":
            for col in numeric_columns:
                df[col].fillna(df[col].mean(), inplace=True)
                
        elif method == "median":
            for col in numeric_columns:
                df[col].fillna(df[col].median(), inplace=True)
        else:
            print("Method unknown")
        
        return df
    
    def normalizer(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        normalize numerical columns
        """
        norm = Normalizer()
        return pd.DataFrame(norm.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def min_max_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        minmax_scaler = MinMaxScaler()
        return pd.DataFrame(minmax_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def standard_scaler(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        scale numerical columns
        """
        standard_scaler = StandardScaler()
        return pd.DataFrame(standard_scaler.fit_transform(df[self.get_numerical_columns(df)]), columns=self.get_numerical_columns(df))
    
    def detect_outliers(self, df:pd.DataFrame, threshold: int) -> list:
        """
        detect the indices of outliers using Z-method 
        """
        z_scores = df.apply(lambda x: np.abs((x - x.mean()) / x.std()))
        tr = threshold
        outliers = np.where(z_scores > tr)
        outlier_indices = [(df.index[i], df.columns[j]) for i, j in zip(*outliers)]
        return outlier_indices
        

    def handle_outliers(self, df:pd.DataFrame, indices:list, method:str) -> pd.DataFrame:
        """
        Handle Outliers of a specified column using the Z method
        """
        if method == 'mean':
            for idx, col_name in indices:
                column_mean = df[col_name].mean()
                df.iloc[idx, df.columns.get_loc(col_name)] = column_mean
        
        elif method == 'mode':
            for idx, col_name in indices:
                column_mode = df[col_name].mode()
                df.iloc[idx, df.columns.get_loc(col_name)] = column_mode

        elif method == 'median':
            for idx, col_name in indices:
                column_median = df[col_name].median()
                df.loc[idx, col_name] = column_median
        else:
            print("Method unknown")
    
        return df
    
    def find_agg(self,df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:
        
        new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\
                            sort_values(by=col_name, ascending=order)[:top]
        
        return new_df

    def convert_bytes_to_megabytes(self,df, bytes_data):

        """
            This function takes the dataframe and the column which has the bytes values
            returns the megabytesof that value
            
            Args:
            -----
            df: dataframe
            bytes_data: column with bytes values
            
            Returns:
            --------
            A series
        """
        
        megabyte = 1*10e+5
        df[bytes_data] = df[bytes_data] / megabyte
        
        return df[bytes_data]
    
    def missing_values_table(self,df):
                mis_val = df.isnull().sum()

                mis_val_percent = 100 * df.isnull().sum() / len(df)

                mis_val_dtype = df.dtypes

                mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)

                mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})

                mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

                print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
            " columns that have missing values.")

                return mis_val_table_ren_columns
import sys

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

sys.path.append('../script')

from utils import Cleaner

from plots import *

from data_extraction import *



clean = Cleaner()

%reload_ext autoreload

%autoreload 2

conn = connection()

data = table_to_sql(conn, "telecom_data")
data.head()
data.info()
top_10 = data["Handset Type"].value_counts()[:10]

plt.figure(figsize= (10,5))

top_10.plot(kind="bar",title="top 10 handset types")

plt.show()
top_3 = data["Handset Manufacturer"].value_counts()[:3]

plt.figure(figsize=(10,5))

top_3.plot(kind="bar",title="top 3 handset manufacturers")

plt.show()
handsets_per_manufacturers=data[data["Handset Manufacturer"].str.contains("Apple|Samsung|Huawei") ][["Handset Manufacturer","Handset Type"]]

plt.figure(figsize=(10,5))

top_five_apple_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Apple')["Handset Type"].value_counts()[:5]

handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Apple')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Apple")

plt.show()
plt.figure(figsize=(10,5))

top_five_samsung_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Samsung')["Handset Type"].value_counts()[:5]

handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Samsung')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Samsung")

plt.show()
plt.figure(figsize=(10,5))

top_five_huawei_handset_type=handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Huawei')["Handset Type"].value_counts()[:5]

handsets_per_manufacturers.groupby(["Handset Manufacturer"]).get_group('Huawei')["Handset Type"].value_counts()[:5].plot(kind='bar', title="Top 5 handset types by Huawei")

plt.show()

session=data.groupby(["MSISDN/Number"]).agg({"Bearer Id":"count"})

session = session.rename(columns={"Bearer Id":"XDR_sessions"})

session = session.sort_values(by=["XDR_sessions"],ascending=False)



session.head(10)

duration = data.groupby(["MSISDN/Number"]).agg({"Dur. (ms)":"sum"})

duration.sort_values(by=["Dur. (ms)"],ascending=False,inplace=True)

duration=duration.rename(columns={"Dur. (ms)":"total_duration(ms) "})

duration.head(10)

total_data = data.groupby(["MSISDN/Number"]).agg({"Total UL (Bytes)":"sum","Total DL (Bytes)":"sum"})

total_data["total_data(bytes)"]=total_data["Total UL (Bytes)"]+total_data["Total DL (Bytes)"]

total_data.sort_values(by=["total_data(bytes)"],ascending=False,inplace=True)

total_data.head(10)




data['social_media'] = data['Social Media DL (Bytes)'] + data['Social Media UL (Bytes)']

data['google'] = data['Google DL (Bytes)'] + data['Google UL (Bytes)']

data['email'] = data['Email DL (Bytes)'] + data['Email UL (Bytes)']

data['youtube'] = data['Youtube DL (Bytes)'] + data['Youtube UL (Bytes)']

data['netflix'] = data['Netflix DL (Bytes)'] + data['Netflix UL (Bytes)']

data['gaming'] = data['Gaming DL (Bytes)'] + data['Gaming UL (Bytes)']

data['other'] = data['Other DL (Bytes)'] + data['Other UL (Bytes)']

data['total_data'] = data['Total UL (Bytes)'] + data['Total DL (Bytes)']
app_total_data = data[['MSISDN/Number','social_media','google', 'email', 'youtube','netflix', 'gaming','other']].copy()

app_total_data.groupby('MSISDN/Number').sum().sample(10)






variables = data[['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'total_data', 'Total DL (Bytes)', 'social_media', 'google', 'email', 'youtube', 'netflix', 'gaming', 'other']].copy()

variables.groupby('MSISDN/Number').sum().sample(10)


non_graphical_univariate = variables.drop('MSISDN/Number', axis=1).describe()

non_graphical_univariate.transpose()

plot_hist(variables,'email','cornflowerblue')

plot_hist(variables,'gaming','cornflowerblue')

plot_hist(variables,'google','cornflowerblue')

plot_hist(variables,'netflix','cornflowerblue')

plot_hist(variables,'other','cornflowerblue')

plot_hist(variables,'social_media','cornflowerblue')

plot_hist(variables,'total_data','cornflowerblue')

plot_hist(variables,'youtube','cornflowerblue')

plot_hist(variables,'Total DL (Bytes)','cornflowerblue')

plot_hist(variables,'Total UL (Bytes)','cornflowerblue')

plot_hist(variables,'Dur. (ms)','cornflowerblue')


agg_data = variables.groupby('MSISDN/Number').sum()

agg_data.head(10)
plot_scatter(agg_data, 'social_media', 'total_data', 'social media data vs total data',  None,  None)

import seaborn as sns

plt.figure(figsize=(15,12))

plt.subplot(2,3,1,title="social media data vs total_data data")

sns.scatterplot(data=agg_data,x="social_media",y="total_data",hue=None,style=None)



plt.subplot(2,3,2,title="email data vs total_data data")

sns.scatterplot(data=agg_data,x="email",y="total_data",hue=None,style=None)



plt.subplot(2,3,3,title="gaming data vs total_data data")

sns.scatterplot(data=agg_data,x="gaming",y="total_data",hue=None,style=None)



plt.subplot(2,3,4,title="google data vs total_data data")

sns.scatterplot(data=agg_data,x="google",y="total_data",hue=None)



plt.subplot(2,3,5,title="netflix data vs total_data data")

sns.scatterplot(data=agg_data,x="netflix",y="total_data",hue="netflix")



plt.subplot(2,3,6,title="youtube data vs total_data data")

sns.scatterplot(data=variables,x="youtube",y="total_data",hue="youtube")



plt.show()


from sklearn.preprocessing import MinMaxScaler





scaled_explore_feature_df = variables[['MSISDN/Number', 'total_data', 'Dur. (ms)']]



scaled_explore_feature_df['Dur. (ms)'] = variables['Dur. (ms)'] /1000



scaled_explore_feature_df = scaled_explore_feature_df.rename(columns={'Dur. (ms)': 'duration'})



scaled_explore_feature_df_agg = scaled_explore_feature_df.groupby('MSISDN/Number').agg({'duration':'sum', 'total_data': 'sum'})



deciles = pd.qcut(scaled_explore_feature_df_agg['duration'], 5, labels=["1st_decile", "2nd_decile",

                                                      "3rd_decile", "4th_decile",

                                                      "5th_decile"])







explore_feature_df_with_decile = scaled_explore_feature_df_agg.copy()



explore_feature_df_with_decile['decile'] = deciles



explore_feature_df_with_decile_agg = explore_feature_df_with_decile.groupby('decile').agg({'total_data': 'sum',

                                                                                           'duration': 'sum'})

explore_feature_df_with_decile_agg
feature2 = variables.drop(['MSISDN/Number', 'Dur. (ms)'], axis=1)

feature2.corr(method='pearson')
plt.figure(figsize=(12,5))

sns.heatmap(feature2.corr(),cmap="YlGnBu")

plt.title("Application Data Correlation")

plt.show()
data_reduction = variables.drop(['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)'], axis=1)

data_reduction.head()

from sklearn.preprocessing import StandardScaler

from sklearn.decomposition import PCA




scaler = StandardScaler()

scaled_explore_feature_df = scaler.fit_transform(data_reduction)




pca = PCA(n_components=2)

pca.fit(scaled_explore_feature_df)

pca_df = pd.DataFrame(pca.transform(scaled_explore_feature_df))




plt.figure(figsize=(8, 6))

plt.scatter(pca_df[0], pca_df[1])

plt.xlabel('PCA 1')

plt.ylabel('PCA 2')

plt.title('PCA')

plt.show()
pca_df.head()
%reload_ext autoreload

%autoreload 2
import pandas as pd
from sqlalchemy import create_engine
import psycopg2


def connection():
    """
    connects to my pg database
    """
    conn = psycopg2.connect(dbname = 'Telecom',
                            user = 'postgres',
                            password = 'kerod53@',
                            host = 'localhost',
                            port = '5432')
    return conn

def table_to_sql(conn, table_name:str) -> pd.DataFrame:
    query = f'SELECT * FROM public.{table_name}'
    data = pd.read_sql_query(query, conn)

    return data
my_parameters = {'dbname': 'Telecom',
                'user': 'postgres',
                'password':'kerod53@',
                'host':'localhost',
                'port': '5432'}

def connection2(self, parameters: dict = my_parameters):
    """
    Connects to the PostgreSQL database. for a given paramters:
    connection_params is a dictionary that define the following:
    {
        'dbname': 'your_database_name',
        'user': 'your_username',
        'password': 'your_password',
        'host': 'your_host',
        'port': 'your_port'
        }
    """
    try:
        conn = psycopg2.connect(**parameters)
        return conn
    except psycopg2.Error as e:
        print(f"Error: Unable to connect to the database. {e}")
        return None

def get_dataframe_sql(self, conn, table_name) -> pd.DataFrame:
    query = f'SELECT * FROM public.{table_name}'
    data = pd.read_sql_query(query, conn)
    return data

def create_engine(self, connection_params: dict = my_parameters):
    """
    creates engine using sqlalchemy for a given paramters:
    """
    engine = create_engine("postgresql://postgres:kerod53@@localhost:5432/Telecom")
    return engine

def write_dataframe_to_table(self, df: pd.DataFrame, table_name: str,engine)->None:
    """
    Writes a pandas dataframe to a new table in the PostgreSQL database.
    """
    df.to_sql(table_name, engine, index=False, if_exists='replace')
    print(f"Dataframe successfully written to the '{table_name}' table.")

def update_table_by_appending(df, table_name, connection_params = my_parameters):
    """
    Appends a pandas dataframe to an existing PostgreSQL table.
    """
    engine = create_engine(f"postgresql://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['dbname']}")
    df.to_sql(table_name, engine, index=False, if_exists='append')
    print(f"Dataframe successfully appended to the '{table_name}' table.")

def delete_table(table_name, connection_params = my_parameters):
    """
    Deletes a table from the PostgreSQL database.
    """
    connection = connect_to_database(connection_params)
    if connection:
        cursor = connection.cursor()
        cursor.execute(f"DROP TABLE IF EXISTS {table_name};")
        connection.commit()
        connection.close()
        print(f"Table '{table_name}' successfully deleted.")
    else:
        print("Error: Unable to connect to the database.")

def connect(self):
    conn = psycopg2.connect(dbname = 'Telecom',
                            user = 'postgres',
                            password = 'kerod53@',
                            host = 'localhost',
                            port = '5432')
    return conn

def get_data_sql(self, conn, table_name):
    query = f'SELECT * FROM public.{table_name}'
    data = pd.read_sql_query(query, conn)
    return data
import imp
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def plot_hist(df:pd.DataFrame, column:str, color:str)->None:
    plt.figure(figsize=(12, 7))
    sns.histplot(data=df, x=column, color=color, kde=True)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()

def plot_count(self,df:pd.DataFrame, column:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.countplot(data=df, x=column)
    plt.title(f'Distribution of {column}', size=20, fontweight='bold')
    plt.show()
    
def plot_bar(self,df:pd.DataFrame, x_col:str, y_col:str, title:str, xlabel:str, ylabel:str)->None:
    plt.figure(figsize=(12, 7))
    sns.barplot(data = df, x=x_col, y=y_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.yticks( fontsize=14)
    plt.xlabel(xlabel, fontsize=16)
    plt.ylabel(ylabel, fontsize=16)
    plt.show()

def plot_heatmap(self,df:pd.DataFrame, title:str, cbar=False)->None:
    plt.figure(figsize=(12, 7))
    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )
    plt.title(title, size=18, fontweight='bold')
    plt.show()

def plot_box(self,df:pd.DataFrame, x_col:str, title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.boxplot(data = df, x=x_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.show()

def plot_box_multi(self,df:pd.DataFrame, x_col:str, y_col:str, title:str) -> None:
    plt.figure(figsize=(12, 7))
    sns.boxplot(data = df, x=x_col, y=y_col)
    plt.title(title, size=20)
    plt.xticks(rotation=75, fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()

def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:
    plt.figure(figsize=(12, 7))
    sns.scatterplot(data = df, x=x_col, y=y_col, hue=hue, style=style)
    plt.title(title, size=20)
    plt.xticks(fontsize=14)
    plt.yticks( fontsize=14)
    plt.show()
import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

import numpy as np

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler

from sklearn import preprocessing

from sklearn.cluster import KMeans

from sklearn.preprocessing import MinMaxScaler

from sklearn.preprocessing import Normalizer

import sys

sys.path.append('../script')

from plots import *

from data_extraction import *

conn = connection()
data = table_to_sql(conn, 'telecom_data')
data.head()

experiance = data[['Bearer Id', 'MSISDN/Number', 'Avg RTT DL (ms)','Avg RTT UL (ms)','TCP DL Retrans. Vol (Bytes)','TCP UL Retrans. Vol (Bytes)','Avg Bearer TP DL (kbps)','Avg Bearer TP UL (kbps)', 'Handset Type']]



experiance.head()

experiance['total_RTT'] = experiance['Avg RTT DL (ms)'] + experiance['Avg RTT UL (ms)']

experiance['total_TCP'] = experiance['TCP DL Retrans. Vol (Bytes)'] + experiance['TCP UL Retrans. Vol (Bytes)']

experiance['total_TP'] = experiance['Avg Bearer TP DL (kbps)'] + experiance['Avg Bearer TP UL (kbps)']



experiance.head()



agg_experiance = experiance.groupby('MSISDN/Number').agg({'total_RTT': 'mean', 'total_TCP': 'mean', 'total_TP': 'mean', 'Handset Type': 'first'})

agg_experiance.rename(columns={'total_RTT': 'avg_RTT', 'total_TCP': 'avg_TCP', 'total_TP': 'avg_TP'}, inplace=True)

agg_experiance.head()

top_10_RTT = agg_experiance.sort_values(by='avg_RTT', ascending=False)

print('The top 10 values of RTT per user are: \n', top_10_RTT['avg_RTT'].head(10))

print('---------------------------------------------------')

print('The bottom to values of RTT per user are: \n',top_10_RTT['avg_RTT'].tail(10))

print('---------------------------------------------------')

freq = agg_experiance['avg_RTT'].value_counts()

print('The most frequent values of RTT are: \n', freq.head(10))

top_10_TP = agg_experiance.sort_values(by='avg_TP', ascending=False)

print('The top 10 values of TP per user are: \n', top_10_TP['avg_TP'].head(10))

print('---------------------------------------------------')

print('The bottom to values of TP per user are: \n',top_10_TP['avg_TP'].tail(10))

print('---------------------------------------------------')

freq = agg_experiance['avg_TP'].value_counts()

print('The most frequent values of TP are: \n', freq.head(10))

top_10_tcp = agg_experiance.sort_values(by='avg_TCP', ascending=False)

print('The top 10 values of TCP per user are: \n', top_10_tcp['avg_TCP'].head(10))

print('---------------------------------------------------')

print('The bottom to values of TCP per user are: \n',top_10_tcp['avg_TCP'].tail(10))

print('---------------------------------------------------')

freq = agg_experiance['avg_TCP'].value_counts()

print('The most frequent values of TCP are: \n', freq.head(10))

freq = agg_experiance['Handset Type'].value_counts()

print('The most frequent handsets are: \n', freq.head(10))

avg_TP_by_handset_type=experiance.groupby('Handset Type').agg({'total_TP':'mean'})

avg_TP_by_handset_type.rename(columns={'total_TP':'avg_tp'},inplace=True)

avg_TP_by_handset_type.head()

avg_TCP_by_handset_type=experiance.groupby('Handset Type').agg({'total_TCP':'mean'})

avg_TCP_by_handset_type.rename(columns={'total_TCP':'avg_tcp'},inplace=True)

avg_TCP_by_handset_type.head()

experiance_cluster = experiance.groupby('MSISDN/Number').agg({'total_RTT': 'mean', 'total_TCP': 'mean', 'total_TP': 'mean'})

experiance_cluster.rename(columns={'total_RTT': 'avg_RTT', 'total_TCP': 'avg_TCP', 'total_TP': 'avg_TP'}, inplace=True)

experiance_cluster.head()

experiance_cluster.reset_index(inplace=True)

experiance_cluster.drop(columns=['MSISDN/Number'], inplace=False)

from sklearn.preprocessing import Normalizer



norm = Normalizer()

normilzed_data = norm.fit_transform(experiance_cluster)

normilized_data = pd.DataFrame(normilzed_data, columns=experiance_cluster.columns)

normilized_data.head()
kmeans = KMeans(n_clusters=3)

kmeans.fit(normilized_data)

labels = kmeans.labels_

agg_experiance['clusters'] = labels



agg_experiance.head()

agg_experiance[agg_experiance['clusters']==0].describe().transpose()

agg_experiance[agg_experiance['clusters']==1].describe().transpose()

agg_experiance[agg_experiance['clusters']==2].describe().transpose()

from urllib.parse import quote_plus

from sqlalchemy import create_engine



password = 'kerod53@'




encoded_password = quote_plus(password)




db_string = f'postgresql://postgres:{encoded_password}@localhost:5432/Telecom'




engine = create_engine(db_string)

table_name = 'user_experiance'

 

agg_experiance.to_sql(table_name, engine, index=False, if_exists='replace')
