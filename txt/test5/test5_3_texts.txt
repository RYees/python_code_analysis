/* Basic Reset */
* {
  margin: 5;
  padding: 5;
  box-sizing: border-box;
}

/* Body Styles */
body {
  font-family: 'Arial', sans-serif;
  background-color:   color:   line-height: 1.5;
  padding: 30px;
}

/* Header Styles */
header {
  background:   color: white;
  padding: 10px 0;
  text-align: center;
}

header h1 {
  margin: 0;
}

/* Navigation Menu Styles */
nav ul {
  list-style: none;
  background-color:   text-align: center;
  padding: 10px;
  margin: 0;
}

nav ul li {
  display: inline;
}

nav ul li a {
  text-decoration: none;
  color: white;
  padding: 10px 15px;
  display: inline-block;
}

nav ul li a:hover {
  background-color: }

/* Main Content Styles */
.container {
  width: 90%;
  margin: auto;
  overflow: hidden;
}

/* Button Styles */
button {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  cursor: pointer;
  border-radius: 4px;
}

button:hover {
  background-color: }

/* Form Styles */
form {
  background-color:   padding: 30px;
  border-radius: 5px;
  margin-left: 5%;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

form label {
  margin-bottom: 5px;
  display: block;
}

form input[type="text"],
form input[type="email"],
form input[type="password"] {
  width: 100%;
  padding: 8px;
  margin-bottom: 10px;
  border: 1px solid   border-radius: 4px;
}

form input[type="submit"] {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  border-radius: 4px;
  cursor: pointer;
}

form input[type="submit"]:hover {
  background-color: }

/* Footer Styles */
footer {
  background:   color: white;
  text-align: center;
  padding: 10px 0;
  margin-top: 20px;
}

/* Utility Classes */
.clearfix::after {
  content: "";
  display: table;
  clear: both;
}

/* Responsive Design */
@media screen and (max-width: 768px) {
  .container,
  nav ul li,
  nav ul li a {
  width: 100%;
  }
  nav ul li {
  display: block;
  }
}
<!DOCTYPE html>
<html>
<head>
  <title>Prompt Result</title>
  <style>
  body {
  font-family: Arial, sans-serif;
  background-color:   margin: 0;
  padding: 0;
  color:   }
  .container {
  width: 80%;
  margin: auto;
  overflow: hidden;
  }
  header {
  background:   color: white;
  padding-top: 30px;
  min-height: 70px;
  border-bottom:   }
  header a {
  color:   text-decoration: none;
  text-transform: uppercase;
  font-size: 16px;
  }
  header ul {
  padding: 0;
  margin: 0;
  list-style: none;
  overflow: hidden;
  }
  header li {
  float: left;
  display: inline;
  padding: 0 20px 0 20px;
  }
  header   float: left;
  }
  header   margin: 0;
  }
  header nav {
  float: right;
  margin-top: 10px;
  }
  header .highlight, header .current a {
  color:   font-weight: bold;
  }
  header a:hover {
  color:   font-weight: bold;
  }
  .scrollable-paragraph {
  width: 60%; /* Narrow width */
  max-height: 400px; /* Maximum height */
  margin: 20px auto; /* Centering the paragraph */
  overflow-y: scroll; /* Enables vertical scrolling */
  padding: 15px;
  border: 1px solid   background-color: white;
  }
  </style>
</head>
<body>
  <header>
  <div class="container">
  <div id="branding">
  <h1>Prompt Generator</h1>
  </div>
  <nav>
  <ul>
  <li class="current"><a href="{% url 'generate' %}">Home</a></li>
  </ul>
  </nav>
  </div>
  </header>
  <div class="container">
  <h2>Generated Prompt</h2>
  <div class="scrollable-paragraph">
  {{ generated_prompt }}
  </div>
  <a href="{% url 'generate' %}">Generate another prompt</a>
  </div>
</body>
</html>
/* Basic Reset */
* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

/* Body Styles */
body {
  font-family: 'Arial', sans-serif;
  background-color:   color:   line-height: 1.5;
  padding: 20px;
}

/* Header Styles */
header {
  background:   color: white;
  padding: 10px 0;
  text-align: center;
}

header h1 {
  margin: 0;
}

/* Navigation Menu Styles */
nav ul {
  list-style: none;
  background-color:   text-align: center;
  padding: 10px;
  margin: 0;
}

nav ul li {
  display: inline;
}

nav ul li a {
  text-decoration: none;
  color: white;
  padding: 10px 15px;
  display: inline-block;
}

nav ul li a:hover {
  background-color: }

/* Main Content Styles */
.container {
  width: 90%;
  margin: auto;
  overflow: hidden;
}

/* Button Styles */
button {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  cursor: pointer;
  border-radius: 4px;
}

button:hover {
  background-color: }

/* Form Styles */
form {
  background-color:   padding: 15px;
  border-radius: 5px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

form label {
  margin-bottom: 5px;
  display: block;
}

form input[type="text"],
form input[type="email"],
form input[type="password"] {
  width: 100%;
  padding: 8px;
  margin-bottom: 10px;
  border: 1px solid   border-radius: 4px;
}

form input[type="submit"] {
  background-color:   color: white;
  border: none;
  padding: 10px 15px;
  border-radius: 4px;
  cursor: pointer;
}

form input[type="submit"]:hover {
  background-color: }

/* Footer Styles */
footer {
  background:   color: white;
  text-align: center;
  padding: 10px 0;
  margin-top: 20px;
}

/* Utility Classes */
.clearfix::after {
  content: "";
  display: table;
  clear: both;
}

/* Responsive Design */
@media screen and (max-width: 768px) {
  .container,
  nav ul li,
  nav ul li a {
  width: 100%;
  }
  nav ul li {
  display: block;
  }
}
from django.shortcuts import HttpResponse, render

from prompt_app.utils.prompt_gen import get_prompt
from .utils.embed_text import embed_text
from .utils.similarity import cosine_similarity

from .models import Prompt, TextChunk
 
def generate_prompt_from_vector(text_chunk, user_question):
  prompt = text_chunk + " \n based on the above data, give an answer to \
  the following question. restrict yourself to the above data only. \
  if you can't get an answer based on the data, you can feel free to \
  say i don't know. here is the question. \n" + user_question +\
  "DONOT add anything from yourself."
  return prompt
 
def generate_prompt(request):
  if request.method == 'POST':
  input_text = request.POST.get('input_text')
  embeded_question = embed_text([input_text])[0]
  highest_similarity = -1
  best_text_chunk = None
  for text_chunk in TextChunk.objects.all():
  similarity = cosine_similarity(embeded_question, text_chunk.embed)
  if similarity > highest_similarity:
  highest_similarity = similarity
  best_text_chunk = text_chunk.chunk
  if best_text_chunk is not None:
  generated_prompt = generate_prompt_from_vector(best_text_chunk, input_text)
  return render(request, 'prompt_app/prompt_result.html', {'generated_prompt': generated_prompt})
  else:
  return HttpResponse("No similar documents found.")
  else:
  return render(request, 'prompt_app/generate_prompt.html')
import fitz  
def extract_text_from_pdf(pdf_path):
  document = fitz.open(pdf_path)
  full_text = ""
  for page_num in range(len(document)):
  page = document.load_page(page_num)
  text = page.get_text()
  full_text += text
  document.close()
  return full_text
 
def chunk_text(text, words_per_chunk, overlap_size):
  words = text.split()
  chunks = []
  i = 0
  while i < len(words):
  end = i + words_per_chunk
  chunk = ' '.join(words[i:end])
  chunks.append(chunk)
  i = end - overlap_size if end - overlap_size > i else end
  return chunks
 
if __name__ == "__main__":
  pdf_path = "data/week6_data.pdf"
  extracted_text = extract_text_from_pdf(pdf_path)
  chunks = chunk_text(extracted_text, 150, 5)
  print(len(chunks))
from collections.abc import Iterable
from django.db import models
from .utils.chunk_pdf import extract_text_from_pdf, chunk_text
from .utils.embed_text import embed_text
import logging

logger = logging.getLogger(__name__)
 
class Prompt(models.Model):
  input_text = models.TextField()
  length_of_line = models.IntegerField(null=True)
  generated_prompt = models.TextField(null=True)
  def __str__(self):
  return f"Prompt: {self.input_text[:50]}..."  
class Document(models.Model):
  pdf_file = models.FileField(upload_to='pdfs/')
  created_at = models.DateTimeField(auto_now_add=True)
  updated_at = models.DateTimeField(auto_now=True)
  def __str__(self):
  return f"PDF File uploaded on {self.created_at}"
  def save(self, *args, **kwargs):
  super().save(*args, **kwargs)
  if self.pdf_file:   text = extract_text_from_pdf(self.pdf_file.path)
  chunks = chunk_text(text)
  embeds = embed_text(chunks)
  for chunk, embed in zip(chunks, embeds):
  TextChunk.objects.create(document=self, chunk=chunk, embed=embed)
  else:
  logger.error("No PDF file to process.")

class TextChunk(models.Model):
  document = models.ForeignKey(Document, on_delete=models.CASCADE)
  chunk = models.TextField()
  embed = models.JSONField(blank=True)
  created_at = models.DateTimeField(auto_now_add=True)
  updated_at = models.DateTimeField(auto_now=True)
from django.contrib import admin
 
from django.contrib import admin
from .models import Document, TextChunk

class DocumentAdmin(admin.ModelAdmin):
  list_display = ('pdf_file', 'created_at', 'updated_at')
  search_fields = ('pdf_file',)
  readonly_fields = ('created_at', 'updated_at')

class TextChunkAdmin(admin.ModelAdmin):
  list_display = ('chunk', 'embed', 'updated_at')
 
admin.site.register(Document, DocumentAdmin)
admin.site.register(TextChunk, TextChunkAdmin)
import fitz  
def extract_text_from_pdf(pdf_path):
  document = fitz.open(pdf_path)
  full_text = ""
  for page_num in range(len(document)):
  page = document.load_page(page_num)
  text = page.get_text()
  full_text += text
  document.close()
  return full_text
 
def chunk_text(text, words_per_chunk=500, overlap_size=20):
  words = text.split()
  chunks = []
  i = 0
  while i < len(words):
  end = i + words_per_chunk
  chunk = ' '.join(words[i:end])
  chunks.append(chunk)
  i = end - overlap_size if end - overlap_size > i else end
  return chunks
 
if __name__ == "__main__":
  pdf_path = "data/week6_data.pdf"
  extracted_text = extract_text_from_pdf(pdf_path)
  chunks = chunk_text(extracted_text, 150, 5)
  print(len(chunks))
from langchain.embeddings.openai import OpenAIEmbeddings
from dotenv import load_dotenv

from . chunk_pdf import extract_text_from_pdf, chunk_text

load_dotenv()

def embed_text(chunks):
  embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
  embeds = embed_model.embed_documents(chunks)
  return embeds
 
if __name__ == "__main__":
  pdf_path = "data/week6_data.pdf"
  text = extract_text_from_pdf(pdf_path)
  chunks = chunk_text(text, 150, 5)
  embeds = embed_text(chunks)
  print(embeds)
"""
Django settings for prompt_generator project.

Generated by 'django-admin startproject' using Django 5.0.1.

For more information on this file, see
https://docs.djangoproject.com/en/5.0/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/5.0/ref/settings/
"""

from pathlib import Path
import os

BASE_DIR = Path(__file__).resolve().parent.parent
 
SECRET_KEY = 'django-insecure-j1ut4aojbrfo13kkj
DEBUG = True

ALLOWED_HOSTS = []
 
INSTALLED_APPS = [
  'django.contrib.admin',
  'django.contrib.auth',
  'django.contrib.contenttypes',
  'django.contrib.sessions',
  'django.contrib.messages',
  'django.contrib.staticfiles',
  'prompt_app',
]

MIDDLEWARE = [
  'django.middleware.security.SecurityMiddleware',
  'django.contrib.sessions.middleware.SessionMiddleware',
  'django.middleware.common.CommonMiddleware',
  'django.middleware.csrf.CsrfViewMiddleware',
  'django.contrib.auth.middleware.AuthenticationMiddleware',
  'django.contrib.messages.middleware.MessageMiddleware',
  'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'prompt_generator.urls'

TEMPLATES = [
  {
  'BACKEND': 'django.template.backends.django.DjangoTemplates',
  'DIRS': [],
  'APP_DIRS': True,
  'OPTIONS': {
  'context_processors': [
  'django.template.context_processors.debug',
  'django.template.context_processors.request',
  'django.contrib.auth.context_processors.auth',
  'django.contrib.messages.context_processors.messages',
  ],
  },
  },
]

WSGI_APPLICATION = 'prompt_generator.wsgi.application'
 
DATABASES = {
  'default': {
  'ENGINE': 'django.db.backends.sqlite3',
  'NAME': BASE_DIR / 'db.sqlite3',
  }
}
 
AUTH_PASSWORD_VALIDATORS = [
  {
  'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
  },
  {
  'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
  },
  {
  'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
  },
  {
  'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
  },
]
 
LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_TZ = True
 
STATIC_URL = '/static/'
STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')
 
DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'
 
iLOGGING = {
  'version': 1,   'disable_existing_loggers': False,
  'handlers': {
  'file': {
  'level': 'DEBUG',
  'class': 'logging.FileHandler',
  'filename': '/path/to/your/logfile.log',
  },
  },
  'loggers': {
  'django': {
  'handlers': ['file'],
  'level': 'DEBUG',
  'propagate': True,
  },
  },
}
from flask import Flask, request
 
app = Flask(__name__)

@app.route("/", methods=['GET', 'POST'])
def hello_world():
  request_data = request.get_json()
  if request.method == 'POST':
  prompt = request_data['prompt']
  return prompt
import { BrowserRouter, Route, Routes } from "react-router-dom";
import PromptPage from "./Pages/promptPage";
function App() {
  return (
  <>
  {/* <BrowserRouter basename="crm"> */}
  <BrowserRouter>
  <div className="fixed left-0 top-0 w-screen z-20">
  {/* <TopNav /> */}
  </div>
  <div className="flex h-screen  overflow-auto scrollbar-w-thin">
  {/* <div className={`${!showSidebar && "hidden"} lg:block z-50`}> */}
  <div className="fixed top-3 left-0 h-screen z-10 mt-16 w-48">
  {/* <CustomerSideBar /> */}
  </div>
  <div
  className={`flex lg:pl-64"
  pr-5 w-full pt-20 h-screen flex-col`}
  >
  <Routes>
  <Route index element={<PromptPage />} />
  </Routes>
  </div>
  </div>
  </BrowserRouter>
  </>
  )

}
 
export default App;
interface Props {
  onHide: () => void,
  add: boolean
}

const PromptPage: React.FC = () => {
  return <>TEst</>
}

export default PromptPage;
import { createSlice, PayloadAction } from "@reduxjs/toolkit";
import { api } from "../utils/api";

export class Prompt {
  id: number = 0;
  name: string = "";
  shortName: string = "";
  description: string = "";
}
export class SearchParams {
  first = 0;
  rows = 7;
  searchText = "";
  sortColumn = "";
  sortDirection: number | null | undefined = 1;
}
interface ResStatus {
  status: number;
  msg: string;
}
interface State {
  prompts: Prompt[];
  prompt: Prompt;
  searchParams: SearchParams;
  resStatus: ResStatus;
  loading: boolean;
}

const initialState: State = {
  prompts: [],
  prompt: {
  id: 0,
  name: "",
  shortName: "",
  description: "",
  },
  searchParams: {
  first: 0,
  rows: 7,
  searchText: "",
  sortColumn: "",
  sortDirection: 1,
  },
  resStatus: {
  status: -1,
  msg: "",
  },
  loading: false,
};
const PromptSlice = createSlice({
  name: "prompt",
  initialState,
  reducers: {
  getPrompts: (
  state: State,
  action: PayloadAction<Prompt[]>
  ) => {
  state.prompts = action.payload;
  },
  addPrompt: (state: State, action: PayloadAction<Prompt>) => {
  state.prompt = action.payload;
  },
  setPrompt: (state: State, action: PayloadAction<Prompt>) => {
  state.prompt = action.payload;
  },
  addResStatus: (state: State, action: PayloadAction<ResStatus>) => {
  state.resStatus = action.payload;
  },
  setSearchParams: (state: State, action: PayloadAction<SearchParams>) => {
  state.searchParams = action.payload;
  },
  setLoading: (state: State, action: PayloadAction<boolean>) => {
  state.loading = action.payload;
  },
  },
});
export const {
  getPrompts,
  addPrompt,
  setLoading,
  addResStatus,
  setSearchParams,
  setPrompt,
} = PromptSlice.actions;
export default PromptSlice.reducer;

const setResStatus = (resStatus: ResStatus) => async (dispatch: any) => {
  dispatch(addResStatus(resStatus));
  setTimeout(() => {
  dispatch(addResStatus({ status: -1, msg: "" }));
  }, 1000);
};

export const fetchPrompts =
  (searchParams: SearchParams): any =>
  async (dispatch: any) => {
  try {
  dispatch(setLoading(true));
  const { first, rows, searchText, sortColumn, sortDirection } =
  searchParams;
  const response = await api.get(
  `/prompts?f=${first}&r=${rows}&st=${searchText}&sc=${sortColumn}&sd=${sortDirection}&`
  );
  let feedData = response.data;
  dispatch(getPrompts(feedData));
  dispatch(setLoading(false));
  } catch (error: any) {
  dispatch(setLoading(false));
  dispatch(
  setResStatus({
  status: error.response.status,
  msg: error.response.data.msg,
  })
  );
  }
  };

export const createPrompt =
  (prompt: Prompt, searchParams: SearchParams): any =>
  async (dispatch: any) => {
  try {
  dispatch(setLoading(true));
  const response = await api.post("/prompts", prompt);
  dispatch(
  setResStatus({
  status: response.status,
  msg: "Prompt created successfully",
  })
  );
  dispatch(fetchPrompts(searchParams));
  dispatch(setLoading(false));
  dispatch(addPrompt(response.data));
  } catch (error: any) {
  dispatch(setLoading(false));
  dispatch(
  setResStatus({
  status: error.response.status,
  msg: error.response.data.msg,
  })
  );
  }
  };

export const updatePrompt =
  (prompt: Prompt, searchParams: SearchParams): any =>
  async (dispatch: any) => {
  try {
  dispatch(setLoading(true));
  const response = await api.put(
  `/prompts/${prompt.id}`,
  prompt,
  );
  dispatch(fetchPrompts(searchParams));
  dispatch(setLoading(false));
  dispatch(
  setResStatus({
  status: response.status,
  msg: "Prompt updated sucessfuly",
  })
  );
  } catch (error: any) {
  dispatch(setLoading(false));
  dispatch(
  setResStatus({
  status: error.response.status,
  msg: error.response.data.msg,
  })
  );
  }
  };

export const deletePrompt =
  (prompt: Prompt): any =>
  async (dispatch: any) => {
  try {
  dispatch(setLoading(true));
  const response = await api.delete(`/prompts/${prompt.id}`,);
  dispatch(setLoading(false));
  dispatch(
  setResStatus({
  status: response.status,
  msg: "Prompt deleted successfully",
  })
  );
  dispatch(fetchPrompts(new SearchParams()));
  } catch (error: any) {
  dispatch(setLoading(false));
  dispatch(
  setResStatus({
  status: error.response.status,
  msg: error.response.data.msg,
  })
  );
  }
  };
from fastapi import FastAPI, HTTPException, Query
from typing import List, Dict

app = FastAPI()

def generate_prompts(description: str, scenarios: List[str], expected_outputs: List[str]) -> List[str]:
  return [f"{description} {scenario} {expected_output}" for scenario, expected_output in zip(scenarios, expected_outputs)]

def evaluate_prompt(description: str, generated_prompt: str) -> float:
  return abs(len(description) - len(generated_prompt))

def generate_evaluation_data(description: str, scenarios: List[str], expected_outputs: List[str]) -> List[Dict[str, float]]:
  evaluation_data = []
  for scenario, expected_output in zip(scenarios, expected_outputs):
  generated_prompt = generate_prompts(description, [scenario], [expected_output])[0]
  evaluation_score = evaluate_prompt(description, generated_prompt)
  evaluation_data.append({"prompt": generated_prompt, "evaluation_score": evaluation_score})
  return evaluation_data

@app.post("/generate_prompts")
def generate_prompts_api(description: str, scenarios: List[str], expected_outputs: List[str]):
  prompts = generate_prompts(description, scenarios, expected_outputs)
  return {"prompts": prompts}

@app.post("/evaluate_prompt")
def evaluate_prompt_api(description: str, generated_prompt: str):
  evaluation_score = evaluate_prompt(description, generated_prompt)
  return {"evaluation_score": evaluation_score}

@app.post("/generate_evaluation_data")
def generate_evaluation_data_api(description: str, scenarios: List[str], expected_outputs: List[str]):
  evaluation_data = generate_evaluation_data(description, scenarios, expected_outputs)
  return {"evaluation_data": evaluation_data}

if __name__ == "__main__":
  import uvicorn
  uvicorn.run(app, host="127.0.0.1", port=8000)
function App() {
  return (
  <>
  <h1 className="text-2xl font-bold underline">Hello world!</h1>
  </>
  );
}

export default App;
@tailwind base;
@tailwind components;
@tailwind utilities;
import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.tsx";
import "./index.css";
import { BrowserRouter } from "react-router-dom";

ReactDOM.createRoot(document.getElementById("root")!).render(
  <React.StrictMode>
  <BrowserRouter>
  <App />
  </BrowserRouter>
  </React.StrictMode>
);
import os

from langchain.chat_models import ChatOpenAI
 
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or "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"
 
chat = ChatOpenAI(
  openai_api_key="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",
  model='gpt-3.5-turbo'

)
 
from langchain.schema import (
  SystemMessage,
  HumanMessage,
  AIMessage

)
 
messages = [
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="Hi AI, how are you today?"),
  AIMessage(content="I'm great thank you. How can I help you?"),
  HumanMessage(content="I'd like to understand string theory.")

]
 
res = chat(messages)

res
print(res.content)

messages.append(res)
 
prompt = HumanMessage(
  content="Why do physicists believe it can produce a 'unified theory'?"

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="What is so special about Llama 2?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="Can you tell me about the LLMChain in LangChain?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
llmchain_information = [
  "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",
  "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",
  "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]
 
source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"
 
augmented_prompt = f"""Using the contexts below, answer the query.
 
Contexts:

{source_knowledge}
 
Query: {query}"""

prompt = HumanMessage(
  content=augmented_prompt

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
from datasets import load_dataset
 
dataset = load_dataset(
  "jamescalam/llama-2-arxiv-papers-chunked",
  split="train"

)
 
dataset
dataset[0]
 
import pinecone
 
pinecone.init(
  api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',
  environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time
 
index_name = 'llama-2-rag'
 
if index_name not in pinecone.list_indexes():
  pinecone.create_index(
  index_name,
  dimension=1536,
  metric='cosine'
  )
  while not pinecone.describe_index(index_name).status['ready']:
  time.sleep(1)
 
index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings
 
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [
  'this is the first chunk of text',
  'then another second chunk of text is here'

]
 
res = embed_model.embed_documents(texts)

len(res), len(res[0])
 
from tqdm.auto import tqdm  
data = dataset.to_pandas()  
batch_size = 100
 
for i in tqdm(range(0, len(data), batch_size)):
  i_end = min(len(data), i+batch_size)
  batch = data.iloc[i:i_end]
  ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]
  texts = [x['chunk'] for _, x in batch.iterrows()]
  embeds = embed_model.embed_documents(texts)
  metadata = [
  {'text': x['chunk'],
  'source': x['source'],
  'title': x['title']} for i, x in batch.iterrows()
  ]
  index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone
 
text_field = "text"  
vectorstore = Pinecone(
  index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"
 
vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):
  results = vectorstore.similarity_search(query, k=3)
  source_knowledge = "\n".join([x.page_content for x in results])
  augmented_prompt = f"""Using the contexts below, answer the query.
  Contexts:
  {source_knowledge}
  Query: {query}"""
  return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(
  content=augment_prompt(query)

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
prompt = HumanMessage(
  content="what safety measures were used in the development of llama 2?"

)
 
res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(
  content=augment_prompt(
  "what safety measures were used in the development of llama 2?"
  )

)
 
res = chat(messages + [prompt])

print(res.content)
'center', margin:"auto" }}>  <ChatContainer >   <MessageList   scrollBehavior="smooth"   typingIndicator={isTyping ? <TypingIndicator content="ChatGPT is typing" /> : null}  >  {messages.map((message, i) => {  return <Message style={{textAlign:'left'}} key={i} model={message} />  })}  </MessageList>  <MessageInput  onSend={handleSend}   style={{ textAlign:"left" }}   placeholder="Type message here"   />   </ChatContainer>  </MainContainer>  </div>  </div>  )
} 
export default Home
2024-01-19 19:53:09,268:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:53:09,360:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,423:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,424:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:55:45,116:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:55:45,211:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:58:24,235:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:35,455:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:46,019:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:03:42,437:logger:An unexpected error occurred: string indices must be integers, not 'str'
2024-01-19 20:29:51,708:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-Hrgzs***************************************WlJy. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 20:29:51,794:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 20:31:23,416:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
2024-01-19 20:35:59,765:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
2024-01-19 19:53:09,268:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:53:09,360:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,423:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,424:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:55:45,116:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:55:45,211:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:58:24,235:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:35,455:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:46,019:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:03:42,437:logger:An unexpected error occurred: string indices must be integers, not 'str'
2024-01-19 20:29:51,708:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-Hrgzs***************************************WlJy. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 20:29:51,794:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 20:31:23,416:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
2024-01-19 20:35:59,765:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
import os
from dotenv import load_dotenv
load_dotenv(".env")

class OPENAI_KEYS:
  def __init__(self):
  self.OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '') or None
 
class VECTORDB_KEYS:
  def __init__(self):
  self.VECTORDB_API_KEY = os.environ.get('VECTORDB_API_KEY', '') or None
  self.VECTORDB_URL = os.environ.get('VECTORDB_URL', '') or None
  self.VECTORDB_MODEL = os.environ.get('VECTORDB_MODEL', '') or None
 
def _get_openai_keys() -> OPENAI_KEYS:
  return OPENAI_KEYS()
 
def _get_vectordb_keys() -> VECTORDB_KEYS:
  return VECTORDB_KEYS()
 
def get_env_manager() -> dict:
  openai_keys = _get_openai_keys().__dict__
  vectordb_keys = _get_vectordb_keys().__dict__
  return {
  'openai_keys': openai_keys,
  'vectordb_keys': vectordb_keys,
  }
from langchain.chat_models import ChatOpenAI
from langchain.agents import AgentType, initialize_agent
from langchain.schema import SystemMessage

from tools import generate_prompts_with_evaluation, get_prompt_ranking_monte_carol_and_elo_rating, generate_evaluation_data

from rag_utils import create_retriever, data_loader

with open("system_message.txt", "r") as file:
  system_message = file.read()

chunks = data_loader
retriever =  create_retriever(chunks)  
def get_agent_executor():
  agent_kwargs = {
  "system_message": SystemMessage(content=system_message),
  "retriever": retriever   }
  analyst_agent_openai = initialize_agent(
  llm=ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),
  agent=AgentType.OPENAI_FUNCTIONS,
  tools=[generate_prompts_with_evaluation, get_prompt_ranking_monte_carol_and_elo_rating, generate_evaluation_data],
  agent_kwargs=agent_kwargs,
  verbose=True,
  max_iterations=20,
  early_stopping_method='generate'
  )
  return analyst_agent_openai
import json
import os

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter  
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Weaviate
 
from datasets import Dataset
import random

import weaviate
from dotenv import load_dotenv,find_dotenv
from weaviate.embedded import EmbeddedOptions

from ragas import evaluate
from ragas.metrics import ( faithfulness, answer_relevancy, context_recall, context_precision)
 
load_dotenv(find_dotenv())
 
def data_loader(file_path= '../prompts/challenge_doc.txt', chunk_size=500, chunk_overlap=50):
  try:
  loader = TextLoader(file_path)
  documents = loader.load()
  text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
  chunks = text_splitter.split_documents(documents)
  print("data loaded to vector database successfully")
  return chunks
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def create_langchain_pipeline(retriever, template, temperature=0):
  try:
  llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=temperature)
  prompt = ChatPromptTemplate.from_template(template)
  rag_chain = (
  {"context": retriever,  "question": RunnablePassthrough()}   | prompt   | llm
  | StrOutputParser()   )
  print("langchain with rag pipeline created successfully.")
  return rag_chain
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def generate_testcase_and_context(questions, ground_truths, retriever, rag_chain):
  try:
  answers = []
  contexts = []
  for query in questions:
  answers.append(rag_chain.invoke(query))
  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])
  data = {
  "question": questions,   "answer": answers,   "contexts": contexts,   "ground_truths": ground_truths   }
  dataset = Dataset.from_dict(data)   print("automatic evaluation data generated succesfully.")
  return  dataset
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def generate_automatic_questions(query, retriever):
  docs = retriever.get_relevant_documents(query)
  return docs
 
def create_retriever(chunks):
  try:
  load_dotenv(find_dotenv())
  client = weaviate.Client(
  embedded_options = EmbeddedOptions()
  )
  vectorstore = Weaviate.from_documents(
  client = client,   documents = chunks,
  embedding = OpenAIEmbeddings(),
  by_text = False
  )
  retriever = vectorstore.as_retriever()
  print("create retriver  succesfully.")
  return retriever
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def monte_carlo_eval(prompt):
  try:
  response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']
  scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}
  trials = 100
  total_score = 0
  for _ in range(trials):
  response = random.choice(response_types)
  total_score += scores[response]
  return total_score / trials
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def elo_eval(prompt, base_rating=1500):
  try:
  outcomes = ['win', 'loss', 'draw']
  outcome = random.choice(outcomes)
  K = 30   R_base = 10 ** (base_rating / 400)
  R_opponent = 10 ** (1600 / 400)   expected_score = R_base / (R_base + R_opponent)
  actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]
  new_rating = base_rating + K * (actual_score - expected_score)
  return new_rating
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):
  """
  Update Elo ratings for a list of prompts based on simulated outcomes.
  Parameters:
  prompts (list): List of prompts to be evaluated.
  elo_ratings (dict): Current Elo ratings for each prompt.
  K (int): Maximum change in rating.
  opponent_rating (int): Fixed rating of the opponent for simulation.
  Returns:
  dict: Updated Elo ratings.
  """
  try:
  for prompt in prompts:
  outcome = random.choice(['win', 'loss', 'draw'])
  actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]
  R_base = 10 ** (elo_ratings[prompt] / 400)
  R_opponent = 10 ** (opponent_rating / 400)
  expected_score = R_base / (R_base + R_opponent)
  elo_ratings[prompt] += K * (actual_score - expected_score)
  return elo_ratings
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def evaluate_prompt(prompts):
  try:
  evaluations = []
  for idx, prompt in enumerate(prompts):
  evaluations.append({   'prompt': prompt,
  'Monte Carlo Evaluation': monte_carlo_eval(prompt),
  'Elo Rating Evaluation': elo_eval(prompt)
  })
  return evaluations
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def load_file(file_path):
  try:
  with open(file_path, 'r') as file:
  file_contents = file.read()   return file_contents
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None  
def ragas_evaulation(response):
  try:
  result = evaluate(
  dataset = response,   metrics=[
  context_precision,
  context_recall,
  faithfulness,
  answer_relevancy,
  ],
  )
  df = result.to_pandas()
  values = df.values.tolist()
  for i in range(len(values)):
  values[i][2] = values[i][2].tolist()
  values[i][3] = values[i][3].tolist()
  columns = df.keys().tolist()
  response = [columns] + values
  return response
  except Exception as e:
  print(f"An unexpected error occurred hola temosa: {e}")
  return None  
def get_generated_prompt_with_evaulation(question):
  try:
  chunks = data_loader()
  retriever = create_retriever(chunks)
  prompt_template = load_file('../prompts/prompt-generation-prompt.txt')
  evaluation_tempate = load_file('../prompts/evaluation-data-generation.txt')
  prompt_rag_chain = create_langchain_pipeline(retriever, prompt_template)
  evaulation_rag_chain = create_langchain_pipeline(retriever, evaluation_tempate, temperature=0.2)
  generated_prompts = prompt_rag_chain.invoke(question)
  prompt_list  = json.loads(generated_prompts)
  questions = [item['prompt'] for item in prompt_list]
  ground_truths = [[item['ground_truth']] for item in prompt_list]
  response = generate_testcase_and_context(questions, ground_truths, retriever, evaulation_rag_chain)
  return ragas_evaulation(response)
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return None
/*   max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
  transform: rotate(0deg);
  }
  to {
  transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
  animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: } */
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}
body{
  background:white;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
  transform: rotate(0deg);
  }
  to {
  transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
  animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
import os

import sys

sys.path.append(os.path.abspath(os.path.join('../utility')))
from rag_utils import  get_generated_prompt_with_evaulation
df = get_generated_prompt_with_evaulation("generate me prompts which help me understand the challenge document")
df
df.columns
import openai  
from sentence_transformers import SentenceTransformer  
import numpy as np

from scipy.spatial.distance import cosine
 
prompts = ["Prompt 1", "Prompt 2", ...]
 
prompt_scores = {prompt: 0 for prompt in prompts}

prompt_wins = {prompt: 0 for prompt in prompts}
 
def compare_prompts(prompt1, prompt2):
  embedder = SentenceTransformer('paraphrase-distilroberta-base-v2')   scores = embedder.encode([prompt1, prompt2])
  similarity = 1 - cosine(scores[0], scores[1])   return similarity
 
for _ in range(num_iterations):
  prompt1, prompt2 = select_prompts_randomly(prompts, prompt_scores)
  comparison_result = compare_prompts(prompt1, prompt2)
  update_scores_and_wins(prompt1, prompt2, comparison_result, prompt_scores, prompt_wins)
 
ranked_prompts = sort_prompts(prompt_scores)  
print("Ranked Prompts:", ranked_prompts)
'center',
  alignItems: 'center',
  }}>
  <h1 style={{ color: 'white', fontFamily: 'Arial, sans-serif', marginBottom: '60px' }}>Promptly Tech</h1>
  <InputComponent
  inputText={inputText}
  setInputText={setInputText}
  handleSubmit={handleSubmit}
  />
  <br />
  <OutputComponent result={data} />
  </div>
  );
};

export default App;
.App {
  text-align: center;
}

.App-logo {
  height: 40vmin;
  pointer-events: none;
}

@media (prefers-reduced-motion: no-preference) {
  .App-logo {
  animation: App-logo-spin infinite 20s linear;
  }
}

.App-header {
  background-color:   min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-link {
  color: }

@keyframes App-logo-spin {
  from {
  transform: rotate(0deg);
  }
  to {
  transform: rotate(360deg);
  }
}
import React from 'react';
import ReactDOM from 'react-dom';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';

ReactDOM.render(
  <React.StrictMode>
  <App />
  </React.StrictMode>,
  document.getElementById('root')
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();
import os

OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")  
import os

from langchain.chat_models import ChatOpenAI
 
OPENAI_API_KEY= os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")  
chat = ChatOpenAI(
  openai_api_key=OPENAI_API_KEY,
  model='gpt-3.5-turbo'

)
 
from langchain.schema import (
  SystemMessage,
  HumanMessage,
  AIMessage

)
 
messages = [
  SystemMessage(content="You are a helpful assistant."),
  HumanMessage(content="Hi AI, how are you today?"),
  AIMessage(content="I'm great thank you. How can I help you?"),
  HumanMessage(content="I'd like to understand string theory.")

]
 
res = chat(messages)

res
print(res.content)

messages.append(res)
 
prompt = HumanMessage(
  content="Why do physicists believe it can produce a 'unified theory'?"

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="What is so special about Llama 2?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
messages.append(res)
 
prompt = HumanMessage(
  content="Can you tell me about the LLMChain in LangChain?"

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
llmchain_information = [
  "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",
  "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",
  "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]
 
source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"
 
augmented_prompt = f"""Using the contexts below, answer the query.
 
Contexts:

{source_knowledge}
 
Query: {query}"""

prompt = HumanMessage(
  content=augmented_prompt

)
 
messages.append(prompt)
 
res = chat(messages)
print(res.content)
 
from datasets import load_dataset
 
dataset = load_dataset(
  "jamescalam/langchain-docs",
  split="train"

)
 
dataset
dataset[0]
 
import os

import pinecone
 
pinecone.init(
  api_key=os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY',
  environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
 
import time
 
index_name = "langchain"
 
if index_name not in pinecone.list_indexes():
  pinecone.create_index(
  index_name,
  dimension=1536,
  metric='cosine'
  )
  while not pinecone.describe_index(index_name).status['ready']:
  time.sleep(1)
 
index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings
 
embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [
  'this is the first chunk of text',
  'then another second chunk of text is here'

]
 
res = embed_model.embed_documents(texts)

len(res), len(res[0])
 
from tqdm.auto import tqdm  
data = dataset.to_pandas()  
batch_size = 100
 
for i in tqdm(range(0, len(data), batch_size)):
  i_end = min(len(data), i+batch_size)
  batch = data.iloc[i:i_end]
  ids = [f"{x['id']}" for _, x in batch.iterrows()]
  texts = [x['text'] for _, x in batch.iterrows()]
  embeds = embed_model.embed_documents(texts)
  metadata = [
  {'text': x['text'],
  'source': x['source']
  } for i, x in batch.iterrows()
  ]
  index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
 
from langchain.vectorstores import Pinecone
 
text_field = "text"  
vectorstore = Pinecone(
  index, embed_model.embed_query, text_field

)

def get_user_input():
  """
  Get user input from the console and return it as a string.
  """
  user_input = input("Enter your query: ")
  return str(user_input)

def get_user_input():
  """
  Get user input from the console and return it as a string.
  """
  user_input = input("Enter your query: ")
  return str(user_input)

query = get_user_input()
 
print(query)

vectorstore.similarity_search(query, k=3)
def augment_prompt(folder_path, query: str):
  results = vectorstore.similarity_search(query, k=3)
  source_knowledge = "\n".join([x.page_content for x in results])
  augmented_prompt = f"""Using the contexts below, answer the query.
  Contexts:
  {source_knowledge}
  Query: {query}"""
  file_path = os.path.join(folder_path, 'context.txt')
  with open(file_path, 'w') as file:
  file.write(source_knowledge)
  return augmented_prompt
 
folder_path = "../prompts"

print(augment_prompt(query, folder_path))
print(augment_prompt(query))

prompt = HumanMessage(
  content=augment_prompt(query)

)
 
messages.append(prompt)
 
res = chat(messages)
 
print(res.content)
prompt = HumanMessage(
  content="what are Agents in langchain?"

)
 
res = chat(messages + [prompt])

print(res.content)
import os
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import sys
sys.path.insert(0, '/home/elias/Documents/10 Academy/WEEK 6/PrecisionRAG-AutomationSuite')
 
from data_generation._data_generation import main as generate_prompt_data, file_reader
from data_generation._evaluation import evaluate
from data_generation.retrive import retrieve_context
import json
 
app = FastAPI()

app.add_middleware(
  CORSMiddleware,
  allow_origins=["*"],
  allow_credentials=True,
  allow_methods=["*"],
  allow_headers=["*"],
)

class InputText (BaseModel):
  inputText: str

@app.post("/apeg")
async def apeg(inputText: InputText):
  generate_prompt_data("5", inputText.inputText)
  script_dir = os.path.dirname(os.path.realpath(__file__))
  base_dir = os.path.dirname(script_dir)
  current_script_directory = os.path.dirname(os.path.realpath(__file__))
  parent_directory = os.path.dirname(current_script_directory)
  grandparent_directory = os.path.dirname(parent_directory)
  file_path = os.path.join(grandparent_directory,"test-dataset/test-data.json")
  print(file_path)
  with open(file_path, 'r') as f:
  prompts = json.load(f)
  print(file_path)
  results = []
  for prompt in prompts:
  context_message = file_reader("prompts/context.txt")
  context = str(context_message)
  prompt_message = file_reader("prompts/data-generation-prompt.txt")
  prompt_text = str(prompt_message)
  evaluation_result = evaluate(prompt_text, prompt['prompt'], context)
  results.append({
  "prompt": prompt['prompt'],
  "classification": evaluation_result['classification'],
  "accuracy": evaluation_result['accuracy'],
  "sufficient_context": context
  })
  return results
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
  'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
  sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  }
  code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
  monospace;
  }
import os
import json
import sys

current_directory = os.getcwd()
print(current_directory)
sys.path.insert(0, '/home/elias/Documents/10 Academy/WEEK 6/PrecisionRAG-AutomationSuite')
 
from dotenv import find_dotenv, load_dotenv
from openai import OpenAI
from data_generation.retrive import retrieve_context

env_file_path = find_dotenv(raise_error_if_not_found=True)
load_dotenv(env_file_path)
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

def get_completion(messages, model="gpt-3.5-turbo", max_tokens=500, temperature=0, stop=None, seed=123, tools=None, logprobs=None, top_logprobs=None):
  params = {
  "messages": messages,
  "model": model,
  "max_tokens": max_tokens,
  "temperature": temperature,
  "stop": stop,
  "seed": seed,
  "logprobs": logprobs,
  "top_logprobs": top_logprobs,
  }
  if tools:
  params["tools"] = tools
  completion = client.chat.completions.create(**params)
  return completion

def file_reader(path):
  fname = os.path.join( path)
  with open(fname, 'r') as f:
  return f.read()

def generate_test_data(prompt, context, num_test_output):
  API_RESPONSE = get_completion([
  {"role": "user", "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)}
  ], logprobs=True, top_logprobs=1)
  return API_RESPONSE.choices[0].message.content

def save_json(test_data):
  file_path = "test-dataset/test-data.json"
  json_object = json.loads(test_data)
  with open(file_path, 'w') as json_file:
  json.dump(json_object, json_file, indent=4)
  print(f"JSON data has been saved to {file_path}")

def main(num_test_output, inputText: str):
  context_message=context=retrieve_context(inputText)
  context = file_reader(os.path.join("prompts/context.txt"))
  prompt = file_reader(os.path.join("prompts/data-generation-prompt.txt"))
  test_data = generate_test_data(prompt, context, num_test_output)
  save_json(test_data)
  print("===========")
  print("Test Data")
  print("===========")
  print(test_data)

if __name__ == "__main__":
  user_input = str(input("inputText: "))
  main("5", user_input)
import os
import sys
import json
sys.path.insert(0, '/home/elias/Documents/10 Academy/WEEK 6/PrecisionRAG-AutomationSuite')
from openai import OpenAI
from data_generation._data_generation import get_completion
from data_generation._data_generation import file_reader
from dotenv import find_dotenv, load_dotenv
import numpy as np

env_file_path = find_dotenv(raise_error_if_not_found=True)
load_dotenv(env_file_path)
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

def evaluate(prompt: str, user_message: str, context: str) -> str:
  """Return the classification of the hallucination.
  @parameter prompt: the prompt to be completed.
  @parameter user_message: the user message to be classified.
  @parameter context: the context of the user message.
  @returns classification: the classification of the hallucination.
  """
  num_test_output = str(10)
  API_RESPONSE = get_completion(
  [
  {
  "role": "system",   "content": prompt.replace("{Context}", context).replace("{Question}", user_message)
  }
  ],
  model="gpt-3.5-turbo",
  logprobs=True,
  top_logprobs=1,
  )
  system_msg = str(API_RESPONSE.choices[0].message.content)
  for logprob in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:
  output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100, 2)}%\n'
  print(output)
  if system_msg == 'true' and np.round(np.exp(logprob.logprob)*100, 2) >= 95.00:
  classification = 'true'
  elif system_msg == 'false' and np.round(np.exp(logprob.logprob)*100, 2) >= 95.00:
  classification = 'false'
  else:
  classification = 'false'
  return classification

if __name__ == "__main__":
  context_message = file_reader("prompts/context.txt")
  prompt_message = file_reader("prompts/generic-evaluation-prompt.txt")
  context = str(context_message)
  prompt = str(prompt_message)
  script_dir = os.path.dirname(os.path.realpath(__file__))
  base_dir = os.path.dirname(script_dir)
  file_path = os.path.join(base_dir, "test-dataset/test-data.json")
  with open(file_path, 'r') as f:
  prompts = json.load(f)
  questions = [prompt['prompt'] for prompt in prompts]
  for question in questions:
  print(evaluate(prompt, question, context))
import React, { useState } from 'react';
import './App.css';

const App = () => {
  const [userMessage, setUserMessage] = useState('');
  const [generatedContent, setGeneratedContent] = useState('');
  // const [selectedOption, setSelectedOption] = useState('Option 1');
  const handleGeneratePrompts = async () => {
  try {
  const response = await fetch('http://127.0.0.1:8000/get_prompt_completion', {
  method: 'POST',
  headers: {
  'Content-Type': 'application/json',
  },
  body: JSON.stringify({ user_message: userMessage }),
  });
  if (!response.ok) {
  throw new Error('Network response was not ok');
  }
  const result = await response.json();
  setGeneratedContent(result.generated_content);
  } catch (error) {
  console.error('Error:', error.message);
  }
  };
  const handleFileUpload = (event) => {
  // Handle file upload logic here
  const file = event.target.files[0];
  console.log('Uploaded file:', file);
  };
  // const handleDropdownChange = (event) => {
  //   setSelectedOption(event.target.value);
  // };
  return (
  <div className="container">
  <header>
  <h1>Promptly</h1>
  </header>
  <main className="main-container">
  {/* Left side: Generate Prompts */}
  <section className="left-section">
  <h2>Generate Prompts</h2>
  <textarea
  placeholder="Enter your prompt..."
  value={userMessage}
  onChange={(e) => setUserMessage(e.target.value)}
  />
  <button onClick={handleGeneratePrompts}>Generate</button>
  {generatedContent && (
  <section className="generated-content">
  <h3>Enhanced Prompt</h3>
  <p>{generatedContent}</p>
  </section>
  )}
  </section>
  <section className="right-section">
  <h2>Enter Contexts or Upload Files</h2>
  <textarea
  placeholder="Enter your context..."
  // value={userContext}
  // onChange={(e) => setUserContext(e.target.value)}
  />
  <label htmlFor="file-upload" className="file-upload-label">
  <span>Upload </span>
  <input
  type="file"
  id="file-upload"
  accept=".csv, .txt"
  onChange={handleFileUpload}
  />
  </label>
  </section>
  </main>
  </div>
  );
};

export default App;
from fastapi import FastAPI, Form, Request
from fastapi.templating import Jinja2Templates
import openai
from llama import Llama
from weaviate import Weaviate

app = FastAPI()

openai.api_key = 'your-gpt-3-api-key'
llama_api_key = 'your-llama-api-key'
weaviate_api_key = 'your-weaviate-api-key'

llama = Llama(api_key=llama_api_key)
weaviate = Weaviate(api_key=weaviate_api_key)

templates = Jinja2Templates(directory="templates")

@app.get("/")
def read_form(request: Request):
  return templates.TemplateResponse("index.html", {"request": request})

@app.post("/generate_prompt")
async def generate_prompt(user_input: str = Form(...)):
  enhanced_prompt = llama.enrich_prompt(user_input)
  response = openai.Completion.create(
  model="text-davinci-003",
  prompt=f"User input: {enhanced_prompt}\nAI response:"
  )
  ai_response = response['choices'][0]['text']
  weaviate.create_object({
  "class": "UserPrompt",
  "properties": {
  "user_input": user_input,
  "enhanced_prompt": enhanced_prompt,
  "ai_response": ai_response
  }
  })
  return templates.TemplateResponse("result.html", {"request": request, "user_input": user_input, "enhanced_prompt": enhanced_prompt, "ai_response": ai_response})
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
load_dotenv()
import os
api_key = os.environ.get("OPENAI_API_KEY")

app = FastAPI()
client = OpenAI()

app.add_middleware(
  CORSMiddleware,
  allow_origins=["*"],   allow_credentials=True,
  allow_methods=["*"],
  allow_headers=["*"],
)
class ChatInput(BaseModel):
  user_message: str
 
@app.post("/get_prompt_completion")
def get_prompt_completion(chat_input: ChatInput):
  prompt = f"You will be provided with a prompt and I want you to improve the prompt into more accurate and detailed one\n\nUser Input: {chat_input.user_message}"
  response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
  {
  "role": "system",
  "content": prompt,
  },
  {"role": "user", "content": " {chat_input.user_message}"},
  ],
  temperature=0.8,
  max_tokens=64,
  top_p=1,
  )
  gemerated_content = response.choices[0].message.content
  return {
  "generated_content": gemerated_content
  }
if __name__ == "__main__":
  import uvicorn
  uvicorn.run(app, host="127.0.0.1", port=8000)
import os
from flask import Flask, render_template, request
from evaluation.evaluator import Evaluator
from prompt_generation.knowledge_retrieval import KnowledgeRetrieval
from prompt_generation.knowledge_integrator import KnowledgeIntegrator
import prompt_generation.config
import pickle
from prompt_generation import config
import config_secret

app = Flask(__name__)

knowledge_retriever = KnowledgeRetrieval(config.knowledge)
knowledge_integrator = KnowledgeIntegrator(model_name='gpt-3.5-turbo-0613', temperature=0)
evaluator = Evaluator(model_name='gpt-3.5-turbo-0613', temperature=0)
answers = []
metrics = {}

@app.route('/')
def index():
  return render_template('index.html', answers=answers, metrics=metrics)

@app.route('/ask', methods=['POST'])
def ask():
  if request.method == 'POST':
  question = request.form['question']
  knowledge = knowledge_retriever.retrieve(question)
  answer = knowledge_integrator.get_answer(config.PROMPT_TEMPLATE, {'question': question, 'knowledge': knowledge})[0]['text']
  evaluation, _ = evaluator.get_evaluation(config.EVALUATION_PROMPT_TEMPLATE,
  {'question': question, 'knowledge': knowledge, 'answer': answer})
  evaluation_text = evaluation['text'].split('\n')
  for row in evaluation_text:
  metric_name = row.split(':')[0].strip()
  value = int(row.split(':')[1].strip())
  if metric_name not in metrics:
  metrics[metric_name] = []
  metrics[metric_name].append(value)
  return render_template('index.html', answers=[{'question': question, 'knowledge': knowledge, 'answer': answer}], metrics=metrics)

if __name__ == '__main__':
  app.run(debug=True)
import os

os.environ["OPENAI_API_KEY"]
import os,sys
from flask import Flask, render_template, request
from evaluation.evaluator import Evaluator
from prompt_generation.knowledge_retrieval import KnowledgeRetrieval
from prompt_generation.knowledge_integrator import KnowledgeIntegrator
import prompt_generation.config
import pickle
from prompt_generation import config
import secret

app = Flask(__name__)

knowledge_retriever = KnowledgeRetrieval(config.knowledge)
knowledge_integrator = KnowledgeIntegrator(model_name='gpt-3.5-turbo-0613', temperature=0)
evaluator = Evaluator(model_name='gpt-3.5-turbo-0613', temperature=0)
answers = []
metrics = {}

@app.route('/')
def index():
  return render_template('frontend/templates/index.html', answers=answers, metrics=metrics)

@app.route('/ask', methods=['POST'])
def ask():
  if request.method == 'POST':
  question = request.form['question']
  knowledge = knowledge_retriever.retrieve(question)
  answer = knowledge_integrator.get_answer(config.PROMPT_TEMPLATE, {'question': question, 'knowledge': knowledge})[0]['text']
  evaluation, _ = evaluator.get_evaluation(config.EVALUATION_PROMPT_TEMPLATE,
  {'question': question, 'knowledge': knowledge, 'answer': answer})
  evaluation_text = evaluation['text'].split('\n')
  for row in evaluation_text:
  metric_name = row.split(':')[0].strip()
  value = int(row.split(':')[1].strip())
  if metric_name not in metrics:
  metrics[metric_name] = []
  metrics[metric_name].append(value)
  return render_template('index.html', answers=[{'question': question, 'knowledge': knowledge, 'answer': answer}], metrics=metrics)

if __name__ == '__main__':
  app.run(debug=True)
