import os

import sys

from IPython.display import Image



sys.path.append(os.path.abspath(os.path.join('../Utils')))
from image_analysis import extract_text_on_image, locate_image_on_image

locate_image_on_image(

    '../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/engagement_instruction.png','../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/_preview.png', prefix='eng_', visualize=True)



path= '../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/engagement_instruction.png'

extract_text_on_image(path)
import cv2
from typing import Tuple, List
import matplotlib.pyplot as plt

import cv2
import pytesseract
from langdetect import detect
from colormap import rgb2hex


def locate_image_on_image(locate_image: str, on_image: str, prefix: str = '', visualize: bool = False, color: Tuple[int, int, int] = (0, 0, 255)):
    try:

        image = cv2.imread(on_image)
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        template = cv2.imread(locate_image, 0)

        result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)
        _, _, _, max_loc = cv2.minMaxLoc(result)

        height, width = template.shape[:2]

        top_left = max_loc
        bottom_right = (top_left[0] + width, top_left[1] + height)

        if visualize:
            cv2.rectangle(image, top_left, bottom_right, color, 1)
            plt.figure(figsize=(10, 10))
            plt.axis('off')
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            plt.imshow(image)

        return {f'{prefix}top_left_pos': top_left, f'{prefix}bottom_right_pos': bottom_right}

    except cv2.error as err:
        print(err)



def extract_text_on_image(image_location: str) -> List[str]:
    """
    Extract text written on images using OCR (Optical Character Recognition).

    Args:
        image_location (str): The path to the image file.

    Returns:
        List[str]: A list of strings containing the extracted text from the image.
    """
    try:
                image = cv2.imread(image_location)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

                blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2)

                contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        text_contours = [cnt for cnt in contours
                        if cv2.contourArea(cnt) > 100 and
                           0.2 < cv2.contourArea(cnt) / (cv2.arcLength(cnt, True) ** 2) < 1.5]

                extracted_text = []
        for cnt in text_contours:
            x, y, w, h = cv2.boundingRect(cnt)
            cropped = gray[y:y + h, x:x + w]
            text = pytesseract.image_to_string(cropped, config='--psm 6')              if text.strip():
                language = detect(text)                  text = text.replace("\n", " ").replace("\x0c", "").replace(" ", " ").strip()
                extracted_text.append(f"{language}:{text}")  
        return extracted_text

    except Exception as e:
                if isinstance(e, pytesseract.TesseractNotFoundError):
            print("Tesseract not found. Please install it!")
        else:
            print(f"An unexpected error occurred: {e}")
        return []


def identify_color_composition(image,
                               tolerance: int = 12,
                               limit: int = 2,
                               visualize: bool = False) -> None:
    """Function that identifies the color composition of a
    given image path."""

    extracted_colors = extcolors.extract_from_path(
        image, tolerance=tolerance, limit=limit)

    identified_colors = color_to_df(extracted_colors)

    if not visualize:
        return identified_colors

    list_color = list(identified_colors['c_code'])
    list_percent = [int(i) for i in list(identified_colors['occurrence'])]

    text_c = [c + ' ' + str(round(p*100/sum(list_percent), 1)) + '%' for c, p in zip(list_color,
                                                                                     list_percent)]
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(100, 100), dpi=10)
    wedges, _ = ax[0].pie(list_percent,
                          labels=text_c,
                          labeldistance=1.05,
                          colors=list_color,
                          textprops={'fontsize': 60, 'color': 'black'}
                          )

    plt.setp(wedges, width=0.3)

        plt.setp(wedges, width=0.36)

    ax[0].set_aspect("equal")
    fig.set_facecolor('grey')

    ax[1].imshow(Image.open(image))

    plt.show()

    return identified_colors


def color_to_df(extracted_colors: tuple):
    """Converts RGB Color values from extcolors output to HEX Values."""

    colors_pre_list = str(extracted_colors).replace(
        '([(', '').replace(')],', '), (').split(', (')[0:-1]
    df_rgb = [i.split('), ')[0] + ')' for i in colors_pre_list]
    df_percent = [i.split('), ')[1].replace(')', '')
                  for i in colors_pre_list]

        df_rgb_values = [(int(i.split(", ")[0].replace("(", "")),
                      int(i.split(", ")[1]),
                      int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]

    df_color_up = [rgb2hex(int(i.split(", ")[0].replace("(", "")),
                           int(i.split(", ")[1]),
                           int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]

    colors_df = pd.DataFrame(zip(df_color_up, df_rgb_values, df_percent),
                             columns=['c_code', 'rgb', 'occurrence'])

    return colors_df
import os

import shutil

import re



asset_folder = "/home/lillian/Documents/TenAcademy/week10/automatic_storyboard_for_Ad/data_preparation/Challenge_Data/Assets"

organized_data = "/home/lillian_assefa/week10/Automated-Storyboard-for-Digital-Ad/Notebook/Data_preparation/organized_data"



os.makedirs(organized_data, exist_ok=True)






for root, dirs, _ in os.walk(asset_folder):

    for folder_name in dirs:

        folder_path = os.path.join(root, folder_name)



        

        images = [file for file in os.listdir(folder_path) if file.endswith(('.jpg', '.png'))]

        videos = [file for file in os.listdir(folder_path) if file.endswith(('.mp4', '.avi'))]



       

        organized_sub_data = os.path.join(organized_data, folder_name)

        os.makedirs(organized_sub_data, exist_ok=True)



     

        images_folder = os.path.join(organized_sub_data, "images")

        os.makedirs(images_folder, exist_ok=True)

        videos_folder = os.path.join(organized_sub_data, "videos")

        os.makedirs(videos_folder, exist_ok=True)



       

        for image in images:

            source_image_path = os.path.join(folder_path, image)

            destination_image_path = os.path.join(images_folder, image)

            shutil.move(source_image_path, destination_image_path)



      

        for video in videos:

            source_video_path = os.path.join(folder_path, video)

            destination_video_path = os.path.join(videos_folder, video)

            shutil.move(source_video_path, destination_video_path)




naming_patterns = set()





for subfolder in os.listdir(organized_data):

    subfolder_path = os.path.join(organized_data, subfolder)



    if os.path.isdir(subfolder_path):

        
        images_folder = os.path.join(subfolder_path, "images")

        
        if os.path.isdir(images_folder):

        


          for image_file in os.listdir(images_folder):

              naming_patterns.add(image_file.lower())



print("Unique naming patterns:")

print("Total length of the parameters are",len(naming_patterns))

for pattern in naming_patterns:

    print(pattern)

    


!pip install numpy

!pip install tensorflow

!pip install -U scikit-learn
import numpy as np

import tensorflow as tf

import sys

from tensorflow.keras.applications import VGG16

from tensorflow.keras.preprocessing import image

from tensorflow.keras.models import Sequential

from tensorflow.keras.applications.vgg16 import preprocess_input

from tensorflow.keras.layers import Dense, Flatten

from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report

from tqdm import tqdm


try:

    from PIL import Image

except ImportError:

    print("PIL.Image import failed. Installing Pillow...")

    import subprocess

    subprocess.check_call([sys.executable, "-m", "pip", "install", "Pillow"])

    from PIL import Image






categories = [

    "Background Image", "Logo", "Call-To-Action (CTA) Button", "Icon",

    "Product Image", "Text Elements", "Infographic", "Banner",

    "Illustration", "Photograph", "Mascot", "Testimonial Quotes",

    "Social Proof", "Seal or Badge", "Graphs and Charts",

    "Decorative Elements", "Interactive Elements", "Animation Frames",

    "Coupon or Offer Code", "Legal Disclaimers or Terms",

    "Contact Information", "Map or Location Image", "QR Code"

]
model = VGG16(weights='imagenet', include_top=False)
!pip install Pillow 
!pip install tqdm
import Image

print(Image.__file__)










































    









import os

import numpy as np

from PIL import Image

from tensorflow.keras.applications.resnet50 import preprocess_input

from tqdm import tqdm



def extract_features(image_dir):

    features_list = []

    labels_list = []

    total_images = 0



    
    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                total_images += len([filename for filename in os.listdir(images_dir) 

                                     if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    progress = tqdm(total=total_images, desc='Processing images', unit='image')

    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                for filename in os.listdir(images_dir):

                    if filename.endswith(('.jpg', '.jpeg', '.png')):

                        img_path = os.path.join(images_dir, filename)

                        img = Image.open(img_path)

                        img = img.resize((224,224))

                        img = img.convert('RGB')

                        img = np.array(img)

                        img = np.expand_dims(img, axis=0)

                        img = preprocess_input(img)



                        features = model.predict(img)

                        features_list.append(features.flatten())



                        class_probs = model.predict(img)

                        predicted_class_index = np.argmax(class_probs[0])

                        if predicted_class_index < len(categories):

                            predicted_category = categories[predicted_class_index]

                            labels_list.append(predicted_category)

                        else:

                            print("Predicted class index out of range:", predicted_class_index)



                        progress.update(1)  


    progress.close()  
    return np.array(features_list), np.array(labels_list)




X, y = extract_features(organized_data)




for i, feature in enumerate(X):

    print(f"Predicted Category: {y[i]}, Features: {feature}")

import os

import numpy as np

import tensorflow as tf

from tensorflow.keras.applications.resnet50 import preprocess_input

from tqdm import tqdm



def load_and_preprocess_image(file_path):

    
    img = tf.io.read_file(file_path)

    img = tf.image.decode_image(img, channels=3)  
    img = tf.image.resize(img, [224, 224])  
    img = preprocess_input(img)  
    

    return img



def extract_features(image_dir):

    features_list = []

    labels_list = []

    total_images = 0



    
    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                total_images += len([filename for filename in os.listdir(images_dir) 

                                     if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    image_paths = []

    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                image_paths.extend([os.path.join(images_dir, filename) 

                                    for filename in os.listdir(images_dir) 

                                    if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    dataset = tf.data.Dataset.from_tensor_slices(image_paths)

    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)

    

    
    progress = tqdm(total=total_images, desc='Processing images', unit='image')

    for img in dataset:

        
        img = tf.expand_dims(img, axis=0)

        

        
        features = model.predict(img)

        features_list.append(features.flatten())



        
        class_probs = model.predict(img)

        predicted_class_index = np.argmax(class_probs[0])

        if predicted_class_index < len(categories):

            predicted_category = categories[predicted_class_index]

            labels_list.append(predicted_category)

        else:

            print("Predicted class index out of range:", predicted_class_index)



        progress.update(1)  


    progress.close()  
    return np.array(features_list), np.array(labels_list)




X, y = extract_features(organized_data)




for i, feature in enumerate(X):

    print(f"Predicted Category: {y[i]}, Features: {feature}")
!pip install pandas
import pandas as pd

import json



with open('concepts.json') as f:

    data = json.load(f)



df = pd.json_normalize(data)

                    







df
assetDf = df['asset_suggestions']

assetDf
data
import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns
df = pd.read_csv("../Data/Challenge_Data/performance_data.csv")
df.head()
df.describe()
df.info()

DataFrame = pd.DataFrame(df, columns=["game_id", "preview_link", "ER", "CTR"])




plt.figure(figsize=(8, 6))

plt.scatter(df["ER"], df["CTR"])

plt.xlabel("ER")

plt.ylabel("CTR")

plt.title("Scatter Plot of ER vs. CTR")

plt.grid(True)

plt.show()




df_selected = df[["ER", "CTR"]]




correlation_matrix = df_selected.corr()




print(correlation_matrix)

df_selected = df[["ER", "CTR"]]




correlation_matrix = df_selected.corr()




plt.figure(figsize=(8, 6))

plt.matshow(correlation_matrix, cmap="coolwarm")

plt.colorbar()

plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)

plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)

plt.title("Heatmap of Correlation Matrix")

plt.show()
import cv2
import numpy as np

def image_overlay(image1, image2, location):
    try:
        h, w = image1.shape[:2]
        h1, w1 = image2.shape[:2]
        x, y = location
        image1[y:y+h1, x:x+w1] = image2
        return image1;
    except cv2.error as err:
        print(err)
def change_image_color(image, origin_color, new_color):
    image[np.where((image==origin_color).all(axis=2))] = new_color
    return image
import os

import sys

from IPython.display import Image



sys.path.append(os.path.abspath(os.path.join('../Utils')))
from image_analysis import extract_text_on_image, locate_image_on_image

locate_image_on_image(

    '../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/engagement_instruction.png','../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/_preview.png', prefix='eng_', visualize=True)



path= '../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/engagement_instruction.png'

extract_text_on_image(path)
import cv2
from typing import Tuple, List
import matplotlib.pyplot as plt

import cv2
import pytesseract
from langdetect import detect
from colormap import rgb2hex


def locate_image_on_image(locate_image: str, on_image: str, prefix: str = '', visualize: bool = False, color: Tuple[int, int, int] = (0, 0, 255)):
    try:

        image = cv2.imread(on_image)
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        template = cv2.imread(locate_image, 0)

        result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)
        _, _, _, max_loc = cv2.minMaxLoc(result)

        height, width = template.shape[:2]

        top_left = max_loc
        bottom_right = (top_left[0] + width, top_left[1] + height)

        if visualize:
            cv2.rectangle(image, top_left, bottom_right, color, 1)
            plt.figure(figsize=(10, 10))
            plt.axis('off')
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            plt.imshow(image)

        return {f'{prefix}top_left_pos': top_left, f'{prefix}bottom_right_pos': bottom_right}

    except cv2.error as err:
        print(err)



def extract_text_on_image(image_location: str) -> List[str]:
    """
    Extract text written on images using OCR (Optical Character Recognition).

    Args:
        image_location (str): The path to the image file.

    Returns:
        List[str]: A list of strings containing the extracted text from the image.
    """
    try:
                image = cv2.imread(image_location)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

                blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2)

                contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        text_contours = [cnt for cnt in contours
                        if cv2.contourArea(cnt) > 100 and
                           0.2 < cv2.contourArea(cnt) / (cv2.arcLength(cnt, True) ** 2) < 1.5]

                extracted_text = []
        for cnt in text_contours:
            x, y, w, h = cv2.boundingRect(cnt)
            cropped = gray[y:y + h, x:x + w]
            text = pytesseract.image_to_string(cropped, config='--psm 6')              if text.strip():
                language = detect(text)                  text = text.replace("\n", " ").replace("\x0c", "").replace(" ", " ").strip()
                extracted_text.append(f"{language}:{text}")  
        return extracted_text

    except Exception as e:
                if isinstance(e, pytesseract.TesseractNotFoundError):
            print("Tesseract not found. Please install it!")
        else:
            print(f"An unexpected error occurred: {e}")
        return []


def identify_color_composition(image,
                               tolerance: int = 12,
                               limit: int = 2,
                               visualize: bool = False) -> None:
    """Function that identifies the color composition of a
    given image path."""

    extracted_colors = extcolors.extract_from_path(
        image, tolerance=tolerance, limit=limit)

    identified_colors = color_to_df(extracted_colors)

    if not visualize:
        return identified_colors

    list_color = list(identified_colors['c_code'])
    list_percent = [int(i) for i in list(identified_colors['occurrence'])]

    text_c = [c + ' ' + str(round(p*100/sum(list_percent), 1)) + '%' for c, p in zip(list_color,
                                                                                     list_percent)]
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(100, 100), dpi=10)
    wedges, _ = ax[0].pie(list_percent,
                          labels=text_c,
                          labeldistance=1.05,
                          colors=list_color,
                          textprops={'fontsize': 60, 'color': 'black'}
                          )

    plt.setp(wedges, width=0.3)

        plt.setp(wedges, width=0.36)

    ax[0].set_aspect("equal")
    fig.set_facecolor('grey')

    ax[1].imshow(Image.open(image))

    plt.show()

    return identified_colors


def color_to_df(extracted_colors: tuple):
    """Converts RGB Color values from extcolors output to HEX Values."""

    colors_pre_list = str(extracted_colors).replace(
        '([(', '').replace(')],', '), (').split(', (')[0:-1]
    df_rgb = [i.split('), ')[0] + ')' for i in colors_pre_list]
    df_percent = [i.split('), ')[1].replace(')', '')
                  for i in colors_pre_list]

        df_rgb_values = [(int(i.split(", ")[0].replace("(", "")),
                      int(i.split(", ")[1]),
                      int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]

    df_color_up = [rgb2hex(int(i.split(", ")[0].replace("(", "")),
                           int(i.split(", ")[1]),
                           int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]

    colors_df = pd.DataFrame(zip(df_color_up, df_rgb_values, df_percent),
                             columns=['c_code', 'rgb', 'occurrence'])

    return colors_df
import os

import shutil

import re



asset_folder = "/home/lillian/Documents/TenAcademy/week10/automatic_storyboard_for_Ad/data_preparation/Challenge_Data/Assets"

organized_data = "/home/lillian_assefa/week10/Automated-Storyboard-for-Digital-Ad/Notebook/Data_preparation/organized_data"



os.makedirs(organized_data, exist_ok=True)






for root, dirs, _ in os.walk(asset_folder):

    for folder_name in dirs:

        folder_path = os.path.join(root, folder_name)



        

        images = [file for file in os.listdir(folder_path) if file.endswith(('.jpg', '.png'))]

        videos = [file for file in os.listdir(folder_path) if file.endswith(('.mp4', '.avi'))]



       

        organized_sub_data = os.path.join(organized_data, folder_name)

        os.makedirs(organized_sub_data, exist_ok=True)



     

        images_folder = os.path.join(organized_sub_data, "images")

        os.makedirs(images_folder, exist_ok=True)

        videos_folder = os.path.join(organized_sub_data, "videos")

        os.makedirs(videos_folder, exist_ok=True)



       

        for image in images:

            source_image_path = os.path.join(folder_path, image)

            destination_image_path = os.path.join(images_folder, image)

            shutil.move(source_image_path, destination_image_path)



      

        for video in videos:

            source_video_path = os.path.join(folder_path, video)

            destination_video_path = os.path.join(videos_folder, video)

            shutil.move(source_video_path, destination_video_path)




naming_patterns = set()





for subfolder in os.listdir(organized_data):

    subfolder_path = os.path.join(organized_data, subfolder)



    if os.path.isdir(subfolder_path):

        
        images_folder = os.path.join(subfolder_path, "images")

        
        if os.path.isdir(images_folder):

        


          for image_file in os.listdir(images_folder):

              naming_patterns.add(image_file.lower())



print("Unique naming patterns:")

print("Total length of the parameters are",len(naming_patterns))

for pattern in naming_patterns:

    print(pattern)

    


!pip install numpy

!pip install tensorflow

!pip install -U scikit-learn
import numpy as np

import tensorflow as tf

import sys

from tensorflow.keras.applications import VGG16

from tensorflow.keras.preprocessing import image

from tensorflow.keras.models import Sequential

from tensorflow.keras.applications.vgg16 import preprocess_input

from tensorflow.keras.layers import Dense, Flatten

from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report

from tqdm import tqdm


try:

    from PIL import Image

except ImportError:

    print("PIL.Image import failed. Installing Pillow...")

    import subprocess

    subprocess.check_call([sys.executable, "-m", "pip", "install", "Pillow"])

    from PIL import Image






categories = [

    "Background Image", "Logo", "Call-To-Action (CTA) Button", "Icon",

    "Product Image", "Text Elements", "Infographic", "Banner",

    "Illustration", "Photograph", "Mascot", "Testimonial Quotes",

    "Social Proof", "Seal or Badge", "Graphs and Charts",

    "Decorative Elements", "Interactive Elements", "Animation Frames",

    "Coupon or Offer Code", "Legal Disclaimers or Terms",

    "Contact Information", "Map or Location Image", "QR Code"

]
model = VGG16(weights='imagenet', include_top=False)
!pip install Pillow 
!pip install tqdm
import Image

print(Image.__file__)










































    









import os

import numpy as np

from PIL import Image

from tensorflow.keras.applications.resnet50 import preprocess_input

from tqdm import tqdm



def extract_features(image_dir):

    features_list = []

    labels_list = []

    total_images = 0



    
    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                total_images += len([filename for filename in os.listdir(images_dir) 

                                     if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    progress = tqdm(total=total_images, desc='Processing images', unit='image')

    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                for filename in os.listdir(images_dir):

                    if filename.endswith(('.jpg', '.jpeg', '.png')):

                        img_path = os.path.join(images_dir, filename)

                        img = Image.open(img_path)

                        img = img.resize((224,224))

                        img = img.convert('RGB')

                        img = np.array(img)

                        img = np.expand_dims(img, axis=0)

                        img = preprocess_input(img)



                        features = model.predict(img)

                        features_list.append(features.flatten())



                        class_probs = model.predict(img)

                        predicted_class_index = np.argmax(class_probs[0])

                        if predicted_class_index < len(categories):

                            predicted_category = categories[predicted_class_index]

                            labels_list.append(predicted_category)

                        else:

                            print("Predicted class index out of range:", predicted_class_index)



                        progress.update(1)  


    progress.close()  
    return np.array(features_list), np.array(labels_list)




X, y = extract_features(organized_data)




for i, feature in enumerate(X):

    print(f"Predicted Category: {y[i]}, Features: {feature}")

import os

import numpy as np

import tensorflow as tf

from tensorflow.keras.applications.resnet50 import preprocess_input

from tqdm import tqdm



def load_and_preprocess_image(file_path):

    
    img = tf.io.read_file(file_path)

    img = tf.image.decode_image(img, channels=3)  
    img = tf.image.resize(img, [224, 224])  
    img = preprocess_input(img)  
    

    return img



def extract_features(image_dir):

    features_list = []

    labels_list = []

    total_images = 0



    
    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                total_images += len([filename for filename in os.listdir(images_dir) 

                                     if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    image_paths = []

    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                image_paths.extend([os.path.join(images_dir, filename) 

                                    for filename in os.listdir(images_dir) 

                                    if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    dataset = tf.data.Dataset.from_tensor_slices(image_paths)

    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)

    

    
    progress = tqdm(total=total_images, desc='Processing images', unit='image')

    for img in dataset:

        
        img = tf.expand_dims(img, axis=0)

        

        
        features = model.predict(img)

        features_list.append(features.flatten())



        
        class_probs = model.predict(img)

        predicted_class_index = np.argmax(class_probs[0])

        if predicted_class_index < len(categories):

            predicted_category = categories[predicted_class_index]

            labels_list.append(predicted_category)

        else:

            print("Predicted class index out of range:", predicted_class_index)



        progress.update(1)  


    progress.close()  
    return np.array(features_list), np.array(labels_list)




X, y = extract_features(organized_data)




for i, feature in enumerate(X):

    print(f"Predicted Category: {y[i]}, Features: {feature}")
!pip install pandas
import pandas as pd

import json



with open('concepts.json') as f:

    data = json.load(f)



df = pd.json_normalize(data)

                    







df
assetDf = df['asset_suggestions']

assetDf
data
import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns
df = pd.read_csv("../Data/Challenge_Data/performance_data.csv")
df.head()
df.describe()
df.info()

DataFrame = pd.DataFrame(df, columns=["game_id", "preview_link", "ER", "CTR"])




plt.figure(figsize=(8, 6))

plt.scatter(df["ER"], df["CTR"])

plt.xlabel("ER")

plt.ylabel("CTR")

plt.title("Scatter Plot of ER vs. CTR")

plt.grid(True)

plt.show()




df_selected = df[["ER", "CTR"]]




correlation_matrix = df_selected.corr()




print(correlation_matrix)

df_selected = df[["ER", "CTR"]]




correlation_matrix = df_selected.corr()




plt.figure(figsize=(8, 6))

plt.matshow(correlation_matrix, cmap="coolwarm")

plt.colorbar()

plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)

plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)

plt.title("Heatmap of Correlation Matrix")

plt.show()
import cv2
import numpy as np

def image_overlay(image1, image2, location):
    try:
        h, w = image1.shape[:2]
        h1, w1 = image2.shape[:2]
        x, y = location
        image1[y:y+h1, x:x+w1] = image2
        return image1;
    except cv2.error as err:
        print(err)
def change_image_color(image, origin_color, new_color):
    image[np.where((image==origin_color).all(axis=2))] = new_color
    return image
from openai import OpenAI
client = OpenAI(
  api_key="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"
)

system_prompt = """
        Assume you are an expert prompt engineer. I will give you a 
        description of an image and you will write me a prompt based 
        on the following format: 
            
            Format : 'subject + style + details + format of output'.
        
        You will add this details by yourself. This is an example prompt 
            Example: 'Draw a ginger-and-white striped cat looking excited 
                      as it chases a mouse around a kitchen, in the style 
                      of an impressionist painter, with light streaming 
                      through the windows and prominent use of blue and yellow.
                      The output of the image has an aspect ratio of 1 : 1 and 
                      the image is used for social media post.'
        This is the kind of prompt I want you to generate.
"""

response = client.chat.completions.create(
  model="gpt-3.5-turbo-0125",
  response_format={ "type": "json_object" },
  messages=[
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "A suspenseful animation of a LEGO CITY set, with buildings, vehicles, and mini-figures coming to life in a dynamic, 3D environment. The animation is designed to captivate the viewer's attention and set the stage for the upcoming challenge."}
  ]
)
print(response.choices[0].message.content)
import pandas as pd

from typing import Tuple

import cv2
df = pd.read_csv('../Data/Challenge_Data/performance_data.csv')
df.head()




df.loc[0].preview_link
def locate_image_on_image(locate_image: str, on_image: str, prefix: str = '', visualize: bool = False, color: Tuple[int, int, int] = (0, 0, 255)):

    try:



        image = cv2.imread(on_image)

        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)



        template = cv2.imread(locate_image, 0)



        result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)

        _, _, _, max_loc = cv2.minMaxLoc(result)



        height, width = template.shape[:2]



        top_left = max_loc

        bottom_right = (top_left[0] + width, top_left[1] + height)



        if visualize:

            cv2.rectangle(image, top_left, bottom_right, color, 1)

            plt.figure(figsize=(10, 10))

            plt.axis('off')

            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            plt.imshow(image)



        return {f'{prefix}top_left_pos': top_left, f'{prefix}bottom_right_pos': bottom_right}



    except cv2.error as err:

        print(err)
locate_image_on_image(

    '..Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/engagement_instruction.png', '..Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/_preview.png', prefix='eng_', visualize=True)
import pytesseract

import cv2

import numpy as np

from typing import List, Tuple
def convert_hex_to_rgb(hex_color: str, normalize: bool = True) -> List[str]:

    """Converts a HEX color to a RGB color



    Args:

        hex_color (str): HEX color code to convert

        normalize (bool, optional): Choice to normalize calculated rgb values . Defaults to True.



    Returns:

        List[str]: List of RGB values in order, normalized or not.

    """

    colors = hex_color[1:]



    
    colors = [int(colors[0:2], base=16),  
              int(colors[2:4], base=16),  
              int(colors[4:6], base=16)]  


    
    if normalize:

        colors = [color / 255 for color in colors]



    return colors



def get_luminance(hex_color: str) -> float:

    """Calculates the luminance of a given HEX color



    Args:

        hex_color (str): HEX color code to calculate luminance for



    Returns:

        float: luminance value of color

    """

    colors = convert_hex_to_rgb(hex_color)



    luminance = colors[0] * 0.2126 + colors[1] * 0.7152 + colors[2] * 0.0722



    return luminance



def fix_image_background(image_path: str):

    identified_colors = identify_color_composition(image_path)

    text_color = identified_colors['c_code'].to_list()[0]

    text_color_rgb = identified_colors['rgb'].to_list()[0]

    luminance = get_luminance(hex_color=text_color)



    if luminance < 140:

        background_color = (255, 255, 255)

    else:

        background_color = (0, 0, 0)



    
    image = cv2.imread(image_path)



    
    image[np.all(image != text_color_rgb, axis=-1)] = background_color



    return image



def extract_text(image_path, tesseract_cmd: str = '', fix_background: bool = False):

    
    try:

        if fix_background:

            text = pytesseract.image_to_string(

                fix_image_background(image_path))

        else:

            text = pytesseract.image_to_string(image_path)



        return text



    except pytesseract.TesseractNotFoundError:

        raise Exception(

            f'Failure: Tesseract is not installed or not available in the defined path {tesseract_cmd}')
from typing import Tuple

from time import sleep

from selenium import webdriver

from selenium.webdriver.chrome.options import Options

from selenium.webdriver.support.ui import WebDriverWait

from selenium.webdriver.common.by import By

from selenium.common.exceptions import TimeoutException, NoSuchElementException

from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

from os import path

from subprocess import Popen, call

import pyautogui

import ffmpeg class CreativeFrameExtractor:

    '''

    Class responsible for Extracting Creative Start and End Frames.

    It requires a chrome webdriver compatible with selenium to be

    installed/included in the run environment path.

    '''



    def __init__(self, preview_url: str,

                 engagement_type: str,

                 save_location: str = '',

                 browser_edges: Tuple[float, float] = (70, 1039)) -> None: 


        self.preview_url = preview_url

        self.engagement_type = engagement_type

        self.browser_edges = browser_edges

        self.file_name = '-'.join(preview_url.split('/')[-3:-1]) 
        self.save_location = save_location

        self.video_name = path.join(self.save_location, self.file_name) 
        self.cmd = f"ffmpeg -f gdigrab -draw_mouse 0 -framerate 60 -i desktop -vcodec libx264rgb {self.video_name}.mkv -y"



        


        
        
        self.opt = Options()

        self.opt.add_argument("--hide-scrollbars")

        self.opt.add_experimental_option(

            "excludeSwitches", ["enable-automation"])

        
        self.capabilities = DesiredCapabilities.CHROME

        self.capabilities["goog:loggingPrefs"] = {"browser": "ALL"}



    def is_status_complete(self, passed_driver) -> bool:

        '''

        Function to check status of the AD-Unit and its completion. 
        
        '''

        
        logs = passed_driver.get_log("browser")



        for log in logs:

            
            if log["source"] == "console-api":

                
                message = log["message"]



                if '"GAME CREATED"' in message or '"DROPPED"' in message:

                    
                    print("Starting Recording AD-UNIT...")

                    print(log)

                    return False



                if '"START"' in message:

                    
                    print("AD-UNIT Engaged...")

                    print(log)

                    return False



                if '"GAME COMPLETE"' in message:

                    
                    print("Stopped Recording AD-UNIT...")

                    print(log)

                    return True



        return False



    @staticmethod

    def terminate(process: Popen[bytes]) -> None:

        '''

        Function to stop/terminate a process.

        '''

        
        if process.poll() is None:

            call("taskkill /F /T /PID " + str(process.pid))



    @staticmethod

    def crop_video(filename: str, x_pos: float = 0, y_pos: float = 70, width: float = 650, height: float = 970) -> None:

        '''

        Function to crop a video with given location and size specific parameters. 
        '''

        print(filename)

        input_video = ffmpeg.input(f"{filename}.mkv")

        cropped_video = ffmpeg.crop(input_video, x_pos, y_pos, width, height)

        output_video = ffmpeg.output(cropped_video, f"{filename}_cropped.mkv")

        ffmpeg.run(output_video)



    def _imitate_engagement(self, ad_size: Tuple[float, float]) -> None:

        '''

        Function to imitate a given engagement type. 
        '''

        center = (ad_size[0]/2, self.browser_edges[0] + (ad_size[1]/2))



        if self.engagement_type == "tap":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.leftClick()



        elif self.engagement_type == "swipe right":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.dragRel(center[0], 0, duration=1)



        elif self.engagement_type == "swipe left":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.dragRel(-center[0], 0, duration=1)



        elif self.engagement_type == "tap and hold":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.click()



        elif self.engagement_type == "scrub":

            pyautogui.moveTo(center[0] - (1/2 * center[0]),

                             center[1] - (2/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)

            pyautogui.dragRel(-center[0], (1/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)

            pyautogui.dragRel(-center[0], (1/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)



    def generate_preview_video(self) -> None:

        '''

        Function to generate preview video and also a cropped version of the video.

        '''

        
        driver = webdriver.Chrome(

            options=self.opt, desired_capabilities=self.capabilities)

        
        driver.maximize_window()



        try:

            
            driver.get(self.preview_url)



            
            canvas = driver.find_element(By.TAG_NAME, "canvas")



            
            video_recording = Popen(self.cmd)



            
            ad_size = (canvas.size.get("width"), canvas.size.get("height"))



            
            self._imitate_engagement(ad_size)



            
            
            WebDriverWait(driver, 100).until(self.is_status_complete)



            sleep(5)



            
            self.terminate(video_recording)



            
            driver.close()



            
            self.crop_video(self.video_name, x_pos=0, y_pos=70,

                            width=ad_size[0], height=ad_size[1])



        except TimeoutException:

            print("TimeOut Exception Fired")

            print("AD-Unit Status Console Logs did not Complete. Engagement Failed.")

            driver.close()



        except NoSuchElementException:

            print(f"AD-Unit Failed to Load: {self.preview_url}")

            driver.close()



    def generate_frames(self) -> None:

        '''

        Function to generate creative start and end frames.

        '''

        
        driver = webdriver.Chrome(

            options=self.opt, desired_capabilities=self.capabilities, )

        
        driver.maximize_window()



        try:

            
            driver.get(self.preview_url)



            
            canvas = driver.find_element(By.TAG_NAME, "canvas")



            
            canvas.screenshot(

                path.join(self.save_location, f'{self.file_name}_start_frame.png'))

            print('Start Frame captured')



            
            ad_size = (canvas.size.get("width"), canvas.size.get("height"))



            
            self._imitate_engagement(ad_size)



            
            
            WebDriverWait(driver, 100).until(self.is_status_complete)



            sleep(5)



            
            canvas.screenshot(path.join(self.save_location,f'{self.file_name}_end_frame.png'))

            print('End Frame Captured')



            
            driver.close()



        except TimeoutException:

            print("TimeOut Exception Fired")

            print("AD-Unit Status Console Logs did not Complete. Engagement Failed.")

            driver.close()



        except NoSuchElementException:

            print(f"AD-Unit Failed to Load: {self.preview_url}")

            driver.close()
import torch

model = torch.hub.load('ultralytics/yolov5', 'custom', path="../models/best.pt")
import os, sys, glob

import pandas as pd



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.symbol_detector import SymbolDetection
sd = SymbolDetection(model, 0.25)
sd.query('../datasets/new_previews/002dbbd85ef3fe6a2e7d0754fb9f9a1a.png')
import cv2



image_path = '../datasets/new_previews/002dbbd85ef3fe6a2e7d0754fb9f9a1a.png'

image = cv2.imread(image_path)



if image is None:

    print(f"Error: Unable to load image at {image_path}")

    exit()



height, width, channels = image.shape

print(f"Image Dimensions: Width={width}, Height={height}, Channels={channels}")



cv2.rectangle(image, (0, 0), (width-1, height-1), (255, 0, 0), 2)



cv2.imshow('Image with Dimensions', image)

cv2.waitKey(0)

cv2.destroyAllWindows()

folder_path = '../datasets/new_previews/'



output = []

image_files = glob.glob(os.path.join(folder_path, '*.jpg')) + glob.glob(os.path.join(folder_path, '*.png'))



for image_file in image_files:

    coords = sd.query(image_file)

    coords.append(

        {

            'img-name': image_file.split('/')[-1]

        }

    )

    output.append(coords)
output[:5]
df = pd.read_csv('../datasets/performance_data.csv')

df
rows = []

for image in output:

    game_id = None 

    single_image = []

    for row_data in image:

        if "img-name" in row_data:

            game_id = row_data['img-name'][:-4]

    

    temp = {'game_id': game_id}

    for row_data in image:

        if "img-name" in row_data:

            continue 

        

        if 'Box' in row_data:

            for attribute in ['x', 'y', 'width', 'height']:

                temp[f"{row_data['label']}_{attribute}"] = row_data['Box'][attribute]

            temp[f'{row_data['label']}_conf'] = row_data['conf']

    

    rows.append(temp)

rows[:5]
df = pd.DataFrame(rows)
df.head()
df.value_counts()
df.to_csv('../datasets/labeled.csv', index=False)
from PIL import Image
import base64
from io import BytesIO

class SymbolDetection:
    """
    A class used to reprsent a Symbol detector
    """
    def __init__(self, model, conf):
        """
        constructs a symbol detector object
        
        parameters:
            model: symbol-detector model
            conf: NMS confidence threshold
        """
        self.model = model
        self.conf = conf

    def query(self, filename):
        """
        Query the symbol-detector object to detect our image

        parameters:
            filename: Path of the input image
        """

                results = self.model(filename)
        objects = results.crop(save=False)
        output = []
        for obj in objects:
            if (self.conf <= float(obj['conf'])):
                width = int(obj['box'][2])
                height = int(obj['box'][3])
                x = int(obj['box'][0])
                y = int(obj['box'][1])
                normalized_x = x / width 
                normalized_y = y / height

                output.append( {
                    "Box": {
                        "x": normalized_x,
                        "y": normalized_y,
                        "width": width,
                        "height": height
                    },
                    "conf": float(obj['conf']),
                    "label": self.model.names[int(obj['cls'])]
                })
        return output
import os,sys

sys.path.append('../scripts/')



from image_generation import ImageGenerator

from image_composer import ImageComposer

concept = {

    "frame_1": {

        "Background": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",

        "CTA Button": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."

    },

    "frame_2": {

        "Interactive Elements": "A digital beat mixer interface with vibrant, touch-responsive Latin American instrument icons like congas, claves, and maracas, each activating a unique sound layer.",

        "Background": "A dynamic, abstract representation of sound waves that move in sync with the user's interactions."

    },

    "frame_3": {

        "Background": "A dynamic, abstract representation of sound waves that move in sync with the user's interactions.",

        "Animation": "A kaleidoscope of colors that dance across the screen, with each beat added, symbolizing the fusion of cultures and music.",

        "CTA Button": "A 'Play Your Mix' button that pulses like a heartbeat, encouraging users to share their creation."

    },

    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."

}
concept = {

    "frame_1": {

        "Background": "A high-resolution 3D animation of a Coca-Cola bottle on a vibrant background. The bottle is detailed with effervescent bubbles rising to the top. Gradually, the bottle transforms into a DJ turntable, featuring a sleek design with illuminated controls and a vinyl record emblazoned with the Coke Studio logo, symbolizing the fusion of refreshment and music.",

        "CTA Button": "'Mix Your Beat' displayed in a bold, dynamic font on a contrasting background. The font style is playful and energetic, designed to evoke a sense of creativity and engagement. The text is surrounded by a subtle glow, suggesting interactivity and inviting the viewer to start their musical adventure."

    },

    "frame_2": {

        "Interactive Elements": "A digital beat mixer interface showcasing vibrant icons of Latin American musical instruments such as congas, claves, and maracas. Each icon is designed to be touch-responsive, illuminating and animating upon interaction to emit distinctive sound layers. The interface is set against a background that visually represents the music's energy through animated, abstract sound waves.",

        "Background": "An abstract, animated visualization of sound waves that dynamically react to interactions on the digital beat mixer. The sound waves pulse, expand, and change colors in real time, creating a visual representation of the music being mixed. The background enhances the immersive experience, reflecting the user's creative input in a visually engaging manner."

    },

    "frame_3": {

        "Background": "An abstract, animated visualization of sound waves that dynamically react to interactions on the digital beat mixer. The sound waves pulse, expand, and change colors in real time, creating a visual representation of the music being mixed. The background enhances the immersive experience, reflecting the user's creative input in a visually engaging manner.",

        "Animation": "A dynamic, colorful animation that bursts into life with each added beat, symbolizing the vibrant energy of Coke Studio's musical fusion. The screen is filled with an ever-changing kaleidoscope of colors, with each new layer of music adding complexity to the visual celebration, reflecting the creativity and diversity of the user's mix.",

        "CTA Button": "A prominently placed 'Play Your Mix' button that pulses in rhythm with the generated music mix. The button's design is inviting, with a visual style that suggests action and progression, encouraging the viewer to engage with their creation and share it. The pulsing effect enhances the sense of urgency and excitement about the music mix."

    },

    "explanation": "These descriptions are crafted to ensure each asset is vividly detailed and self-contained, suitable for generating individual images that collectively build the narrative of an interactive beat mixer. From the initial transformation of a Coca-Cola bottle into a DJ turntable, through the engagement with a virtual beat mixer, to the celebration of a personalized music mix, each description is designed to capture the essence of the interactive experience while being standalone for image generation purposes."

}
concept = {

    "frame_1": {"Background": "In the center of a vibrant scene, a high-resolution 3D Coca-Cola bottle surrounded by effervescent bubbles captures the viewer's attention. As the bubbles rise, the bottle seamlessly transforms into a sleek DJ turntable, complete with illuminated controls and a spinning vinyl record bearing the Coke Studio logo. This imagery symbolizes a fusion of refreshing beverage and rhythmic beats. Directly below this dynamic transformation, the call-to-action 'Mix Your Beat' shines in a bold, dynamic font with a playful energy. The text, surrounded by a subtle glow, invites interaction, set against a backdrop designed to evoke creativity and musical exploration."},

    "frame_2": {"Background": "The viewer is presented with an immersive digital beat mixer interface, populated with vibrant, touch-responsive icons of Latin American musical instruments such as congas, claves, and maracas. Each icon, when interacted with, illuminates and animates to release a distinctive sound layer, set against an animated background of abstract, pulsating sound waves. These waves dynamically react to the mixer's activity, changing colors and patterns in real time to visually represent the user's musical creations, encouraging exploration and engagement through a fusion of visual and auditory feedback."},

    "frame_3": {"Background": "With each beat added to the mix, the screen erupts into a dynamic display of colors, creating a visual celebration that mirrors the energy of Coke Studio's musical fusion. This kaleidoscopic animation evolves with the complexity of the user's mix, symbolizing the blend of cultures and music. At the heart of this vibrant spectacle, a 'Play Your Mix' button pulses in rhythm with the music, its design inviting and central, encouraging the viewer to engage and share their creation. The pulsing button serves as a visual anchor, drawing the viewer's attention and culminating the creative process."}

}
ig = ImageGenerator(concept)

generated_images = ig.generate_images()
image_comp = ImageComposer(600,600, list(generated_images.values()))

generated_frames = image_comp.generate_frames()
import os
import logging
from typing import Literal,List,Tuple,Optional,Dict
import base64
from dotenv import load_dotenv
from io import BytesIO
import uuid
import requests
import replicate
from PIL import Image
import requests
from pydantic import HttpUrl
from http.client import HTTPResponse

load_dotenv()

os.environ["REPLICATE_API_TOKEN"] = 'r8_0gtYm4UC2kwFP09VIDyrCl0hqEdo4Qf1lyvPd'
logging.basicConfig(level=logging.INFO)

class ImageGenerator:
    def __init__(self, asset_suggestions: dict) -> None:
        self.asset_suggestions = asset_suggestions
    def generate_images(self, store_location: str = '../images') -> Dict[str, List[Tuple[str, str]]]:
        keyType = str
        valueType = List[Tuple[str,str]]
        generated_images:Dict[keyType, valueType] = {}
        for frame, elements in self.asset_suggestions.items():
            if frame.startswith('frame'):
                generated_images[frame] = []
                for type, description in elements.items():
                    try:
                                                generated_image_path = ImageGenerator.download_image(ImageGenerator.generate_image(prompt=description)[0], store_location)
                                                generated_images[frame].append((type, *generated_image_path))
                    except Exception as e:
                        print(f"An error occurred while generating image: {e}")
        return generated_images
    @staticmethod
    def generate_image(prompt: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 
                       aspect_ratios_selection: str = "1024*1024", image_seed: int = 1234, sharpness: int = 2) -> Optional[dict]:
        """
        Generates an image based on the given prompt and settings.

        :param prompt: Textual description of the image to generate.
        :param performance_selection: Choice of performance level affecting generation speed and quality.
        :param aspect_ratio: The desired aspect ratio of the generated image.
        :param image_seed: Seed for the image generation process for reproducibility.
        :param sharpness: The sharpness level of the generated image.
        :return: The generated image or None if an error occurred.
        """
        try:
            output = replicate.run(
                "stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b",
                input={
                    "prompt": prompt,
                    "performance_selection": performance_selection,
                    "aspect_ratios_selection": aspect_ratios_selection,
                    "image_seed": image_seed,
                    "sharpness": sharpness
                }
            )
            
            logging.info("Image generated successfully.")
            return output
        except Exception as e:
            logging.error(f"Failed to generate image: {e}")
            return None
    @staticmethod
    def decode_image(base64_data: str) -> Optional[Image.Image]:
        try:
                        image_data = base64.b64decode(base64_data)
                        image_stream = BytesIO(image_data)
                        image = Image.open(image_stream)
            return image
        except Exception as e:
            print(f"An error occurred while decoding the image: {e}")
            return None
    @staticmethod 
    def download_image(url: str, save_path: str) -> Tuple[str, str]:
        try:
            response = requests.get(url)
            
            if response.status_code == 200:
                                                filename = str(uuid.uuid4()) + '.png'
                                complete_path = os.path.join(save_path, filename)
                
                                os.makedirs(os.path.dirname(complete_path), exist_ok=True)
                
                                image = Image.open(BytesIO(response.content))
                                image.save(complete_path, format=image.format)
                logging.info(f"Image saved to {complete_path}")
                return (url, complete_path)
            else:
                raise RuntimeError(f"Failed to download image. Status code: {response.status_code}")
        except Exception as e:
            raise RuntimeError(f"An error occurred: {e}") from e
        
if __name__ =='__main__':
    a = {
    "frame_1": {
        "Animated Element": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",
    },
    "frame_2": {
        "CTA Text": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."
    },
    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."
    }
    test = ImageGenerator(a)

    test.generate_images()
from PIL import Image
import os

def get_image_dimensions(image_path: str) -> tuple:
   
    with Image.open(image_path) as img:
        width, height = img.size

    return width, height

def resize_and_save_image(image_path: str, new_width: int, new_height: int, output_dir: str = '../images/resized') -> None:
   
    os.makedirs(output_dir, exist_ok=True)
    filename = os.path.basename(image_path)
    output_path = os.path.join(output_dir, filename)

    with Image.open(image_path) as img:
        resized_img = img.resize((new_width, new_height))
        resized_img.save(output_path)

    print(f"Resized image saved to {output_path}")
Repository Structure: '
' └── README.md
 '
' Commit History: 
{"insertions": [1], "deletions": [1], "lines": [2], "committed_datetime": ["2024-02-14 11:31:43"], "commit_count": 1} 
 Content: 















import pandas as pd
per_df = pd.read_csv('Challenge_Data/performance_data.csv')

per_df.info()
per_df.head(5)




per_df.loc[5].preview_link
from typing import Tuple

import cv2

import matplotlib.pyplot as plt
def locate_image_on_image(locate_image: str, on_image: str, prefix: str = '', visualize: bool = False, color: Tuple[int, int, int] = (0, 0, 255)):

    try:



        image = cv2.imread(on_image)

        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)



        template = cv2.imread(locate_image, 0)



        result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)

        _, _, _, max_loc = cv2.minMaxLoc(result)



        height, width = template.shape[:2]



        top_left = max_loc

        bottom_right = (top_left[0] + width, top_left[1] + height)



        if visualize:

            cv2.rectangle(image, top_left, bottom_right, color, 1)

            plt.figure(figsize=(10, 10))

            plt.axis('off')

            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            plt.imshow(image)



        return {f'{prefix}top_left_pos': top_left, f'{prefix}bottom_right_pos': bottom_right}



    except cv2.error as err:

        print(err)
locate_image_on_image(

    'Challenge_Data/Assets/4c3bb41d4f40f39842b7b8d3f536366a/engagement_instruction.png', 'Challenge_Data/Assets/4c3bb41d4f40f39842b7b8d3f536366a/_preview.png', prefix='eng_', visualize=True)

locate_image_on_image(

    'Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/engagement_instruction.png', 'Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/_preview.png', prefix='eng_', visualize=True)






import extcolors

import pandas as pd

from colormap import rgb2hex

from matplotlib import pyplot as plt

from PIL import Image
def identify_color_composition(image,

                               tolerance: int = 12,

                               limit: int = 2,

                               visualize: bool = False) -> None:

    """Function that identifies the color composition of a

    given image path."""



    extracted_colors = extcolors.extract_from_path(

        image, tolerance=tolerance, limit=limit)



    identified_colors = color_to_df(extracted_colors)



    if not visualize:

        return identified_colors



    list_color = list(identified_colors['c_code'])

    list_percent = [int(i) for i in list(identified_colors['occurrence'])]



    text_c = [c + ' ' + str(round(p*100/sum(list_percent), 1)) + '%' for c, p in zip(list_color,

                                                                                     list_percent)]

    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(100, 100), dpi=10)

    wedges, _ = ax[0].pie(list_percent,

                          labels=text_c,

                          labeldistance=1.05,

                          colors=list_color,

                          textprops={'fontsize': 60, 'color': 'black'}

                          )



    plt.setp(wedges, width=0.3)



    
    plt.setp(wedges, width=0.36)



    ax[0].set_aspect("equal")

    fig.set_facecolor('grey')



    ax[1].imshow(Image.open(image))



    plt.show()



    return identified_colors





def color_to_df(extracted_colors: tuple):

    """Converts RGB Color values from extcolors output to HEX Values."""



    colors_pre_list = str(extracted_colors).replace(

        '([(', '').replace(')],', '), (').split(', (')[0:-1]

    df_rgb = [i.split('), ')[0] + ')' for i in colors_pre_list]

    df_percent = [i.split('), ')[1].replace(')', '')

                  for i in colors_pre_list]



    
    df_rgb_values = [(int(i.split(", ")[0].replace("(", "")),

                      int(i.split(", ")[1]),

                      int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]



    df_color_up = [rgb2hex(int(i.split(", ")[0].replace("(", "")),

                           int(i.split(", ")[1]),

                           int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]



    colors_df = pd.DataFrame(zip(df_color_up, df_rgb_values, df_percent),

                             columns=['c_code', 'rgb', 'occurrence'])



    return colors_df

identify_color_composition('Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/landing_2.jpg',tolerance=12,limit=8, visualize=True)
identify_color_composition(

    'Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/engagement_instruction.png', visualize=True)
identify_color_composition(

    'Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/cta.png', limit=3 ,visualize=True)









import torch
%cd yolov7

! python detect.py --weights yolov7-tiny.pt --conf 0.4 --img-size 1280 --source /home/aron/Music/10acadamy/week10/Semantic-Image-and-Text-Alignment-Automated-Storyboard-Synthesis-for-Digital-Advertising/Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/_preview.png --project trial --name run

%cd ..



!sudo apt install tesseract-ocr

!pip install pytesseract
import pytesseract

import cv2

import numpy as np

from typing import List, Tuple
def convert_hex_to_rgb(hex_color: str, normalize: bool = True) -> List[str]:

    """Converts a HEX color to a RGB color



    Args:

        hex_color (str): HEX color code to convert

        normalize (bool, optional): Choice to normalize calculated rgb values . Defaults to True.



    Returns:

        List[str]: List of RGB values in order, normalized or not.

    """

    colors = hex_color[1:]



    
    colors = [int(colors[0:2], base=16),  
              int(colors[2:4], base=16),  
              int(colors[4:6], base=16)]  


    
    if normalize:

        colors = [color / 255 for color in colors]



    return colors



def get_luminance(hex_color: str) -> float:

    """Calculates the luminance of a given HEX color



    Args:

        hex_color (str): HEX color code to calculate luminance for



    Returns:

        float: luminance value of color

    """

    colors = convert_hex_to_rgb(hex_color)



    luminance = colors[0] * 0.2126 + colors[1] * 0.7152 + colors[2] * 0.0722



    return luminance



def fix_image_background(image_path: str):

    identified_colors = identify_color_composition(image_path)

    text_color = identified_colors['c_code'].to_list()[0]

    text_color_rgb = identified_colors['rgb'].to_list()[0]

    luminance = get_luminance(hex_color=text_color)



    if luminance < 140:

        background_color = (255, 255, 255)

    else:

        background_color = (0, 0, 0)



    
    image = cv2.imread(image_path)



    
    image[np.all(image != text_color_rgb, axis=-1)] = background_color



    return image



def extract_text(image_path, tesseract_cmd: str = '', fix_background: bool = False):

    
    try:

        if fix_background:

            text = pytesseract.image_to_string(

                fix_image_background(image_path))

        else:

            text = pytesseract.image_to_string(image_path)



        return text



    except pytesseract.TesseractNotFoundError:

        raise Exception(

            f'Failure: Tesseract is not installed or not available in the defined path {tesseract_cmd}')
extract_text('/home/aron/Music/10acadamy/week10/Semantic-Image-and-Text-Alignment-Automated-Storyboard-Synthesis-for-Digital-Advertising/Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/_preview.png')
extract_text('/home/aron/Music/10acadamy/week10/Semantic-Image-and-Text-Alignment-Automated-Storyboard-Synthesis-for-Digital-Advertising/Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/engagement_instruction.png', fix_background=True)
extract_text('/home/aron/Music/10acadamy/week10/Semantic-Image-and-Text-Alignment-Automated-Storyboard-Synthesis-for-Digital-Advertising/Challenge_Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/cta.png', fix_background=True)





from typing import Tuple

from time import sleep

from selenium import webdriver

from selenium.webdriver.firefox.options import Options

from selenium.webdriver.support.ui import WebDriverWait

from selenium.webdriver.common.by import By

from selenium.common.exceptions import TimeoutException, NoSuchElementException

from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

from os import path

from subprocess import Popen, call

import pyautogui

import ffmpeg class CreativeFrameExtractor:

    '''

    Class responsible for Extracting Creative Start and End Frames.

    It requires a chrome webdriver compatible with selenium to be

    installed/included in the run environment path.

    '''



    def __init__(self, preview_url: str,

                 engagement_type: str,

                 save_location: str = '',

                 browser_edges: Tuple[float, float] = (70, 1039)) -> None: 


        self.preview_url = preview_url

        self.engagement_type = engagement_type

        self.browser_edges = browser_edges

        self.file_name = '-'.join(preview_url.split('/')[-3:-1]) 
        self.save_location = save_location

        self.video_name = path.join(self.save_location, self.file_name) 
        self.cmd = f"ffmpeg -f gdigrab -draw_mouse 0 -framerate 60 -i desktop -vcodec libx264rgb {self.video_name}.mkv -y"



        


        
        
        self.opt = Options()

        self.opt.add_argument("--hide-scrollbars")

        
        self.capabilities = DesiredCapabilities.CHROME

        self.capabilities["goog:loggingPrefs"] = {"browser": "ALL"}



    def is_status_complete(self, passed_driver) -> bool:

        '''

        Function to check status of the AD-Unit and its completion. 
        
        '''

        
        logs = passed_driver.get_log("browser")



        for log in logs:

            
            if log["source"] == "console-api":

                
                message = log["message"]



                if '"GAME CREATED"' in message or '"DROPPED"' in message:

                    
                    print("Starting Recording AD-UNIT...")

                    print(log)

                    return False



                if '"START"' in message:

                    
                    print("AD-UNIT Engaged...")

                    print(log)

                    return False



                if '"GAME COMPLETE"' in message:

                    
                    print("Stopped Recording AD-UNIT...")

                    print(log)

                    return True



        return False

    @staticmethod

    def terminate(process: Popen[bytes]) -> None:

        '''

        Function to stop/terminate a process.

        '''

        
        if process.poll() is None:

            call("taskkill /F /T /PID " + str(process.pid))



    @staticmethod

    def crop_video(filename: str, x_pos: float = 0, y_pos: float = 70, width: float = 650, height: float = 970) -> None:

        '''

        Function to crop a video with given location and size specific parameters. 
        '''

        print(filename)

        input_video = ffmpeg.input(f"{filename}.mkv")

        cropped_video = ffmpeg.crop(input_video, x_pos, y_pos, width, height)

        output_video = ffmpeg.output(cropped_video, f"{filename}_cropped.mkv")

        ffmpeg.run(output_video)



    def _imitate_engagement(self, ad_size: Tuple[float, float]) -> None:

        '''

        Function to imitate a given engagement type. 
        '''

        center = (ad_size[0]/2, self.browser_edges[0] + (ad_size[1]/2))



        if self.engagement_type == "tap":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.leftClick()



        elif self.engagement_type == "swipe right":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.dragRel(center[0], 0, duration=1)



        elif self.engagement_type == "swipe left":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.dragRel(-center[0], 0, duration=1)



        elif self.engagement_type == "tap and hold":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.click()



        elif self.engagement_type == "scrub":

            pyautogui.moveTo(center[0] - (1/2 * center[0]),

                             center[1] - (2/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)

            pyautogui.dragRel(-center[0], (1/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)

            pyautogui.dragRel(-center[0], (1/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)



    def generate_preview_video(self) -> None:

        '''

        Function to generate preview video and also a cropped version of the video.

        '''

        
        driver = webdriver.Chrome(options=self.opt)

        
        driver.maximize_window()



        try:

            
            driver.get(self.preview_url)



            
            canvas = driver.find_element(By.TAG_NAME, "canvas")



            
            video_recording = Popen(self.cmd)



            
            ad_size = (canvas.size.get("width"), canvas.size.get("height"))



            
            self._imitate_engagement(ad_size)



            
            
            WebDriverWait(driver, 100).until(self.is_status_complete)



            sleep(5)



            
            self.terminate(video_recording)



            
            driver.close()



            
            self.crop_video(self.video_name, x_pos=0, y_pos=70,

                            width=ad_size[0], height=ad_size[1])



        except TimeoutException:

            print("TimeOut Exception Fired")

            print("AD-Unit Status Console Logs did not Complete. Engagement Failed.")

            driver.close()



        except NoSuchElementException:

            print(f"AD-Unit Failed to Load: {self.preview_url}")

            driver.close()



    def generate_frames(self) -> None:

        '''

        Function to generate creative start and end frames.

        '''

        
        driver = webdriver.Chrome(

            options=self.opt, desired_capabilities=self.capabilities, )

        
        driver.maximize_window()



        try:

            
            driver.get(self.preview_url)



            
            canvas = driver.find_element(By.TAG_NAME, "canvas")



            
            canvas.screenshot(

                path.join(self.save_location, f'{self.file_name}_start_frame.png'))

            print('Start Frame captured')



            
            ad_size = (canvas.size.get("width"), canvas.size.get("height"))



            
            self._imitate_engagement(ad_size)



            
            
            WebDriverWait(driver, 100).until(self.is_status_complete)



            sleep(5)



            
            canvas.screenshot(path.join(self.save_location,f'{self.file_name}_end_frame.png'))

            print('End Frame Captured')



            
            driver.close()



        except TimeoutException:

            print("TimeOut Exception Fired")

            print("AD-Unit Status Console Logs did not Complete. Engagement Failed.")

            driver.close()



        except NoSuchElementException:

            print(f"AD-Unit Failed to Load: {self.preview_url}")

            driver.close()


ext = CreativeFrameExtractor(

    'https://s3.us-east-1.amazonaws.com/a.futureadlabs.com-us-east-1-backup/us-east-1/games/5a48ffcbf611f167ecbb884e807f31f6/7e6dcb347f24c7843a8d/index.html', 'tap', save_location='./extracted_images/')

ext.generate_frames()

ext.generate_preview_video()
import cv2

import numpy as np

from PIL import Image, ImageDraw, ImageFont

import json




with open('/home/aron/Music/10acadamy/week10/Semantic-Image-and-Text-Alignment-Automated-Storyboard-Synthesis-for-Digital-Advertising/Data/concepts.json') as f:

    data = json.load(f)

data





for frame_data in data:

    description = frame_data['concept']

    
    text_image = Image.new('RGB', (800, 200), color='white')

    draw = ImageDraw.Draw(text_image)

    font = ImageFont.truetype("arial.ttf", 24)

    draw.text((10, 10), description, font=font, fill='black')

    text_image_np = np.array(text_image)

    text_image_np = cv2.cvtColor(text_image_np, cv2.COLOR_RGB2BGR)

    cv2.imwrite(f'frame_{frame_data["frame"]}_text.png', text_image_np)




for frame_data in data:

    frame = frame_data['frame']

    for asset_data in frame_data['asset_suggestions']:

        asset_name = asset_data['asset_name']

        asset_description = asset_data['asset_description']

        
        image = np.ones((600, 800, 3), dtype=np.uint8) * 255

        font = cv2.FONT_HERSHEY_SIMPLEX

        cv2.putText(image, asset_description, (10, 50), font, 1, (0, 0, 0), 2, cv2.LINE_AA)

        cv2.imwrite(f'frame_{frame}_{asset_name}.png', image) 

 

from openai import OpenAI
client = OpenAI(
  api_key="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"
)

system_prompt = """
        Assume you are an expert prompt engineer. I will give you a 
        description of an image and you will write me a prompt based 
        on the following format: 
            
            Format : 'subject + style + details + format of output'.
        
        You will add this details by yourself. This is an example prompt 
            Example: 'Draw a ginger-and-white striped cat looking excited 
                      as it chases a mouse around a kitchen, in the style 
                      of an impressionist painter, with light streaming 
                      through the windows and prominent use of blue and yellow.
                      The output of the image has an aspect ratio of 1 : 1 and 
                      the image is used for social media post.'
        This is the kind of prompt I want you to generate.
"""

response = client.chat.completions.create(
  model="gpt-3.5-turbo-0125",
  response_format={ "type": "json_object" },
  messages=[
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "A suspenseful animation of a LEGO CITY set, with buildings, vehicles, and mini-figures coming to life in a dynamic, 3D environment. The animation is designed to captivate the viewer's attention and set the stage for the upcoming challenge."}
  ]
)
print(response.choices[0].message.content)
import pandas as pd

from typing import Tuple

import cv2
df = pd.read_csv('../Data/Challenge_Data/performance_data.csv')
df.head()




df.loc[0].preview_link
def locate_image_on_image(locate_image: str, on_image: str, prefix: str = '', visualize: bool = False, color: Tuple[int, int, int] = (0, 0, 255)):

    try:



        image = cv2.imread(on_image)

        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)



        template = cv2.imread(locate_image, 0)



        result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)

        _, _, _, max_loc = cv2.minMaxLoc(result)



        height, width = template.shape[:2]



        top_left = max_loc

        bottom_right = (top_left[0] + width, top_left[1] + height)



        if visualize:

            cv2.rectangle(image, top_left, bottom_right, color, 1)

            plt.figure(figsize=(10, 10))

            plt.axis('off')

            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            plt.imshow(image)



        return {f'{prefix}top_left_pos': top_left, f'{prefix}bottom_right_pos': bottom_right}



    except cv2.error as err:

        print(err)
locate_image_on_image(

    '..Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/engagement_instruction.png', '..Data/Assets/5a48ffcbf611f167ecbb884e807f31f6/_preview.png', prefix='eng_', visualize=True)
import pytesseract

import cv2

import numpy as np

from typing import List, Tuple
def convert_hex_to_rgb(hex_color: str, normalize: bool = True) -> List[str]:

    """Converts a HEX color to a RGB color



    Args:

        hex_color (str): HEX color code to convert

        normalize (bool, optional): Choice to normalize calculated rgb values . Defaults to True.



    Returns:

        List[str]: List of RGB values in order, normalized or not.

    """

    colors = hex_color[1:]



    
    colors = [int(colors[0:2], base=16),  
              int(colors[2:4], base=16),  
              int(colors[4:6], base=16)]  


    
    if normalize:

        colors = [color / 255 for color in colors]



    return colors



def get_luminance(hex_color: str) -> float:

    """Calculates the luminance of a given HEX color



    Args:

        hex_color (str): HEX color code to calculate luminance for



    Returns:

        float: luminance value of color

    """

    colors = convert_hex_to_rgb(hex_color)



    luminance = colors[0] * 0.2126 + colors[1] * 0.7152 + colors[2] * 0.0722



    return luminance



def fix_image_background(image_path: str):

    identified_colors = identify_color_composition(image_path)

    text_color = identified_colors['c_code'].to_list()[0]

    text_color_rgb = identified_colors['rgb'].to_list()[0]

    luminance = get_luminance(hex_color=text_color)



    if luminance < 140:

        background_color = (255, 255, 255)

    else:

        background_color = (0, 0, 0)



    
    image = cv2.imread(image_path)



    
    image[np.all(image != text_color_rgb, axis=-1)] = background_color



    return image



def extract_text(image_path, tesseract_cmd: str = '', fix_background: bool = False):

    
    try:

        if fix_background:

            text = pytesseract.image_to_string(

                fix_image_background(image_path))

        else:

            text = pytesseract.image_to_string(image_path)



        return text



    except pytesseract.TesseractNotFoundError:

        raise Exception(

            f'Failure: Tesseract is not installed or not available in the defined path {tesseract_cmd}')
from typing import Tuple

from time import sleep

from selenium import webdriver

from selenium.webdriver.chrome.options import Options

from selenium.webdriver.support.ui import WebDriverWait

from selenium.webdriver.common.by import By

from selenium.common.exceptions import TimeoutException, NoSuchElementException

from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

from os import path

from subprocess import Popen, call

import pyautogui

import ffmpeg class CreativeFrameExtractor:

    '''

    Class responsible for Extracting Creative Start and End Frames.

    It requires a chrome webdriver compatible with selenium to be

    installed/included in the run environment path.

    '''



    def __init__(self, preview_url: str,

                 engagement_type: str,

                 save_location: str = '',

                 browser_edges: Tuple[float, float] = (70, 1039)) -> None: 


        self.preview_url = preview_url

        self.engagement_type = engagement_type

        self.browser_edges = browser_edges

        self.file_name = '-'.join(preview_url.split('/')[-3:-1]) 
        self.save_location = save_location

        self.video_name = path.join(self.save_location, self.file_name) 
        self.cmd = f"ffmpeg -f gdigrab -draw_mouse 0 -framerate 60 -i desktop -vcodec libx264rgb {self.video_name}.mkv -y"



        


        
        
        self.opt = Options()

        self.opt.add_argument("--hide-scrollbars")

        self.opt.add_experimental_option(

            "excludeSwitches", ["enable-automation"])

        
        self.capabilities = DesiredCapabilities.CHROME

        self.capabilities["goog:loggingPrefs"] = {"browser": "ALL"}



    def is_status_complete(self, passed_driver) -> bool:

        '''

        Function to check status of the AD-Unit and its completion. 
        
        '''

        
        logs = passed_driver.get_log("browser")



        for log in logs:

            
            if log["source"] == "console-api":

                
                message = log["message"]



                if '"GAME CREATED"' in message or '"DROPPED"' in message:

                    
                    print("Starting Recording AD-UNIT...")

                    print(log)

                    return False



                if '"START"' in message:

                    
                    print("AD-UNIT Engaged...")

                    print(log)

                    return False



                if '"GAME COMPLETE"' in message:

                    
                    print("Stopped Recording AD-UNIT...")

                    print(log)

                    return True



        return False



    @staticmethod

    def terminate(process: Popen[bytes]) -> None:

        '''

        Function to stop/terminate a process.

        '''

        
        if process.poll() is None:

            call("taskkill /F /T /PID " + str(process.pid))



    @staticmethod

    def crop_video(filename: str, x_pos: float = 0, y_pos: float = 70, width: float = 650, height: float = 970) -> None:

        '''

        Function to crop a video with given location and size specific parameters. 
        '''

        print(filename)

        input_video = ffmpeg.input(f"{filename}.mkv")

        cropped_video = ffmpeg.crop(input_video, x_pos, y_pos, width, height)

        output_video = ffmpeg.output(cropped_video, f"{filename}_cropped.mkv")

        ffmpeg.run(output_video)



    def _imitate_engagement(self, ad_size: Tuple[float, float]) -> None:

        '''

        Function to imitate a given engagement type. 
        '''

        center = (ad_size[0]/2, self.browser_edges[0] + (ad_size[1]/2))



        if self.engagement_type == "tap":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.leftClick()



        elif self.engagement_type == "swipe right":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.dragRel(center[0], 0, duration=1)



        elif self.engagement_type == "swipe left":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.dragRel(-center[0], 0, duration=1)



        elif self.engagement_type == "tap and hold":

            pyautogui.moveTo(center[0], center[1], duration=1)

            pyautogui.click()



        elif self.engagement_type == "scrub":

            pyautogui.moveTo(center[0] - (1/2 * center[0]),

                             center[1] - (2/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)

            pyautogui.dragRel(-center[0], (1/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)

            pyautogui.dragRel(-center[0], (1/3 * center[1]), duration=0.2)

            pyautogui.dragRel(center[0], 0, duration=0.2)



    def generate_preview_video(self) -> None:

        '''

        Function to generate preview video and also a cropped version of the video.

        '''

        
        driver = webdriver.Chrome(

            options=self.opt, desired_capabilities=self.capabilities)

        
        driver.maximize_window()



        try:

            
            driver.get(self.preview_url)



            
            canvas = driver.find_element(By.TAG_NAME, "canvas")



            
            video_recording = Popen(self.cmd)



            
            ad_size = (canvas.size.get("width"), canvas.size.get("height"))



            
            self._imitate_engagement(ad_size)



            
            
            WebDriverWait(driver, 100).until(self.is_status_complete)



            sleep(5)



            
            self.terminate(video_recording)



            
            driver.close()



            
            self.crop_video(self.video_name, x_pos=0, y_pos=70,

                            width=ad_size[0], height=ad_size[1])



        except TimeoutException:

            print("TimeOut Exception Fired")

            print("AD-Unit Status Console Logs did not Complete. Engagement Failed.")

            driver.close()



        except NoSuchElementException:

            print(f"AD-Unit Failed to Load: {self.preview_url}")

            driver.close()



    def generate_frames(self) -> None:

        '''

        Function to generate creative start and end frames.

        '''

        
        driver = webdriver.Chrome(

            options=self.opt, desired_capabilities=self.capabilities, )

        
        driver.maximize_window()



        try:

            
            driver.get(self.preview_url)



            
            canvas = driver.find_element(By.TAG_NAME, "canvas")



            
            canvas.screenshot(

                path.join(self.save_location, f'{self.file_name}_start_frame.png'))

            print('Start Frame captured')



            
            ad_size = (canvas.size.get("width"), canvas.size.get("height"))



            
            self._imitate_engagement(ad_size)



            
            
            WebDriverWait(driver, 100).until(self.is_status_complete)



            sleep(5)



            
            canvas.screenshot(path.join(self.save_location,f'{self.file_name}_end_frame.png'))

            print('End Frame Captured')



            
            driver.close()



        except TimeoutException:

            print("TimeOut Exception Fired")

            print("AD-Unit Status Console Logs did not Complete. Engagement Failed.")

            driver.close()



        except NoSuchElementException:

            print(f"AD-Unit Failed to Load: {self.preview_url}")

            driver.close()
import torch

model = torch.hub.load('ultralytics/yolov5', 'custom', path="../models/best.pt")
import os, sys, glob

import pandas as pd



rpath = os.path.abspath('..')

if rpath not in sys.path:

    sys.path.insert(0, rpath)



from scripts.symbol_detector import SymbolDetection
sd = SymbolDetection(model, 0.25)
sd.query('../datasets/new_previews/002dbbd85ef3fe6a2e7d0754fb9f9a1a.png')
import cv2



image_path = '../datasets/new_previews/002dbbd85ef3fe6a2e7d0754fb9f9a1a.png'

image = cv2.imread(image_path)



if image is None:

    print(f"Error: Unable to load image at {image_path}")

    exit()



height, width, channels = image.shape

print(f"Image Dimensions: Width={width}, Height={height}, Channels={channels}")



cv2.rectangle(image, (0, 0), (width-1, height-1), (255, 0, 0), 2)



cv2.imshow('Image with Dimensions', image)

cv2.waitKey(0)

cv2.destroyAllWindows()

folder_path = '../datasets/new_previews/'



output = []

image_files = glob.glob(os.path.join(folder_path, '*.jpg')) + glob.glob(os.path.join(folder_path, '*.png'))



for image_file in image_files:

    coords = sd.query(image_file)

    coords.append(

        {

            'img-name': image_file.split('/')[-1]

        }

    )

    output.append(coords)
output[:5]
df = pd.read_csv('../datasets/performance_data.csv')

df
rows = []

for image in output:

    game_id = None 

    single_image = []

    for row_data in image:

        if "img-name" in row_data:

            game_id = row_data['img-name'][:-4]

    

    temp = {'game_id': game_id}

    for row_data in image:

        if "img-name" in row_data:

            continue 

        

        if 'Box' in row_data:

            for attribute in ['x', 'y', 'width', 'height']:

                temp[f"{row_data['label']}_{attribute}"] = row_data['Box'][attribute]

            temp[f'{row_data['label']}_conf'] = row_data['conf']

    

    rows.append(temp)

rows[:5]
df = pd.DataFrame(rows)
df.head()
df.value_counts()
df.to_csv('../datasets/labeled.csv', index=False)
from PIL import Image
import base64
from io import BytesIO

class SymbolDetection:
    """
    A class used to reprsent a Symbol detector
    """
    def __init__(self, model, conf):
        """
        constructs a symbol detector object
        
        parameters:
            model: symbol-detector model
            conf: NMS confidence threshold
        """
        self.model = model
        self.conf = conf

    def query(self, filename):
        """
        Query the symbol-detector object to detect our image

        parameters:
            filename: Path of the input image
        """

                results = self.model(filename)
        objects = results.crop(save=False)
        output = []
        for obj in objects:
            if (self.conf <= float(obj['conf'])):
                width = int(obj['box'][2])
                height = int(obj['box'][3])
                x = int(obj['box'][0])
                y = int(obj['box'][1])
                normalized_x = x / width 
                normalized_y = y / height

                output.append( {
                    "Box": {
                        "x": normalized_x,
                        "y": normalized_y,
                        "width": width,
                        "height": height
                    },
                    "conf": float(obj['conf']),
                    "label": self.model.names[int(obj['cls'])]
                })
        return output
import os,sys

sys.path.append('../scripts/')



from image_generation import ImageGenerator

from image_composer import ImageComposer

concept = {

    "frame_1": {

        "Background": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",

        "CTA Button": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."

    },

    "frame_2": {

        "Interactive Elements": "A digital beat mixer interface with vibrant, touch-responsive Latin American instrument icons like congas, claves, and maracas, each activating a unique sound layer.",

        "Background": "A dynamic, abstract representation of sound waves that move in sync with the user's interactions."

    },

    "frame_3": {

        "Background": "A dynamic, abstract representation of sound waves that move in sync with the user's interactions.",

        "Animation": "A kaleidoscope of colors that dance across the screen, with each beat added, symbolizing the fusion of cultures and music.",

        "CTA Button": "A 'Play Your Mix' button that pulses like a heartbeat, encouraging users to share their creation."

    },

    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."

}
concept = {

    "frame_1": {

        "Background": "A high-resolution 3D animation of a Coca-Cola bottle on a vibrant background. The bottle is detailed with effervescent bubbles rising to the top. Gradually, the bottle transforms into a DJ turntable, featuring a sleek design with illuminated controls and a vinyl record emblazoned with the Coke Studio logo, symbolizing the fusion of refreshment and music.",

        "CTA Button": "'Mix Your Beat' displayed in a bold, dynamic font on a contrasting background. The font style is playful and energetic, designed to evoke a sense of creativity and engagement. The text is surrounded by a subtle glow, suggesting interactivity and inviting the viewer to start their musical adventure."

    },

    "frame_2": {

        "Interactive Elements": "A digital beat mixer interface showcasing vibrant icons of Latin American musical instruments such as congas, claves, and maracas. Each icon is designed to be touch-responsive, illuminating and animating upon interaction to emit distinctive sound layers. The interface is set against a background that visually represents the music's energy through animated, abstract sound waves.",

        "Background": "An abstract, animated visualization of sound waves that dynamically react to interactions on the digital beat mixer. The sound waves pulse, expand, and change colors in real time, creating a visual representation of the music being mixed. The background enhances the immersive experience, reflecting the user's creative input in a visually engaging manner."

    },

    "frame_3": {

        "Background": "An abstract, animated visualization of sound waves that dynamically react to interactions on the digital beat mixer. The sound waves pulse, expand, and change colors in real time, creating a visual representation of the music being mixed. The background enhances the immersive experience, reflecting the user's creative input in a visually engaging manner.",

        "Animation": "A dynamic, colorful animation that bursts into life with each added beat, symbolizing the vibrant energy of Coke Studio's musical fusion. The screen is filled with an ever-changing kaleidoscope of colors, with each new layer of music adding complexity to the visual celebration, reflecting the creativity and diversity of the user's mix.",

        "CTA Button": "A prominently placed 'Play Your Mix' button that pulses in rhythm with the generated music mix. The button's design is inviting, with a visual style that suggests action and progression, encouraging the viewer to engage with their creation and share it. The pulsing effect enhances the sense of urgency and excitement about the music mix."

    },

    "explanation": "These descriptions are crafted to ensure each asset is vividly detailed and self-contained, suitable for generating individual images that collectively build the narrative of an interactive beat mixer. From the initial transformation of a Coca-Cola bottle into a DJ turntable, through the engagement with a virtual beat mixer, to the celebration of a personalized music mix, each description is designed to capture the essence of the interactive experience while being standalone for image generation purposes."

}
concept = {

    "frame_1": {"Background": "In the center of a vibrant scene, a high-resolution 3D Coca-Cola bottle surrounded by effervescent bubbles captures the viewer's attention. As the bubbles rise, the bottle seamlessly transforms into a sleek DJ turntable, complete with illuminated controls and a spinning vinyl record bearing the Coke Studio logo. This imagery symbolizes a fusion of refreshing beverage and rhythmic beats. Directly below this dynamic transformation, the call-to-action 'Mix Your Beat' shines in a bold, dynamic font with a playful energy. The text, surrounded by a subtle glow, invites interaction, set against a backdrop designed to evoke creativity and musical exploration."},

    "frame_2": {"Background": "The viewer is presented with an immersive digital beat mixer interface, populated with vibrant, touch-responsive icons of Latin American musical instruments such as congas, claves, and maracas. Each icon, when interacted with, illuminates and animates to release a distinctive sound layer, set against an animated background of abstract, pulsating sound waves. These waves dynamically react to the mixer's activity, changing colors and patterns in real time to visually represent the user's musical creations, encouraging exploration and engagement through a fusion of visual and auditory feedback."},

    "frame_3": {"Background": "With each beat added to the mix, the screen erupts into a dynamic display of colors, creating a visual celebration that mirrors the energy of Coke Studio's musical fusion. This kaleidoscopic animation evolves with the complexity of the user's mix, symbolizing the blend of cultures and music. At the heart of this vibrant spectacle, a 'Play Your Mix' button pulses in rhythm with the music, its design inviting and central, encouraging the viewer to engage and share their creation. The pulsing button serves as a visual anchor, drawing the viewer's attention and culminating the creative process."}

}
ig = ImageGenerator(concept)

generated_images = ig.generate_images()
image_comp = ImageComposer(600,600, list(generated_images.values()))

generated_frames = image_comp.generate_frames()
import os
import logging
from typing import Literal,List,Tuple,Optional,Dict
import base64
from dotenv import load_dotenv
from io import BytesIO
import uuid
import requests
import replicate
from PIL import Image
import requests
from pydantic import HttpUrl
from http.client import HTTPResponse

load_dotenv()

os.environ["REPLICATE_API_TOKEN"] = 'r8_0gtYm4UC2kwFP09VIDyrCl0hqEdo4Qf1lyvPd'
logging.basicConfig(level=logging.INFO)

class ImageGenerator:
    def __init__(self, asset_suggestions: dict) -> None:
        self.asset_suggestions = asset_suggestions
    def generate_images(self, store_location: str = '../images') -> Dict[str, List[Tuple[str, str]]]:
        keyType = str
        valueType = List[Tuple[str,str]]
        generated_images:Dict[keyType, valueType] = {}
        for frame, elements in self.asset_suggestions.items():
            if frame.startswith('frame'):
                generated_images[frame] = []
                for type, description in elements.items():
                    try:
                                                generated_image_path = ImageGenerator.download_image(ImageGenerator.generate_image(prompt=description)[0], store_location)
                                                generated_images[frame].append((type, *generated_image_path))
                    except Exception as e:
                        print(f"An error occurred while generating image: {e}")
        return generated_images
    @staticmethod
    def generate_image(prompt: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 
                       aspect_ratios_selection: str = "1024*1024", image_seed: int = 1234, sharpness: int = 2) -> Optional[dict]:
        """
        Generates an image based on the given prompt and settings.

        :param prompt: Textual description of the image to generate.
        :param performance_selection: Choice of performance level affecting generation speed and quality.
        :param aspect_ratio: The desired aspect ratio of the generated image.
        :param image_seed: Seed for the image generation process for reproducibility.
        :param sharpness: The sharpness level of the generated image.
        :return: The generated image or None if an error occurred.
        """
        try:
            output = replicate.run(
                "stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b",
                input={
                    "prompt": prompt,
                    "performance_selection": performance_selection,
                    "aspect_ratios_selection": aspect_ratios_selection,
                    "image_seed": image_seed,
                    "sharpness": sharpness
                }
            )
            
            logging.info("Image generated successfully.")
            return output
        except Exception as e:
            logging.error(f"Failed to generate image: {e}")
            return None
    @staticmethod
    def decode_image(base64_data: str) -> Optional[Image.Image]:
        try:
                        image_data = base64.b64decode(base64_data)
                        image_stream = BytesIO(image_data)
                        image = Image.open(image_stream)
            return image
        except Exception as e:
            print(f"An error occurred while decoding the image: {e}")
            return None
    @staticmethod 
    def download_image(url: str, save_path: str) -> Tuple[str, str]:
        try:
            response = requests.get(url)
            
            if response.status_code == 200:
                                                filename = str(uuid.uuid4()) + '.png'
                                complete_path = os.path.join(save_path, filename)
                
                                os.makedirs(os.path.dirname(complete_path), exist_ok=True)
                
                                image = Image.open(BytesIO(response.content))
                                image.save(complete_path, format=image.format)
                logging.info(f"Image saved to {complete_path}")
                return (url, complete_path)
            else:
                raise RuntimeError(f"Failed to download image. Status code: {response.status_code}")
        except Exception as e:
            raise RuntimeError(f"An error occurred: {e}") from e
        
if __name__ =='__main__':
    a = {
    "frame_1": {
        "Animated Element": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",
    },
    "frame_2": {
        "CTA Text": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."
    },
    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."
    }
    test = ImageGenerator(a)

    test.generate_images()
from PIL import Image
import os

def get_image_dimensions(image_path: str) -> tuple:
   
    with Image.open(image_path) as img:
        width, height = img.size

    return width, height

def resize_and_save_image(image_path: str, new_width: int, new_height: int, output_dir: str = '../images/resized') -> None:
   
    os.makedirs(output_dir, exist_ok=True)
    filename = os.path.basename(image_path)
    output_path = os.path.join(output_dir, filename)

    with Image.open(image_path) as img:
        resized_img = img.resize((new_width, new_height))
        resized_img.save(output_path)

    print(f"Resized image saved to {output_path}")
Repository Structure: '
' ├── Deeplabv3
│   └── asset_inference.ipynb
├── main.py
├── LICENSE
├── Dockerfile
├── .gitignore
├── tests
│   └── test.py
├── tutorial
│   ├── image_generator.py
│   ├── images
│   ├── image_composer.py
│   ├── test.py
│   └── pipeline.ipynb
├── README.md
├── app
│   ├── algorithm.py
│   ├── __init__.py
│   ├── notebook.ipynb
│   ├── storyboard_visualizer.py
│   ├── two_pass.py
│   └── utils.py
├── requirements.txt
└── note.ipynb
 '
' Commit History: 
{"insertions": [287], "deletions": [0], "lines": [287], "committed_datetime": ["2024-02-17 23:34:33"], "commit_count": 1} 
 Content: 
from typing import List, Literal, Tuple

import itertools

import random

from algorithm import optimize_blend

from collections import defaultdict

from PIL import Image

from pprint import pprint

import torch.nn.functional as F

VERTICAL_POSITIONING = {'Logo': [1], 'CTA Button': [1, 2, 3], 'Icon': [1, 2, 3], 'Product Image': [2],

                        'Text Elements': [1, 3], 'Infographic': [2], 'Banner': [1], 'Illustration': [2], 'Photograph': [2],

                        'Mascot': [2], 'Testimonial Quotes': [2], 'Social Proof': [2, 1, 3], 'Seal or Badge': [3, 1, 2],

                        'Graphs and Charts': [2], 'Decorative Elements': [3], 'Interactive Elements': [2], 'Animation': [2],

                        'Coupon or Offer Code': [3], 'Legal Disclaimers or Terms': [3], 'Contact Information': [3, 1, 2],

                        'Map or Location Image': [3], 'QR Code': [3, 1, 2]}



HORIZONTAL_POSITIONING = {'Logo': [1], 'CTA Button': [2, 1, 3], 'Icon': [1], 'Product Image': [1],

                          'Text Elements': [1], 'Infographic': [1], 'Banner': [2], 'Illustration': [2], 'Photograph': [2],

                          'Mascot': [1], 'Testimonial Quotes': [2], 'Social Proof': [3, 1, 2], 'Seal or Badge': [3, 1, 2],

                          'Graphs and Charts': [1], 'Decorative Elements': [3], 'Interactive Elements': [2], 'Animation': [2],

                          'Coupon or Offer Code': [3], 'Legal Disclaimers or Terms': [3], 'Contact Information': [3, 1, 2],

                          'Map or Location Image': [3], 'QR Code': [3, 1, 2]}





class ImageComposer:

    categories = Literal["Background", "Logo", "CTA Button", "Icon", "Product Image", "Text Elements", "Infographic", "Banner", "Illustration", "Photograph", "Mascot", "Testimonial Quotes", "Social Proof", "Seal or Badge", "Graphs and Charts", "Decorative Elements", "Interactive Elements", "Animation", "Coupon or Offer Code", "Legal Disclaimers or Terms", "Contact Information", "Map or Location Image", "QR Code"]

    PositionSegment = Tuple[float, float]

    AlignmentPosition = Tuple[int, int]

    AlignmentPositions = List[AlignmentPosition]

    frame_images = List[Tuple[categories, str, str]]



    def __init__(self, width:int, height: int, frames: List[frame_images]) -> None:

        self.width = width

        self.height = height

        self.frames = frames

        self.segments = ImageComposer.get_image_position_segments(width, height)

        self.generated_frames = []



    def generate_frames(self):

        self.compose_frames()

        return self.generated_frames



    def compose_frames(self) -> None:

        self.generated_frames = []



        for frame in self.frames:

            
            background_path = None

            mask_path = None

            target_path = None

            for category, url_path, local_path in frame:

                if category == 'Background':

                    background_path = local_path

                elif category == 'Mask':  
                    mask_path = local_path

                else:

                    target_path = local_path

            

            if background_path is None or target_path is None:

                continue  


            
            blended_image = optimize_blend(background_path, mask_path, target_path)



            
            self.generated_frames.append(blended_image)

    @staticmethod

    def compute_positions(elements: List[categories]) -> List[AlignmentPositions]:

        possible_positions = []



        
        for element in elements:

            vertical_options = VERTICAL_POSITIONING[element]

            horizontal_options = HORIZONTAL_POSITIONING[element]

            combinations = list(itertools.product(vertical_options, horizontal_options))

            possible_positions.append(combinations)



        return possible_positions



    @staticmethod

    def select_diverse_positions(possible_positions: List[AlignmentPositions]) -> AlignmentPositions:

        position_frequency = defaultdict(int)



        def update_position_frequency(selected_position):

            position_frequency[selected_position] += 1



        selected_positions = []



        for positions in possible_positions:

            sorted_combinations = sorted(positions, key=lambda x: position_frequency[x])



            lowest_frequency = position_frequency[sorted_combinations[0]]

            lowest_freq_combinations = [pos for pos in sorted_combinations if position_frequency[pos] == lowest_frequency]



            selected_position = random.choice(lowest_freq_combinations)

            selected_positions.append(selected_position)



            update_position_frequency(selected_position)



        return selected_positions





    @staticmethod

    def get_image_position_segments(width: float, height: float, vm: float = 0.6, vo: float = 0.2, hm: float = 0.6, ho: float = 0.2) -> Tuple[List[PositionSegment], List[PositionSegment]]:

        """Divide Image based on percentage for vertical and horizontal segments."""



        if vm + vo * 2 > 1 or hm + ho * 2 > 1:

            raise ValueError("Sum of percentages exceeds 100% for either vertical or horizontal segments.")



        vertical_mid = height * vm

        vertical_outer = height * vo

        horizontal_mid = width * hm

        horizontal_outer = width * ho



        vertical_segments = [

            (0, vertical_outer),

            (vertical_outer, vertical_outer + vertical_mid),

            (vertical_outer + vertical_mid, height)

        ]



        horizontal_segments = [

            (0, horizontal_outer),

            (horizontal_outer, horizontal_outer + horizontal_mid),

            (horizontal_outer + horizontal_mid, width)

        ]



        segements = []

        for vs in vertical_segments:

            vs_items = []

            for hs in horizontal_segments:

                vs_items.append((vs, hs))

            segements.append(vs_items)





        return segements



    def calculate_adjusted_element_positions(self, elements_positions, padding=10):

        element_details = []

        segment_elements = {}



        
        for i, (v_pos, h_pos) in enumerate(elements_positions):

            segment_key = (v_pos, h_pos)

            if segment_key not in segment_elements:

                segment_elements[segment_key] = []

            segment_elements[segment_key].append(i)



        for segment_key, elements in segment_elements.items():

            v_pos, h_pos = segment_key

            segment = self.segments[v_pos-1][h_pos-1]

            vertical_segment, horizontal_segment = segment

            num_elements = len(elements)



            x_start, x_end = horizontal_segment

            y_start, y_end = vertical_segment

            segment_width = (x_end - x_start) - 2 * padding

            segment_height = (y_end - y_start) - 2 * padding



            
            is_vertical = segment_height > segment_width

            if is_vertical:

                space_per_element = segment_height / num_elements

            else:

                space_per_element = segment_width / num_elements



            for index, _ in enumerate(elements):

                if is_vertical:

                    element_x_start = x_start + padding

                    element_y_start = y_start + padding + index * space_per_element

                    element_width = segment_width

                    element_height = space_per_element

                else:

                    element_x_start = x_start + padding + index * space_per_element

                    element_y_start = y_start + padding

                    element_width = space_per_element

                    element_height = segment_height



                element_details.append({

                    "start_point": (element_x_start, element_y_start),

                    "dimensions": (element_width, element_height)

                })



        return element_details



    @staticmethod

    @staticmethod

    def resize_image(image, target_width, target_height):

        """

        Resize an image to fit within target dimensions while maintaining aspect ratio.

        """

        original_width, original_height = image.size

        ratio = min(target_width / original_width, target_height / original_height)

        new_width = int(original_width * ratio)

        new_height = int(original_height * ratio)

        resized_image = image.resize((new_width, new_height), Image.LANCZOS)  
        return resized_image





    def create_combined_image(self, background_path: str, elements: List[Tuple[str, int|float, int|float]]):

        """

        Create a combined image based on background and elements' positioning and sizing.



        :param background_path: Path to the background image.

        :param elements: A list of tuples containing 'image_path', 'start_point', and 'dimensions'.

        """

        
        background = Image.open(background_path).convert("RGBA")



        
        for element in elements:

            
            image_path = element[0]

            element_image = Image.open(image_path).convert("RGBA")



            
            target_width, target_height = element[2]

            resized_element_image = ImageComposer.resize_image(element_image, target_width, target_height)



            
            start_x, start_y = element[1]

            offset_x = start_x + (target_width - resized_element_image.size[0]) / 2

            offset_y = start_y + (target_height - resized_element_image.size[1]) / 2



            
            background = algorithm.optimize_blend(background, resized_element_image, (int(offset_x), int(offset_y)))



        return background





if __name__ == "__main__":

    ic = ImageComposer(320, 500, [[('Logo', 'url_path', 'local_path'),

                                   ('CTA Button', 'url_path', 'local_path'),

                                   ('Icon', 'url_path', 'local_path'),

                                   ('Product Image', 'url_path', 'local_path'),

                                   ('Text Elements', 'url_path', 'local_path')]])

    possibilties = ImageComposer.compute_positions(["Logo", "CTA Button", "Icon", "Product Image", "Text Elements"])

    pprint(possibilties)

    print("======================================================")

    diverse = ImageComposer.select_diverse_positions(possibilties)

    pprint(diverse)



    print(ic.calculate_adjusted_element_positions(diverse))
 

 

import torch
from PIL import Image
from io import BytesIO
import os
from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler, StableDiffusionImg2ImgPipeline
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import replicate
from typing import Literal, Optional, Tuple


model_id = "stabilityai/stable-diffusion-2"
device = 'cuda:0'
scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder='scheduler')
text2img_pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16).to(device)
img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
bert_model = AutoModel.from_pretrained("bert-base-uncased").to(device)



def gen_img2img(prompt: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 
                            aspect_ratios_selection: str = "1024,1024",image_seed: int = 1234, sharpness: int = 2, cn_img1: str =  "Img", cn_type1= "ImagePrompt", 
                            ):
    pass

img2img_replicate = gen_img2img()
    



def generate_images(df, out_dir):
    for index, row in df.iterrows():
        prompts = row["summarized_story"]
        story_type = row["story_type"]
        if len(prompts)>0:
            init_prompt = prompts[0]
            init_image = text2img_pipe(init_prompt).images[0]
            final_index = story_type+"/"+row["story_name"]+"_segment_0"
            save_path = os.path.join(out_dir,str(final_index) +'.jpg')
            init_image.save(save_path)
            
            for idx, prompt in enumerate(prompts[1:]):    
                images = img2img_pipe(prompt=prompt, image=init_image, strength=0.90, guidance_scale=7.5).images
                final_index = story_type+"/"+row["story_name"]+"_segment_"+ str(idx+1)
                save_path = os.path.join(out_dir,str(final_index) +'.jpg')
                images[0].save(save_path)


def generate_images_baseline0(prompts, prefix=""):
    generated_images = []
    if len(prompts)>0:
        for idx, prompt in enumerate(prompts):    
            image = text2img_pipe(prefix+prompt).images[0]            
            generated_images.append(image)
    return generated_images


def generate_images_baseline(prompts, strength, guidance_scale, prefix=""):
    if len(prompts)>0:
        init_prompt = prompts[0]
        init_image = text2img_pipe(prefix+init_prompt).images[0]
        generated_images = [init_image]
        for idx, prompt in enumerate(prompts[1:]):    
            images = img2img_pipe(prompt=prefix+prompt, image=init_image, strength=strength, guidance_scale=guidance_scale).images
            generated_images.append(images[0])
    return generated_images

def bert_sentence_embedding(sentence):
    tokens = bert_tokenizer(sentence, return_tensors="pt")
    with torch.no_grad():
        output = bert_model(**tokens.to(device))
        cls_embedding = output.last_hidden_state[:, 0, :].cpu().numpy()
    return cls_embedding

def findSimilarPromptBert(query, string_list):
    if not string_list:
        return None
    query_embedding = bert_sentence_embedding(query)
    string_embeddings = [bert_sentence_embedding(s) for s in string_list]

    similarities = [cosine_similarity(query_embedding, s.reshape(1, -1))[0, 0] for s in string_embeddings]
    most_similar_index = similarities.index(max(similarities))
    return most_similar_index


def generate_images_proposed(story, strength, guidance_scale, prefix=""):
    init_prompt = story[0]
        init_image = text2img_pipe(prefix+init_prompt).images[0]
    generated_images = [init_image]
    for idx, prompt in enumerate(story[1:]):
        most_similar_index = findSimilarPromptBert(prompt, story[:idx+1])
        print("CURRENT PROMPT:",prompt)
        print("SIMILAR PROMPT", story[most_similar_index])
        images = img2img_pipe(prompt=prefix+prompt, image=generated_images[most_similar_index], strength=strength, guidance_scale=guidance_scale).images
        generated_images.append(images[0])
    return generated_images
! pip install colabcode
import torch 

import requests

from PIL import Image

from io import BytesIO

import matplotlib.pyplot as plt

import replicate, json, sys, os

from typing import Literal, Optional, Tuple
data = {

        "concept": "Escape Challenge Teaser",

        "implementation": {

            "frame_1": {

                "description": "The ad begins with a suspenseful animation of a LEGO CITY set, with the tagline 'YOUR CITY, NO LIMITS' and a countdown timer.",

                "interaction_type": "Tap",

                "next_frame": "frame_2",

                "duration": "5 seconds"

            },

            "frame_2": {

                "description": "The scene transitions to a series of quick, exciting clips showcasing various challenges in the 'ULTIMATE ESCAPE CHALLENGE'.",

                "interaction_type": "Swipe",

                "next_frame": "frame_3",

                "duration": "10 seconds"

            },

            "frame_3": {

                "description": "The final frame reveals the LEGO CITY 2024 logo and a 'Play Now' button, inviting users to join the challenge on the LEGO website.",

                "interaction_type": "Tap",

                "next_frame": "end-of-advertisement",

                "duration": "5 seconds"

            }

        },

        "explanation": "This concept aligns with LEGO CITY's brand identity of providing fun and entertainment. It appeals to the target demographic's interest in games and challenges, and it achieves the campaign's objectives of enhancing audience engagement, increasing product and brand awareness, and driving traffic to the brand's website. The concept is scalable and can be adapted for various platforms and audiences. It can be executed within the given budget constraints.",

        "asset_suggestions": [

            {

                "frame_1": {

                    "Background Animation": "A suspenseful animation of a LEGO CITY set, with buildings, vehicles, and mini-figures coming to life in a dynamic, 3D environment. The animation is designed to captivate the viewer's attention and set the stage for the upcoming challenge.",

                    "Tagline": "'YOUR CITY, NO LIMITS' appears in bold, vibrant letters, reinforcing the campaign's theme of boundless creativity and adventure.",

                    "Countdown Timer": "A digital countdown timer, styled to resemble LEGO bricks, adds an element of suspense and anticipation, building up to the reveal of the 'ULTIMATE ESCAPE CHALLENGE'."

                },

                "frame_2": {

                    "Video Clips": "A series of quick, exciting clips showcasing various challenges in the 'ULTIMATE ESCAPE CHALLENGE'. Each clip features different LEGO CITY sets and mini-figures, highlighting the diversity and expansiveness of the LEGO CITY universe.",

                    "Swipe Indicator": "A subtle, animated swipe indicator encourages viewers to interact with the ad and explore the different challenges."

                },

                "frame_3": {

                    "Logo": "The LEGO CITY 2024 logo, designed with the iconic LEGO brick pattern, reinforces brand recognition and ties the ad back to the product line.",

                    "CTA Button": "A 'Play Now' button, styled to resemble a LEGO brick, invites users to join the challenge on the LEGO website. The button is strategically placed to be easily noticeable and accessible, encouraging viewers to take immediate action."

                },

                "explanation": "This variation aligns with the creative brief by effectively promoting the LEGO CITY 2024 product line and the 'ULTIMATE ESCAPE CHALLENGE'. The suspenseful animation and countdown timer in the first frame capture the viewer's attention and set the stage for the challenge. The quick, exciting clips in the second frame showcase the diversity and expansiveness of the LEGO CITY universe, appealing to the target audience's interest in games and challenges. The LEGO CITY 2024 logo and 'Play Now' button in the final frame reinforce brand recognition and drive traffic to the brand's website, achieving the campaign's objectives of enhancing audience engagement and increasing product and brand awareness."

            },

            {

                "frame_1": {

                    "Background": "A lively, animated cityscape made entirely of LEGO CITY sets. The cityscape should be filled with LEGO characters engaging in various activities, showcasing the fun and entertainment offered by the LEGO CITY universe.",

                    "Logo": "The LEGO logo appears prominently at the center of the frame, reinforcing brand recognition.",

                    "Tagline": "'YOUR CITY, NO LIMITS' appears in bold, colorful font, capturing the frame's message and the campaign's overarching theme.",

                    "Interactive Element": "A 'Tap to Begin' button, inviting viewers to interact with the ad."

                },

                "frame_2": {

                    "Background": "A blank, grid-like canvas that serves as the foundation for the viewer's LEGO city.",

                    "Toolbox": "A toolbox filled with various LEGO CITY sets appears at the bottom of the screen. Each set should be clearly visible and identifiable, showcasing the product range.",

                    "Interactive Element": "The 'Drag and Drop' feature, allowing viewers to select LEGO sets from the toolbox and place them onto the canvas."

                },

                "frame_3": {

                    "Background": "The viewer's city, coming to life as they add LEGO sets. The city should be animated, with LEGO characters and vehicles moving around.",

                    "Sound Effects": "Sound effects that correspond to the viewer's actions, enhancing the immersive experience.",

                    "Interactive Element": "The 'Tap to Animate' feature, allowing viewers to animate their city by tapping on the LEGO sets."

                },

                "frame_4": {

                    "Background": "The viewer's fully built and animated LEGO city.",

                    "CTA": "A 'Start Challenge' button that invites the viewer to start the 'ULTIMATE ESCAPE CHALLENGE', driving traffic to the brand's website.",

                    "Interactive Element": "The 'Tap to Start Challenge' feature, prompting viewers to interact with the ad and engage with the brand."

                },

                "explanation": "This variation aligns with the concept and brief by leveraging interactive elements and product imagery to engage the target audience and promote the LEGO CITY product line. The lively cityscape in Frame 1 captures the viewer's attention and sets the stage for the interactive experience. The toolbox in Frame 2 showcases the product range and allows viewers to engage with the ad by building their own city. The animations and sound effects in Frame 3 enhance the immersive experience and foster creativity, aligning with the brand's identity. The 'Start Challenge' button in Frame 4 drives traffic to the brand's website, achieving the campaign's objectives. The assets across frames work together to create a cohesive narrative that resonates with the target audience and promotes the LEGO CITY brand."

            },

            {

                "frame_1": {

                    "Background": "A vibrant, detailed image of a LEGO CITY set, showcasing key features like buildings, vehicles, and mini-figures",

                    "Interactive Element": "'Choose Your Adventure' prompt, a large, colorful button placed strategically to draw attention",

                    "Logo": "LEGO logo placed subtly in the corner to reinforce brand recognition"

                },

                "frame_2": {

                    "Animation": "A unique, engaging animation that brings the chosen part of the city to life, highlighting the fun and creativity of LEGO CITY",

                    "Interactive Element": "Swipe functionality to allow users to navigate through the story",

                    "Text": "Brief, compelling text that narrates the story and enhances the viewer's immersion"

                },

                "frame_3": {

                    "CTA": "'Continue the Adventure' button, designed to be eye-catching and compelling, prompting users to visit the LEGO website",

                    "Ending Scene": "A cliffhanger scene that leaves viewers eager to continue the adventure",

                    "Logo": "LEGO logo, slightly more prominent to reinforce brand identity"

                },

                "explanation": "This variation aligns with the creative brief by leveraging the interactive storytelling concept to engage the target audience of boys aged 6-8. The detailed LEGO CITY set image in frame 1 captures viewers' attention and invites them to interact with the ad. The animation in frame 2 immerses viewers in the LEGO CITY universe, enhancing their engagement and increasing product awareness. The cliffhanger and CTA in frame 3 drive traffic to the LEGO website, contributing to the campaign's objectives. The consistent use of the LEGO logo across frames reinforces brand recognition."

            },

            {

                "frame_1": {

                    "Background": "A vibrant, bustling LEGO CITY, meticulously crafted with LEGO bricks, showcasing a bird's-eye view. The city is alive with miniature LEGO citizens, vehicles, and buildings, reflecting the brand's commitment to detail and creativity.",

                    "Interactive Element": "A 'Start Tour' button, designed in LEGO's signature bright colors, inviting users to tap and begin their exploration of the city.",

                    "Logo": "The LEGO logo, subtly placed in the corner to reinforce brand recognition without distracting from the city view."

                },

                "frame_2": {

                    "Background": "A first-person view of the LEGO CITY, immersing users in the cityscape. The city is filled with diverse LEGO buildings, each showcasing unique architectural designs and features.",

                    "Interactive Element": "Swipe functionality, allowing users to navigate through the city. Tappable buildings, each revealing fun facts or trivia about the building when tapped, engaging users and enhancing their knowledge of LEGO CITY.",

                    "Text": "Brief, engaging descriptions that pop up when a building is tapped, providing information about the building in a fun and educational manner."

                },

                "frame_3": {

                    "Background": "A final view of the LEGO CITY, with the cityscape subtly transitioning into the LEGO CITY 2024 logo.",

                    "Interactive Element": "A 'Join the City' button, designed in LEGO's signature colors, directing users to the LEGO website.",

                    "Logo": "The LEGO CITY 2024 logo, prominently displayed to reinforce product awareness and brand recognition."

                },

                "explanation": "This variation aligns with the 'City Tour' concept by providing an interactive tour of a LEGO CITY, engaging users and enhancing their knowledge of the product. The use of vibrant colors and detailed LEGO buildings reflects the brand's commitment to creativity and detail, appealing to the target demographic's interest in exploration and discovery. The interactive elements, such as the 'Start Tour' and 'Join the City' buttons and the tappable buildings, enhance audience engagement and drive traffic to the brand's website, achieving the campaign's objectives. The LEGO and LEGO CITY 2024 logos reinforce brand and product awareness, contributing to the campaign's success."

            },

            {

                "frame_1": {

                    "Background": "A vibrant, bustling LEGO CITY set, showcasing key features like the police station, fire station, and various vehicles",

                    "Foreground": "A large, eye-catching 'Test Your Knowledge' prompt in LEGO's signature yellow and red colors",

                    "Interactive Element": "A 'Tap to Start' button, inviting users to engage with the ad"

                },

                "frame_2": {

                    "Background": "A series of LEGO CITY scenes related to the quiz questions",

                    "Foreground": "Multiple-choice questions in playful, kid-friendly fonts",

                    "Interactive Element": "Answer options that users can tap on to proceed"

                },

                "frame_3": {

                    "Background": "A celebratory scene with LEGO characters cheering",

                    "Foreground": "The user's score displayed in large, bold numbers",

                    "Interactive Element": "A 'Learn More' button, directing users to the LEGO website"

                },

                "explanation": "This variation emphasizes the interactive and educational aspects of the LEGO CITY Quiz concept. The vibrant LEGO CITY set in the first frame captures the viewer's attention and sets the stage for the quiz. The multiple-choice questions in the second frame engage the viewer's curiosity and stimulate their knowledge about LEGO CITY. The celebratory scene in the third frame rewards the viewer's participation and encourages them to learn more about LEGO CITY on the brand's website. This variation aligns with the campaign's objectives of enhancing audience engagement, increasing product and brand awareness, and driving traffic to the brand's website."

            }

        ]

    }
data['implementation']['frame_1']
for frame_name, frame_data in data['implementation'].items():

    print(f"Frame {frame_name}")

    print(frame_data)

combined_prompt = []

for frame_name, frame_data in data['implementation'].items():

    combined_prompt.append(frame_data['description'])

    

print(combined_prompt)

import replicate

import logging

import requests

os.environ["REPLICATE_API_TOKEN"] = "r8_UbIKKDB3ZgnPyZWGCRWv8janUuULY6C2mIzxQ"
import os

import requests




SAVE_DIR = "/home/biniyam/TenAcademy/StoryBoard-Generation/images"




os.makedirs(SAVE_DIR, exist_ok=True)



def generate_image(prompt: str, filename: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 

                       aspect_ratios_selection: str = "1024*1024", image_seed: int = 1234, sharpness: int = 2) -> Optional[dict]:

        """

        Generates an image based on the given prompt and settings.



        :param prompt: Textual description of the image to generate.

        :param filename: The filename to save the image as.

        :param performance_selection: Choice of performance level affecting generation speed and quality.

        :param aspect_ratio: The desired aspect ratio of the generated image.

        :param image_seed: Seed for the image generation process for reproducibility.

        :param sharpness: The sharpness level of the generated image.

        :return: The generated image or None if an error occurred.

        """

        try:

            output = replicate.run(

                "konieshadow/fooocus-api-anime:a750658f54c4f8bec1c8b0e352ce2666c22f2f919d391688ff4fc16e48b3a28f",

                input={

                    "prompt": prompt,

                    "performance_selection": performance_selection,

                    "aspect_ratios_selection": aspect_ratios_selection,

                    "image_seed": image_seed,

                    "sharpness": sharpness

                }

            )

            logging.info("Image generated successfully.")

            

            
            if isinstance(output, list) and all(isinstance(item, str) for item in output):

                for url in output:

                    
                    response = requests.get(url)

                    if response.status_code == 200:

                        
                        image_path = os.path.join(SAVE_DIR, filename)

                        

                        
                        with open(image_path, 'wb') as f:

                            f.write(response.content)

                        

                        logging.info(f"Image saved to: {image_path}")

                    else:

                        logging.warning(f"Failed to download image from URL: {url}")

            else:

                logging.warning("Invalid output format: expecting a list of image URLs.")

            

            return output

        except Exception as e:

            logging.error(f"Failed to generate image: {e}")

            return None




init_prompt = combined_prompt[0]

generate_image(init_prompt, "frame_1.jpg")

generate_image(init_prompt, performance_selection="Extreme Speed", aspect_ratios_selection="1024*1024", image_seed=1234, sharpness=2)
def gen_from_img(prompt: str, filename: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 

                       aspect_ratios_selection: str = "1024*1024", image_seed: int = 1234, sharpness: int = 2, cn_img1: str = "https://replicate.delivery/pbxt/sZzW9Zcnv8okM5wKEtU12ZnIUdUh2r3HpHwAnUm2oU5033lE/0970c0d5-71e2-4ddf-9cdc-b770e33a223e.png") -> Optional[dict]:

        """

        Generates an image based on the given prompt and settings.



        :param prompt: Textual description of the image to generate.

        :param filename: The filename to save the image as.

        :param performance_selection: Choice of performance level affecting generation speed and quality.

        :param aspect_ratio: The desired aspect ratio of the generated image.

        :param image_seed: Seed for the image generation process for reproducibility.

        :param sharpness: The sharpness level of the generated image.

        :return: The generated image or None if an error occurred.

        """

        try:

            output = replicate.run(

                "konieshadow/fooocus-api-anime:a750658f54c4f8bec1c8b0e352ce2666c22f2f919d391688ff4fc16e48b3a28f",

                input={

                    "prompt": prompt,

                    "performance_selection": performance_selection,

                    "aspect_ratios_selection": aspect_ratios_selection,

                    "image_seed": image_seed,

                    "sharpness": sharpness,

                    "cn_img1": cn_img1,

                    "cn_stop1": 1

                }

            )

            logging.info("Image generated successfully.")

            

            if isinstance(output, list) and all(isinstance(item, str) for item in output):

                for url in output:

                    
                    response = requests.get(url)

                    if response.status_code == 200:

                        
                        image_path = os.path.join(SAVE_DIR, filename)

                        

                        
                        with open(image_path, 'wb') as f:

                            f.write(response.content)

                        

                        logging.info(f"Image saved to: {image_path}")

                    else:

                        logging.warning(f"Failed to download image from URL: {url}")

            else:

                logging.warning("Invalid output format: expecting a list of image URLs.")

            

            return output

        except Exception as e:

            logging.error(f"Failed to generate image: {e}")

            return None





prompt = combined_prompt[1]
prompt
gen_from_img(prompt=prompt, filename="frame_2.jpg", performance_selection="Extreme Speed", aspect_ratios_selection="1024*1024", image_seed=1234, sharpness=2, cn_img1='https://replicate.delivery/pbxt/sZzW9Zcnv8okM5wKEtU12ZnIUdUh2r3HpHwAnUm2oU5033lE/0970c0d5-71e2-4ddf-9cdc-b770e33a223e.png')
import sys

import os



sys.path.append('/home/biniyam/TenAcademy/StoryBoard-Generation/src')
from image_composer import ImageComposer

from image_generator import ImageGenerater

from storyboard_visualizer import StoryBoard
concept = {

    "frame_1": {

        "Background": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",

        "CTA Button": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."

    },

    "frame_2": {

        "Interactive Elements": "A digital beat mixer interface with vibrant, touch-responsive Latin American instrument icons like congas, claves, and maracas, each activating a unique sound layer.",

        "Background": "A dynamic, abstract representation of sound waves that move in sync with the user's interactions."

    },

    "frame_3": {

        "Background": "A dynamic, abstract representation of sound waves that move in sync with the user's interactions.",

        "Animation": "A kaleidoscope of colors that dance across the screen, with each beat added, symbolizing the fusion of cultures and music.",

        "CTA Button": "A 'Play Your Mix' button that pulses like a heartbeat, encouraging users to share their creation."

    },

    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."

}
concept = {

    "frame_1": {

        "Background": "A high-resolution 3D animation of a Coca-Cola bottle on a vibrant background. The bottle is detailed with effervescent bubbles rising to the top. Gradually, the bottle transforms into a DJ turntable, featuring a sleek design with illuminated controls and a vinyl record emblazoned with the Coke Studio logo, symbolizing the fusion of refreshment and music.",

        "CTA Button": "'Mix Your Beat' displayed in a bold, dynamic font on a contrasting background. The font style is playful and energetic, designed to evoke a sense of creativity and engagement. The text is surrounded by a subtle glow, suggesting interactivity and inviting the viewer to start their musical adventure."

    },

    "frame_2": {

        "Interactive Elements": "A digital beat mixer interface showcasing vibrant icons of Latin American musical instruments such as congas, claves, and maracas. Each icon is designed to be touch-responsive, illuminating and animating upon interaction to emit distinctive sound layers. The interface is set against a background that visually represents the music's energy through animated, abstract sound waves.",

        "Background": "An abstract, animated visualization of sound waves that dynamically react to interactions on the digital beat mixer. The sound waves pulse, expand, and change colors in real time, creating a visual representation of the music being mixed. The background enhances the immersive experience, reflecting the user's creative input in a visually engaging manner."

    },

    "frame_3": {

        "Background": "An abstract, animated visualization of sound waves that dynamically react to interactions on the digital beat mixer. The sound waves pulse, expand, and change colors in real time, creating a visual representation of the music being mixed. The background enhances the immersive experience, reflecting the user's creative input in a visually engaging manner.",

        "Animation": "A dynamic, colorful animation that bursts into life with each added beat, symbolizing the vibrant energy of Coke Studio's musical fusion. The screen is filled with an ever-changing kaleidoscope of colors, with each new layer of music adding complexity to the visual celebration, reflecting the creativity and diversity of the user's mix.",

        "CTA Button": "A prominently placed 'Play Your Mix' button that pulses in rhythm with the generated music mix. The button's design is inviting, with a visual style that suggests action and progression, encouraging the viewer to engage with their creation and share it. The pulsing effect enhances the sense of urgency and excitement about the music mix."

    },

    "explanation": "These descriptions are crafted to ensure each asset is vividly detailed and self-contained, suitable for generating individual images that collectively build the narrative of an interactive beat mixer. From the initial transformation of a Coca-Cola bottle into a DJ turntable, through the engagement with a virtual beat mixer, to the celebration of a personalized music mix, each description is designed to capture the essence of the interactive experience while being standalone for image generation purposes."

}
concept = {

    "frame_1": {"Background": "In the center of a vibrant scene, a high-resolution 3D Coca-Cola bottle surrounded by effervescent bubbles captures the viewer's attention. As the bubbles rise, the bottle seamlessly transforms into a sleek DJ turntable, complete with illuminated controls and a spinning vinyl record bearing the Coke Studio logo. This imagery symbolizes a fusion of refreshing beverage and rhythmic beats. Directly below this dynamic transformation, the call-to-action 'Mix Your Beat' shines in a bold, dynamic font with a playful energy. The text, surrounded by a subtle glow, invites interaction, set against a backdrop designed to evoke creativity and musical exploration."},

    "frame_2": {"Background": "The viewer is presented with an immersive digital beat mixer interface, populated with vibrant, touch-responsive icons of Latin American musical instruments such as congas, claves, and maracas. Each icon, when interacted with, illuminates and animates to release a distinctive sound layer, set against an animated background of abstract, pulsating sound waves. These waves dynamically react to the mixer's activity, changing colors and patterns in real time to visually represent the user's musical creations, encouraging exploration and engagement through a fusion of visual and auditory feedback."},

    "frame_3": {"Background": "With each beat added to the mix, the screen erupts into a dynamic display of colors, creating a visual celebration that mirrors the energy of Coke Studio's musical fusion. This kaleidoscopic animation evolves with the complexity of the user's mix, symbolizing the blend of cultures and music. At the heart of this vibrant spectacle, a 'Play Your Mix' button pulses in rhythm with the music, its design inviting and central, encouraging the viewer to engage and share their creation. The pulsing button serves as a visual anchor, drawing the viewer's attention and culminating the creative process."}

}
ig = ImageGenerater(concept)

generated_images = ig.generate_images()
ic = ImageComposer(600,600, list(generated_images.values()))

generated_frames = ic.generate_frames()
generated_frames[2]
StoryBoard.combine_images_horizontally(generated_frames)
from typing import Literal, Optional, Tuple
import logging
import base64
from io import BytesIO
import os

import replicate
from PIL import Image
import requests
from pydantic import HttpUrl
from collections import Union



os.environ["REPLICATE_API_TOKEN"] = "r8_UbIKKDB3ZgnPyZWGCRWv8janUuULY6C2mIzxQ"
logging.basicConfig(level=logging.INFO)





class ImageGenerater:
    def __init__(self, asset_suggestions: dict) -> None:
        self.asset_suggestions = asset_suggestions

    def generate_images(self, store_location: str ='./images') -> dict:
        generated_images = {}
        for frame, elements in self.asset_suggestions.items():
            if frame.startswith('frame'):
                generated_images[frame] = []
                for type, description in elements.items():
                    downloaded_image = ImageGenerater.download_image(ImageGenerater.generate_image(prompt=description)[0], store_location)
                    generated_images[frame].append((type, *downloaded_image))

        return generated_images


    @staticmethod
    def generate_image(prompt: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 
                       aspect_ratios_selection: str = "1024*1024", image_seed: int = 1234, sharpness: int = 2) -> Optional[dict]:
        """
        Generates an image based on the given prompt and settings.

        :param prompt: Textual description of the image to generate.
        :param performance_selection: Choice of performance level affecting generation speed and quality.
        :param aspect_ratio: The desired aspect ratio of the generated image.
        :param image_seed: Seed for the image generation process for reproducibility.
        :param sharpness: The sharpness level of the generated image.
        :return: The generated image or None if an error occurred.
        """
        try:
            output = replicate.run(
                "konieshadow/fooocus-api-anime:a750658f54c4f8bec1c8b0e352ce2666c22f2f919d391688ff4fc16e48b3a28f",
                input={
                    "prompt": prompt,
                    "performance_selection": performance_selection,
                    "aspect_ratios_selection": aspect_ratios_selection,
                    "image_seed": image_seed,
                    "sharpness": sharpness
                }
            )
            logging.info("Image generated successfully.")
            return output
        except Exception as e:
            logging.error(f"Failed to generate image: {e}")
            return None
        
    img_path = generated_images
        
    @staticmethod
    def generate_from_image(prompt: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 
                            aspect_ratios_selection: str = "1024,1024",image_seed: int = 1234, sharpness: int = 2, cn_img1: str =  "Img", cn_type1= "ImagePrompt", 
                            ):
        pass
        
    @staticmethod
    def decode_image(base64_data: str) -> Optional[Image.Image]:
        """
        Converts a base64 image into pillow iamge object.

        :param base64_data: Textual base64 image data.
        :return: Converted pillow image.
        """
        image_data = base64.b64decode(base64_data)
        image_stream = BytesIO(image_data)
        return(Image.open(image_stream))
    
    @staticmethod
    def download_image(url: HttpUrl, save_path: str) -> Tuple[str, str]:
        """
        Downloads provided url data to given location.

        :param url: HTTP Url of the file.
        :param save_path: Folder location to save the data.
        :return: Tuple of the url and save location.
        """

        try:
            response = requests.get(url)
            
            if response.status_code == 200:
                save_path = os.path.join(save_path, os.path.basename(url))
                image = Image.open(BytesIO(response.content))
                image.save(save_path)
                logging.info(f"Image saved to {save_path}")
                return (url, save_path)
            else:
                raise RuntimeError(f"Failed to download image. Status code: {response.status_code}") from None
        except Exception as e:
            raise RuntimeError(f"An error occurred: {e}") from e
        
        
if __name__ == "__main__":
                    a = {
    "frame_1": {
        "Animated Element": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",
    },
    "frame_2": {
        "CTA Text": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."
    },
    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."
    }
    test = ImageGenerater(a)

    test.generate_images()
import pandas as pd

import numpy as np



from diffusers import DiffusionPipeline

import torch 
model_id = 'stabilityai/stable-diffusion-xl-base-1.0'
data = pd.read_json("/home/biniyam/TenAcademy/StoryBoard-Generation/data/concepts.json")

frames = []

df = pd.DataFrame(data)

df.head()
df.shape
df['asset_suggestions'][0][0]['frame_1']['Countdown Timer']
df['asset_suggestions'][0][0]['frame_3']
df['implementation'][4]
def decrypt_caesar(ciphertext, shift):

    decrypted_text = ''

    for char in ciphertext:

        if char.isalpha():

            shifted = ord(char) - shift

            if char.islower():

                if shifted < ord('a'):

                    shifted += 26

            elif char.isupper():

                if shifted < ord('A'):

                    shifted += 26

            decrypted_text += chr(shifted)

        else:

            decrypted_text += char

    return decrypted_text



def sophisticated_decrypt(ciphertext):

    
    
    
    

    
    for shift in range(1, 26):

        decrypted_text = decrypt_caesar(ciphertext, shift)

        
        
        
        if 'the' in decrypted_text.lower():

            return decrypted_text

    

    
    return None




encrypted_message = "Uvrtm vmt qp WQ qpm tvmk $10 Rwqxsr Vrmxqpsvgx"

decrypted_message = sophisticated_decrypt(encrypted_message)

if decrypted_message:

    print("Decrypted message:", decrypted_message)

else:

    print("Failed to decrypt the message.")
import openai

import os

import dotenv

from dotenv import load_dotenv
load_dotenv()

api_key = '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'

print(api_key)
from openai import OpenAI



client = OpenAI()



response = client.images.generate(

    model="dall-e-3",

    prompt="The ad begins with a suspenseful animation of a LEGO CITY set, with the tagline 'YOUR CITY, NO LIMITS' and a countdown timer.",

    size="1024x1024",

    quality="hd",

    style='vivid',

    n=1

)
import openai

import os

import dotenv

from dotenv import load_dotenv
load_dotenv()

api_key = os.getenv("OPENAI_API")

print(api_key)
import logging

from openai import OpenAI




logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger(__name__)



client = OpenAI(api_key=api_key)




prompt = "The ad begins with a suspenseful animation of a LEGO CITY set, with the tagline 'YOUR CITY, NO LIMITS' and a countdown timer."



response = client.images.generate(

    model="dall-e-3",

    prompt=prompt,

    size="1024x1024",

    quality="hd",

    style='vivid',

    n=1,

)



logger.debug("Request Data: %s", response)

image_url = response.data[0].url

image_url
import requests

import os



def download_image(url, folder_path):

    
    response = requests.get(url)

    

    
    if response.status_code == 200:

        
        filename = url.split("/")[-1].split("?")[0] + ".png"

        
        if not os.path.exists(folder_path):

            os.makedirs(folder_path)

        
        with open(os.path.join(folder_path, filename), 'wb') as f:

            f.write(response.content)

        print(f"Image downloaded successfully at {os.path.join(folder_path, filename)}")

    else:

        print("Failed to download image")




url = image_url

folder_path = "/home/biniyam/TenAcademy/StoryBoard-Generation/images"

download_image(url, folder_path)

frame_prompts = [

    "A collage of various travel experiences, meticulously crafted with high-resolution images showcasing diverse destinations and activities - from backpacking in the Himalayas, to luxury cruises in the Mediterranean, to cultural tours in South America. The text 'Your Travel, Your Way' appears in bold, sophisticated typography, overlaying the collage, encapsulating the brand's personalized approach. The Inspiring Travel logo is subtly placed in the top right corner to reinforce brand recognition.",

    "A first-person view of the diverse range of travel experiences offered by Inspiring Travel, immersing users in the world of exploration and adventure. Each image represents a unique travel experience - a private safari tour, a culinary journey in Italy, a historical exploration in Egypt. A soothing, confident voice of a Travel Specialist provides a voiceover detailing the customization options available, enhancing the immersive experience. Swipe functionality allows users to navigate through the images at their own pace, reinforcing the 'Your Travel, Your Way' message.",

    "A captivating image of a world map, symbolizing the endless possibilities with Inspiring Travel. A prominent 'Start Your Journey' button, designed to prompt viewer interaction and response, is placed centrally on the world map. The Inspiring Travel logo is subtly placed in the top right corner to reinforce brand recognition.",

    "A series of images showcasing the diverse range of travel experiences offered by Inspiring Travel, with each image representing a unique destination or activity. A voiceover by a Travel Specialist details the customization options available, enhancing the immersive experience. Swipe functionality allows users to navigate through the images, reinforcing the 'Your Travel, Your Way' message.",

    "An immersive video montage showcasing various travel experiences offered by Inspiring Travel - from breathtaking landscapes to cultural festivals. A warm, engaging voice of a Travel Specialist provides a voiceover, detailing the customization options available. Swipe functionality allows users to navigate through the video at their own pace, reinforcing the brand's personalized approach.",

    "A stunning image of a globe, symbolizing the endless possibilities with Inspiring Travel. A prominent 'Start Your Journey' button, designed to prompt viewer interaction and response, is placed centrally on the globe. The Inspiring Travel logo is subtly placed in the top right corner to reinforce brand recognition."

]



from openai import OpenAI

client = OpenAI(api_key=api_key)



for prompt in frame_prompts:

    response = client.images.generate(

        model="dall-e-3",

        prompt=prompt,

        quality="hd",

        n=1,

        size="1024x1024",

        style="vivid"

    )

    download_image(response.data[0].url, folder_path="/home/biniyam/TenAcademy/StoryBoard-Generation/images")



import cv2

import os

import numpy as np

import random



def resize_images(image, width):

    aspect_ratio = image.shape[1] / image.shape[0]

    height = int(width / aspect_ratio)

    resized_image = cv2.resize(image, (width, height))

    return resized_image



def create_storyboard(image_folder, output_path, storyboard_size=(1024,1024), images_per_row=3):

    images = []

    for img_name in os.listdir(image_folder):

        img_path = os.path.join(image_folder, img_name)

        image = cv2.imread(img_path)

        if image is not None:

            image = resize_images(image, storyboard_size[0] // images_per_row)

            images.append(image)

            

    rows = len(images) // images_per_row

    if len(images) % images_per_row != 0:

        rows += 1

        

    storyboard = np.zeros((storyboard_size[1], storyboard_size[0], 3), dtype=np.uint8)

    

    for i, img in enumerate(images):

        x_offset = (i % images_per_row) * (storyboard_size[0] // images_per_row)

        y_offset = (i // images_per_row) * (storyboard_size[1] // rows)

        

        storyboard[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img

        

    cv2.imwrite(output_path, storyboard)

    cv2.imshow("Storyboard", storyboard)

    cv2.waitKey(0)

    cv2.destroyAllWindows()

    

        
image_folder = "/home/biniyam/TenAcademy/StoryBoard-Generation/images/lego_city"

output_path = "/home/biniyam/TenAcademy/StoryBoard-Generation/images/lego_city/storyboard.png"

create_storyboard(image_folder, output_path)
import pandas as pd

import matplotlib.pyplot as plt

import os
data = pd.read_csv("../Data/performance_data.csv")

data.head()
data.loc[0]["preview_link"]
jsoN = pd.read_json("../concepts.json")

jsoN.head()
type(jsoN)
jsoN["implementation"]
from typing import Tuple

import cv2
def locate_image_on_image(locate_image: str, on_image: str, prefix: str = '', visualize: bool = False, color: Tuple[int, int, int] = (0, 0, 255)):

    try:



        image = cv2.imread(on_image)

        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)



        template = cv2.imread(locate_image, 0)



        result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)

        _, _, _, max_loc = cv2.minMaxLoc(result)



        height, width = template.shape[:2]



        top_left = max_loc

        bottom_right = (top_left[0] + width, top_left[1] + height)



        if visualize:

            cv2.rectangle(image, top_left, bottom_right, color, 1)

            plt.figure(figsize=(10, 10))

            plt.axis('off')

            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            plt.imshow(image)



        return {f'{prefix}top_left_pos': top_left, f'{prefix}bottom_right_pos': bottom_right}



    except cv2.error as err:

        print(err)
locate_image_on_image(

    '../Data/Assets/0a22f881b77f00220f2034c21a18b854/engagement_instruction_1.png', 

    '../Data/Assets/0a22f881b77f00220f2034c21a18b854/_preview.png',

    prefix='eng_', visualize=True

)
locate_image_on_image(

    '../Data/Assets/0a22f881b77f00220f2034c21a18b854/game_endfrmae.png', 

    '../Data/Assets/0a22f881b77f00220f2034c21a18b854/end-willowgrove.png',

    prefix='eng_', visualize=True

)
from diffusers import StableDiffusionPipeline

import torch

import time
model_id = "runwayml/stable-diffusion-v1-5"

pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

pipe = pipe.to("cuda")



prompt = "a photo of an astronaut riding a horse on mars"

image = pipe(prompt).images[0]  

    

image.save("astronaut_rides_horse.png")
import replicate

import os



os.environ["REPLICATE_API_TOKEN"] = "r8_AuRvVRUzKg61aDzFyhqPkrbp2rwAINA0oWSl4"



output = replicate.run(

  "stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b",

  input={"prompt": "an iguana on the beach, pointillism",

         "performance_selection": "Extreme Speed",

         "image_speed": 1234,

         "sharpness": 2,

         "aspect_ratios_selection": "1152*896"}

)

imag = output
output = replicate.run(

  "konieshadow/fooocus-api:fda927242b1db6affa1ece4f54c37f19b964666bf23b0d06ae2439067cd344a4",

  input={"prompt": "eagle flying",

         "performance_selection": "Extreme Speed",

         "image_speed": 1234,

         "sharpness": 2,

         "aspect_ratios_selection": "1152*896"}

)

imag2 = output
output = replicate.run(

    "stability-ai/stable-diffusion-inpainting:95b7223104132402a9ae91cc677285bc5eb997834bd2349fa486f53910fd68b3",

    input={

        "mask": imag2[0],

        "image": imag[0],

        "width": 512,

        "height": 512,

        
        "scheduler": "DPMSolverMultistep",

        "num_outputs": 1,

        "guidance_scale": 7.5,

        "num_inference_steps": 25

    }

)

print(output)
import yolov5




model = yolov5.load("fcakyon/yolov5s-v7.0")




model.conf = 0.25  
model.iou = 0.45   
model.agnostic = False   
model.multi_label = False   
model.max_det = 1000    



img = '../Data/Assets/0a22f881b77f00220f2034c21a18b854/end-willowgrove.png'




results = model(img)




results = model(img, size=650)




results = model(img, augment= True)




predictions = results.pred[0]

boxes = predictions[:,:4]      
scores = predictions[:, 4]

categories = predictions[:, 5]




results.show()




results.save(save_dir = 'results/')


from ultralytics import YOLO




model = YOLO("yolov8n.yaml")    



results = model.train(data="./config.yaml", epochs=1)   
import torch




model = torch.hub.load("ultralytics/yolov5", "yolov5s")  



img = "https://ultralytics.com/images/zidane.jpg"  



results = model(img)




results.print()
from typing import List, Literal, Tuple
import itertools
import random
from collections import defaultdict

from PIL import Image
from pprint import pprint


VERTICAL_POSITIONING = {'Logo': [1], 'CTA Button': [1, 2, 3], 'Icon': [1, 2, 3], 'Product Image': [2],
               'Text Elements': [1,3], 'Infographic': [2], 'Banner': [1], 'Illustration': [2], 'Photograph': [2],
               'Mascot': [2], 'Testimonial Quotes': [2], 'Social Proof': [2, 1, 3], 'Seal or Badge': [3, 1, 2],
               'Graphs and Charts': [2], 'Decorative Elements': [3], 'Interactive Elements': [2], 'Interactive Icons': [2],
               'Animation': [2], 'Coupon or Offer Code': [3], 'Legal Disclaimers or Terms': [3],
               'Contact Information': [3, 1, 2], 'Map or Location Image': [3], 'QR Code': [3, 1, 2]}

HORIZONTAL_POSITIONING = {'Logo': [1], 'CTA Button': [2, 1, 3], 'Icon': [1], 'Product Image': [1],
                          'Text Elements': [1], 'Infographic': [1], 'Banner': [2], 'Illustration': [2],
                          'Photograph': [2], 'Mascot': [1], 'Testimonial Quotes': [2], 'Social Proof': [3, 1, 2],
                          'Seal or Badge': [3, 1, 2], 'Graphs and Charts': [1], 'Decorative Elements': [3],
                          'Interactive Elements': [2], 'Interactive Icons': [2], 'Animation': [2], 'Coupon or Offer Code': [3],
                          'Legal Disclaimers or Terms': [3], 'Contact Information': [3, 1, 2],
                          'Map or Location Image': [3], 'QR Code': [3, 1, 2]}


class ImageComposer:
    categories = Literal["Background", "Logo", "CTA Button", "Icon", "Product Image", "Text Elements", "Infographic", "Banner", "Illustration", "Photograph", "Mascot", "Testimonial Quotes", "Social Proof", "Seal or Badge", "Graphs and Charts", "Decorative Elements", "Interactive Elements", "Interactive Icons", "Animation", "Coupon or Offer Code", "Legal Disclaimers or Terms", "Contact Information", "Map or Location Image", "QR Code"]
    PositionSegment = Tuple[float, float]
    AlignmentPosition = Tuple[int, int]
    AlignmentPositions = List[AlignmentPosition]
    frame_images = List[Tuple[categories, str, str]]

    def __init__(self, width:int, height: int, frames: List[frame_images]) -> None:
        self.width = width
        self.height = height
        self.frames = frames
        self.segments = ImageComposer.get_image_position_segments(width, height)
        self.generated_frames = []

    def generate_frames(self):
        self.compose_frames()
        return self.generated_frames

    def compose_frames(self) -> None:
        self.generated_frames = []

        for frame in self.frames:
                        placement_items = []
            for index, item in enumerate(frame):
                if item[0] == "Background":
                    background_index = index
                    continue
                placement_items.append(item)
            
            background = frame[background_index]

            possibilties = ImageComposer.compute_positions([item[0] for item in placement_items])
            identified_locations = ImageComposer.select_diverse_positions(possibilties)
            adjusted_positions = self.calculate_adjusted_element_positions(identified_locations)
            placement_values = [(x[2], *list(y.values())) for x, y in zip(placement_items, adjusted_positions)]
                        self.generated_frames.append(self.create_combined_image(background[2], placement_values))

    @staticmethod
    def compute_positions(elements: List[categories]) -> List[AlignmentPositions]:
        possible_positions = []

                for element in elements:
            vertical_options = VERTICAL_POSITIONING[element]
            horizontal_options = HORIZONTAL_POSITIONING[element]
            combinations = list(itertools.product(vertical_options, horizontal_options))
            possible_positions.append(combinations)

        return possible_positions
    
    @staticmethod
    def select_diverse_positions(possible_positions: List[AlignmentPositions]) -> AlignmentPositions:
        position_frequency = defaultdict(int)

        def update_position_frequency(selected_position):
            position_frequency[selected_position] += 1

        selected_positions = []
        
        for positions in possible_positions:
            sorted_combinations = sorted(positions, key=lambda x: position_frequency[x])
            
            lowest_frequency = position_frequency[sorted_combinations[0]]
            lowest_freq_combinations = [pos for pos in sorted_combinations if position_frequency[pos] == lowest_frequency]
            
            selected_position = random.choice(lowest_freq_combinations)
            selected_positions.append(selected_position)
            
            update_position_frequency(selected_position)
        
        return selected_positions


    @staticmethod
    def get_image_position_segments(width: float, height: float, vm: float = 0.6, vo: float = 0.2, hm: float = 0.6, ho: float = 0.2) -> Tuple[List[PositionSegment], List[PositionSegment]]:
        """Divide Image based on percentage for vertical and horizontal segments."""
        
        if vm + vo * 2 > 1 or hm + ho * 2 > 1:
            raise ValueError("Sum of percentages exceeds 100% for either vertical or horizontal segments.")
        
        vertical_mid = height * vm
        vertical_outer = height * vo
        horizontal_mid = width * hm
        horizontal_outer = width * ho

        vertical_segments = [
            (0, vertical_outer),
            (vertical_outer, vertical_outer + vertical_mid),
            (vertical_outer + vertical_mid, height)
        ]
        
        horizontal_segments = [
            (0, horizontal_outer),
            (horizontal_outer, horizontal_outer + horizontal_mid),
            (horizontal_outer + horizontal_mid, width)
        ]

        segements = []
        for vs in vertical_segments:
            vs_items = []
            for hs in horizontal_segments:
                vs_items.append((vs, hs))
            segements.append(vs_items)


        return segements
    
    def calculate_adjusted_element_positions(self, elements_positions, padding=10):
        element_details = []
        segment_elements = {}

                for i, (v_pos, h_pos) in enumerate(elements_positions):
            segment_key = (v_pos, h_pos)
            if segment_key not in segment_elements:
                segment_elements[segment_key] = []
            segment_elements[segment_key].append(i)
        
        for segment_key, elements in segment_elements.items():
            v_pos, h_pos = segment_key
            segment = self.segments[v_pos-1][h_pos-1]
            vertical_segment, horizontal_segment = segment
            num_elements = len(elements)
            
            x_start, x_end = horizontal_segment
            y_start, y_end = vertical_segment
            segment_width = (x_end - x_start) - 2 * padding
            segment_height = (y_end - y_start) - 2 * padding
            
                        is_vertical = segment_height > segment_width
            if is_vertical:
                space_per_element = segment_height / num_elements
            else:
                space_per_element = segment_width / num_elements
            
            for index, _ in enumerate(elements):
                if is_vertical:
                    element_x_start = x_start + padding
                    element_y_start = y_start + padding + index * space_per_element
                    element_width = segment_width
                    element_height = space_per_element
                else:
                    element_x_start = x_start + padding + index * space_per_element
                    element_y_start = y_start + padding
                    element_width = space_per_element
                    element_height = segment_height
                
                element_details.append({
                    "start_point": (element_x_start, element_y_start),
                    "dimensions": (element_width, element_height)
                })

        return element_details
    
    @staticmethod
    def resize_image(image, target_width, target_height):
        """
        Resize an image to fit within target dimensions while maintaining aspect ratio.
        """
        original_width, original_height = image.size
        ratio = min(target_width / original_width, target_height / original_height)
        new_width = int(original_width * ratio)
        new_height = int(original_height * ratio)
        resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
        return resized_image

    def create_combined_image(self, background_path: str, elements: List[Tuple[str, int|float, int|float]]) -> Image.Image:
        """
        Create a combined image based on background and elements' positioning and sizing.
        
        :param background_path: Path to the background image.
        :param elements: A list of dictionaries, each containing 'image_path', 'start_point', and 'dimensions'.
        """
                background = Image.open(background_path).convert("RGBA")
        
        for element in elements:
                        image_path = element[0]
            image = Image.open(image_path).convert("RGBA")
            
                        target_width, target_height = element[2]
            resized_image = ImageComposer.resize_image(image, target_width, target_height)
            
                        start_x, start_y = element[1]
            offset_x = start_x + (target_width - resized_image.size[0]) / 2
            offset_y = start_y + (target_height - resized_image.size[1]) / 2
            
                        background.paste(resized_image, (int(offset_x), int(offset_y)), resized_image)
        
        return background
    

if __name__ == "__main__":
    ic = ImageComposer(320, 500, [[('Logo', 'url_path', 'local_path'), 
                                   ('Call-To-Action (CTA) Button', 'url_path', 'local_path'),
                                   ('Icon', 'url_path', 'local_path'),
                                   ('Product Image', 'url_path', 'local_path'),
                                   ('Text Elements', 'url_path', 'local_path')]])
    possibilties = ImageComposer.compute_positions(["Logo", "Call-To-Action (CTA) Button", "Icon", "Product Image", "Text Elements"])
    pprint(possibilties)
    print("======================================================")
    diverse = ImageComposer.select_diverse_positions(possibilties)
    pprint(diverse)

    print(ic.calculate_adjusted_element_positions(diverse))
from typing import Literal, Optional, Tuple
import logging
import base64
from io import BytesIO
import os

import replicate
from PIL import Image
import requests
from pydantic import HttpUrl

os.environ["REPLICATE_API_TOKEN"] = "r8_AuRvVRUzKg61aDzFyhqPkrbp2rwAINA0oWSl4"
logging.basicConfig(level=logging.INFO)

class ImageGenerater:
    def __init__(self, asset_suggestions: dict) -> None:
        self.asset_suggestions = asset_suggestions

    def generate_images(self, store_location: str ='./images') -> dict:
        generated_images = {}
        for frame, elements in self.asset_suggestions.items():
            if frame.startswith('frame'):
                generated_images[frame] = []
                for type, description in elements.items():
                    downloaded_image = ImageGenerater.download_image(ImageGenerater.generate_image(prompt=description)[0], store_location)
                    generated_images[frame].append((type, *downloaded_image))

        return generated_images


    @staticmethod
    def generate_image(prompt: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed", 
                       aspect_ratios_selection: str = "1024*1024", image_seed: int = 1234, sharpness: int = 2) -> Optional[dict]:
        """
        Generates an image based on the given prompt and settings.

        :param prompt: Textual description of the image to generate.
        :param performance_selection: Choice of performance level affecting generation speed and quality.
        :param aspect_ratio: The desired aspect ratio of the generated image.
        :param image_seed: Seed for the image generation process for reproducibility.
        :param sharpness: The sharpness level of the generated image.
        :return: The generated image or None if an error occurred.
        """
        try:
            output = replicate.run(
                "stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b",
                input={
                    "prompt": prompt,
                    "performance_selection": performance_selection,
                    "aspect_ratios_selection": aspect_ratios_selection,
                    "image_seed": image_seed,
                    "sharpness": sharpness
                }
            )
            logging.info("Image generated successfully.")
            return output
        except Exception as e:
            logging.error(f"Failed to generate image: {e}")
            return None
        
    @staticmethod
    def decode_image(base64_data: str) -> Optional[Image.Image]:
        """
        Converts a base64 image into pillow iamge object.

        :param base64_data: Textual base64 image data.
        :return: Converted pillow image.
        """
        image_data = base64.b64decode(base64_data)
        image_stream = BytesIO(image_data)
        return(Image.open(image_stream))
    
    @staticmethod
    def download_image(url: HttpUrl, save_path: str) -> Tuple[str, str]:
        """
        Downloads provided url data to given location.

        :param url: HTTP Url of the file.
        :param save_path: Folder location to save the data.
        :return: Tuple of the url and save location.
        """

        try:
            response = requests.get(url)
            
            if response.status_code == 200:
                save_path = os.path.join(save_path, os.path.basename(url))
                image = Image.open(BytesIO(response.content))
                image.save(save_path)
                logging.info(f"Image saved to {save_path}")
                return (url, save_path)
            else:
                raise RuntimeError(f"Failed to download image. Status code: {response.status_code}") from None
        except Exception as e:
            raise RuntimeError(f"An error occurred: {e}") from e
        
        
if __name__ == "__main__":
    a = {
    "frame_1": {
        "Animated Element": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",
    },
    "frame_2": {
        "CTA Text": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."
    },
    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."
    }
    test = ImageGenerater(a)

    test.generate_images()
from data_ingest import logger
from data_ingest.pipeline.stage_01_data_ingestion import DataIngestionTrainingPipeline

STAGE_NAME = "Data Ingestion stage"


try:
    logger.info(f">>>>>> stage {STAGE_NAME} started <<<<<<")
    obj = DataIngestionTrainingPipeline()
    obj.main()
    logger.info(f">>>>>> stage {STAGE_NAME} completed <<<<<<\n\nx==========x")
except Exception as e:
    logger.exception(e)
    raise e
import os

import sys

from IPython.display import Image

import io

import base64



sys.path.append(os.path.abspath(os.path.join('../../scripts')))

from image_generator.image_generator_with_foocus import ImageGenerater
prompt = "Background: A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable withImageGenerater a vinyl record that has the Coke Studio logo."
generateImage = ImageGenerater.generate_image(prompt=prompt, aspect_ratios_selection='1024*960')
generateImage[0]
ImageGenerater.download_image(

    url=generateImage[0],

    save_path='../../storyboard_assets/frame1/',

    image_name='background'

)
ImageGenerater.download_image(

    url=generateImage[0],

    save_path='../../images_generator/',

    image_name='foocus'

)
from typing import Literal, Optional, Tuple
import logging
import base64
from io import BytesIO
import os

import replicate
from PIL import Image
import requests
from pydantic import HttpUrl

os.getenv("REPLICATE_API_TOKEN")
logging.basicConfig(level=logging.INFO)

class ImageGenerater:
    def __init__(self, asset_suggestions: dict) -> None:
        self.asset_suggestions = asset_suggestions

    def generate_images(self, store_location: str ='./images') -> dict:
        generated_images = {}
        for frame, elements in self.asset_suggestions.items():
            if frame.startswith('frame'):
                generated_images[frame] = []
                for type, description in elements.items():
                    downloaded_image = ImageGenerater.download_image(ImageGenerater.generate_image(prompt=description)[0], store_location)
                    generated_images[frame].append((type, *downloaded_image))

        return generated_images


    @staticmethod
    def generate_image(prompt: str, performance_selection: Literal['Speed', 'Quality', 'Extreme Speed'] = "Extreme Speed",
                       aspect_ratios_selection: str = "1024*1024", image_seed: int = 1234, sharpness: int = 2) -> Optional[dict]:
        """
        Generates an image based on the given prompt and settings.

        :param prompt: Textual description of the image to generate.
        :param performance_selection: Choice of performance level affecting generation speed and quality.
        :param aspect_ratio: The desired aspect ratio of the generated image.
        :param image_seed: Seed for the image generation process for reproducibility.
        :param sharpness: The sharpness level of the generated image.
        :return: The generated image or None if an error occurred.
        """
        try:
            output = replicate.run(
                "konieshadow/fooocus-api-anime:a750658f54c4f8bec1c8b0e352ce2666c22f2f919d391688ff4fc16e48b3a28f",
                input={
                    "prompt": prompt,
                    "performance_selection": performance_selection,
                    "aspect_ratios_selection": aspect_ratios_selection,
                    "image_seed": image_seed,
                    "sharpness": sharpness
                }
            )
            logging.info("Image generated successfully.")
            return output
        except Exception as e:
            logging.error(f"Failed to generate image: {e}")
            return None

    @staticmethod
    def decode_image(base64_data: str) -> Optional[Image.Image]:
        """
        Converts a base64 image into pillow iamge object.

        :param base64_data: Textual base64 image data.
        :return: Converted pillow image.
        """
        image_data = base64.b64decode(base64_data)
        image_stream = BytesIO(image_data)
        return(Image.open(image_stream))

    @staticmethod
    def download_image(url: HttpUrl, save_path: str) -> Tuple[str, str]:
        """
        Downloads provided url data to given location.

        :param url: HTTP Url of the file.
        :param save_path: Folder location to save the data.
        :return: Tuple of the url and save location.
        """

        try:
            response = requests.get(url)

            if response.status_code == 200:
                save_path = os.path.join(save_path, os.path.basename(url))
                image = Image.open(BytesIO(response.content))
                image.save(save_path)
                logging.info(f"Image saved to {save_path}")
                return (url, save_path)
            else:
                raise RuntimeError(f"Failed to download image. Status code: {response.status_code}") from None
        except Exception as e:
            raise RuntimeError(f"An error occurred: {e}") from e


if __name__ == "__main__":
                    a = {
    "frame_1": {
        "Animated Element": "A high-resolution 3D Coca-Cola bottle center-screen, bubbles rising to the top, transitioning into a sleek DJ turntable with a vinyl record that has the Coke Studio logo.",
    },
    "frame_2": {
        "CTA Text": "'Mix Your Beat' in bold, playful font pulsating to the rhythm of a subtle background beat, positioned at the bottom of the screen."
    },
    "explanation": "This variation emphasizes the joy and interactivity of music mixing, with each frame building on the last to create a crescendo of engagement. The 3D bottle-to-turntable animation captures attention, the interactive beat mixer sustains engagement, and the vibrant animations encourage sharing, aligning with the campaign's objectives of engagement and message recall."
    }
    test = ImageGenerater(a)

    test.generate_images()
import logging
import matplotlib.pyplot as plt
from keras_cv.models import StableDiffusion
import os

class ImageTextModel:
    def __init__(self, img_width=512, img_height=512, jit_compile=False):
        self.model = StableDiffusion(img_width=img_width, img_height=img_height, jit_compile=jit_compile)
        self.logger = logging.getLogger(__name__)

    def generate_images_from_text(self, text, batch_size=3):
        try:
            images = self.model.text_to_image(text, batch_size=batch_size)
            self.logger.info("Images generated successfully.")
        except Exception as e:
            self.logger.error(f"Error generating images: {e}")
            return None
        return images

    def plot_images(self, images):
        try:
            plt.figure(figsize=(20, 20))
            for i in range(len(images)):
                ax = plt.subplot(1, len(images), i + 1)
                plt.imshow(images[i])
                plt.axis("off")
            plt.show()
            self.logger.info("Images plotted successfully.")
        except Exception as e:
            self.logger.error(f"Error plotting images: {e}")

    def save_images(self, images, folder_path):
        try:
            os.makedirs(folder_path, exist_ok=True)
            for i, image in enumerate(images):
                plt.imsave(os.path.join(folder_path, f"image_{i}.png"), image)
            self.logger.info("Images saved successfully.")
        except Exception as e:
            self.logger.error(f"Error saving images: {e}")
from typing import List, Literal, Tuple
import itertools
import random
from collections import defaultdict

from PIL import Image
from pprint import pprint


VERTICAL_POSITIONING = {'Logo': [1], 'CTA Button': [1, 2, 3], 'Icon': [1, 2, 3], 'Product Image': [2],
               'Text Elements': [1,3], 'Infographic': [2], 'Banner': [1], 'Illustration': [2], 'Photograph': [2],
               'Mascot': [2], 'Testimonial Quotes': [2], 'Social Proof': [2, 1, 3], 'Seal or Badge': [3, 1, 2],
               'Graphs and Charts': [2], 'Decorative Elements': [3], 'Interactive Elements': [2],
               'Animation': [2], 'Coupon or Offer Code': [3], 'Legal Disclaimers or Terms': [3],
               'Contact Information': [3, 1, 2], 'Map or Location Image': [3], 'QR Code': [3, 1, 2]}

HORIZONTAL_POSITIONING = {'Logo': [1], 'CTA Button': [2, 1, 3], 'Icon': [1], 'Product Image': [1],
                          'Text Elements': [1], 'Infographic': [1], 'Banner': [2], 'Illustration': [2],
                          'Photograph': [2], 'Mascot': [1], 'Testimonial Quotes': [2], 'Social Proof': [3, 1, 2],
                          'Seal or Badge': [3, 1, 2], 'Graphs and Charts': [1], 'Decorative Elements': [3],
                          'Interactive Elements': [2], 'Animation': [2], 'Coupon or Offer Code': [3],
                          'Legal Disclaimers or Terms': [3], 'Contact Information': [3, 1, 2],
                          'Map or Location Image': [3], 'QR Code': [3, 1, 2]}


class ImageComposer:
    categories = Literal["Background", "Logo", "CTA Button", "Icon", "Product Image", "Text Elements", "Infographic", "Banner", "Illustration", "Photograph", "Mascot", "Testimonial Quotes", "Social Proof", "Seal or Badge", "Graphs and Charts", "Decorative Elements", "Interactive Elements", "Animation", "Coupon or Offer Code", "Legal Disclaimers or Terms", "Contact Information", "Map or Location Image", "QR Code"]
    PositionSegment = Tuple[float, float]
    AlignmentPosition = Tuple[int, int]
    AlignmentPositions = List[AlignmentPosition]
    frame_images = List[Tuple[categories, str, str]]

    def __init__(self, width:int, height: int, frames: List[frame_images]) -> None:
        self.width = width
        self.height = height
        self.frames = frames
        self.segments = ImageComposer.get_image_position_segments(width, height)
        self.generated_frames = []

    def generate_frames(self):
        self.compose_frames()
        return self.generated_frames

    def compose_frames(self) -> None:
        self.generated_frames = []

        for frame in self.frames:
                        placement_items = []
            for index, item in enumerate(frame):
                if item[0] == "Background":
                    background_index = index
                    continue
                placement_items.append(item)

            background = frame[background_index]

            possibilties = ImageComposer.compute_positions([item[0] for item in placement_items])
            identified_locations = ImageComposer.select_diverse_positions(possibilties)
            adjusted_positions = self.calculate_adjusted_element_positions(identified_locations)
            placement_values = [(x[2], *list(y.values())) for x, y in zip(placement_items, adjusted_positions)]
                        self.generated_frames.append(self.create_combined_image(background[2], placement_values))

    @staticmethod
    def compute_positions(elements: List[categories]) -> List[AlignmentPositions]:
        possible_positions = []

                for element in elements:
            vertical_options = VERTICAL_POSITIONING[element]
            horizontal_options = HORIZONTAL_POSITIONING[element]
            combinations = list(itertools.product(vertical_options, horizontal_options))
            possible_positions.append(combinations)

        return possible_positions

    @staticmethod
    def select_diverse_positions(possible_positions: List[AlignmentPositions]) -> AlignmentPositions:
        position_frequency = defaultdict(int)

        def update_position_frequency(selected_position):
            position_frequency[selected_position] += 1

        selected_positions = []

        for positions in possible_positions:
            sorted_combinations = sorted(positions, key=lambda x: position_frequency[x])

            lowest_frequency = position_frequency[sorted_combinations[0]]
            lowest_freq_combinations = [pos for pos in sorted_combinations if position_frequency[pos] == lowest_frequency]

            selected_position = random.choice(lowest_freq_combinations)
            selected_positions.append(selected_position)

            update_position_frequency(selected_position)

        return selected_positions


    @staticmethod
    def get_image_position_segments(width: float, height: float, vm: float = 0.6, vo: float = 0.2, hm: float = 0.6, ho: float = 0.2) -> Tuple[List[PositionSegment], List[PositionSegment]]:
        """Divide Image based on percentage for vertical and horizontal segments."""

        if vm + vo * 2 > 1 or hm + ho * 2 > 1:
            raise ValueError("Sum of percentages exceeds 100% for either vertical or horizontal segments.")

        vertical_mid = height * vm
        vertical_outer = height * vo
        horizontal_mid = width * hm
        horizontal_outer = width * ho

        vertical_segments = [
            (0, vertical_outer),
            (vertical_outer, vertical_outer + vertical_mid),
            (vertical_outer + vertical_mid, height)
        ]

        horizontal_segments = [
            (0, horizontal_outer),
            (horizontal_outer, horizontal_outer + horizontal_mid),
            (horizontal_outer + horizontal_mid, width)
        ]

        segements = []
        for vs in vertical_segments:
            vs_items = []
            for hs in horizontal_segments:
                vs_items.append((vs, hs))
            segements.append(vs_items)


        return segements

    def calculate_adjusted_element_positions(self, elements_positions, padding=10):
        element_details = []
        segment_elements = {}

                for i, (v_pos, h_pos) in enumerate(elements_positions):
            segment_key = (v_pos, h_pos)
            if segment_key not in segment_elements:
                segment_elements[segment_key] = []
            segment_elements[segment_key].append(i)

        for segment_key, elements in segment_elements.items():
            v_pos, h_pos = segment_key
            segment = self.segments[v_pos-1][h_pos-1]
            vertical_segment, horizontal_segment = segment
            num_elements = len(elements)

            x_start, x_end = horizontal_segment
            y_start, y_end = vertical_segment
            segment_width = (x_end - x_start) - 2 * padding
            segment_height = (y_end - y_start) - 2 * padding

                        is_vertical = segment_height > segment_width
            if is_vertical:
                space_per_element = segment_height / num_elements
            else:
                space_per_element = segment_width / num_elements

            for index, _ in enumerate(elements):
                if is_vertical:
                    element_x_start = x_start + padding
                    element_y_start = y_start + padding + index * space_per_element
                    element_width = segment_width
                    element_height = space_per_element
                else:
                    element_x_start = x_start + padding + index * space_per_element
                    element_y_start = y_start + padding
                    element_width = space_per_element
                    element_height = segment_height

                element_details.append({
                    "start_point": (element_x_start, element_y_start),
                    "dimensions": (element_width, element_height)
                })

        return element_details

    @staticmethod
    def resize_image(image, target_width, target_height):
        """
        Resize an image to fit within target dimensions while maintaining aspect ratio.
        """
        original_width, original_height = image.size
        ratio = min(target_width / original_width, target_height / original_height)
        new_width = int(original_width * ratio)
        new_height = int(original_height * ratio)
        resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
        return resized_image

    def create_combined_image(self, background_path: str, elements: List[Tuple[str, int|float, int|float]]) -> Image.Image:
        """
        Create a combined image based on background and elements' positioning and sizing.

        :param background_path: Path to the background image.
        :param elements: A list of dictionaries, each containing 'image_path', 'start_point', and 'dimensions'.
        """
                background = Image.open(background_path).convert("RGBA")

        for element in elements:
                        image_path = element[0]
            image = Image.open(image_path).convert("RGBA")

                        target_width, target_height = element[2]
            resized_image = ImageComposer.resize_image(image, target_width, target_height)

                        start_x, start_y = element[1]
            offset_x = start_x + (target_width - resized_image.size[0]) / 2
            offset_y = start_y + (target_height - resized_image.size[1]) / 2

                        background.paste(resized_image, (int(offset_x), int(offset_y)), resized_image)

        return background


if __name__ == "__main__":
    ic = ImageComposer(320, 500, [[('Logo', 'url_path', 'local_path'),
                                   ('Call-To-Action (CTA) Button', 'url_path', 'local_path'),
                                   ('Icon', 'url_path', 'local_path'),
                                   ('Product Image', 'url_path', 'local_path'),
                                   ('Text Elements', 'url_path', 'local_path')]])
    possibilties = ImageComposer.compute_positions(["Logo", "Call-To-Action (CTA) Button", "Icon", "Product Image", "Text Elements"])
    pprint(possibilties)
    print("======================================================")
    diverse = ImageComposer.select_diverse_positions(possibilties)
    pprint(diverse)

    print(ic.calculate_adjusted_element_positions(diverse))
from typing import List
from PIL import Image

class StoryBoard:

    @staticmethod
    def combine_images_horizontally(images: List[Image.Image], separation_space=100, vertical_padding=200, background_color=(255, 255, 255)):
        """
        Combines multiple images into a new image, displayed horizontally on a larger background.
        Images are centered horizontally within the background and have vertical padding.

        :param images: loaded pillow images.
        :param separation_space: Space between images in pixels.
        :param vertical_padding: Vertical padding for the top and bottom of the images.
        :param background_color: Background color of the new image as an RGB tuple.
        :return: Combined image.
        """
        widths, heights = zip(*(i.size for i in images))

                total_images_width = sum(widths) + separation_space * (len(images) - 1)
        max_height = max(heights) + vertical_padding * 2

                background_width = total_images_width + vertical_padding * 2          background_height = max_height

                background = Image.new('RGB', (background_width, background_height), color=background_color)

                x_offset = (background_width - total_images_width) // 2

                for img in images:
            y_offset = (background_height - img.height) // 2
            background.paste(img, (x_offset, y_offset))
            x_offset += img.width + separation_space

        return background

if __name__ == "__main__":
        images = [Image.open('./resized_images/01d8fa51-ca7b-499d-9938-c13f3b496439.png'),
              Image.open('./resized_images/0c29fdce-588c-4e2c-930a-07d9c3216e1d.png'),
              Image.open('./resized_images/9ec00576-2a1e-437e-9f13-355b7cfb4a14.png')]

    image = StoryBoard.combine_images_horizontally(images)
    image.show()
from openai import OpenAI
from dotenv import load_dotenv
import base64
import logging
from typing import Literal, Optional, Tuple
import os
import requests
from PIL import Image
from typing import Tuple
from io import BytesIO

class ImageGenerate:
    def __init__(self)-> None:
                load_dotenv()
                self.api_key = os.getenv("OPENAI_API_KEY")
                if not self.api_key:
            raise ValueError("API key is not set. Make sure it is available in your .env file.")
                self.client = OpenAI(api_key=self.api_key)
    @staticmethod
    def generate_images(asset_suggestions: dict, store_location: str ='./images') -> dict:
        generated_images = {}
        for frame, elements in asset_suggestions.items():
            if frame.startswith('frame'):
                generated_images[frame] = []
                for type, description in elements.items():
                    downloaded_image = ImageGenerate.download_image(ImageGenerate.generate_image(prompt=description), store_location,type)
                    generated_images[frame].append((type, *downloaded_image))

        return generated_images

    @staticmethod
    def generate_image(prompt: str) -> str:
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                quality="hd",
                n=1,
            )
            image_url = response.data[0].url
            logging.info("Image generated successfully.")
            return image_url
        except Exception as e:
            logging.error(f"Error while generating image: {e}")
            return ""

    @staticmethod
    def decode_image(base64_data: str) -> Optional[Image.Image]:
        """
        Converts a base64 image into pillow image object.

        :param base64_data: Textual base64 image data.
        :return: Converted pillow image.
        """
        image_data = base64.b64decode(base64_data)
        image_stream = BytesIO(image_data)
        return Image.open(image_stream)

    @staticmethod
    def download_image(url: str, save_path: str, image_name: str) -> Tuple[str, str]:
        """
        Downloads an image from the provided URL and saves it to the specified location with the given image name.

        :param url: HTTP URL of the image.
        :param save_path: Folder location to save the image.
        :param image_name: Name to save the image with.
        :return: Tuple containing the URL of the image and the path where it is saved.
        """
        try:
            response = requests.get(url)
            response.raise_for_status()  
                        image_name += ".png"

                        os.makedirs(save_path, exist_ok=True)

                        save_file_path = os.path.join(save_path, image_name)

                        with open(save_file_path, 'wb') as f:
                f.write(response.content)

            logging.info(f"Image saved to {save_file_path}")
            return save_file_path

        except requests.RequestException as e:
            raise RuntimeError(f"Failed to download image from {url}: {e}") from e

        except OSError as e:
            raise RuntimeError(f"Error occurred while saving the image: {e}") from e
!pip install transformers diffusers accelerate

!pip install xformers
from diffusers import StableDiffusionPipeline

import torch
model_id = "dreamlike-art/dreamlike-photoreal-2.0"

pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

pipe = pipe.to("cuda")
prompts = ["A suspenseful animation of a LEGO CITY set, with buildings, vehicles, and mini-figures coming to life in a dynamic, 3D environment. The animation is designed to captivate the viewer's attention and set the stage for the upcoming challenge",

          "YOUR CITY, NO LIMITS' appears in bold, vibrant letters, reinforcing the campaign's theme of boundless creativity and adventure",

          "A digital countdown timer, styled to resemble LEGO bricks, adds an element of suspense and anticipation, building up to the reveal of the 'ULTIMATE ESCAPE CHALLENGE'"]





images = []
for i, prompt in enumerate(prompts):

    image = pipe(prompt).images[0]

    image.save(f'../images/picture_{i}.jpg')

    images.append(image)
from typing import List, Literal, Tuple
import itertools
import random
from collections import defaultdict

from PIL import Image
from pprint import pprint


VERTICAL_POSITIONING = {'Logo': [1], 'CTA Button': [1, 2, 3], 'Icon': [1, 2, 3], 'Product Image': [2],
               'Text Elements': [1,3], 'Infographic': [2], 'Banner': [1], 'Illustration': [2], 'Photograph': [2],
               'Mascot': [2], 'Testimonial Quotes': [2], 'Social Proof': [2, 1, 3], 'Seal or Badge': [3, 1, 2],
               'Graphs and Charts': [2], 'Decorative Elements': [3], 'Interactive Elements': [2],
               'Animation': [2], 'Coupon or Offer Code': [3], 'Legal Disclaimers or Terms': [3],
               'Contact Information': [3, 1, 2], 'Map or Location Image': [3], 'QR Code': [3, 1, 2]}

HORIZONTAL_POSITIONING = {'Logo': [1], 'CTA Button': [2, 1, 3], 'Icon': [1], 'Product Image': [1],
                          'Text Elements': [1], 'Infographic': [1], 'Banner': [2], 'Illustration': [2],
                          'Photograph': [2], 'Mascot': [1], 'Testimonial Quotes': [2], 'Social Proof': [3, 1, 2],
                          'Seal or Badge': [3, 1, 2], 'Graphs and Charts': [1], 'Decorative Elements': [3],
                          'Interactive Elements': [2], 'Animation': [2], 'Coupon or Offer Code': [3],
                          'Legal Disclaimers or Terms': [3], 'Contact Information': [3, 1, 2],
                          'Map or Location Image': [3], 'QR Code': [3, 1, 2]}


class ImageComposer:
    categories = Literal["Background", "Logo", "CTA Button", "Icon", "Product Image", "Text Elements", "Infographic", "Banner", "Illustration", "Photograph", "Mascot", "Testimonial Quotes", "Social Proof", "Seal or Badge", "Graphs and Charts", "Decorative Elements", "Interactive Elements", "Animation", "Coupon or Offer Code", "Legal Disclaimers or Terms", "Contact Information", "Map or Location Image", "QR Code"]
    PositionSegment = Tuple[float, float]
    AlignmentPosition = Tuple[int, int]
    AlignmentPositions = List[AlignmentPosition]
    frame_images = List[Tuple[categories, str, str]]

    def __init__(self, width:int, height: int, frames: List[frame_images]) -> None:
        self.width = width
        self.height = height
        self.frames = frames
        self.segments = ImageComposer.get_image_position_segments(width, height)
        self.generated_frames = []

    def generate_frames(self):
        self.compose_frames()
        return self.generated_frames

    def generate_frames(self):
        self.compose_frames()
        return self.generated_frames

    def compose_frames(self) -> None:
        self.generated_frames = []

        for frame in self.frames:
            background_element = None
            other_elements = []

                        for index, item in enumerate(frame):
                if item[0] == "Background":
                    background_element = item
                else:
                    other_elements.append(item)

                        if background_element:
                                self.generated_frames.append(self.create_combined_image(background_element[2], other_elements))
            else:
                                self.generated_frames.append(self.create_combined_image(None, other_elements))


    @staticmethod
    def compute_positions(elements: List[categories]) -> List[AlignmentPositions]:
        possible_positions = []

                for element in elements:
            vertical_options = VERTICAL_POSITIONING[element]
            horizontal_options = HORIZONTAL_POSITIONING[element]
            combinations = list(itertools.product(vertical_options, horizontal_options))
            possible_positions.append(combinations)

        return possible_positions
    
    @staticmethod
    def select_diverse_positions(possible_positions: List[AlignmentPositions]) -> AlignmentPositions:
        position_frequency = defaultdict(int)

        def update_position_frequency(selected_position):
            position_frequency[selected_position] += 1

        selected_positions = []
        
        for positions in possible_positions:
            sorted_combinations = sorted(positions, key=lambda x: position_frequency[x])
            
            lowest_frequency = position_frequency[sorted_combinations[0]]
            lowest_freq_combinations = [pos for pos in sorted_combinations if position_frequency[pos] == lowest_frequency]
            
            selected_position = random.choice(lowest_freq_combinations)
            selected_positions.append(selected_position)
            
            update_position_frequency(selected_position)
        
        return selected_positions


    @staticmethod
    def get_image_position_segments(width: float, height: float, vm: float = 0.6, vo: float = 0.2, hm: float = 0.6, ho: float = 0.2) -> Tuple[List[PositionSegment], List[PositionSegment]]:
        """Divide Image based on percentage for vertical and horizontal segments."""
        
        if vm + vo * 2 > 1 or hm + ho * 2 > 1:
            raise ValueError("Sum of percentages exceeds 100% for either vertical or horizontal segments.")
        
        vertical_mid = height * vm
        vertical_outer = height * vo
        horizontal_mid = width * hm
        horizontal_outer = width * ho

        vertical_segments = [
            (0, vertical_outer),
            (vertical_outer, vertical_outer + vertical_mid),
            (vertical_outer + vertical_mid, height)
        ]
        
        horizontal_segments = [
            (0, horizontal_outer),
            (horizontal_outer, horizontal_outer + horizontal_mid),
            (horizontal_outer + horizontal_mid, width)
        ]

        segements = []
        for vs in vertical_segments:
            vs_items = []
            for hs in horizontal_segments:
                vs_items.append((vs, hs))
            segements.append(vs_items)


        return segements
    
    def calculate_adjusted_element_positions(self, elements_positions, padding=10):
        element_details = []
        segment_elements = {}

                for i, (v_pos, h_pos) in enumerate(elements_positions):
            segment_key = (v_pos, h_pos)
            if segment_key not in segment_elements:
                segment_elements[segment_key] = []
            segment_elements[segment_key].append(i)
        
        for segment_key, elements in segment_elements.items():
            v_pos, h_pos = segment_key
            segment = self.segments[v_pos-1][h_pos-1]
            vertical_segment, horizontal_segment = segment
            num_elements = len(elements)
            
            x_start, x_end = horizontal_segment
            y_start, y_end = vertical_segment
            segment_width = (x_end - x_start) - 2 * padding
            segment_height = (y_end - y_start) - 2 * padding
            
                        is_vertical = segment_height > segment_width
            if is_vertical:
                space_per_element = segment_height / num_elements
            else:
                space_per_element = segment_width / num_elements
            
            for index, _ in enumerate(elements):
                if is_vertical:
                    element_x_start = x_start + padding
                    element_y_start = y_start + padding + index * space_per_element
                    element_width = segment_width
                    element_height = space_per_element
                else:
                    element_x_start = x_start + padding + index * space_per_element
                    element_y_start = y_start + padding
                    element_width = space_per_element
                    element_height = segment_height
                
                element_details.append({
                    "start_point": (element_x_start, element_y_start),
                    "dimensions": (element_width, element_height)
                })

        return element_details
    
    @staticmethod
    def resize_image(image, target_width, target_height):
        """
        Resize an image to fit within target dimensions while maintaining aspect ratio.
        """
        original_width, original_height = image.size
        ratio = min(target_width / original_width, target_height / original_height)
        new_width = int(original_width * ratio)
        new_height = int(original_height * ratio)
        resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)
        return resized_image

    def create_combined_image(self, background_path: str, elements: List[Tuple[str, int|float, int|float]]) -> Image.Image:
        """
        Create a combined image based on background and elements' positioning and sizing.
        
        :param background_path: Path to the background image.
        :param elements: A list of dictionaries, each containing 'image_path', 'start_point', and 'dimensions'.
        """
                background = Image.open(background_path).convert("RGBA")
        
        for element in elements:
                        image_path = element[0]
            image = Image.open(image_path).convert("RGBA")
            
                        target_width, target_height = element[2]
            resized_image = ImageComposer.resize_image(image, target_width, target_height)
            
                        start_x, start_y = element[1]
            offset_x = start_x + (target_width - resized_image.size[0]) / 2
            offset_y = start_y + (target_height - resized_image.size[1]) / 2
            
                        background.paste(resized_image, (int(offset_x), int(offset_y)), resized_image)
        
        return background
    

if __name__ == "__main__":
    ic = ImageComposer(320, 500, [[('Logo', 'url_path', 'local_path'), 
                                   ('Call-To-Action (CTA) Button', 'url_path', 'local_path'),
                                   ('Icon', 'url_path', 'local_path'),
                                   ('Product Image', 'url_path', 'local_path'),
                                   ('Text Elements', 'url_path', 'local_path')]])
    possibilties = ImageComposer.compute_positions(["Logo", "Call-To-Action (CTA) Button", "Icon", "Product Image", "Text Elements"])
    pprint(possibilties)
    print("======================================================")
    diverse = ImageComposer.select_diverse_positions(possibilties)
    pprint(diverse)

    print(ic.calculate_adjusted_element_positions(diverse))
from typing import Literal, Optional, Tuple 
import logging 
import base64
from io import BytesIO
import os
from PIL import Image

import replicate
from pydantic import HttpUrl
import requests


os.environ['REPLICATE_API_TOKEN'] = 'YOUR_API_TOKEN'
logging.basicConfig(level=logging.INFO)


class ImageGenerator:
    """
    A class for generating and working with images using the Replicate API.

    Attributes:
        asset_suggestions (dict): A dictionary containing frame and elements for image generation.
    """
    def __init__(self, asset_suggestions: dict) -> None:
        """
        Initialize the ImageGenerator object with asset suggestions.

        Args:
            asset_suggestions (dict): A dictionary containing frame and elements for image generation.
        """
        self.asset_suggestions = asset_suggestions
    
    def generate_images(self, store_location: str ='./images') -> dict:
        """
        Generate images based on the provided asset suggestions.

        Args:
            store_location (str, optional): The directory path to store the generated images. Defaults to './images'.

        Returns:
            dict: A dictionary containing generated images with frame and element details.
        """
        generated_images = {}
        for frame, elements in self.asset_suggestions.items():
            if frame.startswith('frame'):
                generated_images[frame] = []
                for type, description in elements.items():
                    downloaded_image = ImageGenerator.download_image(
                        ImageGenerator.generate_images(prompt=description)[0], store_location)
                    generated_images[frame].append((type, *downloaded_image))
        
        return generated_images
    
    @staticmethod
    def generate_images(prompt: str) -> Optional[list]:
        """
        Generate images using the Replicate API based on the provided prompt.

        Args:
            prompt (str): The prompt for image generation.

        Returns:
            list: A list containing image details, or None if an error occurs.
        """
        focus_api = os.environ.get('FOCUS_API')
        
        try:
            output = replicate.run(
                    'konieshadow/fooocus-api:{focus_api}',
                    input={
                        'prompt': prompt,
                        'num_images': 1
                    }
                )
            logging.info('Image generated successfully')
            return output
        except Exception as e:
            logging.error(f'Error generating image: {e}')
            return None
        
    @staticmethod
    def decode_image(base64_data: str) -> Optional[Image.Image]:
        """
        Decode base64-encoded image data and return the PIL Image object.

        Args:
            base64_data (str): Base64-encoded image data.

        Returns:
            Optional[Image.Image]: The PIL Image object, or None if decoding fails.
        """
        image_data = base64.b64decode(base64_data)
        image_stream = BytesIO(image_data)
        return Image.open(image_stream)
    
    @staticmethod
    def download_image(url: HttpUrl, save_path: str) -> Tuple[str, str]:
        """
        Download an image from the given URL and save it to the specified path.

        Args:
            url (HttpUrl): The URL of the image to be downloaded.
            save_path (str): The local path where the image should be saved.

        Returns:
            Tuple[str, str]: A tuple containing the original URL and the local path where the image is saved.
        """
        try:
            response = requests.get(url)
            if response.status_code == 200:
                image = Image.open(BytesIO(response.content))
                image.save(save_path)
                logging.info(f'Image downloaded successfully at {save_path}')
                return (url, save_path)
            
        except Exception as e:
            raise RuntimeError(f'Error downloading image: {e}')
import os

import sys

from IPython.display import Image



sys.path.append(os.path.abspath(os.path.join('../Utils')))
from image_analysis import extract_text_on_image, locate_image_on_image

locate_image_on_image(

    '../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/engagement_instruction.png','../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/_preview.png', prefix='eng_', visualize=True)



path= '../Data/Challenge_Data/Assets/2aab06df5509c6727fd35c7344796eb1/engagement_instruction.png'

extract_text_on_image(path)
import cv2
from typing import Tuple, List
import matplotlib.pyplot as plt

import cv2
import pytesseract
from langdetect import detect
from colormap import rgb2hex


def locate_image_on_image(locate_image: str, on_image: str, prefix: str = '', visualize: bool = False, color: Tuple[int, int, int] = (0, 0, 255)):
    try:

        image = cv2.imread(on_image)
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        template = cv2.imread(locate_image, 0)

        result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)
        _, _, _, max_loc = cv2.minMaxLoc(result)

        height, width = template.shape[:2]

        top_left = max_loc
        bottom_right = (top_left[0] + width, top_left[1] + height)

        if visualize:
            cv2.rectangle(image, top_left, bottom_right, color, 1)
            plt.figure(figsize=(10, 10))
            plt.axis('off')
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            plt.imshow(image)

        return {f'{prefix}top_left_pos': top_left, f'{prefix}bottom_right_pos': bottom_right}

    except cv2.error as err:
        print(err)



def extract_text_on_image(image_location: str) -> List[str]:
    """
    Extract text written on images using OCR (Optical Character Recognition).

    Args:
        image_location (str): The path to the image file.

    Returns:
        List[str]: A list of strings containing the extracted text from the image.
    """
    try:
                image = cv2.imread(image_location)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

                blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2)

                contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        text_contours = [cnt for cnt in contours
                        if cv2.contourArea(cnt) > 100 and
                           0.2 < cv2.contourArea(cnt) / (cv2.arcLength(cnt, True) ** 2) < 1.5]

                extracted_text = []
        for cnt in text_contours:
            x, y, w, h = cv2.boundingRect(cnt)
            cropped = gray[y:y + h, x:x + w]
            text = pytesseract.image_to_string(cropped, config='--psm 6')              if text.strip():
                language = detect(text)                  text = text.replace("\n", " ").replace("\x0c", "").replace(" ", " ").strip()
                extracted_text.append(f"{language}:{text}")  
        return extracted_text

    except Exception as e:
                if isinstance(e, pytesseract.TesseractNotFoundError):
            print("Tesseract not found. Please install it!")
        else:
            print(f"An unexpected error occurred: {e}")
        return []


def identify_color_composition(image,
                               tolerance: int = 12,
                               limit: int = 2,
                               visualize: bool = False) -> None:
    """Function that identifies the color composition of a
    given image path."""

    extracted_colors = extcolors.extract_from_path(
        image, tolerance=tolerance, limit=limit)

    identified_colors = color_to_df(extracted_colors)

    if not visualize:
        return identified_colors

    list_color = list(identified_colors['c_code'])
    list_percent = [int(i) for i in list(identified_colors['occurrence'])]

    text_c = [c + ' ' + str(round(p*100/sum(list_percent), 1)) + '%' for c, p in zip(list_color,
                                                                                     list_percent)]
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(100, 100), dpi=10)
    wedges, _ = ax[0].pie(list_percent,
                          labels=text_c,
                          labeldistance=1.05,
                          colors=list_color,
                          textprops={'fontsize': 60, 'color': 'black'}
                          )

    plt.setp(wedges, width=0.3)

        plt.setp(wedges, width=0.36)

    ax[0].set_aspect("equal")
    fig.set_facecolor('grey')

    ax[1].imshow(Image.open(image))

    plt.show()

    return identified_colors


def color_to_df(extracted_colors: tuple):
    """Converts RGB Color values from extcolors output to HEX Values."""

    colors_pre_list = str(extracted_colors).replace(
        '([(', '').replace(')],', '), (').split(', (')[0:-1]
    df_rgb = [i.split('), ')[0] + ')' for i in colors_pre_list]
    df_percent = [i.split('), ')[1].replace(')', '')
                  for i in colors_pre_list]

        df_rgb_values = [(int(i.split(", ")[0].replace("(", "")),
                      int(i.split(", ")[1]),
                      int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]

    df_color_up = [rgb2hex(int(i.split(", ")[0].replace("(", "")),
                           int(i.split(", ")[1]),
                           int(i.split(", ")[2].replace(")", ""))) for i in df_rgb]

    colors_df = pd.DataFrame(zip(df_color_up, df_rgb_values, df_percent),
                             columns=['c_code', 'rgb', 'occurrence'])

    return colors_df
import os

import shutil

import re



asset_folder = "/home/lillian/Documents/TenAcademy/week10/automatic_storyboard_for_Ad/data_preparation/Challenge_Data/Assets"

organized_data = "/home/lillian_assefa/week10/Automated-Storyboard-for-Digital-Ad/Notebook/Data_preparation/organized_data"



os.makedirs(organized_data, exist_ok=True)






for root, dirs, _ in os.walk(asset_folder):

    for folder_name in dirs:

        folder_path = os.path.join(root, folder_name)



        

        images = [file for file in os.listdir(folder_path) if file.endswith(('.jpg', '.png'))]

        videos = [file for file in os.listdir(folder_path) if file.endswith(('.mp4', '.avi'))]



       

        organized_sub_data = os.path.join(organized_data, folder_name)

        os.makedirs(organized_sub_data, exist_ok=True)



     

        images_folder = os.path.join(organized_sub_data, "images")

        os.makedirs(images_folder, exist_ok=True)

        videos_folder = os.path.join(organized_sub_data, "videos")

        os.makedirs(videos_folder, exist_ok=True)



       

        for image in images:

            source_image_path = os.path.join(folder_path, image)

            destination_image_path = os.path.join(images_folder, image)

            shutil.move(source_image_path, destination_image_path)



      

        for video in videos:

            source_video_path = os.path.join(folder_path, video)

            destination_video_path = os.path.join(videos_folder, video)

            shutil.move(source_video_path, destination_video_path)




naming_patterns = set()





for subfolder in os.listdir(organized_data):

    subfolder_path = os.path.join(organized_data, subfolder)



    if os.path.isdir(subfolder_path):

        
        images_folder = os.path.join(subfolder_path, "images")

        
        if os.path.isdir(images_folder):

        


          for image_file in os.listdir(images_folder):

              naming_patterns.add(image_file.lower())



print("Unique naming patterns:")

print("Total length of the parameters are",len(naming_patterns))

for pattern in naming_patterns:

    print(pattern)

    


!pip install numpy

!pip install tensorflow

!pip install -U scikit-learn
import numpy as np

import tensorflow as tf

import sys

from tensorflow.keras.applications import VGG16

from tensorflow.keras.preprocessing import image

from tensorflow.keras.models import Sequential

from tensorflow.keras.applications.vgg16 import preprocess_input

from tensorflow.keras.layers import Dense, Flatten

from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report

from tqdm import tqdm


try:

    from PIL import Image

except ImportError:

    print("PIL.Image import failed. Installing Pillow...")

    import subprocess

    subprocess.check_call([sys.executable, "-m", "pip", "install", "Pillow"])

    from PIL import Image






categories = [

    "Background Image", "Logo", "Call-To-Action (CTA) Button", "Icon",

    "Product Image", "Text Elements", "Infographic", "Banner",

    "Illustration", "Photograph", "Mascot", "Testimonial Quotes",

    "Social Proof", "Seal or Badge", "Graphs and Charts",

    "Decorative Elements", "Interactive Elements", "Animation Frames",

    "Coupon or Offer Code", "Legal Disclaimers or Terms",

    "Contact Information", "Map or Location Image", "QR Code"

]
model = VGG16(weights='imagenet', include_top=False)
!pip install Pillow 
!pip install tqdm
import Image

print(Image.__file__)










































    









import os

import numpy as np

from PIL import Image

from tensorflow.keras.applications.resnet50 import preprocess_input

from tqdm import tqdm



def extract_features(image_dir):

    features_list = []

    labels_list = []

    total_images = 0



    
    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                total_images += len([filename for filename in os.listdir(images_dir) 

                                     if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    progress = tqdm(total=total_images, desc='Processing images', unit='image')

    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                for filename in os.listdir(images_dir):

                    if filename.endswith(('.jpg', '.jpeg', '.png')):

                        img_path = os.path.join(images_dir, filename)

                        img = Image.open(img_path)

                        img = img.resize((224,224))

                        img = img.convert('RGB')

                        img = np.array(img)

                        img = np.expand_dims(img, axis=0)

                        img = preprocess_input(img)



                        features = model.predict(img)

                        features_list.append(features.flatten())



                        class_probs = model.predict(img)

                        predicted_class_index = np.argmax(class_probs[0])

                        if predicted_class_index < len(categories):

                            predicted_category = categories[predicted_class_index]

                            labels_list.append(predicted_category)

                        else:

                            print("Predicted class index out of range:", predicted_class_index)



                        progress.update(1)  


    progress.close()  
    return np.array(features_list), np.array(labels_list)




X, y = extract_features(organized_data)




for i, feature in enumerate(X):

    print(f"Predicted Category: {y[i]}, Features: {feature}")

import os

import numpy as np

import tensorflow as tf

from tensorflow.keras.applications.resnet50 import preprocess_input

from tqdm import tqdm



def load_and_preprocess_image(file_path):

    
    img = tf.io.read_file(file_path)

    img = tf.image.decode_image(img, channels=3)  
    img = tf.image.resize(img, [224, 224])  
    img = preprocess_input(img)  
    

    return img



def extract_features(image_dir):

    features_list = []

    labels_list = []

    total_images = 0



    
    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                total_images += len([filename for filename in os.listdir(images_dir) 

                                     if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    image_paths = []

    for subfolder in os.listdir(image_dir):

        subfolder_path = os.path.join(image_dir, subfolder)

        if os.path.isdir(subfolder_path):

            images_dir = os.path.join(subfolder_path, 'images')

            if os.path.isdir(images_dir):

                image_paths.extend([os.path.join(images_dir, filename) 

                                    for filename in os.listdir(images_dir) 

                                    if filename.endswith(('.jpg', '.jpeg', '.png'))])



    
    dataset = tf.data.Dataset.from_tensor_slices(image_paths)

    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)

    

    
    progress = tqdm(total=total_images, desc='Processing images', unit='image')

    for img in dataset:

        
        img = tf.expand_dims(img, axis=0)

        

        
        features = model.predict(img)

        features_list.append(features.flatten())



        
        class_probs = model.predict(img)

        predicted_class_index = np.argmax(class_probs[0])

        if predicted_class_index < len(categories):

            predicted_category = categories[predicted_class_index]

            labels_list.append(predicted_category)

        else:

            print("Predicted class index out of range:", predicted_class_index)



        progress.update(1)  


    progress.close()  
    return np.array(features_list), np.array(labels_list)




X, y = extract_features(organized_data)




for i, feature in enumerate(X):

    print(f"Predicted Category: {y[i]}, Features: {feature}")
!pip install pandas
import pandas as pd

import json



with open('concepts.json') as f:

    data = json.load(f)



df = pd.json_normalize(data)

                    







df
assetDf = df['asset_suggestions']

assetDf
data
import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns
df = pd.read_csv("../Data/Challenge_Data/performance_data.csv")
df.head()
df.describe()
df.info()

DataFrame = pd.DataFrame(df, columns=["game_id", "preview_link", "ER", "CTR"])




plt.figure(figsize=(8, 6))

plt.scatter(df["ER"], df["CTR"])

plt.xlabel("ER")

plt.ylabel("CTR")

plt.title("Scatter Plot of ER vs. CTR")

plt.grid(True)

plt.show()




df_selected = df[["ER", "CTR"]]




correlation_matrix = df_selected.corr()




print(correlation_matrix)

df_selected = df[["ER", "CTR"]]




correlation_matrix = df_selected.corr()




plt.figure(figsize=(8, 6))

plt.matshow(correlation_matrix, cmap="coolwarm")

plt.colorbar()

plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)

plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)

plt.title("Heatmap of Correlation Matrix")

plt.show()
import cv2
import numpy as np

def image_overlay(image1, image2, location):
    try:
        h, w = image1.shape[:2]
        h1, w1 = image2.shape[:2]
        x, y = location
        image1[y:y+h1, x:x+w1] = image2
        return image1;
    except cv2.error as err:
        print(err)
def change_image_color(image, origin_color, new_color):
    image[np.where((image==origin_color).all(axis=2))] = new_color
    return image
