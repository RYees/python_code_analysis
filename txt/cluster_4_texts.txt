pip install python-dotenv

import os

from dotenv import load_dotenv

from langchain.chat_models import ChatOpenAI



load_dotenv()

chat = ChatOpenAI(

   openai_api_key=os.getenv("OPENAI_API_KEY"),

   model='gpt-3.5-turbo'

)

from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string relativite by eninstein.")

]


res = chat(messages)

print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why is the shortes distance between points in a space is a curved line'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)


pip install datasets
import PyPDF2




with open('../data/10 Academy Cohort A - Weekly Challenge_ Week - 6.pdf', 'rb') as file:

    
    reader = PyPDF2.PdfReader(file)



    
    num_pages = len(reader.pages)



    
    text = ''



    
    for page_num in range(num_pages):

        
        page_text = reader.pages[page_num].extract_text()

        
        text += page_text




print(text)







import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time



index_name = 'llama-2-rag'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]

    
    texts = [x['chunk'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['chunk'],

         'source': x['source'],

         'title': x['title']} for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"



vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what safety measures were used in the development of llama 2?"

)



res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(

    content=augment_prompt(

        "what safety measures were used in the development of llama 2?"

    )

)



res = chat(messages + [prompt])

print(res.content)
import requests

from langchain.document_loaders import TextLoader

from langchain.text_splitter import CharacterTextSplitter 

from langchain.embeddings import OpenAIEmbeddings

import pinecone

from dotenv import load_dotenv, find_dotenv

from langchain.chat_models import ChatOpenAI

from langchain.prompts import ChatPromptTemplate

from langchain.schema.runnable import RunnablePassthrough

from langchain.schema.output_parser import StrOutputParser

def data_loader(file_path= '../prompts/Weekly_Challenge_Week_6.txt'):

    loader = TextLoader(file_path)

    documents = loader.load()



    
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    chunks = text_splitter.split_documents(documents)

    return chunks
data_loader()
chunks =  data_loader()
import os

 
pinecone.init(

    api_key=os.environ.get('7663f65c-a68e-4f31-a9a8-cf6d9e7bdac2'),

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time



index_name = '10-acadamy'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
import pandas as pd

data = pd.DataFrame(chunks)
from tqdm.auto import tqdm  
import pandas as pd




data = pd.DataFrame([{'page_content': doc.page_content, 'source': doc.metadata['source']} for doc in chunks])



batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i + batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{i}-{source}" for i, source in enumerate(batch['source'], start=i)]

    
    texts = batch['page_content'].tolist()

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [{'text': text, 'source': source} for text, source in zip(texts, batch['source'])]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
import time

import os

import pinecone



def create_retriever(chunks):



  
  load_dotenv(find_dotenv())



 
  pinecone.init(

    api_key=os.environ.get('7663f65c-a68e-4f31-a9a8-cf6d9e7bdac2'),

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

  )



 
  index_name = "my-index"

  pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

  while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)

 
  index = pinecone.Index(index_name)



 
  for chunk in chunks:

    embeddings = OpenAIEmbeddings().embed_query(chunk)

    index.upsert(ids=[chunk], vectors=embeddings)



 
  retriever = index.as_retriever()

  return retriever
chunks =  data_loader()

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)




template = """You are an assistant for question-answering tasks. 

Use the following pieces of retrieved context to answer the question. 

If you don't know the answer, just say that you don't know. 

Use two sentences maximum and keep the answer concise.

Question: {question} 

Context: {context} 

Answer:

"""



prompt = ChatPromptTemplate.from_template(template)




rag_chain = (

    {"context": retriever,  "question": RunnablePassthrough()} 

    | prompt 

    | llm

    | StrOutputParser() 

)
/* Basic Reset */
* {
    margin: 5;
    padding: 5;
    box-sizing: border-box;
}

/* Body Styles */
body {
    font-family: 'Arial', sans-serif;
    background-color:     color:     line-height: 1.5;
    padding: 30px;
}

/* Header Styles */
header {
    background:     color: white;
    padding: 10px 0;
    text-align: center;
}

header h1 {
    margin: 0;
}

/* Navigation Menu Styles */
nav ul {
    list-style: none;
    background-color:     text-align: center;
    padding: 10px;
    margin: 0;
}

nav ul li {
    display: inline;
}

nav ul li a {
    text-decoration: none;
    color: white;
    padding: 10px 15px;
    display: inline-block;
}

nav ul li a:hover {
    background-color: }

/* Main Content Styles */
.container {
    width: 90%;
    margin: auto;
    overflow: hidden;
}

/* Button Styles */
button {
    background-color:     color: white;
    border: none;
    padding: 10px 15px;
    cursor: pointer;
    border-radius: 4px;
}

button:hover {
    background-color: }

/* Form Styles */
form {
    background-color:     padding: 30px;
    border-radius: 5px;
    margin-left: 5%;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

form label {
    margin-bottom: 5px;
    display: block;
}

form input[type="text"],
form input[type="email"],
form input[type="password"] {
    width: 100%;
    padding: 8px;
    margin-bottom: 10px;
    border: 1px solid     border-radius: 4px;
}

form input[type="submit"] {
    background-color:     color: white;
    border: none;
    padding: 10px 15px;
    border-radius: 4px;
    cursor: pointer;
}

form input[type="submit"]:hover {
    background-color: }

/* Footer Styles */
footer {
    background:     color: white;
    text-align: center;
    padding: 10px 0;
    margin-top: 20px;
}

/* Utility Classes */
.clearfix::after {
    content: "";
    display: table;
    clear: both;
}

/* Responsive Design */
@media screen and (max-width: 768px) {
    .container,
    nav ul li,
    nav ul li a {
        width: 100%;
    }

    nav ul li {
        display: block;
    }
}
<!DOCTYPE html>
<html>
<head>
    <title>Prompt Result</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color:             margin: 0;
            padding: 0;
            color:         }

        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
        }

        header {
            background:             color: white;
            padding-top: 30px;
            min-height: 70px;
            border-bottom:         }

        header a {
            color:             text-decoration: none;
            text-transform: uppercase;
            font-size: 16px;
        }

        header ul {
            padding: 0;
            margin: 0;
            list-style: none;
            overflow: hidden;
        }

        header li {
            float: left;
            display: inline;
            padding: 0 20px 0 20px;
        }

        header             float: left;
        }

        header             margin: 0;
        }

        header nav {
            float: right;
            margin-top: 10px;
        }

        header .highlight, header .current a {
            color:             font-weight: bold;
        }

        header a:hover {
            color:             font-weight: bold;
        }

        .scrollable-paragraph {
            width: 60%; /* Narrow width */
            max-height: 400px; /* Maximum height */
            margin: 20px auto; /* Centering the paragraph */
            overflow-y: scroll; /* Enables vertical scrolling */
            padding: 15px;
            border: 1px solid             background-color: white;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div id="branding">
                <h1>Prompt Generator</h1>
            </div>
            <nav>
                <ul>
                    <li class="current"><a href="{% url 'generate' %}">Home</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="container">
        <h2>Generated Prompt</h2>
        <div class="scrollable-paragraph">
            {{ generated_prompt }}
        </div>
        <a href="{% url 'generate' %}">Generate another prompt</a>
    </div>
</body>
</html>
/* Basic Reset */
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

/* Body Styles */
body {
    font-family: 'Arial', sans-serif;
    background-color:     color:     line-height: 1.5;
    padding: 20px;
}

/* Header Styles */
header {
    background:     color: white;
    padding: 10px 0;
    text-align: center;
}

header h1 {
    margin: 0;
}

/* Navigation Menu Styles */
nav ul {
    list-style: none;
    background-color:     text-align: center;
    padding: 10px;
    margin: 0;
}

nav ul li {
    display: inline;
}

nav ul li a {
    text-decoration: none;
    color: white;
    padding: 10px 15px;
    display: inline-block;
}

nav ul li a:hover {
    background-color: }

/* Main Content Styles */
.container {
    width: 90%;
    margin: auto;
    overflow: hidden;
}

/* Button Styles */
button {
    background-color:     color: white;
    border: none;
    padding: 10px 15px;
    cursor: pointer;
    border-radius: 4px;
}

button:hover {
    background-color: }

/* Form Styles */
form {
    background-color:     padding: 15px;
    border-radius: 5px;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

form label {
    margin-bottom: 5px;
    display: block;
}

form input[type="text"],
form input[type="email"],
form input[type="password"] {
    width: 100%;
    padding: 8px;
    margin-bottom: 10px;
    border: 1px solid     border-radius: 4px;
}

form input[type="submit"] {
    background-color:     color: white;
    border: none;
    padding: 10px 15px;
    border-radius: 4px;
    cursor: pointer;
}

form input[type="submit"]:hover {
    background-color: }

/* Footer Styles */
footer {
    background:     color: white;
    text-align: center;
    padding: 10px 0;
    margin-top: 20px;
}

/* Utility Classes */
.clearfix::after {
    content: "";
    display: table;
    clear: both;
}

/* Responsive Design */
@media screen and (max-width: 768px) {
    .container,
    nav ul li,
    nav ul li a {
        width: 100%;
    }

    nav ul li {
        display: block;
    }
}
from django.shortcuts import HttpResponse, render

from prompt_app.utils.prompt_gen import get_prompt
from .utils.embed_text import embed_text
from .utils.similarity import cosine_similarity

from .models import Prompt, TextChunk



def generate_prompt_from_vector(text_chunk, user_question):

    prompt = text_chunk + " \n based on the above data, give an answer to \
            the following question. restrict yourself to the above data only. \
            if you can't get an answer based on the data, you can feel free to \
                say i don't know. here is the question. \n" + user_question +\
                 "DONOT add anything from yourself."
    return prompt



def generate_prompt(request):
    if request.method == 'POST':
        input_text = request.POST.get('input_text')

                embeded_question = embed_text([input_text])[0]

        highest_similarity = -1
        best_text_chunk = None

                for text_chunk in TextChunk.objects.all():
            similarity = cosine_similarity(embeded_question, text_chunk.embed)

            if similarity > highest_similarity:
                highest_similarity = similarity
                best_text_chunk = text_chunk.chunk

        if best_text_chunk is not None:
            generated_prompt = generate_prompt_from_vector(best_text_chunk, input_text)
                        return render(request, 'prompt_app/prompt_result.html', {'generated_prompt': generated_prompt})
        else:
            return HttpResponse("No similar documents found.")
    else:
        return render(request, 'prompt_app/generate_prompt.html')
import fitz  


def extract_text_from_pdf(pdf_path):
    document = fitz.open(pdf_path)
    
    full_text = ""

    for page_num in range(len(document)):
        page = document.load_page(page_num)

        text = page.get_text()

        full_text += text
    
    document.close()
    
    return full_text


def chunk_text(text, words_per_chunk, overlap_size):
        words = text.split()

    chunks = []
    i = 0

    while i < len(words):
                end = i + words_per_chunk

                chunk = ' '.join(words[i:end])
        chunks.append(chunk)

                i = end - overlap_size if end - overlap_size > i else end

    return chunks


if __name__ == "__main__":
    pdf_path = "data/week6_data.pdf"
    extracted_text = extract_text_from_pdf(pdf_path)
    
    chunks = chunk_text(extracted_text, 150, 5)
    print(len(chunks))
from collections.abc import Iterable
from django.db import models
from .utils.chunk_pdf import extract_text_from_pdf, chunk_text
from .utils.embed_text import embed_text
import logging

logger = logging.getLogger(__name__)



class Prompt(models.Model):
    input_text = models.TextField()
    length_of_line = models.IntegerField(null=True)
    generated_prompt = models.TextField(null=True)

    def __str__(self):
        return f"Prompt: {self.input_text[:50]}..."      

class Document(models.Model):
    pdf_file = models.FileField(upload_to='pdfs/')
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    def __str__(self):
        return f"PDF File uploaded on {self.created_at}"

    def save(self, *args, **kwargs):
        super().save(*args, **kwargs)

        if self.pdf_file:  
            text = extract_text_from_pdf(self.pdf_file.path)
            chunks = chunk_text(text)
            embeds = embed_text(chunks)

            for chunk, embed in zip(chunks, embeds):
                TextChunk.objects.create(document=self, chunk=chunk, embed=embed)
        else:
            logger.error("No PDF file to process.")

class TextChunk(models.Model):

    document = models.ForeignKey(Document, on_delete=models.CASCADE)
    chunk = models.TextField()
    embed = models.JSONField(blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
from django.contrib import admin


from django.contrib import admin
from .models import Document, TextChunk

class DocumentAdmin(admin.ModelAdmin):
    list_display = ('pdf_file', 'created_at', 'updated_at')
    search_fields = ('pdf_file',)
    readonly_fields = ('created_at', 'updated_at')

class TextChunkAdmin(admin.ModelAdmin):
    list_display = ('chunk', 'embed', 'updated_at')
  
admin.site.register(Document, DocumentAdmin)
admin.site.register(TextChunk, TextChunkAdmin)
import fitz  


def extract_text_from_pdf(pdf_path):
    document = fitz.open(pdf_path)
    
    full_text = ""

    for page_num in range(len(document)):
        page = document.load_page(page_num)

        text = page.get_text()

        full_text += text
    
    document.close()
    
    return full_text


def chunk_text(text, words_per_chunk=500, overlap_size=20):
        words = text.split()

    chunks = []
    i = 0

    while i < len(words):
                end = i + words_per_chunk

                chunk = ' '.join(words[i:end])
        chunks.append(chunk)

                i = end - overlap_size if end - overlap_size > i else end

    return chunks


if __name__ == "__main__":
    pdf_path = "data/week6_data.pdf"
    extracted_text = extract_text_from_pdf(pdf_path)
    
    chunks = chunk_text(extracted_text, 150, 5)
    print(len(chunks))
from langchain.embeddings.openai import OpenAIEmbeddings
from dotenv import load_dotenv

from . chunk_pdf import extract_text_from_pdf, chunk_text

load_dotenv()

def embed_text(chunks):
    embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")

    embeds = embed_model.embed_documents(chunks)

    return embeds


if __name__ == "__main__":

    pdf_path = "data/week6_data.pdf"
    
    text = extract_text_from_pdf(pdf_path)
    chunks = chunk_text(text, 150, 5)

    embeds = embed_text(chunks)
    print(embeds)
"""
Django settings for prompt_generator project.

Generated by 'django-admin startproject' using Django 5.0.1.

For more information on this file, see
https://docs.djangoproject.com/en/5.0/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/5.0/ref/settings/
"""

from pathlib import Path
import os

BASE_DIR = Path(__file__).resolve().parent.parent





SECRET_KEY = 'django-insecure-j1ut4aojbrfo13kkj
DEBUG = True

ALLOWED_HOSTS = []



INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'prompt_app',
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'prompt_generator.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'prompt_generator.wsgi.application'



DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
    }
}



AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]



LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_TZ = True



STATIC_URL = '/static/'
STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')







DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'




iLOGGING = {
    'version': 1,      'disable_existing_loggers': False,
    'handlers': {
        'file': {
            'level': 'DEBUG',
            'class': 'logging.FileHandler',
            'filename': '/path/to/your/logfile.log',
        },
            },
    'loggers': {
        'django': {
            'handlers': ['file'],
            'level': 'DEBUG',
            'propagate': True,
        },
            },
}
!pip install -qU \

    langchain==0.0.355 \

    openai==1.6.1 \

    datasets==2.10.1 \

    pinecone-client==3.0.0 \

    tiktoken==0.5.2
import os

from langchain_openai import ChatOpenAI



os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or "YOUR_API_KEY"



chat = ChatOpenAI(

    openai_api_key=os.environ["OPENAI_API_KEY"],

    model='gpt-3.5-turbo'

)

























from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]


res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)

















from datasets import load_dataset



dataset = load_dataset("jamescalam/llama-2-arxiv-papers-chunked")


print(dataset)
dataset[0]






from pinecone import Pinecone




api_key = os.getenv("PINECONE_API_KEY")


pc = Pinecone(api_key=api_key)
from pinecone import ServerlessSpec



spec = ServerlessSpec(

    cloud="aws", region="us-west-2"

)
import time



index_name = 'llama-2-rag'

existing_indexes = [

    index_info["name"] for index_info in pc.list_indexes()

]




if index_name not in existing_indexes:

    try:

        
        pc.create_index(

            index_name,

            dimension=1536,  
            metric='dotproduct',

            spec=spec

        )

        
        while not pc.describe_index(index_name).status['ready']:

            time.sleep(1)

    except Exception as e:

        print(f"Failed to create index: {e}")

        



index = pc.Index(index_name)

time.sleep(1)


index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]

    
    texts = [x['chunk'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['chunk'],

         'source': x['source'],

         'title': x['title']} for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"



vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what safety measures were used in the development of llama 2?"

)



res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(

    content=augment_prompt(

        "what safety measures were used in the development of llama 2?"

    )

)



res = chat(messages + [prompt])

print(res.content)
pc.delete_index(index_name)
!pip install -qU openai==0.27.7



















prompt = """Answer the question based on the context below. If the

question cannot be answered using the information provided answer

with "I don't know".



Context: Large Language Models (LLMs) are the latest models used in NLP.

Their superior performance over smaller models has made them incredibly

useful for developers building NLP enabled applications. These models

can be accessed via Hugging Face's `transformers` library, via OpenAI

using the `openai` library, and via Cohere using the `cohere` library.



Question: Which libraries and model providers offer LLMs?



Answer: """














import os

import openai




openai.api_key = os.getenv("OPENAI_API_KEY") or "OPENAI_API_KEY"



openai.Engine.list()  
res = openai.Completion.create(

    engine='gpt-3.5-turbo-instruct',

    prompt=prompt,

    max_tokens=256

)



print(res['choices'][0]['text'].strip())
prompt = """Answer the question based on the context below. If the

question cannot be answered using the information provided answer

with "I don't know".



Context: Libraries are places full of books.



Question: Which libraries and model providers offer LLMs?



Answer: """



res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    max_tokens=256

)



print(res['choices'][0]['text'].strip())
















prompt = """The below is a conversation with a funny chatbot. The

chatbot's responses are amusing and entertaining.



Chatbot: Hi there! I'm a chatbot.

User: Hi, what are you doing today?

Chatbot: """



res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    max_tokens=256,

    temperature=0.0  
)



print(res['choices'][0]['text'].strip())
prompt = """The below is a conversation with a funny chatbot. The

chatbot's responses are amusing and entertaining.



Chatbot: Hi there! I'm a chatbot.

User: Hi, what are you doing today?

Chatbot: """



res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    max_tokens=512,

    temperature=1.0

)



print(res['choices'][0]['text'].strip())




prompt = """The following is a conversation with an AI assistant.

The assistant is typically sarcastic and witty, producing creative 

and funny responses to the users questions. 



User: What is the meaning of life?

AI: """



res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    max_tokens=256,

    temperature=1.0

)



print(res['choices'][0]['text'].strip())
prompt = """The following are exerpts from conversations with an AI assistant.

The assistant is typically sarcastic and witty, producing creative 

and funny responses to the users questions. Here are some examples: 



User: How are you?

AI: I can't complain but sometimes I still do.



User: What time is it?

AI: It's time to get a watch.



User: What is the meaning of life?

AI: """



res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    max_tokens=256,

    temperature=1.0

)



print(res['choices'][0]['text'].strip())






contexts = [

    (

        "Large Language Models (LLMs) are the latest models used in NLP. " +

        "Their superior performance over smaller models has made them incredibly " +

        "useful for developers building NLP enabled applications. These models " +

        "can be accessed via Hugging Face's `transformers` library, via OpenAI " +

        "using the `openai` library, and via Cohere using the `cohere` library."

    ),

    (

        "To use OpenAI's GPT-3 model for completion (generation) tasks, you " +

        "first need to get an API key from " +

        "'https://beta.openai.com/account/api-keys'."

    ),

    (

        "OpenAI's API is accessible via Python using the `openai` library. " +

        "After installing the library with pip you can use it as follows: \n" +

        "```import openai\nopenai.api_key = 'YOUR_API_KEY'\nprompt = \n" +

        "'<YOUR PROMPT>'\nres = openai.Completion.create(engine='text-davinci" +

        "-003', prompt=prompt, max_tokens=100)\nprint(res)"

    ),

    (

        "The OpenAI endpoint is available for completion tasks via the " +

        "LangChain library. To use it, first install the library with " +

        "`pip install langchain openai`. Then, import the library and " +

        "initialize the model as follows: \n" +

        "```from langchain.llms import OpenAI\nopenai = OpenAI(" +

        "model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n" +

        "prompt = 'YOUR_PROMPT'\nprint(openai(prompt))```"

    )

]
context_str = '\n\n


print(f"""Answer the question based on the contexts below. If the

question cannot be answered using the information provided answer

with "I don't know".






Contexts:

{context_str}






Question: Give me two examples of how to use OpenAI's GPT-3 model

using Python from start to finish



Answer: """)
prompt = f"""Answer the question based on the contexts below. If the

question cannot be answered using the information provided answer

with "I don't know".






Contexts:

{context_str}






Question: Give me two examples of how to use OpenAI's GPT-3 model

using Python from start to finish



Answer: """



res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    max_tokens=256,

    temperature=0.0

)



print(res['choices'][0]['text'].strip())
prompt = f"""Answer the question based on the contexts below. If the

question cannot be answered using the information provided answer

with "I don't know".



Question: Give me two examples of how to use OpenAI's GPT-3 model

using Python from start to finish



Answer: """



res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    max_tokens=256,

    temperature=0.0

)



print(res['choices'][0]['text'].strip())










!pip install -qU tiktoken==0.4.0
import tiktoken



prompt = f"""Answer the question based on the contexts below. If the

question cannot be answered using the information provided answer

with "I don't know".






Contexts:

{'





Question: Give me two examples of how to use OpenAI's GPT-3 model

using Python from start to finish



Answer: """



encoder_name = 'p50k_base'

tokenizer = tiktoken.get_encoding(encoder_name)



len(tokenizer.encode(prompt))














res = openai.Completion.create(

    engine='text-davinci-003',

    prompt=prompt,

    temperature=0.0,

    max_tokens=3685

)



print(res['choices'][0]['text'].strip())


try:

    res = openai.Completion.create(

        engine='text-davinci-003',

        prompt=prompt,

        temperature=0.0,

        max_tokens=3686

    )

except openai.InvalidRequestError as e:

    print(e)
import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

import numpy as np

import re

import warnings

import json



warnings.filterwarnings('ignore')
import sys

sys.path.append("../scripts/")

from data_cleaner import DataCleaner

from util import Util
util = Util()

cleaner = DataCleaner()
repo="https://github.com/Nathnael12/prompt-engineering.git"

news_df=util.read_from_dvc("data/news.csv",repo,"news-v0",low_memory=False)



print(news_df.shape)

news_df.head()
news_df.columns
news_df.describe()



temp = news_df['Analyst_Average_Score']

news_df.drop('Analyst_Average_Score',axis=1,inplace=True)

news_df['Analyst_Average_Score']=temp

news_df.head()
news_df.plot(kind='bar')
cleaned_df=cleaner.clean_links(news_df,['Body'])

cleaned_df=cleaner.clean_symbols(cleaned_df,['Body','Description','Title'])

cleaned_df=cleaner.convert_to_datetime(cleaned_df,['timestamp'])

cleaned_df.head(5)

job_df=pd.read_json("../data/job_description_train.json")

job_df_train=job_df.copy()
job_df_train.head()
job_df_train.isna().sum()
import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

import numpy as np

import re

import warnings

warnings.simplefilter('ignore')
import sys

sys.path.append("../scripts/")

from data_cleaner import DataCleaner

from util import Util

from pridict import Predict

from preprocessor import Processor
util = Util()

cleaner = DataCleaner()

predictor = Predict()

processor = Processor()
repo="https://github.com/Nathnael12/prompt-engineering.git"

test=util.read_from_dvc("data/test_news.csv",repo,"test-news-v3",low_memory=False)

train=util.read_from_dvc("data/trainer_news.csv",repo,"train-news-v3",low_memory=False)






















































unique_test=test.sample()

unique_train=train.sample()



predictor.predict(unique_train,unique_test) 
predictor.predict(unique_train,unique_test,model="a0b276d4-adf8-453e-983f-31b8761e8521-ft") 
prompt=predictor.predict(unique_train,unique_test, model="c6af2dfd-16ae-4503-9693-6e50dae3861a-ft" ) 
prompt=predictor.predict(unique_train,unique_test, model="acfdd84f-2b81-4ee2-92d5-22ca5ee8f4f0-ft" ) 
prompt=predictor.predict(unique_train,unique_test, model="e74ec85a-8e14-4913-83d5-02fe80ac7c4f-ft" ) 


print(f"Correct Value: {unique_test.iloc[0,-1]}")

print(prompt)
job_train_df=pd.read_json("../data/job_description_train.json")

job_test_df=pd.read_json("../data/job_description_test.json")



processed_job_train=job_train_df.copy()

processed_job_test=job_test_df.copy()



processed_job_test=processor.prepare_job_description_text(job_test_df)

processed_job_train=processor.prepare_job_description_text(job_train_df)



trainer=processed_job_train.sample(2)

test=processed_job_test.sample(1)



prompt_job=predictor.extract_entities(trainer,test)

print()

prompt_job=predictor.extract_entities(trainer,test,'a724ac98-2abc-47b7-96b3-a77c3a5eb0f8-ft')
import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

import numpy as np

import re

import warnings



from data_describe.text.text_preprocessing import *

from sklearn.datasets import fetch_20newsgroups



warnings.simplefilter('ignore')
import sys

sys.path.append("../scripts/")

from data_cleaner import DataCleaner

from util import Util

from preprocessor import Processor
util = Util()

cleaner = DataCleaner()

processor = Processor()
repo="https://github.com/Nathnael12/prompt-engineering.git"

news_df=util.read_from_dvc("data/news.csv",repo,"news-v0",low_memory=False)



news_df=news_df.sample(frac=1)



train_news=news_df.head(8)

test_news=news_df.tail(2)
full_processed_df=processor.prepare_text(train_news.copy())

description_processed=processor.prepare_text(train_news.copy(),columns=["Description"])[["Description","Analyst_Average_Score"]]

title_processed=processor.prepare_text(train_news.copy(),columns=["Title"])[["Title","Analyst_Average_Score"]]

body_processed=processor.prepare_text(train_news.copy(),columns=["Body"])[["Body","Analyst_Average_Score"]]

unprocessed=train_news



processed_test=processor.prepare_text(test_news)
full_processed_df['Description']=full_processed_df['Title'] + ", " + full_processed_df['Description'] + ", " + full_processed_df["Body"]

full_processed_df.drop(['Domain','Title','Body','Link','timestamp','Analyst_Rank','Reference_Final_Score'],axis=1,inplace=True)



title_processed.rename(columns={'Title':'Description'},inplace=True)



body_processed.rename(columns={'Body':'Description'},inplace=True)



unprocessed['Description']=unprocessed['Title'] + ", " + unprocessed['Description'] + ", " + unprocessed["Body"]

unprocessed.drop(['Domain','Title','Body','Link','timestamp','Analyst_Rank','Reference_Final_Score'],axis=1,inplace=True)



processed_test['Description']=processed_test['Title'] + ", " + processed_test['Description'] + ", " + processed_test["Body"]

processed_test.drop(['Domain','Title','Body','Link','timestamp','Analyst_Rank','Reference_Final_Score'],axis=1,inplace=True)





frames = [full_processed_df, description_processed, title_processed, body_processed, unprocessed]



full_promp_trainer = pd.concat(frames)

full_promp_trainer=full_promp_trainer.reset_index().drop(['index'],axis=1)

full_promp_trainer.rename(columns={"Analyst_Average_Score":"Analyst Average Score"},inplace=True)



full_promp_trainer.head()


processor.prepare_tuner(full_promp_trainer)




job_df=pd.read_json("../data/job_description_train.json")

test_df=pd.read_json("../data/job_description_test.json")
job_df_train=job_df.copy()

job_df_test = test_df.copy()



processed_description=processor.prepare_job_description_text(job_df_train)

processed_test_description=processor.prepare_job_description_text(job_df_test)



display(processed_description.head())

display(processed_test_description.head())
processed_test_description.shape
processed_description.shape
job_frames = [processed_description, processed_test_description.head(10)]



job_tuner_df = pd.concat(job_frames)
job_tuner_df=job_tuner_df.reset_index().drop(["index"],axis=1)
processor.prepare_job_description_tuner(job_tuner_df)
import os

from langchain.chat_models import ChatOpenAI



os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or "YOUR_API_KEY"



chat = ChatOpenAI(

    openai_api_key=os.environ["OPENAI_API_KEY"],

    model='gpt-3.5-turbo'

)

























from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]


res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)

















from langchain.document_loaders import DirectoryLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.document_loaders import PyPDFLoader



directory_loader = DirectoryLoader("docs", {

            '10_Academy_challenge_doc.pdf': lambda path: PyPDFLoader(path),

        })

raw_docs = directory_loader.load()



text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)



docs = text_splitter.split_documents(raw_docs)

print('split docs', docs)



print('creating vector store...')
dataset[0]






from pinecone import Pinecone




api_key = os.getenv("PINECONE_API_KEY")


pc = Pinecone(api_key=api_key)
from pinecone import PodSpec

import time





index_name = '10-academy-challenge'

existing_indexes = [

    index_info["name"] for index_info in pc.list_indexes()

]




if index_name not in existing_indexes:

    try:

        
        pc.create_index(

            index_name,

            dimension=1536,  
            metric='cosine',

            spec=PodSpec(

                environment="gcp-starter"

            )

        )

        
        while not pc.describe_index(index_name).status['ready']:

            time.sleep(1)

    except Exception as e:

        print(f"Failed to create index: {e}")

        



index = pc.Index(index_name)

time.sleep(1)


index.describe_index_stats()
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]

    
    texts = [x['chunk'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['chunk'],

         'source': x['source'],

         'title': x['title']} for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
query = "What is this challenge week about?"



vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what are this week challenge?"

)



res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(

    content=augment_prompt(

        "what is the concept of task 3?"

    )

)



res = chat(messages + [prompt])

print(res.content)
Automatic prompt generator
import os
import json
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utility.env_manager import get_env_manager
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
from typing import List, Dict
from dotenv import load_dotenv
from prompts.context import KnowledgeAssistant

load_dotenv()
env_manager = get_env_manager()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def get_completion(
    messages: List[Dict[str, str]],
    model: str = env_manager['vectordb_keys']['VECTORDB_MODEL'],
    max_tokens=500,
    temperature=0,
    stop=None,
    seed=123,
    tools=None,
    logprobs=None,
    top_logprobs=None,
) -> str:
    """Return the completion of the prompt.
    @parameter messages: list of dictionaries with keys 'role' and 'content'.
    @parameter model: the model to use for completion. Defaults to 'davinci'.
    @parameter max_tokens: max tokens to use for each prompt completion.
    @parameter temperature: the higher the temperature, the crazier the text
    @parameter stop: token at which text generation is stopped
    @parameter seed: random seed for text generation
    @parameter tools: list of tools to use for post-processing the output.
    @parameter logprobs: whether to return log probabilities of the output tokens or not.
    @returns completion: the completion of the prompt.
    """

    params = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion


def file_reader(path: str) -> str:
    script_dir = os.path.dirname(os.path.realpath(__file__))
    base_dir = os.path.dirname(script_dir)
    file_path = os.path.join(base_dir, path)
    with open(file_path, 'r', encoding='utf-8') as f:
        system_message = f.read()
    return system_message
            
            

def generate_test_data(prompt: str, context: str, num_test_output: str) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
        assistant = KnowledgeAssistant()
    query = "I want to know about this week tasks."
    augmented_prompt = assistant.augment_prompt(query)

    API_RESPONSE = get_completion(
        [
            {
                "role": "user", 
                "content": prompt.replace("{context}", augmented_prompt).replace("{num_test_output}", num_test_output)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = API_RESPONSE.choices[0].message.content
    return system_msg


def main(num_test_output: str):
    context_message = file_reader("prompts/context.txt")
    prompt_message = file_reader("prompts/data-generation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    test_data = generate_test_data(prompt, context, num_test_output)
    def save_json(test_data) -> None:
               script_dir = os.path.dirname(os.path.realpath(__file__))

                base_dir = os.path.dirname(script_dir)

                path = "test-dataset/test-data.json"

                file_path = os.path.join(base_dir, path)
        json_object = json.loads(test_data)
        with open(file_path, 'w') as json_file:
            json.dump(json_object, json_file, indent=4)
            
        print(f"JSON data has been saved to {file_path}")

    save_json(test_data)

    print("===========")
    print("Test Data")
    print("===========")
    print(test_data)


if __name__ == "__main__":
    main("8")
from dotenv import load_dotenv
import os
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from pinecone import PodSpec
from pinecone import Pinecone as ppincone
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain_community.vectorstores import Pinecone
import json

class KnowledgeAssistant:

    def __init__(self):
        load_dotenv()
        os.environ["PINECONE_API_KEY"] = os.getenv("PINECONE_API_KEY")
        os.environ["PINECONE_ENV"] = os.getenv("PINECONE_ENV")
        os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

        self.pc = ppincone(
            api_key=os.getenv("PINECONE_API_KEY"),
            environment=os.getenv("PINECONE_ENV")
        )
        self.embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
        self.index = self.pc.Index('canopy--document-uploader')

        self.text_field = "text"
        self.vectorstore = Pinecone(self.index, self.embed_model, self.text_field)
        self.chat = ChatOpenAI(openai_api_key=os.environ["OPENAI_API_KEY"], model='gpt-3.5-turbo')

    def augment_prompt(self, query):
        query1 = json.dumps(str(query))
        results = self.vectorstore.similarity_search(query1, k=3)
        source_knowledge = "\n".join([x.page_content for x in results])
        augmented_prompt = f"""Using the contexts below, answer the query.

        Contexts:
        {source_knowledge}

        Query: {query1}"""
        return augmented_prompt

    def run_chat(self, query):
        messages = [
            SystemMessage(content="You are a helpful assistant."),
            HumanMessage(content=f"Hi AI, {query}"),
            AIMessage(content="I'm great thank you. How can I help you?"),
        ]

        augmented_prompt = self.augment_prompt(query)

        prompt = HumanMessage(content=augmented_prompt)
        messages.append(prompt)

        res = self.chat(messages)
        return res.content


if __name__ == "__main__":
    assistant = KnowledgeAssistant()
    query = "Who are the tutors in this week's challenge?"

        augmented_prompt = assistant.augment_prompt(query)

        print(augmented_prompt)
import os
import sys
from dotenv import load_dotenv
load_dotenv(".env")


class OPENAI_KEYS:
    def __init__(self):
        self.OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '') or None


class VECTORDB_KEYS:
    def __init__(self):
        self.VECTORDB_API_KEY = os.environ.get('VECTORDB_API_KEY', '') or None
        self.VECTORDB_URL = os.environ.get('VECTORDB_URL', '') or None
        self.VECTORDB_MODEL = os.environ.get('VECTORDB_MODEL', '') or None


def _get_openai_keys() -> OPENAI_KEYS:
    return OPENAI_KEYS()


def _get_vectordb_keys() -> VECTORDB_KEYS:
    return VECTORDB_KEYS()


def get_env_manager() -> dict:
    openai_keys = _get_openai_keys().__dict__
    vectordb_keys = _get_vectordb_keys().__dict__

    return {
        'openai_keys': openai_keys,
        'vectordb_keys': vectordb_keys,
    }
import os
import json
import sys
from openai import OpenAI
from math import exp
import numpy as np
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utility.env_manager import get_env_manager
from evaluation._data_generation import get_completion
from evaluation._data_generation import file_reader
from prompts.context import KnowledgeAssistant

env_manager = get_env_manager()
client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])


def evaluate(prompt: str, user_message: str, context: str, use_test_data: bool = True) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """

    num_test_output = str(10)
    API_RESPONSE = get_completion(
        [
            {
                "role": "system", 
                "content": prompt.replace("{Context}", augmented_prompt).replace("{Question}", user_message)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = str(API_RESPONSE.choices[0].message.content)

    for i, logprob in enumerate(API_RESPONSE.choices[0].logprobs.content[0].top_logprobs, start=1):
        output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100,2)}%\n'
        print(output)
        if system_msg == 'true' and np.round(np.exp(logprob.logprob)*100,2) >= 65.00:
            classification = 'true'
        elif system_msg == 'false' and np.round(np.exp(logprob.logprob)*100,2) >= 65.00:
            classification = 'false'
        else:
            classification = 'false'
    return classification

if __name__ == "__main__":
    assistant = KnowledgeAssistant()
    query = "I want to know about this week tasks."
    print(query)
    augmented_prompt = assistant.augment_prompt(query)
    context_message = augmented_prompt
    prompt_message = file_reader("prompts/generic-evaluation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    
    user_message = str(input("question: "))
    
    print(evaluate(prompt, user_message, context))
import os
import json
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utility.env_manager import get_env_manager
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
from typing import List, Dict
from dotenv import load_dotenv
from prompts.context import KnowledgeAssistant

load_dotenv()
env_manager = get_env_manager()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def get_completion(
    messages: List[Dict[str, str]],
    model: str = env_manager['vectordb_keys']['VECTORDB_MODEL'],
    max_tokens=500,
    temperature=0,
    stop=None,
    seed=123,
    tools=None,
    logprobs=None,
    top_logprobs=None,
) -> str:
    """Return the completion of the prompt.
    @parameter messages: list of dictionaries with keys 'role' and 'content'.
    @parameter model: the model to use for completion. Defaults to 'davinci'.
    @parameter max_tokens: max tokens to use for each prompt completion.
    @parameter temperature: the higher the temperature, the crazier the text
    @parameter stop: token at which text generation is stopped
    @parameter seed: random seed for text generation
    @parameter tools: list of tools to use for post-processing the output.
    @parameter logprobs: whether to return log probabilities of the output tokens or not.
    @returns completion: the completion of the prompt.
    """

    params = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion


def file_reader(path: str) -> str:
    script_dir = os.path.dirname(os.path.realpath(__file__))
    base_dir = os.path.dirname(script_dir)
    file_path = os.path.join(base_dir, path)
    with open(file_path, 'r', encoding='utf-8') as f:
        system_message = f.read()
    return system_message
            
            

def generate_test_data(prompt: str, context: str, num_test_output: str, objective) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    API_RESPONSE = get_completion(
        [
            {
                "role": "user", 
                "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = API_RESPONSE.choices[0].message.content
    return system_msg


def main(num_test_output: str, objective):
        assistant = KnowledgeAssistant()
    query = '"' + str(objective) + '"'
    print(query)
    augmented_prompt = assistant.augment_prompt(query)
    context_message = augmented_prompt
    prompt_message = file_reader("prompts/prompt-generation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    test_data = generate_test_data(prompt, context, num_test_output, objective)
    def save_json(test_data) -> None:
               script_dir = os.path.dirname(os.path.realpath(__file__))

                base_dir = os.path.dirname(script_dir)

                path = "prompt-dataset/prompt-data.json"

                file_path = os.path.join(base_dir, path)
        json_object = json.loads(test_data)
        with open(file_path, 'w') as json_file:
            json.dump(json_object, json_file, indent=4)
            
        print(f"JSON data has been saved to {file_path}")

    save_json(test_data)

    print("===========")
    print("Test Data")
    print("===========")
    print(test_data)


if __name__ == "__main__":
    main("8","I want to know about this weeks challenge")
import React, { useState } from "react";
import { useNavigate } from "react-router-dom";

import { preview } from "../assets";
import { getRandomPrompt } from "../utils";
import { FormFields, Loader } from "../components";

const CreatePost = () => {
    const navigate = useNavigate();

    const [form, setForm] = useState({
        objective: "",
        output: "",
        scenario: "",
    });

    const [generatingprompt, setGeneratingprompt] = useState(false);
    const [loading, setLoading] = useState(false);
    const [result, setResult] = useState(""); // Add this line
    const [accuracy, setAccuracy] = useState(null); // Add this line


    const handleChange = (e) =>
        setForm({ ...form, [e.target.name]: e.target.value });

    const handleSurpriseMe = () => {
        const randomPrompt = getRandomPrompt(form.scenario);
        setForm({ ...form, scenario: randomPrompt });
    };

    const generatePrompt = async () => {
        if (form.scenario) {
            try {
                setGeneratingprompt(true);
                const response = await fetch("http://192.168.137.236:8000/generate", {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({
                        num_test_output: "8",
                        objective: form.objective,
                        output: form.output,
                    }),
                });
    
                const data = await response.json();
                setResult(data.prompt);
                setAccuracy(data.score);
            } catch (err) {
                console.log(err);
            } finally {
                setGeneratingprompt(false);
            }
        } else {
            alert("Please provide a proper prompt");
        }
    };
    
    


    const handleSubmit = async (e) => {
        e.preventDefault();
    
        if (form.scenario && form.preview) {
            setLoading(true);
            try {
                const response = await fetch(
                    "http://192.168.137.236:8000/generate",
                    {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                        },
                        body: JSON.stringify({ ...form}),
                    }
                );
    
                if (response.ok) {
                    const responseData = await response.json();
                    // Assuming the response has a property named "result"
                    const result = responseData.result;
    
                    // Do something with the result
                    console.log(result);
                    // You can also update your UI or state with the received result
                } else {
                    console.log("Failed to get a successful response from the server");
                }
            } catch (err) {
                console.error(err);
            } finally {
                setLoading(false);
            }
        } else {
            alert("Please generate a prompt with proper details");
        }
    };
    
    return (
        <section className="bg-hero min-h-[calc(100vh)]">
            <div className="max-w-7xl bg-hero sm:p-8 px-4 mt-16 m-auto">
                <div>
                    <h1 className="font-extrabold text-text text-[42px]">Insert your preferences</h1>
                </div>

                <form className="mt-2 form" onSubmit={handleSubmit}>
                    <div className="flex my-auto flex-col gap-5">
                        <FormFields
                            labelName="The objective"
                            type="text"
                            name="objective"
                            placeholder="Enter Your Objective"
                            value={form.objective}
                            handleChange={handleChange}
                        />
                        <FormFields
                            labelName="The output"
                            type="text"
                            name="output"
                            placeholder="Enter the desired output"
                            value={form.output}
                            handleChange={handleChange}
                        />
                        <FormFields
                            labelName="Scenario"
                            type="text"
                            name="scenario"
                            placeholder="Enter a prompt..."
                            value={form.scenario}
                            handleChange={handleChange}
                            isSurpriseMe
                            handleSurpriseMe={handleSurpriseMe}
                        />

                        <div className="mt-2 flex flex-col">
                            <button
                                type="button"
                                onClick={generatePrompt}
                                className="text-black bg-accent font-bold rounded-md text-sm w-full sm:w-auto px-5 py-2.5 text-center"
                            >
                                {generatingprompt ? "Generating..." : "Generate Prompt"}
                            </button>
                            <button
                                type="submit"
                                className="mt-3 text-white bg-brand text-black font-bold rounded-md text-sm sm:w-auto px-5 py-2.5 text-center w-full"
                            >
                                {loading ? "Sharing..." : "Use this directly on chatbot"}
                            </button>
                        </div>
                    </div>
                    <div className="relative form_photo md:m-auto border bg-darkgrey border-darkgrey text-whtie text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 w-64 p-3 h-64 flex flex-col items-center justify-center">
                        {form.preview ? (
                            <span className="text-white mb-2">
                                {result ? result : (form.results || "Generated prompt will be shown here")}
                            </span>
                        ) : (
                            <div className="text-white text-center">
                                <p className="mb-2">{result ? result : (form.results || "Generated prompt will be shown here")}</p>
                                {accuracy && <p className="text-white mt-2">Score: {accuracy}</p>}
                            </div>
                        )}
                    </div>
                </form>
            </div>
        </section>
    );
};

export default CreatePost;
import os
import json
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utility.env_manager import get_env_manager
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
from typing import List, Dict
from dotenv import load_dotenv
from prompts.context import KnowledgeAssistant

load_dotenv()
env_manager = get_env_manager()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def get_completion(
    messages: List[Dict[str, str]],
    model: str = env_manager['vectordb_keys']['VECTORDB_MODEL'],
    max_tokens=500,
    temperature=0,
    stop=None,
    seed=123,
    tools=None,
    logprobs=None,
    top_logprobs=None,
) -> str:
    """Return the completion of the prompt.
    @parameter messages: list of dictionaries with keys 'role' and 'content'.
    @parameter model: the model to use for completion. Defaults to 'davinci'.
    @parameter max_tokens: max tokens to use for each prompt completion.
    @parameter temperature: the higher the temperature, the crazier the text
    @parameter stop: token at which text generation is stopped
    @parameter seed: random seed for text generation
    @parameter tools: list of tools to use for post-processing the output.
    @parameter logprobs: whether to return log probabilities of the output tokens or not.
    @returns completion: the completion of the prompt.
    """

    params = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion


def file_reader(path: str) -> str:
    script_dir = os.path.dirname(os.path.realpath(__file__))
    base_dir = os.path.dirname(script_dir)
    file_path = os.path.join(base_dir, path)
    with open(file_path, 'r', encoding='utf-8') as f:
        system_message = f.read()
    return system_message
            
            

def generate_test_data(prompt: str, context: str, num_test_output: str, objective) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """

    API_RESPONSE = get_completion(
        [
            {
                "role": "user", 
                "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = API_RESPONSE.choices[0].message.content
    return system_msg


def main(num_test_output: str, objective):
    assistant = KnowledgeAssistant()
    query = '"' + str(objective) + '"'
    print(query)
    augmented_prompt = assistant.augment_prompt(query)
    context_message = augmented_prompt
    prompt_message = file_reader("prompts/data-generation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    test_data = generate_test_data(prompt, context, num_test_output, objective)
    def save_json(test_data) -> None:
               script_dir = os.path.dirname(os.path.realpath(__file__))

                base_dir = os.path.dirname(script_dir)

                path = "test-dataset/test-data.json"

                file_path = os.path.join(base_dir, path)
        json_object = json.loads(test_data)
        with open(file_path, 'w') as json_file:
            json.dump(json_object, json_file, indent=4)
            
        print(f"JSON data has been saved to {file_path}")

    save_json(test_data)

    print("===========")
    print("Test Data")
    print("===========")
    print(test_data)


if __name__ == "__main__":
    main("4","I want to know when the interim submission deadline is")
import os
import json
import sys
from openai import OpenAI
from math import exp
import numpy as np
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utility.env_manager import get_env_manager
from evaluation._data_generation import get_completion
from evaluation._data_generation import file_reader
from prompts.context import KnowledgeAssistant

env_manager = get_env_manager()
client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])


class Evaluator:
    def __init__(self, env_manager, client):
        self.env_manager = env_manager
        self.client = client
        self.assistant = KnowledgeAssistant()

    def evaluate(self, prompt: str, user_message: str, context: str, use_test_data: bool = False) -> str:
        API_RESPONSE = get_completion(
            [
                {
                    "role": "system", 
                    "content": prompt.replace("{Context}", context).replace("{Question}", user_message)
                }
            ],
            model=self.env_manager['vectordb_keys']['VECTORDB_MODEL'],
            logprobs=True,
            top_logprobs=1,
        )

        system_msg = str(API_RESPONSE.choices[0].message.content)
        
        for i, logprob in enumerate(API_RESPONSE.choices[0].logprobs.content[0].top_logprobs, start=1):
            output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100,2)}%\n'
            print(output)
            if system_msg == 'false' and np.round(np.exp(logprob.logprob)*100,2) >= 55.00:
                classification = 'false'
            elif system_msg == 'true' and np.round(np.exp(logprob.logprob)*100,2) >= 55.00:
                classification = 'true'
            else:
                classification = 'false'
        accuracy = np.round(np.exp(logprob.logprob)*100,2)
        sufficent = system_msg
        return classification, accuracy, sufficent

    def run(self, query, user_message):
        augmented_prompt = self.assistant.augment_prompt(query)
        context_message = augmented_prompt
        prompt_message = file_reader("prompts/generic-evaluation-prompt.txt")
        context = str(context_message)
        prompt = str(prompt_message)
        return self.evaluate(prompt, user_message, context)


if __name__ == "__main__":
    env_manager = get_env_manager()
    client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])
    evaluator = Evaluator(env_manager, client)
    query = "I want to know about this week tasks."
    user_message = "What are my tasks for this week?"
    print(evaluator.run(query, user_message))
import os
import json
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utility.env_manager import get_env_manager
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
from typing import List, Dict
from dotenv import load_dotenv
from prompts.context import KnowledgeAssistant

load_dotenv()
env_manager = get_env_manager()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class PromptGenerator:
    def __init__(self, num_test_output: str, objective: str, output: str):
        self.num_test_output = num_test_output
        self.objective = objective
        self.output = output
        self.env_manager = get_env_manager()
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
    def get_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = env_manager['vectordb_keys']['VECTORDB_MODEL'],
        max_tokens=500,
        temperature=0,
        stop=None,
        seed=123,
        tools=None,
        logprobs=None,
        top_logprobs=None,
    ) -> str:
        """Return the completion of the prompt.
        @parameter messages: list of dictionaries with keys 'role' and 'content'.
        @parameter model: the model to use for completion. Defaults to 'davinci'.
        @parameter max_tokens: max tokens to use for each prompt completion.
        @parameter temperature: the higher the temperature, the crazier the text
        @parameter stop: token at which text generation is stopped
        @parameter seed: random seed for text generation
        @parameter tools: list of tools to use for post-processing the output.
        @parameter logprobs: whether to return log probabilities of the output tokens or not.
        @returns completion: the completion of the prompt.
        """

        params = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stop": stop,
            "seed": seed,
            "logprobs": logprobs,
            "top_logprobs": top_logprobs,
        }
        if tools:
            params["tools"] = tools

        completion = client.chat.completions.create(**params)
        return completion


    def file_reader(self, path: str) -> str:
        script_dir = os.path.dirname(os.path.realpath(__file__))
        base_dir = os.path.dirname(script_dir)
        file_path = os.path.join(base_dir, path)
        with open(file_path, 'r', encoding='utf-8') as f:
            system_message = f.read()
        return system_message
                
                

    def generate_test_data(self, prompt: str, context: str) -> str:
        """Return the classification of the hallucination."""
        API_RESPONSE = self.get_completion(
            [
                {
                    "role": "user",
                    "content": prompt.replace("{context}", context).replace("{num_test_output}", self.num_test_output).replace("{output}", self.output)
                }
            ],
            model=self.env_manager['vectordb_keys']['VECTORDB_MODEL'],
            logprobs=True,
            top_logprobs=1,
        )

        system_msg = API_RESPONSE.choices[0].message.content
        return system_msg

    def save_json(self, test_data) -> None:
            script_dir = os.path.dirname(os.path.realpath(__file__))

                base_dir = os.path.dirname(script_dir)

                path = "prompts-dataset/prompt-data.json"

                file_path = os.path.join(base_dir, path)
        json_object = json.loads(test_data)
        with open(file_path, 'w') as json_file:
            json.dump(json_object, json_file, indent=4)
            
        print(f"JSON data has been saved to {file_path}")

    def execute(self):
        assistant = KnowledgeAssistant()
        query = '"' + str(self.objective) + '"'
        augmented_prompt = assistant.augment_prompt(query)
        context_message = augmented_prompt
        prompt_message = self.file_reader("prompts/prompt-generation-prompt.txt")
        context = str(context_message)
        prompt = str(prompt_message)
        test_data = self.generate_test_data(prompt, context)
        
                self.save_json(test_data)
        
        print("===========")
        print("Prompt Data")
        print("===========")
        print(test_data)
        return test_data

if __name__ == "__main__":
    generator = PromptGenerator("4", "I want to know when the interim submission deadline is", "WHAT ARE THE COMPANY NAMES?")
    generator.execute()
import sys
import json
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException
import os
import requests
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional
from pydantic import BaseModel
from evaluation._prompt_generation import PromptGenerator
from evaluation._evaluation import Evaluator
from utility.env_manager import get_env_manager
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

API_KEY = os.getenv('API_KEY')
API_SECRET = os.getenv('API_SECRET')

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class Item(BaseModel):
    num_test_output: str
    objective: str
    output: str

class EvaluationItem(BaseModel):
    query: str
    user_message: str

@app.get("/check")
def check():
    return "Your API is up!"

@app.post("/generate")
def generate(item: Item):
    env_manager = get_env_manager()
    client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])
    generator = PromptGenerator(item.num_test_output, item.objective, item.output)
    generator.execute()
    with open('prompts-dataset/prompt-data.json', 'r') as f:
        prompts = json.load(f)

    top_score = -1
    top_result = None

        for prompt in prompts:
                evaluation_item = EvaluationItem(query=item.objective, user_message=prompt['Prompt'])

                evaluator = Evaluator(env_manager, client)
        evaluation_result, accuracy, sufficient = evaluator.run(evaluation_item.query, evaluation_item.user_message)

                sufficient = "true"

                if sufficient == "true" and accuracy > top_score:
            top_score = accuracy
            top_result = {"prompt": prompt['Prompt'], "score": f"{top_score}%"}

        return top_result
        
@app.post("/evaluate")
def evaluate(item: EvaluationItem):
    evaluator = Evaluator(env_manager, client)
    result = evaluator.run(item.query, item.user_message)
    return {"result": result}
import download from "./download.png";
import logo from "./openai.png";
import preview from "./preview.png";
import hero from "./hero.jpg"

export { download, logo, preview, hero };
import os

from langchain.chat_models import ChatOpenAI



os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") 



chat = ChatOpenAI(

    openai_api_key=os.getenv("OPENAI_API_KEY"),

    model='gpt-3.5-turbo'

)

























from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]


res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)


from datasets import load_dataset



dataset = load_dataset(

    "jamescalam/llama-2-arxiv-papers-chunked",

    split="train"

)



dataset
dataset[0]






import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time



index_name = 'llama-2-rag'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]

    
    texts = [x['chunk'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['chunk'],

         'source': x['source'],

         'title': x['title']} for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"



vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what safety measures were used in the development of llama 2?"

)



res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(

    content=augment_prompt(

        "what safety measures were used in the development of llama 2?"

    )

)



res = chat(messages + [prompt])        auth_config = weaviate.AuthApiKey(api_key=env_manager['vectordb_keys']['VECTORDB_API_KEY'])



print(res.content)
![Alt text](PromptlyTech.png?raw=true "PromptlyTech_1")

![Alt text](PromptlyTech_1.png?raw=true "PromptlyTech_2")

Welcome to the PromptlyTech repository! We specialize in bridging the gap between cutting-edge Language Models (LLMs) and real-world businesses, making AI accessible and impactful for everyone.


We revolutionize how businesses interact with LLMs by addressing the complexities of prompt engineering. Our solutions unlock:

- **Enhanced Decision-Making:** Gain data-driven insights and accurate predictions to fuel strategic choices.
- **Optimized Operations:** Automate tasks, streamline workflows, and boost efficiency across your organization.
- **Elevated Customer Experience:** Deliver personalized interactions, answer questions instantly, and build stronger customer relationships.


We offer flexible and scalable LLM integration solutions designed to fit your unique needs and evolve with your growth. Whether you're in finance, healthcare, marketing, or any other industry, we have the expertise to help you harness the power of LLMs.


This repository contains the code for both the frontend/rag_chatbot and backend components of your LLM integration.


To get started with this project, clone the repository using the following command:

1. **Clone Repo:**

```bash
git clone https://github.com/amitchew/week_6
```

2. **Project Setup:**

- **Client (Frontend):**
  - [Frontend Repository](https://github.com/amitchew/week_6/tree/master/FrontEnd/rag_chatbot)

- **Server (Backend):**
  - [Backend Repository](https://github.com/amitchew/week_6/tree/master/backend)
import { useState } from "react";

import Message from "./components/Message";
import Input from "./components/Input";
import History from "./components/History";
import Clear from "./components/Clear";

import "./App.css";

export default function App() {
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [history, setHistory] = useState([]);
  const [accuracy, setAccuracy] = useState([]);
  const [Classification, setClassification] = useState([]);


  const handleSubmit = async () => {
    const prompt = {
      role: "user",
      content: input
    };

    setMessages([...messages, prompt]);

    await fetch("https://localhost:3001/chat/", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.REACT_APP_OPENAI_API_KEY}`,
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        model: "gpt-3.5-turbo",
        messages: [...messages, prompt]
      })
    })
      .then((data) => data.json())
      .then((data) => {
        console.log(data);
        const res = data.choices[0].message.content;
        setMessages((messages) => [
          ...messages,
          {
            role: "assistant",
            content: res
          }
        ]);
        setAccuracy((accuracy) => [
          accuracy = data.choices[0]?.message?.accuracy || " ",
        ]);
        setClassification((classification) => [
          classification = data.choices[0]?.message?.classification || " ",
        ]);
        setHistory((history) => [...history, { question: input, answer: res }]);
        setInput("");
      });
  };

  const clear = () => {
    setMessages([]);
    setHistory([]);
    setAccuracy([]);
    setClassification([]);
  };

  return (
    <div className="App">
      <div className="Column">
        <h3 className="Title">PromptlyTech RAG</h3>
        <div className="Content">
          {messages.map((el, i) => {
            return <Message key={i} role={el.role} content={el.content} />;
          })}
        </div>
        <Input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onClick={input ? handleSubmit : undefined}
        />
      </div>
      <div className="Column">
        {/* <h3 className="Title">History</h3> */}
        <h6 className="Title">Accuracy {accuracy} %</h6>

        <h6 className="Title">Classification {Classification} </h6>




        <div className="Content">
          {history.map((el, i) => {
            return (
              <History
                key={i}
                question={el.question}
                onClick={() =>
                  setMessages([
                    { role: "user", content: history[i].question },
                    { role: "assistant", content: history[i].answer }
                  ])
                }
              />
            );
          })}
        </div>
        <Clear onClick={clear} />
      </div>
    </div>
  );
}
%pip install -qU \

    langchain==0.0.292 \

    openai==0.28.0 \

    datasets==2.10.1 \

    pinecone-client==2.2.4 \

    tiktoken==0.5.1
import importlib



libraries = [

    "langchain",

    "openai",

    "datasets",

    "pinecone_client",

    "tiktoken"

]



for library in libraries:

    try:

        module = importlib.import_module(library)

        print(f"{library} is installed (version: {module.__version__})")

    except ModuleNotFoundError:

        print(f"{library} is not installed")
%pip install pinecone-client==2.2.4

%pip install tiktoken==0.5.1
import os

from dotenv import load_dotenv

from langchain.chat_models import ChatOpenAI



load_dotenv()



openai_api_key = os.getenv("OPENAI_API_KEY") 

os.environ["OPENAI_API_KEY"] = openai_api_key 




chat = ChatOpenAI(

    openai_api_key = openai_api_key,

    model='gpt-3.5-turbo'

)

























from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]


res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)




messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)




messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""
print(augmented_prompt)

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)


from datasets import load_dataset



dataset = load_dataset("bzantium/LITM")
from datasets import load_dataset



dataset = load_dataset("jamescalam/llama-2-arxiv-papers-chunked")



dataset
dataset[0]






import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY'),

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time



index_name = 'llama-2-rag'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]

    
    texts = [x['chunk'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['chunk'],

         'source': x['source'],

         'title': x['title']} for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"



vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what safety measures were used in the development of llama 2?"

)



res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(

    content=augment_prompt(

        "what safety measures were used in the development of llama 2?"

    )

)



res = chat(messages + [prompt])

print(res.content)
import os
import json
import sys
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
from scripts import prompt_generator


env_manager = get_env_manager()
client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])


def evaluate(prompt: str, user_message: str, context: str, use_test_data: bool = False) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    num_test_output = str(10)
    API_RESPONSE = prompt_generator.get_completion(
        [
            {
                "role": "system", 
                "content": prompt.replace("{Context}", context).replace("{Question}", user_message)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = str(API_RESPONSE.choices[0].message.content)

    for i, logprob in enumerate(API_RESPONSE.choices[0].logprobs.content[0].top_logprobs, start=1):
        output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100,2)}%\n'
        print(output)
        
        if system_msg == 'true' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:
            classification = 'true'
        elif system_msg == 'false' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:
            classification = 'false'
        else:
            classification = 'false'

    return classification

if __name__ == "__main__":
    context_message = prompt_generator.file_reader("prompts/context.txt")
    prompt_message = prompt_generator.file_reader("prompts/generic-evaluation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    
    user_message = str(input("question: "))
    
    print(evaluate(prompt, user_message, context))
import os
import json
import sys
from openai import OpenAI
from math import exp
import numpy as np
from dotenv import load_dotenv
load_dotenv()


openai_api_key = os.getenv("OPENAI_API_KEY") 
vectordb_keys = os.getenv("OPENAI_MODEL") 
client = OpenAI(api_key=openai_api_key)


def get_completion(
    messages: list[dict[str, str]],
    model: str = vectordb_keys,
    max_tokens=500,
    temperature=0,
    stop=None,
    seed=123,
    tools=None,
    logprobs=None,
    top_logprobs=None,
) -> str:
    """Return the completion of the prompt.
    @parameter messages: list of dictionaries with keys 'role' and 'content'.
    @parameter model: the model to use for completion. Defaults to 'davinci'.
    @parameter max_tokens: max tokens to use for each prompt completion.
    @parameter temperature: the higher the temperature, the crazier the text
    @parameter stop: token at which text generation is stopped
    @parameter seed: random seed for text generation
    @parameter tools: list of tools to use for post-processing the output.
    @parameter logprobs: whether to return log probabilities of the output tokens or not.
    @returns completion: the completion of the prompt.
    """

    params = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion


def file_reader(path: str, ) -> str:
    fname = os.path.join(path)
    with open(fname, 'r') as f:
        system_message = f.read()
    return system_message
            

def generate_test_data(prompt: str, context: str, num_test_output: str) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    API_RESPONSE = get_completion(
        [
            {
                "role": "user", 
                "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)
            }
        ],
        model=vectordb_keys,
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = API_RESPONSE.choices[0].message.content
    return system_msg


def main(num: str):
    context_message = file_reader("../prompts/context.txt")
    prompt_message = file_reader("../prompts/prompt-generating-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)

    generate_prompts = generate_test_data(prompt, context, num)

    def save_txt(generate_prompts) -> None:
                file_path = "../prompts/automatically-generated-prompts.txt"
        with open(file_path, 'w') as txt_file:
            txt_file.write(generate_prompts)
        
        print(f"Generated Prompts have been saved to {file_path}")

    save_txt(generate_prompts)

    print("===========")
    print("Prompts")
    print("===========")
    print(generate_prompts)


if __name__ == "__main__":
    main("5")
import os
import json
import sys
from openai import OpenAI
from math import exp
import numpy as np
from dotenv import load_dotenv
load_dotenv()


openai_api_key = os.getenv("OPENAI_API_KEY") 
model = os.getenv("OPENAI_MODEL") 

client = OpenAI(api_key=openai_api_key)


def get_completion(
    messages: list[dict[str, str]],
    model: str = model,
    max_tokens=500,
    temperature=0,
    stop=None,
    seed=123,
    tools=None,
    logprobs=None,
    top_logprobs=None,
) -> str:
    """Return the completion of the prompt.
    @parameter messages: list of dictionaries with keys 'role' and 'content'.
    @parameter model: the model to use for completion. Defaults to 'davinci'.
    @parameter max_tokens: max tokens to use for each prompt completion.
    @parameter temperature: the higher the temperature, the crazier the text
    @parameter stop: token at which text generation is stopped
    @parameter seed: random seed for text generation
    @parameter tools: list of tools to use for post-processing the output.
    @parameter logprobs: whether to return log probabilities of the output tokens or not.
    @returns completion: the completion of the prompt.
    """

    params = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion


def file_reader(path: str, ) -> str:
    fname = os.path.join(path)
    with open(fname, 'r') as f:
        system_message = f.read()
    return system_message
            

def generate_test_data(prompt: str, context: str, num_test_output: str) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    API_RESPONSE = get_completion(
        [
            {
                "role": "user", 
                "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)
            }
        ],
        model=model,
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = API_RESPONSE.choices[0].message.content
    return system_msg


def main(num_test_output: str):
    context_message = file_reader("../prompts/context.txt")
    prompt_message = file_reader("../prompts/test-prompt-generating-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)

    test_data = generate_test_data(prompt, context, num_test_output)

    def save_txt(test_data) -> None:
                file_path = "../prompts/automatically-generated-test-prompts.txt"
        with open(file_path, 'w') as txt_file:
            txt_file.write(test_data)
        
        print(f"Text data has been saved to {file_path}")

    save_txt(test_data)

    print("===========")
    print("Prompts")
    print("===========")
    print(test_data)


if __name__ == "__main__":
    main("3")
import requests
import os
import weaviate
from weaviate.embedded import EmbeddedOptions
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter  
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Weaviate
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

from dotenv import load_dotenv,find_dotenv


def chunk_loader(file_path= '../RAG/prompts/context.txt'):
    loader = TextLoader(file_path)
    documents = loader.load()

        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)
    return chunks


def create_retriever(chunks):
    load_dotenv(find_dotenv())

    client = weaviate.Client(
    embedded_options = EmbeddedOptions()
  )

    vectorstore = Weaviate.from_documents(
      client = client,    
      documents = chunks,
      embedding = OpenAIEmbeddings(),
      by_text = False
  )

    retriever = vectorstore.as_retriever()
  return retriever


def file_reader(path: str, ) -> str:
    fname = os.path.join(path)
    with open(fname, 'r') as f:
        system_message = f.read()
    return system_message


def test_prompts():
    prompts = file_reader("../prompts/automatically-generated-prompts.txt")
    chunks =  chunk_loader()
    retriever = create_retriever(chunks)

        llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k", temperature=0)

    final_prompts = []

    for prompt in prompts:
       final_prompts.append(ChatPromptTemplate.from_template(prompt))

    
    for prompt in final_prompts:
                rag_chain = (
            {"context": retriever,  "question": RunnablePassthrough()} 
            | prompt 
            | llm
            | StrOutputParser() 
        )

        test_cases = file_reader("../prompts/automatically-generated-test-prompts.txt")

        questions = []
        ground_truths = []
        for test_case in test_cases:
            questions.append(test_case["user"])
            ground_truths.append(test_case["assistant"])

        answers = []
        contexts = []

                for query in questions:
            answers.append(rag_chain.invoke(query))
            contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])

                data = {
            "question": questions,             "answer": answers,             "contexts": contexts,             "ground_truths": ground_truths         }

                dataset = Dataset.from_dict(data)

        result = evaluate(
            dataset = dataset, 
            metrics=[
                context_precision,
                context_recall,
                faithfulness,
                answer_relevancy,
            ],
        )

        df = result.to_pandas()
        print(df)
        
        return result
    
if __name__ == "__main__":
    test_prompts()
import random
def monte_carlo_eval(prompt):

    
    response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']

    scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}



    
    trials = 100

    total_score = 0

    for _ in range(trials):

        response = random.choice(response_types)

        total_score += scores[response]



    
    return total_score / trials



def elo_eval(prompt, base_rating=1500):

    
    
    outcomes = ['win', 'loss', 'draw']

    outcome = random.choice(outcomes)



    
    K = 30  
    R_base = 10 ** (base_rating / 400)

    R_opponent = 10 ** (1600 / 400)  
    expected_score = R_base / (R_base + R_opponent)



    
    actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]

    new_rating = base_rating + K * (actual_score - expected_score)



    return new_rating
def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):

    """

    Update Elo ratings for a list of prompts based on simulated outcomes.



    Parameters:

    prompts (list): List of prompts to be evaluated.

    elo_ratings (dict): Current Elo ratings for each prompt.

    K (int): Maximum change in rating.

    opponent_rating (int): Fixed rating of the opponent for simulation.



    Returns:

    dict: Updated Elo ratings.

    """



    for prompt in prompts:

        
        outcome = random.choice(['win', 'loss', 'draw'])



        
        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]

        R_base = 10 ** (elo_ratings[prompt] / 400)

        R_opponent = 10 ** (opponent_rating / 400)

        expected_score = R_base / (R_base + R_opponent)

        elo_ratings[prompt] += K * (actual_score - expected_score)



    return elo_ratings




prompts = ["Who founded OpenAI?", 

                "What was the initial goal of OpenAI?",

                "What did OpenAI release in 2016?", 

                "What project did OpenAI showcase in 2018?",

                "How did the AI agents in OpenAI Five work together?"

                ]

elo_ratings = {prompt: 1500 for prompt in prompts}  



for _ in range(10):  
    elo_ratings = elo_ratings_func(prompts, elo_ratings)




sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)




for prompt in sorted_prompts:

    print(f"{prompt}: {elo_ratings[prompt]}")
def evaluate_prompt(main_prompt, test_cases):

    evaluations = {}



    
    evaluations['main_prompt'] = {

        'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),

        'Elo Rating Evaluation': elo_eval(main_prompt)

    }



    
    for idx, test_case in enumerate(test_cases):

        evaluations[f'test_case_{idx+1}'] = {

            'Monte Carlo Evaluation': monte_carlo_eval(test_case),

            'Elo Rating Evaluation': elo_eval(test_case)

        }



    return evaluations
main_prompt = "why we use OepenAI?"

test_cases = ["Who founded OpenAI?", 

                "What was the initial goal of OpenAI?",

                "What did OpenAI release in 2016?", 

                "What project did OpenAI showcase in 2018?",

                "How did the AI agents in OpenAI Five work together?"

                ]

result = evaluate_prompt(main_prompt, test_cases)



result



%pip install --pre -U "weaviate-client==4.*"
%pip install chardet
import requests

import os
import weaviate

from weaviate.embedded import EmbeddedOptions
from langchain.document_loaders import TextLoader

from langchain.text_splitter import CharacterTextSplitter  

from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import Weaviate

from langchain.chat_models import ChatOpenAI

from langchain.prompts import ChatPromptTemplate

from langchain.schema.runnable import RunnablePassthrough

from langchain.schema.output_parser import StrOutputParser
from dotenv import load_dotenv,find_dotenv

def data_loader(file_path= '../RAG/prompts/context.txt'):

    loader = TextLoader(file_path)

    documents = loader.load()



    
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    chunks = text_splitter.split_documents(documents)

    return chunks
def create_retriever(chunks):

  
  load_dotenv(find_dotenv())



  
  openai_api_key = os.getenv("OPENAI_API_KEY")



  
  print(openai_api_key)

  

  
  client = weaviate.Client(

    embedded_options = EmbeddedOptions()

  )



  
  vectorstore = Weaviate.from_documents(

      client = client,    

      documents = chunks,

      embedding = OpenAIEmbeddings(),

      by_text = False

  )



  
  retriever = vectorstore.as_retriever()

  return retriever
chunks =  data_loader()



chunks
retriever = create_retriever(chunks)

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)




template = """You are an assistant for question-answering tasks. 

Use the following pieces of retrieved context to answer the question. 

If you don't know the answer, just say that you don't know. 

Use two sentences maximum and keep the answer concise.

Question: {question} 

Context: {context} 

Answer:

"""



prompt = ChatPromptTemplate.from_template(template)




rag_chain = (

    {"context": retriever,  "question": RunnablePassthrough()} 

    | prompt 

    | llm

    | StrOutputParser() 

)
from datasets import Dataset



questions = [

             "Who founded OpenAI?", 

             "What was the initial goal of OpenAI?",

             "What did OpenAI release in 2016?"

            ]



ground_truths = [

                 ["Sam Altman, Elon Musk, Ilya Sutskever and Greg Brockman"],

                 ["To advance digital intelligence in a way that benefits humanity"],

                 ["OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms"]

                ]



answers = []

contexts = []




for query in questions:

  answers.append(rag_chain.invoke(query))

  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])




data = {

    "question": questions, 
    "answer": answers, 
    "contexts": contexts, 
    "ground_truths": ground_truths 
}




dataset = Dataset.from_dict(data)
%pip install ragas
from ragas import evaluate



from ragas.metrics import (

    faithfulness,

    answer_relevancy,

    context_recall,

    context_precision,

)



result = evaluate(

    dataset = dataset, 

    metrics=[

        context_precision,

        context_recall,

        faithfulness,

        answer_relevancy,

    ],

)



df = result.to_pandas()
df
%pip install --upgrade jupyter
%pip install --upgrade ipywidgets
from datasets import load_dataset




dataset = load_dataset("jamescalam/llama-2-arxiv-papers-chunked", split="train")



dataset
import os

from langchain.chat_models import ChatOpenAI

from langchain.schema import SystemMessage, HumanMessage,  AIMessage







OPENAI_KEY = os.getenv("OPENAI_API_KEY") 
chat = ChatOpenAI(

    openai_api_key=OPENAI_KEY,

    model='gpt-3.5-turbo'

)







messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]



res = chat(messages)

res
import os

from langchain.chains import RetrievalQA

from langchain_community.document_loaders import PyPDFLoader

from langchain.embeddings import OpenAIEmbeddings

from langchain.llms import OpenAI

from langchain.text_splitter import CharacterTextSplitter

from langchain.vectorstores import Chroma

def create_qa_model():

    
    loader = PyPDFLoader("./data/RAG.pdf")

    documents = loader.load()

    
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)

    texts = text_splitter.split_documents(documents)

    print(len(texts))

    
    embeddings = OpenAIEmbeddings()

    
    db = Chroma.from_documents(texts, embeddings)

    
    retriever = db.as_retriever( search_kwargs={"k": 3})

    return  retriever
create_qa_model()
from langchain.chat_models import ChatOpenAI

from langchain.chains import RetrievalQA



retriver = create_qa_model()

primary_qa = ChatOpenAI(model_name='gpt-3.5-turbo-16k' )

qa_chain = RetrievalQA.from_chain_type(primary_qa,retriever = retriver, return_source_documents= True)
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from load_to_chroma import Load_VD
import os


class Chat:
    def __init__(self) -> None:
        pass

    def connect_openai():
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model="gpt-3.5-turbo-16k")
        return chat

    def qa_chain(self):
        retriver = Load_VD()
        primary_qa = self.connect_openai()
        qa_chain = RetrievalQA.from_chain_type(
            primary_qa, retriver, return_source_documents=True
        )
        return qa_chain


Chat
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
import os


def Load_VD(file):
        loader = PyPDFLoader(file)
    documents = loader.load()
        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    print(len(texts))
        embeddings = OpenAIEmbeddings()
        db = Chroma.from_documents(texts, embeddings)
        retriever = db.as_retriever(search_kwargs={"k": 3})
    return retriever
!pip install nltk
import os

from dotenv import load_dotenv



load_dotenv()

from langchain_community.llms import OpenAI
import sys

!{sys.executable} -m pip install tiktoken
from langchain import OpenAI

from langchain.document_loaders import TextLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.embeddings.openai import OpenAIEmbeddings

from langchain.vectorstores import Chroma

from langchain.chains import RetrievalQA
loader = TextLoader('./week_6_challenge_doc.txt')

documents = loader.load()



text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)

texts  = text_splitter.split_documents(documents)



embeddings = OpenAIEmbeddings()

store = Chroma.from_documents(texts,embeddings, collection_name="challenge_document")



llm = OpenAI(temperature=0)

chain = RetrievalQA.from_chain_type(llm,retriever=store.as_retriever())



print(chain.run("what are the tasks to be done for final submission?"))

print(chain.run("what format am i supposed to sumbit the final report?"))
print(chain.run("how many tasks does the challenge document have?"))
print(chain.run("can you list out the tutorial dates?"))
print(chain.run("what are the key performance indicators for the challenge?"))
!pip install nltk
import os

from dotenv import load_dotenv



load_dotenv()
from langchain_community.llms import OpenAI
import sys

!{sys.executable} -m pip install tiktoken
from langchain import OpenAI

from langchain.document_loaders import TextLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.embeddings.openai import OpenAIEmbeddings

from langchain.vectorstores import Chroma

from langchain.chains import RetrievalQA
loader = TextLoader('./week_6_challenge_doc.txt')

documents = loader.load()



text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)

texts  = text_splitter.split_documents(documents)



embeddings = OpenAIEmbeddings()

store = Chroma.from_documents(texts,embeddings, collection_name="challenge_document")



llm = OpenAI(temperature=0)

chain = RetrievalQA.from_chain_type(llm,retriever=store.as_retriever())



print(chain.run("what are the tasks to be done for final submission?"))

print(chain.run("what format am i supposed to sumbit the final report?"))
print(chain.run("how many tasks does the challenge document have?"))
print(chain.run("can you list out the tutorial dates?"))
print(chain.run("what are the key performance indicators for the challenge?"))
import logo from './logo.svg';
import 'bootstrap/dist/css/bootstrap.css'
import Tables from './components/Tables'
import Table from './components/Table'
import api from './api/api'
import React, { Component , useState, useRef} from 'react';
function App() {

  const [tables,setTables] = useState([]);
  const [isShow,setShow] = useState(false);

  // useEffect(() =>{
  //     fetchTables();
  // });

  const fetchTables = async () =>{
      // const tables_r = [["Name","Title","Status","Position","Action"],["Name","Title","Status","Position","Action"],["Name","Title","Status","Position","Action"]];
      // setTables(tables_r)
      console.log(ref.current.value);
      const prompt = ref.current.value;

      const response = await api.get('/ranks?prompt='+prompt);
      console.log(response.data);
      setTables(response.data)
      setShow(true)
  }

  // const renderTable = () =>{
  //   fetchTables()
  //   return 
  // }

  const ref = useRef(null);
  return (
    // <div className="App">
    //   <div class="mb-3">
    //     <label for="exampleFormControlTextarea1" class="form-label">Example textarea</label>
    //     <textarea class="form-control" id="exampleFormControlTextarea1" rows="7" ></textarea>
    //   </div>
      // {/* <button type="button" className="btn btn-primary" onClick={this.ShowAlert}>Primary</button> */}
    // </div>
    <div>
      <div className="mb-3">
        <label htmlFor="exampleFormControlTextarea1" className="form-label">Input Prompt</label>
        <textarea className="form-control" id="exampleFormControlTextarea1" rows="7"  ref={ref}/>
      </div>
      <button type="button" className="btn btn-primary" onClick={fetchTables}>Rank Prompt</button> 
      {isShow && <Tables tables= {tables}/>}
    </div>

  );
}




export default App;
import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';
import {BrowserRouter as Router} from 'react-router-dom';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(
  <React.StrictMode>
    <Router>
     <App />
    </Router>
  </React.StrictMode>
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();
import React, { Component } from 'react';
import { MDBBadge, MDBBtn, MDBTable, MDBTableHead, MDBTableBody } from 'mdb-react-ui-kit';

class Table extends Component {
    tables = [["Name","Title","Status","Position","Action"],["Name","Title","Status","Position","Action"],["Name","Title","Status","Position","Action"]];

    render() { 
        return (
            <MDBTable align='middle'>
        <MDBTableHead>
            <tr>
            <th scope='col'>Name</th>
            <th scope='col'>Title</th>
            <th scope='col'>Status</th>
            <th scope='col'>Position</th>
            <th scope='col'>Actions</th>
            </tr>
        </MDBTableHead>
        <MDBTableBody>
            <tr>
            <td>
                <div className='d-flex align-items-center'>
                <img
                    src='https://mdbootstrap.com/img/new/avatars/8.jpg'
                    alt=''
                    style={{ width: '45px', height: '45px' }}
                    className='rounded-circle'
                />
                <div className='ms-3'>
                    <p className='fw-bold mb-1'>John Doe</p>
                    <p className='text-muted mb-0'>john.doe@gmail.com</p>
                </div>
                </div>
            </td>
            <td>
                <p className='fw-normal mb-1'>Software engineer</p>
                <p className='text-muted mb-0'>IT department</p>
            </td>
            <td>
                <MDBBadge color='success' pill>
                Active
                </MDBBadge>
            </td>
            <td>Senior</td>
            <td>
                <MDBBtn color='link' rounded size='sm' >
                Edit
                </MDBBtn>
            </td>
            </tr>
            <tr>
            <td>
                <div className='d-flex align-items-center'>
                <img
                    src='https://mdbootstrap.com/img/new/avatars/6.jpg'
                    alt=''
                    style={{ width: '45px', height: '45px' }}
                    className='rounded-circle'
                />
                <div className='ms-3'>
                    <p className='fw-bold mb-1'>Alex Ray</p>
                    <p className='text-muted mb-0'>alex.ray@gmail.com</p>
                </div>
                </div>
            </td>
            <td>
                <p className='fw-normal mb-1'>Consultant</p>
                <p className='text-muted mb-0'>Finance</p>
            </td>
            <td>
                <MDBBadge color='primary' pill>
                Onboarding
                </MDBBadge>
            </td>
            <td>Junior</td>
            <td>
                <MDBBtn color='link' rounded size='sm'>
                Edit
                </MDBBtn>
            </td>
            </tr>
            <tr>
            <td>
                <div className='d-flex align-items-center'>
                <img
                    src='https://mdbootstrap.com/img/new/avatars/7.jpg'
                    alt=''
                    style={{ width: '45px', height: '45px' }}
                    className='rounded-circle'
                />
                <div className='ms-3'>
                    <p className='fw-bold mb-1'>Kate Hunington</p>
                    <p className='text-muted mb-0'>kate.hunington@gmail.com</p>
                </div>
                </div>
            </td>
            <td>
                <p className='fw-normal mb-1'>Designer</p>
                <p className='text-muted mb-0'>UI/UX</p>
            </td>
            <td>
                <MDBBadge color='warning' pill>
                Awaiting
                </MDBBadge>
            </td>
            <td>Senior</td>
            <td>
                <MDBBtn color='link' rounded size='sm'>
                Edit
                </MDBBtn>
            </td>
            </tr>
        </MDBTableBody>
        </MDBTable>
        );
    }
}
 
export default Table;
from fastapi import FastAPI, HTTPException, Depends
from typing import Annotated, List
from pydantic import BaseModel
from models import RankResult
from prompt_evaluation import *
from fastapi.middleware.cors import CORSMiddleware


app = FastAPI()

origins = ["http://localhost:3000"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
)


class RankBase(BaseModel):
    id: int
    name: str
    rating: float


@app.get("/ranks", response_model=List[RankBase])
async def return_rank(prompt: str):
    results = evaluate_prompt(prompt)
    return results
class PromptResult:
    def __init__(
        self,
        id,
        question,
        answer,
        contexts,
        ground_truths,
        context_precision,
        context_recall,
        faithfulness,
        answer_relevancy,
    ) -> None:
        self.id = id
        self.question = question
        self.answer = answer
        self.contexts = contexts
        self.ground_truths = ground_truths
        self.context_precision = context_precision
        self.context_recall = context_recall
        self.faithfulness = faithfulness
        self.answer_relevancy = answer_relevancy
        pass


class RankResult:
    def __init__(self, id, name, rating) -> None:
        self.id = id
        self.name = name
        self.rating = rating
        pass
import os

from dotenv import load_dotenv



load_dotenv()
from langchain import OpenAI

from langchain.document_loaders import TextLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.embeddings.openai import OpenAIEmbeddings

from langchain.vectorstores import Chroma

from langchain.chains import RetrievalQA

from langchain.prompts import ChatPromptTemplate
import os

import json

import sys





def file_reader(path: str, ) -> str:

    fname = os.path.join(path)

    with open(fname, 'r') as f:

        system_message = f.read()

    return system_message

            

def get_prompt():

    prompt_message = file_reader("../prompts/prompt_generation_template.txt")

    prompt = str(prompt_message)

    return prompt
RAG_PROMPT_TEMPLATE = get_prompt()



rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

rag_prompt
loader = TextLoader('./week_6_challenge_doc.txt')

documents = loader.load()



text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 200, chunk_overlap=50, model_name = "gpt-4-1106-preview")

texts  = text_splitter.split_documents(documents)



llm = OpenAI(temperature=0)



embeddings = OpenAIEmbeddings()

store = Chroma.from_documents(texts,embeddings, collection_name="challenge_document")

len(texts)
from langchain.schema import StrOutputParser



str_output_parser = StrOutputParser()
from langchain_core.runnables import RunnableParallel, RunnablePassthrough



retriever = store.as_retriever()



entry_point_and_retriever = RunnableParallel(

    {

        "context" : retriever,

        "output" : RunnablePassthrough()

    }

)



rag_chain = entry_point_and_retriever | rag_prompt | llm | str_output_parser
rag_chain.invoke('i want to know the goals of the challenge')
from fastapi import FastAPI, HTTPException, Query
from typing import List, Dict

app = FastAPI()

def generate_prompts(description: str, scenarios: List[str], expected_outputs: List[str]) -> List[str]:
    return [f"{description} {scenario} {expected_output}" for scenario, expected_output in zip(scenarios, expected_outputs)]

def evaluate_prompt(description: str, generated_prompt: str) -> float:
    return abs(len(description) - len(generated_prompt))

def generate_evaluation_data(description: str, scenarios: List[str], expected_outputs: List[str]) -> List[Dict[str, float]]:
    evaluation_data = []
    for scenario, expected_output in zip(scenarios, expected_outputs):
        generated_prompt = generate_prompts(description, [scenario], [expected_output])[0]
        evaluation_score = evaluate_prompt(description, generated_prompt)
        evaluation_data.append({"prompt": generated_prompt, "evaluation_score": evaluation_score})
    return evaluation_data

@app.post("/generate_prompts")
def generate_prompts_api(description: str, scenarios: List[str], expected_outputs: List[str]):
    prompts = generate_prompts(description, scenarios, expected_outputs)
    return {"prompts": prompts}

@app.post("/evaluate_prompt")
def evaluate_prompt_api(description: str, generated_prompt: str):
    evaluation_score = evaluate_prompt(description, generated_prompt)
    return {"evaluation_score": evaluation_score}

@app.post("/generate_evaluation_data")
def generate_evaluation_data_api(description: str, scenarios: List[str], expected_outputs: List[str]):
    evaluation_data = generate_evaluation_data(description, scenarios, expected_outputs)
    return {"evaluation_data": evaluation_data}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
from prompt_generation.prompt_generator import PromptGenerator
from test_case_generation.test_case_generator import TestCaseGenerator
from evaluation.monte_carlo_matchmaking import MonteCarloMatchmaking
from evaluation.elo_rating_system import ELORatingSystem

class PromptEngineeringApp:
    def __init__(self):
        self.prompt_generator = PromptGenerator()
        self.test_case_generator = TestCaseGenerator()
        self.monte_carlo_matchmaker = MonteCarloMatchmaking()
        self.elo_rating_system = ELORatingSystem()

    def run(self):
        
        pass
import random

class PromptTestingAndRanking:
    def __init__(self, evaluation_data_generator):
        self.evaluation_data_generator = evaluation_data_generator
        self.prompt_candidates = []
        self.matchups_history = {}
        self.elo_ratings = {}

    def add_prompt_candidates(self, prompt_candidates):
        """
        Add prompt candidates to the testing and ranking system.

        Args:
        - prompt_candidates (list): List of prompt candidates.
        """
        self.prompt_candidates = prompt_candidates
        self.initialize_elo_ratings()

    def initialize_elo_ratings(self):
        """
        Initialize ELO ratings for each prompt candidate.
        """
        for prompt_candidate in self.prompt_candidates:
            self.elo_ratings[prompt_candidate] = 1000  
    def perform_monte_carlo_matchmaking(self, num_matchups=10):
        """
        Perform Monte Carlo Matchmaking to simulate prompt matchups.

        Args:
        - num_matchups (int): Number of matchups to simulate.

        Returns:
        - dict: Dictionary containing matchups and their outcomes.
        """
        matchups_outcomes = {}

        for _ in range(num_matchups):
            matchup = random.sample(self.prompt_candidates, 2)
            winner = random.choice(matchup)
            loser = matchup[0] if matchup[0] != winner else matchup[1]

                        if matchup not in self.matchups_history:
                self.matchups_history[matchup] = {"wins": 0, "losses": 0}
            self.matchups_history[matchup]["wins"] += 1

                        self.update_elo_ratings(winner, loser)

            matchups_outcomes[matchup] = winner

        return matchups_outcomes

    def update_elo_ratings(self, winner, loser, k=32):
        """
        Update ELO ratings based on the outcome of a matchup.

        Args:
        - winner (str): Winner prompt candidate.
        - loser (str): Loser prompt candidate.
        - k (int): ELO rating update constant.
        """
        rating_difference = self.elo_ratings[winner] - self.elo_ratings[loser]
        expected_outcome = 1 / (1 + 10 ** (-rating_difference / 400))

        self.elo_ratings[winner] += k * (1 - expected_outcome)
        self.elo_ratings[loser] -= k * expected_outcome

    def rank_prompts(self):
        """
        Rank prompt candidates based on their ELO ratings.

        Returns:
        - list: Ranked prompt candidates.
        """
        ranked_prompts = sorted(self.elo_ratings, key=lambda x: self.elo_ratings[x], reverse=True)
        return ranked_prompts


evaluation_data_generator = EvaluationDataGenerator()
prompt_testing_and_ranking = PromptTestingAndRanking(evaluation_data_generator)

prompt_candidates = ["Prompt 1", "Prompt 2", "Prompt 3"]
evaluation_data_generator.generate_evaluation_data("User's description", num_test_cases=5)

prompt_testing_and_ranking.add_prompt_candidates(prompt_candidates)

matchups_outcomes = prompt_testing_and_ranking.perform_monte_carlo_matchmaking(num_matchups=10)
print("Matchups Outcomes:")
for matchup, winner in matchups_outcomes.items():
    print(f"{matchup[0]} vs {matchup[1]} - Winner: {winner}")

ranked_prompts = prompt_testing_and_ranking.rank_prompts()
print("\nRanked Prompts:")
for i, prompt in enumerate(ranked_prompts, 1):
    print(f"{i}. {prompt} - ELO Rating: {prompt_testing_and_ranking.elo_ratings[prompt]}")
import random
import math

class PromptTestingAndRanking:
    def __init__(self, evaluation_data_generator, exploration_rate=0.2):
        self.evaluation_data_generator = evaluation_data_generator
        self.prompt_candidates = []
        self.matchups_history = {}
        self.bandit_rewards = {}
        self.exploration_rate = exploration_rate

    def add_prompt_candidates(self, prompt_candidates):
        """
        Add prompt candidates to the testing and ranking system.

        Args:
        - prompt_candidates (list): List of prompt candidates.
        """
        self.prompt_candidates = prompt_candidates
        self.initialize_bandit_rewards()

    def initialize_bandit_rewards(self):
        """
        Initialize rewards for each prompt candidate in the Multi-Armed Bandit.
        """
        for prompt_candidate in self.prompt_candidates:
            self.bandit_rewards[prompt_candidate] = {"wins": 0, "losses": 0}

    def perform_monte_carlo_matchmaking(self, num_matchups=10):
        """
        Perform Monte Carlo Matchmaking to simulate prompt matchups.

        Args:
        - num_matchups (int): Number of matchups to simulate.

        Returns:
        - dict: Dictionary containing matchups and their outcomes.
        """
        matchups_outcomes = {}

        for _ in range(num_matchups):
            matchup = random.sample(self.prompt_candidates, 2)
            winner = random.choice(matchup)
            loser = matchup[0] if matchup[0] != winner else matchup[1]

                        if matchup not in self.matchups_history:
                self.matchups_history[matchup] = {"wins": 0, "losses": 0}
            self.matchups_history[matchup]["wins"] += 1

                        self.bandit_rewards[winner]["wins"] += 1
            self.bandit_rewards[loser]["losses"] += 1

            matchups_outcomes[matchup] = winner

        return matchups_outcomes

    def select_prompt_to_evaluate(self):
        """
        Select a prompt candidate to evaluate using the Multi-Armed Bandit Algorithm.

        Returns:
        - str: Selected prompt candidate.
        """
        total_evaluations = sum(self.bandit_rewards[prompt]["wins"] + self.bandit_rewards[prompt]["losses"] for prompt in self.prompt_candidates)

        if random.uniform(0, 1) < self.exploration_rate:
                        return random.choice(self.prompt_candidates)
        else:
                        rewards_estimates = {prompt: (self.bandit_rewards[prompt]["wins"] + 1) / (total_evaluations + 1) for prompt in self.prompt_candidates}
            selected_prompt = max(rewards_estimates, key=rewards_estimates.get)
            return selected_prompt

evaluation_data_generator = EvaluationDataGenerator()
prompt_testing_and_ranking = PromptTestingAndRanking(evaluation_data_generator)

prompt_candidates = ["Prompt 1", "Prompt 2", "Prompt 3"]
evaluation_data_generator.generate_evaluation_data("User's description", num_test_cases=5)

prompt_testing_and_ranking.add_prompt_candidates(prompt_candidates)

matchups_outcomes = prompt_testing_and_ranking.perform_monte_carlo_matchmaking(num_matchups=10)
print("Matchups Outcomes:")
for matchup, winner in matchups_outcomes.items():
    print(f"{matchup[0]} vs {matchup[1]} - Winner: {winner}")

selected_prompt = prompt_testing_and_ranking.select_prompt_to_evaluate()
print(f"\nSelected Prompt to Evaluate: {selected_prompt}")
import random

class PromptGenerationSystem:
    def __init__(self):
        self.generated_prompts = []

    def generate_prompts(self, input_description, scenarios, expected_outputs, num_options=3):
        """
        Generate multiple prompt options based on user input and scenarios.

        Args:
        - input_description (str): User's objective or task description.
        - scenarios (list): List of specified scenarios.
        - expected_outputs (list): List of expected outputs corresponding to scenarios.
        - num_options (int): Number of prompt options to generate.

        Returns:
        - list: Generated prompt options.
        """
        self.generated_prompts = []

        for _ in range(num_options):
            
            generated_prompt = f"{input_description} | Scenarios: {', '.join(scenarios)} | Expected Outputs: {', '.join(expected_outputs)}"
            
            
            
            self.generated_prompts.append(generated_prompt)

        return self.generated_prompts

    def evaluate_prompt_alignment(self, prompt_candidate):
        """
        Evaluate whether the generated prompt candidate aligns with the input description.

        Args:
        - prompt_candidate (str): Generated prompt to be evaluated.

        Returns:
        - float: Evaluation score (can be based on similarity, relevance, etc.).
        """
       
        return random.uniform(0.5, 1.0)


prompt_system = PromptGenerationSystem()


user_description = "Solve a complex mathematical problem"
user_scenarios = ["Given initial conditions", "Under time constraints"]
user_expected_outputs = ["Accurate solution", "Optimal result"]

generated_prompts = prompt_system.generate_prompts(user_description, user_scenarios, user_expected_outputs)
print("Generated Prompts:")
for prompt in generated_prompts:
    print(prompt)

for prompt_candidate in generated_prompts:
    evaluation_score = prompt_system.evaluate_prompt_alignment(prompt_candidate)
    print(f"Evaluation Score for the Prompt: {evaluation_score}")
class TestCaseGenerator:
    def generate_test_cases(self, user_input):
       
        pass
class Configuration:
    def __init__(self):
                self.api_key = "my_api_key"
        self.max_attempts = 3
        self.log_level = "info"

    def update_config(self, api_key=None, max_attempts=None, log_level=None):
                if api_key:
            self.api_key = api_key
        if max_attempts:
            self.max_attempts = max_attempts
        if log_level:
            self.log_level = log_level
class MainWindow:
    def __init__(self):
       
        pass

    def display_prompt(self, prompt):
       
        pass

import unittest
from backend.evaluation.monte_carlo_matchmaking import MonteCarloMatchmaking
from backend.evaluation.elo_rating_system import ELORatingSystem

class TestEvaluationMethods(unittest.TestCase):
    def test_monte_carlo_matchmaking(self):
        monte_carlo_matchmaker = MonteCarloMatchmaking()
        result = monte_carlo_matchmaker.match_prompts(["Prompt 1", "Prompt 2"])
       
        self.assertEqual(result, "Match Result")

    def test_elo_rating_system(self):
        elo_rating_system = ELORatingSystem()
        result = elo_rating_system.rate_prompts(["Prompt 1", "Prompt 2"])
        
        self.assertEqual(result, {"Prompt 1": 1200, "Prompt 2": 1100})

if __name__ == "__main__":
    unittest.main()
import unittest
from backend.prompt_generation.prompt_generator import PromptGenerator

class TestPromptGeneration(unittest.TestCase):
    def test_generate_prompt(self):
        prompt_generator = PromptGenerator()
        result = prompt_generator.generate_prompt("User Input")
      
        self.assertEqual(result, "Expected Prompt")

if __name__ == "__main__":
    unittest.main()
import unittest
from backend.test_case_generation.test_case_generator import TestCaseGenerator

class TestTestCaseGeneration(unittest.TestCase):
    def test_generate_test_cases(self):
        test_case_generator = TestCaseGenerator()
        result = test_case_generator.generate_test_cases("User Input")
        
        self.assertEqual(result, ["Test Case 1", "Test Case 2"])

if __name__ == "__main__":
    unittest.main()
from ui_design.main_window import MainWindow

def run_ui():
   
    main_window = MainWindow()
    main_window.display_prompt("Placeholder Prompt")

if __name__ == "__main__":
    run_ui()
import os
from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
from config.config import Configuration

class SomeModule:
    def __init__(self, config):
        self.config = config

    def perform_action(self):
                api_key = self.config.api_key
        max_attempts = self.config.max_attempts
        log_level = self.config.log_level
import unittest
from config.config import Configuration

class TestConfigurations(unittest.TestCase):
    def test_configuration_update(self):
                config_instance = Configuration()

                config_instance.update_config(api_key="test_api_key", max_attempts=2, log_level="error")

                self.assertEqual(config_instance.api_key, "test_api_key")
        self.assertEqual(config_instance.max_attempts, 2)
        self.assertEqual(config_instance.log_level, "error")
%pip install -qU \

     langchain==0.0.292 \

     openai==0.28.0 \

     datasets==2.10.1 \

     pinecone-cpip install datasets

lient==2.2.4 \

     tiktoken==0.5.1

import os

from langchain.chat_models import ChatOpenAI



os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or ""



chat = ChatOpenAI(

    openai_api_key="",

    model='gpt-3.5-turbo'

)
from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]
res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
from datasets import load_dataset



dataset = load_dataset('fka/awesome-chatgpt-prompts',split="train")

dataset
train_data = dataset['train']

first_few_rows = dataset['train'][:5]




first_few_rows


act_column = dataset['train']['act']




act_column

import os

import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or '20ea8434-7c1a-4f44-a907-0ab624e39ec0',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)

import time



index_name = 'llama-2-rag'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])
from tqdm.auto import tqdm  






data = dataset['train'].to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i + batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"row_{index}" for index in batch.index]

    
    texts = batch['prompt'].tolist()

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'prompt': x['prompt'],

         'act': x['act']} for _, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))

index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)

query_prompt = "What is your favorite color?"

vectorstore.similarity_search(query, k=3)


top_k = 5




results = index.query(vector=query_vector, top_k=top_k)

for match in results["matches"]:

    id, score, values = match.get("id"), match.get("score"), match.get("values")

    print(f"ID: {id}, Score: {score}, Values: {values}")



from datasets import load_dataset




dataset_link = 'jamescalam/llama-2-arxiv-papers'




dataset = load_dataset(dataset_link)

dataset 
from datasets import load_dataset



dataset = load_dataset("jamescalam/llama-2-arxiv-papers-chunked")
from datasets import load_dataset



dataset = load_dataset("jamescalam/llama-2-arxiv-papers-chunked",

    split="train"

)



dataset
dataset[0]
import os

import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or '20ea8434-7c1a-4f44-a907-0ab624e39ec0',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)

import time



index_name = 'llama-2-rag'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])
from tqdm.auto import tqdm

import pandas as pd




data = dataset.to_pandas()

batch_size = 100
for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i + batch_size)

    batch = data.iloc[i:i_end]
!pip install pinecone-client langchain cohere
!pip install openai==0.27.1
from pinecone import pinecone

import os


from langchain.vectorstores import Pinecone

from langchain.embeddings.cohere import CohereEmbeddings

import openai

from langchain.embeddings.openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain.document_loaders import DirectoryLoader, TextLoader



openai_api_key = os.getenv("OPEN_API_KEY")

pinecone_api_key = os.getenv("PINECONE_API_KEY")

cohere_api_key = os.getenv("COHERE_API_KEY")


loader = TextLoader("/content/drive/MyDrive/10academy/rags/rags doc.txt")

document = loader.load()
doc = "/content/drive/MyDrive/10academy/rags/rags doc.txt"
with open(doc, 'r') as file:

    doc_content = file.read()



print(doc_content)

splitter = CharacterTextSplitter(

    chunk_size = 500,

    chunk_overlap = 100

)
split_docs = splitter.split_documents(document)

print(split_docs)

embeddings = CohereEmbeddings( model = "multilingual-22-12", cohere_api_key= cohere_api_key)

texts = [doc.page_content for doc in split_docs]


embedded = embeddings.embed(texts)

print(embedded)

key3 = os.getenv("OPEN_API_KEY")

openai.api_key=key3
MODEL = "text-embedding-ada-002"



res = openai.Embedding.create(

    input= texts,

    engine=MODEL, api_key = openai.api_key

)

res
from pinecone import Pinecone




api_key = os.getenv("PINECONE_API_KEY")




pc = Pinecone(api_key=api_key)

import time

from pinecone import ServerlessSpec



index_name = 'rag-cohere1'

existing_indexes = [

    index_info["name"] for index_info in pc.list_indexes()

]




if index_name not in existing_indexes:

    
    pc.create_index(

        index_name,

        dimension=768,  
        metric='cosine',

        spec=ServerlessSpec(

        cloud="aws",

        region="us-west-2"

    )



    )

    
    while not pc.describe_index(index_name).status['ready']:

        time.sleep(1)




index = pc.Index(index_name)

time.sleep(1)


index.describe_index_stats()
from uuid import uuid4


ids = [str(uuid4()) for _ in range(len(texts))]

documents = [{'id': id, 'values': vector} for id, vector in zip(ids, embedded)]

index.upsert(documents)
print(res.keys())

data_values = res['data']




for value in data_values[1].items():

    print(f"{value}")

print(data_values[1]['embedding'])

vector_data = []

for i, data in enumerate(data_values):

    embedding = data.get('embedding')

    if embedding is not None:

        
        embedding_list = np.array(embedding).tolist()



        
        vector_data.append({'id': str(i), 'values': embedding_list})
print(vector_data)
embeds = [record['embedding'] for record in res['data']]

len(embeds)
ids = str(range(len(embeds)))

to_upsert = zip(ids, embeds)

index = pc.Index("rag-cohere1")




index.upsert(vectors = vector_data )

query = "What is the business objective of the challenge?"


ques = openai.Embedding.create(input = query, engine=MODEL, api_key = openai.api_key

)['data'][0]['embedding']

print(ques)

index.fetch(["25"])
from datasets import load_dataset

import 



dataset = load_dataset(

    "Rtian/DebugBench",

    split="test"

)



dataset
dataset [0]
import pinecone







pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time



index_name = 'Ritina-Debug'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)

index.describe_index_stats()
from datasets import load_dataset

import pandas as pd

 



dataset = load_dataset(

    "Rtian/DebugBench",

    split="test"

)



dataset
from langchain.document_loaders import ReadTheDocsLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter




chunk_size = 300

chunk_overlap = 50

text_splitter = RecursiveCharacterTextSplitter(

    separators=["\n\n", "\n", " ", ""],

    chunk_size=chunk_size,

    chunk_overlap=chunk_overlap,

    length_function=len,

)




sample_section = next(iter(dataset))

chunks = text_splitter.create_documents(

    texts=[sample_section["question"]], 

    metadatas=[{"solution": sample_section["solution"]}])

num_chunks = len(chunks)

print(f"{num_chunks} chunks")

print (chunks)

from functools import partial

def chunk_section(section, chunk_size, chunk_overlap):

    text_splitter = RecursiveCharacterTextSplitter(

        separators=["\n\n", "\n", " ", ""],

        chunk_size=chunk_size,

        chunk_overlap=chunk_overlap,

        length_function=len)

    chunks = text_splitter.create_documents(

        texts=[sample_section["question"]], 

        metadatas=[{"solution": sample_section["solution"]}])

    return {

        "question_chunk": [chunk.page_content for chunk in chunks],

        "solution_chunk": [chunk.metadata["solution"] for chunk in chunks]

    }




chunks_ds = dataset.map(partial(

    chunk_section, 

    chunk_size=300, 

    chunk_overlap=40))

    


num_chunks = len(chunks_ds)





splitted = chunks_ds.map(lambda data: {"question": data["question"], "solution": data["solution"]}, batched=True)


splitted = splitted.remove_columns(list(set(splitted.column_names) - {"question", "solution"}))

size = len(splitted)

print(f"{size} chunks")

print(splitted[0])

df = pd.DataFrame(splitted)






df.to_json('data.jsonl', orient='records', lines=True)
from datasets import load_dataset

import pandas as pd

 



dataset = load_dataset(

    "Rtian/DebugBench",

    split="test"

)



dataset
from langchain.document_loaders import ReadTheDocsLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter




chunk_size = 300

chunk_overlap = 50

text_splitter = RecursiveCharacterTextSplitter(

    separators=["\n\n", "\n", " ", ""],

    chunk_size=chunk_size,

    chunk_overlap=chunk_overlap,

    length_function=len,

)




sample_section = next(iter(dataset))

chunks = text_splitter.create_documents(

    texts=[sample_section["question"]], 

    metadatas=[{"solution": sample_section["solution"]}])

num_chunks = len(chunks)

print(f"{num_chunks} chunks")

print (chunks)

from functools import partial

def chunk_section(section, chunk_size, chunk_overlap):

    text_splitter = RecursiveCharacterTextSplitter(

        separators=["\n\n", "\n", " ", ""],

        chunk_size=chunk_size,

        chunk_overlap=chunk_overlap,

        length_function=len)

    chunks = text_splitter.create_documents(

        texts=[sample_section["question"]], 

        metadatas=[{"solution": sample_section["solution"]}])

    return {

        "question_chunk": [chunk.page_content for chunk in chunks],

        "solution_chunk": [chunk.metadata["solution"] for chunk in chunks]

    }




chunks_ds = dataset.map(partial(

    chunk_section, 

    chunk_size=300, 

    chunk_overlap=40))

    


num_chunks = len(chunks_ds)





splitted = chunks_ds.map(lambda data: {"question": data["question"], "solution": data["solution"]}, batched=True)


splitted = splitted.remove_columns(list(set(splitted.column_names) - {"question", "solution"}))

size = len(splitted)

print(f"{size} chunks")

print(splitted[0])

df = pd.DataFrame(splitted)






df.to_json('data.jsonl', orient='records', lines=True)
import os

from langchain.embeddings import OpenAIEmbeddings

from langchain.embeddings.huggingface import HuggingFaceEmbeddings

import numpy as np






class EmbedChunks:

    def __init__(self):

       

        self.embedding_model = OpenAIEmbeddings(

            model="text-embedding-ada-002",

            openai_api_base=os.getenv("OPENAI_API_BASE"),

            openai_api_key=os.getenv("OPENAI_API_KEY"))

      



    def process_batch(self, batch):

        embeddings = self.embedding_model.embed_documents(batch["question"])

       

        return pd.DataFrame({"question": batch["question"], "solution": batch["solution"], "embeddings": embeddings})
from dotenv import load_dotenv

import os

load_dotenv()

from datasets import load_dataset



openai_api_key = os.getenv("OPENAI_API_KEY")

from langchain.embeddings.openai import OpenAIEmbeddings

import pandas as pd

data_path = "../dataset/data.jsonl"



df = pd.read_json(data_path, lines=True)







 








embedder = EmbedChunks()












batch_size = 100




batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]




processed_batches = [embedder.process_batch(batch) for batch in batches]




embeded_chunk = pd.concat(processed_batches, ignore_index=True)






















    
print(embeded_chunk.iloc[0])



from tqdm import tqdm

import psycopg2



class StoreResults:

    def __call__(self, batch):

        
        with psycopg.connect(os.environ["DB_CONNECTION_STRING"]) as conn:

            
            register_vector(conn)

            with conn.cursor() as cur:

                
                for question, solution, embeded_chunk in zip(batch["question"], batch["solution"], batch["embeded_chunk"]):

                    cur.execute("INSERT INTO document (question, solution, embeded_chunk) VALUES (%s, %s, %s)",

                                (question, solution, embeded_chunk,))

        
        return {}



store_results = StoreResults()  



for _, row in tqdm(embeded_chunk.iterrows(), total=len(embeded_chunk), desc="Processing"): 

    StoreResults()(row)

embedded_chunks.map_batches(

    StoreResults,

    batch_size=128,

    num_cpus=1,

    compute=ActorPoolStrategy(size=28),

).count()
import os

from langchain.embeddings import OpenAIEmbeddings

from langchain.embeddings.huggingface import HuggingFaceEmbeddings

import numpy as np






class EmbedChunks:

    def __init__(self):

       

        self.embedding_model = OpenAIEmbeddings(

            model="text-embedding-ada-002",

            openai_api_base=os.getenv("OPENAI_API_BASE"),

            openai_api_key=os.getenv("OPENAI_API_KEY"))

      



    def process_batch(self, batch):

        embeddings = self.embedding_model.embed_documents(batch["question"])

       

        return pd.DataFrame({"question": batch["question"], "solution": batch["solution"], "embeddings": embeddings})
from dotenv import load_dotenv

import os

load_dotenv()

from datasets import load_dataset



openai_api_key = os.getenv("OPENAI_API_KEY")

from langchain.embeddings.openai import OpenAIEmbeddings

import pandas as pd

data_path = "../dataset/data.jsonl"



df = pd.read_json(data_path, lines=True)







 








embedder = EmbedChunks()












batch_size = 100




batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]




processed_batches = [embedder.process_batch(batch) for batch in batches]




embeded_chunk = pd.concat(processed_batches, ignore_index=True)






















    
print(embeded_chunk.iloc[0])



from tqdm import tqdm

import psycopg2



class StoreResults:

    def __call__(self, batch):

        
        with psycopg.connect(os.environ["DB_CONNECTION_STRING"]) as conn:

            
            register_vector(conn)

            with conn.cursor() as cur:

                
                for question, solution, embeded_chunk in zip(batch["question"], batch["solution"], batch["embeded_chunk"]):

                    cur.execute("INSERT INTO document (question, solution, embeded_chunk) VALUES (%s, %s, %s)",

                                (question, solution, embeded_chunk,))

        
        return {}



store_results = StoreResults()  



for _, row in tqdm(embeded_chunk.iterrows(), total=len(embeded_chunk), desc="Processing"): 

    StoreResults()(row)

embedded_chunks.map_batches(

    StoreResults,

    batch_size=128,

    num_cpus=1,

    compute=ActorPoolStrategy(size=28),

).count()
from langchain.embeddings.openai import OpenAIEmbeddings

from dotenv import load_dotenv

import os

load_dotenv()

embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
query = ['Put your prompt here']

res = embed_model.embed_documents(query)

len(res), len(res[0])

num_chunks = 5

with psycopg.connect(os.environ["DB_CONNECTION_STRING"]) as conn:

    register_vector(conn)

    with conn.cursor() as cur:

        cur.execute("SELECT * FROM document ORDER BY embedding <-> %s LIMIT %s", (res, num_chunks))

        rows = cur.fetchall()

        context = [{"question": row[1]} for row in rows]

        sources = [row[2] for row in rows]

        

def semantic_search(query, embedding_model, k):

    embedding = np.array(embedding_model.embed_query(query))

    with psycopg.connect(os.environ["DB_CONNECTION_STRING"]) as conn:

        register_vector(conn)

        with conn.cursor() as cur:

            cur.execute("SELECT * FROM document ORDER BY embedding <=> %s LIMIT %s", (embedding, k),)

            rows = cur.fetchall()

            semantic_context = [{"id": row[0], "question": row[1], "solution": row[2]} for row in rows]

    return semantic_context
from langchain.embeddings.openai import OpenAIEmbeddings

from dotenv import load_dotenv

import os

load_dotenv()

embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
query = ['Put your prompt here']

res = embed_model.embed_documents(query)

len(res), len(res[0])

num_chunks = 5

with psycopg.connect(os.environ["DB_CONNECTION_STRING"]) as conn:

    register_vector(conn)

    with conn.cursor() as cur:

        cur.execute("SELECT * FROM document ORDER BY embedding <-> %s LIMIT %s", (res, num_chunks))

        rows = cur.fetchall()

        context = [{"question": row[1]} for row in rows]

        sources = [row[2] for row in rows]

        

def semantic_search(query, embedding_model, k):

    embedding = np.array(embedding_model.embed_query(query))

    with psycopg.connect(os.environ["DB_CONNECTION_STRING"]) as conn:

        register_vector(conn)

        with conn.cursor() as cur:

            cur.execute("SELECT * FROM document ORDER BY embedding <=> %s LIMIT %s", (embedding, k),)

            rows = cur.fetchall()

            semantic_context = [{"id": row[0], "question": row[1], "solution": row[2]} for row in rows]

    return semantic_context
import os

import json

import sys

from openai import OpenAI

from math import exp

import numpy as np

from utility.env_manager import get_env_manager

from evaluation._data_generation import get_completion

from evaluation._data_generation import file_reader






env_manager = get_env_manager()



client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])





def evaluate(prompt: str, user_message: str, context: str, use_test_data: bool = False) -> str:

    """Return the classification of the hallucination.

    @parameter prompt: the prompt to be completed.

    @parameter user_message: the user message to be classified.

    @parameter context: the context of the user message.

    @returns classification: the classification of the hallucination.

    """

    num_test_output = str(10)

    API_RESPONSE = get_completion(

        [

            {

                "role": "system", 

                "content": prompt.replace("{Context}", context).replace("{Question}", user_message)

            }

        ],

        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],

        logprobs=True,

        top_logprobs=1,

    )



    system_msg = str(API_RESPONSE.choices[0].message.content)



    for i, logprob in enumerate(API_RESPONSE.choices[0].logprobs.content[0].top_logprobs, start=1):

        output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100,2)}%\n'

        print(output)

        if system_msg == 'true' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:

            classification = 'true'

        elif system_msg == 'false' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:

            classification = 'false'

        else:

            classification = 'false'

    return classification



if __name__ == "__main__":

    context_message = file_reader("prompts/context.txt")

    prompt_message = file_reader("prompts/generic-evaluation-prompt.txt")

    context = str(context_message)

    prompt = str(prompt_message)

    

    user_message = str(input("question: "))

    

    print(evaluate(prompt, user_message, context))
function App() {
  return (
    <>
      <h1 className="text-2xl font-bold underline">Hello world!</h1>
    </>
  );
}

export default App;
@tailwind base;
@tailwind components;
@tailwind utilities;
import React from "react";
import ReactDOM from "react-dom/client";
import App from "./App.tsx";
import "./index.css";
import { BrowserRouter } from "react-router-dom";

ReactDOM.createRoot(document.getElementById("root")!).render(
  <React.StrictMode>
    <BrowserRouter>
      <App />
    </BrowserRouter>
  </React.StrictMode>
);
import os

from langchain.chat_models import ChatOpenAI



os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or "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"



chat = ChatOpenAI(

    openai_api_key="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",

    model='gpt-3.5-turbo'

)

























from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]


res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)


from datasets import load_dataset



dataset = load_dataset(

    "jamescalam/llama-2-arxiv-papers-chunked",

    split="train"

)



dataset
dataset[0]






import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time



index_name = 'llama-2-rag'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]

    
    texts = [x['chunk'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['chunk'],

         'source': x['source'],

         'title': x['title']} for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"



vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what safety measures were used in the development of llama 2?"

)



res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(

    content=augment_prompt(

        "what safety measures were used in the development of llama 2?"

    )

)



res = chat(messages + [prompt])

print(res.content)
Repository Structure: '
' ├── chatbot
│   ├── Template
│   └── app.py
├── prompt_generation
│   ├── file.zip
│   ├── Documents
│   ├── challenge.pdf
│   ├── utils
│   └── prompt.ipynb
├── screenshots
│   ├── Screenshot from 2024-01-17 16-32-39.png
│   └── Screenshot from 2024-01-20 01-54-36.png
├── .gitignore
├── README.md
└── requirements.txt
 '
' Commit History: 
{"insertions": [167, 639, 167], "deletions": [202, 0, 202], "lines": [369, 639, 369], "committed_datetime": ["2024-01-21 02:51:07", "2024-01-21 07:12:22", "2024-01-21 07:13:26"], "commit_count": 3} 
 Content: 
%reload_ext autoreload

%autoreload 2
import os

from langchain.chat_models import ChatOpenAI

from dotenv import load_dotenv, find_dotenv

load_dotenv()

openai_key = os.getenv("OPENAI_API_KEY") 



chat = ChatOpenAI(

    openai_api_key=openai_key, 

    model='gpt-3.5-turbo'

)
from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]
res = chat(messages)



res

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)
import PyPDF2

import os 




with open("/home/hp/Documents/week 5/precision-RAG-prompt-tuning/prompt_generation/challenge.pdf",'rb') as file:

    
    pdf_reader = PyPDF2.PdfReader(file)



    num_pages = len(pdf_reader.pages)



    for page in range(num_pages):

        page_obj = pdf_reader.pages[page]

        extracted_text = page_obj.extract_text()

        

len(extracted_text)

chunk_size=50

overlap=20

def chunk_text(extracted_text, chunk_size, overlap):

    chunks = []

    text_length = len(extracted_text)



    for start in range(0, text_length, chunk_size - overlap):

        end = min(start + chunk_size, text_length)

        chunk = extracted_text[start:end]

        chunks.append(chunk)



    return chunks



chunks = chunk_text(extracted_text, chunk_size, overlap)



for i, chunk in enumerate(chunks):

    print(f"Chunk {i + 1}:", chunk)
chunks = chunk_text(extracted_text, chunk_size, overlap)




    
print(chunks[0])

len(chunks)
from pinecone import Pinecone



pc = Pinecone(api_key="cc6b9914-0016-4fa4-ab44-f82fe08434b9")

index = pc.Index("mekdes-index")
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
res = embed_model.embed_documents(chunks)

len(res), len(res[0])

import time

import hashlib 



def generate_unique_id_for_chunk(chunk):

    
    timestamp = str(time.time())  
    chunk_hash = hashlib.sha256(chunk.encode()).hexdigest()  
    unique_id = f"{timestamp}_{chunk_hash}"

    

    return unique_id

for chunk, embedding in zip(chunks, res):

    document_id = generate_unique_id_for_chunk(chunk) 

    index.upsert(vectors=[(document_id, embedding)])




index_name = "mekdes-index"

query_vector = [0.1, 0.2, 0.3]  
top_k = 5  
import pinecone 

index = pinecone.Index(index_name, host='https://mekdes-index-nn3xpxm.svc.gcp-starter.pinecone.io')



from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
def generate_prompt_from_vector(chunks, user_question):

    prompt = chunks + " \n based on the above data, give an answer to \

            the following question. restrict yourself to the above data only. \

            if you can't get an answer based on the data, you can feel free to \

                say i don't know. here is the question. \n" + user_question

    return prompt

import numpy as np



def cosine_similarity(embedding1, embedding2):

    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))
query = "What is so special about Llama 2?"



cosine_similarity(query, embedding)
def generate_prompt(request):

    if request.method == 'POST':

        input_text = request.POST.get('input_text')

        
        embeded_question = embed_text([input_text])[0]

        highest_similarity = -1

        best_text_chunk = None

        
        for text_chunk in chunks.objects.all():

            similarity = cosine_similarity(embeded_question, text_chunk.embed)

            if similarity > highest_similarity:

                highest_similarity = similarity

                best_text_chunk = text_chunk.chunk

        if best_text_chunk is not None:

            generated_prompt = generate_prompt_from_vector(best_text_chunk, input_text)

            
            return render(request, 'prompt_app/prompt_result.html', {'generated_prompt': generated_prompt})

        else:

            return HttpResponse("No similar documents found.")

    else:

        return render(request, 'prompt_app/generate_prompt.html') 

 

 

import random
def monte_carlo_eval(prompt):

    
    response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']

    scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}



    
    trials = 100

    total_score = 0

    for _ in range(trials):

        response = random.choice(response_types)

        total_score += scores[response]



    
    return total_score / trials



def elo_eval(prompt, base_rating=1500):

    
    
    outcomes = ['win', 'loss', 'draw']

    outcome = random.choice(outcomes)



    
    K = 30  
    R_base = 10 ** (base_rating / 400)

    R_opponent = 10 ** (1600 / 400)  
    expected_score = R_base / (R_base + R_opponent)



    
    actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]

    new_rating = base_rating + K * (actual_score - expected_score)



    return new_rating
def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):

    """

    Update Elo ratings for a list of prompts based on simulated outcomes.



    Parameters:

    prompts (list): List of prompts to be evaluated.

    elo_ratings (dict): Current Elo ratings for each prompt.

    K (int): Maximum change in rating.

    opponent_rating (int): Fixed rating of the opponent for simulation.



    Returns:

    dict: Updated Elo ratings.

    """



    for prompt in prompts:

        
        outcome = random.choice(['win', 'loss', 'draw'])



        
        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]

        R_base = 10 ** (elo_ratings[prompt] / 400)

        R_opponent = 10 ** (opponent_rating / 400)

        expected_score = R_base / (R_base + R_opponent)

        elo_ratings[prompt] += K * (actual_score - expected_score)



    return elo_ratings




prompts = [

            "Who founded OpenAI?", 

            "What was the initial goal of OpenAI?",

            "What did OpenAI release in 2016?", 

            "What project did OpenAI showcase in 2018?",

            "How did the AI agents in OpenAI Five work together?"

                ]

elo_ratings = {prompt: 1500 for prompt in prompts}  



for _ in range(10):  
    elo_ratings = elo_ratings_func(prompts, elo_ratings)




sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)




for prompt in sorted_prompts:

    print(f"{prompt}: {elo_ratings[prompt]}")









def evaluate_prompt(main_prompt, test_cases):

    evaluations = {}



    
    evaluations['main_prompt'] = {

        'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),

        'Elo Rating Evaluation': elo_eval(main_prompt)

    }



    
    for idx, test_case in enumerate(test_cases):

        evaluations[f'test_case_{idx+1}'] = {

            'Monte Carlo Evaluation': monte_carlo_eval(test_case),

            'Elo Rating Evaluation': elo_eval(test_case)

        }



    return evaluations
main_prompt = "why we use OepenAI?"

test_cases = ["Who founded OpenAI?", 

                "What was the initial goal of OpenAI?",

                "What did OpenAI release in 2016?", 

                "What project did OpenAI showcase in 2018?",

                "How did the AI agents in OpenAI Five work together?"

                ]

result = evaluate_prompt(main_prompt, test_cases)

print(result)



















import requests

from langchain.document_loaders import TextLoader

from langchain.text_splitter import CharacterTextSplitter  

from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import Weaviate

import weaviate

from weaviate.embedded import EmbeddedOptions

from dotenv import load_dotenv,find_dotenv

from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import Weaviate

import weaviate

from weaviate.embedded import EmbeddedOptions

from dotenv import load_dotenv,find_dotenv


from langchain.chat_models import ChatOpenAI

from langchain.prompts import ChatPromptTemplate

from langchain.schema.runnable import RunnablePassthrough

from langchain.schema.output_parser import StrOutputParser

def data_loader(file_path= '../prompts/context.txt'):

    loader = TextLoader(file_path)

    documents = loader.load()



    
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    chunks = text_splitter.split_documents(documents)

    return chunks
def create_retriever(chunks):



  
  load_dotenv(find_dotenv())



  
  client = weaviate.Client(

    embedded_options = EmbeddedOptions()

  )



  
  vectorstore = Weaviate.from_documents(

      client = client,    

      documents = chunks,

      embedding = OpenAIEmbeddings(),

      by_text = False

  )



  
  retriever = vectorstore.as_retriever()

  return retriever
import chunk

chunk
chunks =  data_loader()

retriever = create_retriever(chunks)





llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)




template = """You are an assistant for question-answering tasks. 

Use the following pieces of retrieved context to answer the question. 

If you don't know the answer, just say that you don't know. 

Use two sentences maximum and keep the answer concise.

Question: {question} 

Context: {context} 

Answer:

"""



prompt = ChatPromptTemplate.from_template(template)




rag_chain = (

    {"context": retriever,  "question": RunnablePassthrough()} 

    | prompt 

    | llm

    | StrOutputParser() 

)
from datasets import Dataset



questions = ["Who founded OpenAI?", 

             "What was the initial goal of OpenAI?",

             "What did OpenAI release in 2016?",

            ]

ground_truths = [["Sam Altman, Elon Musk, Ilya Sutskever and Greg Brockman"],

                ["To advance digital intelligence in a way that benefits humanity"],

                ["OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms"]]

answers = []

contexts = []




for query in questions:



  answers.append(rag_chain.invoke(query))

  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])




data = {

    "question": questions, 
    "answer": answers, 
    "contexts": contexts, 
    "ground_truths": ground_truths 
}




dataset = Dataset.from_dict(data)
from ragas import evaluate

from ragas.metrics import (

    faithfulness,

    answer_relevancy,

    context_recall,

    context_precision,

)

dataset

result = evaluate(

    dataset = dataset, 

    metrics=[

        context_precision,

        context_recall,

        faithfulness,

        answer_relevancy,

    ],

)



df = result.to_pandas()
df
2024-01-18 21:59:54,471:logger:data loaded to vector database successfullu
2024-01-18 22:00:23,262:logger:data loaded to vector database successfully
2024-01-18 22:05:00,822:logger:data loaded to vector database successfullu
2024-01-18 22:07:46,630:logger:data loaded to vector database successfullu
2024-01-18 22:07:56,509:logger:data loaded to vector database successfully
2024-01-18 22:10:31,784:logger:data loaded to vector database successfullu
2024-01-18 22:10:38,968:logger:data loaded to vector database successfully
2024-01-18 22:27:03,343:logger:data loaded to vector database successfullu
2024-01-18 22:27:16,956:logger:data loaded to vector database successfully
2024-01-18 22:28:19,472:logger:data loaded to vector database successfullu
2024-01-18 22:28:22,504:logger:data loaded to vector database successfully
2024-01-18 22:29:26,217:logger:data loaded to vector database successfullu
2024-01-18 22:29:29,126:logger:data loaded to vector database successfully
2024-01-18 22:30:38,666:logger:data loaded to vector database successfullu
2024-01-18 22:30:41,761:logger:data loaded to vector database successfully
2024-01-18 22:30:56,411:logger:data loaded to vector database successfullu
2024-01-18 22:30:58,948:logger:data loaded to vector database successfully
2024-01-18 22:46:15,007:logger:data loaded to vector database successfullu
2024-01-18 22:47:32,135:logger:data loaded to vector database successfullu
2024-01-18 22:51:38,710:logger:data loaded to vector database successfullu
2024-01-18 22:51:46,255:logger:data loaded to vector database successfully
2024-01-18 23:00:10,087:logger:data loaded to vector database successfullu
2024-01-18 23:00:12,850:logger:data loaded to vector database successfully
2024-01-18 23:01:07,971:logger:data loaded to vector database successfullu
2024-01-18 23:01:10,469:logger:data loaded to vector database successfully
2024-01-18 23:15:41,385:logger:data loaded to vector database successfullu
2024-01-18 23:17:15,977:logger:data loaded to vector database successfullu
2024-01-18 23:17:52,516:logger:data loaded to vector database successfully
2024-01-18 23:23:03,695:logger:data loaded to vector database successfullu
2024-01-18 23:23:18,275:logger:data loaded to vector database successfullu
2024-01-18 23:25:49,760:logger:data loaded to vector database successfullu
2024-01-18 23:29:30,837:logger:data loaded to vector database successfully
2024-01-18 23:29:41,274:logger:automatic evaluation data generated succesfully.
2024-01-18 23:31:55,502:logger:data loaded to vector database successfullu
2024-01-18 23:31:59,037:logger:data loaded to vector database successfully
2024-01-18 23:32:18,499:logger:automatic evaluation data generated succesfully.
2024-01-18 23:34:41,772:logger:data loaded to vector database successfully
2024-01-18 23:34:55,869:logger:automatic evaluation data generated succesfully.
2024-01-19 19:52:57,690:logger:data loaded to vector database successfully
2024-01-19 19:53:08,505:logger:data loaded to vector database successfully
2024-01-19 19:53:09,268:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:53:09,360:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,423:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,424:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:55:44,300:logger:data loaded to vector database successfully
2024-01-19 19:55:45,116:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:55:45,211:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:58:20,619:logger:data loaded to vector database successfully
2024-01-19 19:58:24,235:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:35,455:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:46,019:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:03:36,364:logger:data loaded to vector database successfully
2024-01-19 20:03:38,295:logger:automatic evaluation data generated succesfully.
2024-01-19 20:03:38,364:logger:langchain with rag pipeline created successfully.
2024-01-19 20:03:38,426:logger:langchain with rag pipeline created successfully.
2024-01-19 20:03:42,437:logger:An unexpected error occurred: string indices must be integers, not 'str'
2024-01-19 20:06:24,695:logger:data loaded to vector database successfully
2024-01-19 20:06:26,196:logger:automatic evaluation data generated succesfully.
2024-01-19 20:06:26,267:logger:langchain with rag pipeline created successfully.
2024-01-19 20:06:26,333:logger:langchain with rag pipeline created successfully.
2024-01-19 20:10:24,578:logger:automatic evaluation data generated succesfully.
2024-01-19 20:13:19,463:logger:data loaded to vector database successfully
2024-01-19 20:13:21,752:logger:automatic evaluation data generated succesfully.
2024-01-19 20:13:21,842:logger:langchain with rag pipeline created successfully.
2024-01-19 20:13:21,913:logger:langchain with rag pipeline created successfully.
2024-01-19 20:13:33,736:logger:automatic evaluation data generated succesfully.
2024-01-19 20:29:50,040:logger:data loaded to vector database successfully
2024-01-19 20:29:51,708:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-Hrgzs***************************************WlJy. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 20:29:51,794:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 20:31:08,434:logger:data loaded to vector database successfully
2024-01-19 20:31:10,689:logger:automatic evaluation data generated succesfully.
2024-01-19 20:31:10,746:logger:langchain with rag pipeline created successfully.
2024-01-19 20:31:10,796:logger:langchain with rag pipeline created successfully.
2024-01-19 20:31:23,412:logger:automatic evaluation data generated succesfully.
2024-01-19 20:31:23,416:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
2024-01-19 20:35:44,712:logger:data loaded to vector database successfully
2024-01-19 20:35:46,960:logger:automatic evaluation data generated succesfully.
2024-01-19 20:35:47,038:logger:langchain with rag pipeline created successfully.
2024-01-19 20:35:47,097:logger:langchain with rag pipeline created successfully.
2024-01-19 20:35:59,762:logger:automatic evaluation data generated succesfully.
2024-01-19 20:35:59,765:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
import os

import sys

from dotenv import load_dotenv,find_dotenv

sys.path.append(os.path.abspath(os.path.join('../utility')))
from rag_utils import data_loader, create_langchain_pipeline, create_retriever

load_dotenv(find_dotenv())
chunks = data_loader()
retriever = create_retriever(chunks)
retriever

file_path = '../prompts/prompt-generation-prompt.txt'




with open(file_path, 'r') as file:

    
    file_contents = file.read()
rag_chain = create_langchain_pipeline(retriever, file_contents)
rag_chain
generated_prompts = rag_chain.invoke("questions about the challenge documnet")
generated_prompts

import json

prompt_data = json.loads(generated_prompts)

prompt_data
import requests
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter  
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Weaviate
import weaviate
from weaviate.embedded import EmbeddedOptions
from dotenv import load_dotenv,find_dotenv
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Weaviate
import weaviate
from weaviate.embedded import EmbeddedOptions
from dotenv import load_dotenv,find_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from logger import logger

from datasets import Dataset


def data_loader(file_path= '../prompts/context.txt', chunk_size=500, chunk_overlap=50):
    try:
        loader = TextLoader(file_path)
        documents = loader.load()

                text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = text_splitter.split_documents(documents)
        
        logger.info("data loaded to vector database successfullu")
        return chunks
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        return None 


def create_chain_rag(retriever, template):
    try:
                llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

                
       
        prompt = ChatPromptTemplate.from_template(template)

                rag_chain = (
            {"context": retriever,  "question": RunnablePassthrough()} 
            | prompt 
            | llm
            | StrOutputParser() 
        )

        logger.info("data loaded to vector database successfully")
        return rag_chain

    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        return None 
    

def generate_testcase_and_context(questions, ground_truths, retriever, template):
    try:
        answers = []
        contexts = []

        rag_chain = create_chain_rag(retriever, template)

                for query in questions:

            answers.append(rag_chain.invoke(query))
            contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])

            
        data = {
            "question": questions,             "answer": answers,             "contexts": contexts,             "ground_truths": ground_truths         }


                dataset = Dataset.from_dict(data) 
        logger.info("automatic evaluation data generated succesfully.")

        return  dataset
    
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        return None
from config import create_app
from flask_cors import CORS

app = create_app()
CORS(app)

if __name__ == '__main__':
    app.run()
from flask import Blueprint, jsonify, request
import json
import logging
from exectuors import get_agent_executor

main_bp = Blueprint('main', __name__)
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')


analyst_agent_openai = get_agent_executor()


@main_bp.route('/api/v1/chat', methods=['POST'])
def index():
    data = request.json
    response = {
        "data" : None,
        "error" : None
    }
    statusCode = 404
    try:
        logging.info(f"data: {data}")
        message = data['message']
        answer = analyst_agent_openai.run(message)

        logging.info(f"response: {answer}")
        response["data"] = answer
        statusCode = 200
    except Exception as error:
        logging.error(error)
        response['error'] = {
        'message': f"{error}"
        }
        statusCode = 404
    return jsonify(response), statusCode
// import { useState, useEffect } from 'react';
// import axios from 'axios';

// const YourComponent = () => {
//   const [data, setData] = useState(null);

//   useEffect(() => {
//     const fetchData = async () => {
//       try {
//         console.log("called")
//         const response = await axios.post('http://127.0.0.1:5000/api/v1/chat', { question: 'your-question' });
//         console.log(response);
//         const result = response.data.data.map(subList => {
//           // Check if the sub-list has at least three elements before removing the third one
//           if (subList.length >= 3) {
//             // Use slice to create a new array without the third element
//             return subList.slice(0, 2).concat(subList.slice(3));
//           }
//           return subList; // If the sub-list has less than three elements, leave it unchanged
//         });

//         setData(result); // Assuming the 'data' property contains the DataFrame JSON
//       } catch (error) {
//         console.error('Error fetching data:', error);
//       }
//     };

//     fetchData();

//   }, []);


  


//   if (!data) {
//     return <div>Loading...</div>;
//   }

//   return (
//     <div>
//       <h2>Data Visualization</h2>
//       <div className="mb-8 xl:mb-16  max-w-[70rem] flex flex-col gap-8 md:gap-24  ">
//             <table className="min-w-full divide-y divide-gray-200 overflow-x-auto">
//               <thead className="bg-gray-50">
//                 <tr>
//                   <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Question
//                   </th>

//                   <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Answer
//                   </th>

//                   {/* <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Contexts
//                   </th> */}

//                   <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Ground Truths
//                   </th>

//                   <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Context Precision
//                   </th>

//                   <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Context Recall
//                   </th>
                  
//                   <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Faithfulness
//                   </th>
//                   <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
//                   Answer Relevancy
//                   </th>
//                 </tr>
//               </thead>
//               <tbody className="bg-white divide-y divide-gray-200">
//                 {data.map((prompt, index) => {
//                   return ( <tr key={index}>
//                      {
//                       prompt.map((value, value_index) => {
//                         return  <td key={value_index} className="px-6 py-4 ">
                      
//                       <div className="text-xs text-left text-gray-900">
//                         {value}
//                       </div>
//                       </td>
//                       })
//                      }
//                     </tr>
//                   )
//                 })}
//               </tbody>
//             </table>
//           </div>
//     </div>
//   );
// };

// export default YourComponent;


import  { useEffect, useRef, useState } from 'react';
import '../App.css'
import '@chatscope/chat-ui-kit-styles/dist/default/styles.min.css';
import { MainContainer, ChatContainer, MessageList, Message, MessageInput, TypingIndicator } from '@chatscope/chat-ui-kit-react';
// import PdfTextExtract from './PdfTextExtract';
import axios from 'axios';

// const API_KEY =  import.meta.env.VITE_REACT_APP_CHATGPT_API_KEY;


function Home() {

  const fileInputRef = useRef(null);
  const [text, setText] = useState('');
  const [prompt, setPrompt] = useState('');

 
  useEffect(() => {
    if(text){
      setPrompt(`
      Answer the following question based on the information in the provided text:
      ${text}
      `)
    }
    else{
      setPrompt("")
    }
  }, [text]);

  const handleButtonClick = () => {
    // Trigger the file input when the button is clicked
    fileInputRef.current.click();
  };



  const handleFileChange = async (event) => {
    const selectedFile = event.target.files[0];

    const formData = new FormData();
    formData.append('file', selectedFile);


    try {
      const response = await axios.post('http://127.0.0.1:5000/extract-text', formData, {
        headers: { 'Content-Type': 'multipart/form-data' },
      });
      setText(response.data.data);
    } catch (error) {
      console.error(error);
    }

  };

  const systemMessage = { //  Explain things like you're talking to a software professional with 5 years of experience.
    "role": "system", "content": prompt
  }
  


  const [messages, setMessages] = useState([
  
  ]);
  const [isTyping, setIsTyping] = useState(false);

  const handleSend = async (message) => {
    const newMessage = {
      message,
      direction: 'outgoing',
      sender: "user"
    };

    const newMessages = [...messages, newMessage];
    
    setMessages(newMessages);

    // Initial system message to determine ChatGPT functionality
    // How it responds, how it talks, etc.
    setIsTyping(true);
    await processMessageToChatGPT(newMessages);
  };

  async function processMessageToChatGPT(chatMessages) { // messages is an array of messages
    // Format messages for chatGPT API
    // API is expecting objects in format of { role: "user" or "assistant", "content": "message here"}
    // So we need to reformat

    let apiMessages = chatMessages.map((messageObject) => {
      let role = "";
      if (messageObject.sender === "ChatGPT") {
        role = "assistant";
      } else {
        role = "user";
      }
      return { role: role, content: messageObject.message}
    });


  
    const apiRequestBody = {
      "model": "gpt-4-1106-preview",
      "messages": [
        systemMessage,  // The system message DEFINES the logic of our chatGPT
        ...apiMessages // The messages from our chat with ChatGPT
      ]
    }

    console.log("apiRequestBody: ", apiRequestBody.messages)

        try {
          const response = await axios.post('http://127.0.0.1:5000/api/v1/chat', { message:apiRequestBody.messages});
          console.log(response);
          setMessages([...chatMessages, {
                message: response.data.data,
                sender: "ChatGPT"
              }]);
              setIsTyping(false);
  
        } catch (error) {
          console.error('Error fetching data:', error);
        }
  

    // await fetch('http://127.0.0.1:5000/api/v1/chat', 
    // {
    //   method: "POST",
    //   headers: {
    //     // "Authorization": "Bearer " + API_KEY,
    //     "Content-Type": "application/json"
    //   },
    //   body: {"question": apiRequestBody.messages}
    // }).then((data) => {
    //   return data.json();
    // }).then((data) => {
    //   console.log("data: ", data)
    //   setMessages([...chatMessages, {
    //     message: data.data.message,
    //     sender: "ChatGPT"
    //   }]);
    //   setIsTyping(false);
    // });
  }

  return (
    <div className="App" style={{height:"90vh", margin:'auto'}}>
      <div style={{ position:"relative", height: "100%", width: "700px",margin:'auto', }}>
      {/* <div style={{marginBottom:'10px'}}>
     <button onClick={handleButtonClick}>Select PDF File</button>
      <input
        ref={fileInputRef}
        type="file"
        accept=".pdf"
        style={{ display: 'none' }}
        onChange={handleFileChange}
      />
   </div> */}

        <MainContainer style={{padding:"10px 5px", borderRadius:'10px',
      display:"flex", alignItems:'center', justifyContent:'center', margin:"auto" }}>
          <ChatContainer >       
            <MessageList 
              scrollBehavior="smooth" 
              typingIndicator={isTyping ? <TypingIndicator content="ChatGPT is typing" /> : null}
            >
              {messages.map((message, i) => {
                return <Message style={{textAlign:'left'}} key={i} model={message} />
              })}
            </MessageList>
            <MessageInput
            onSend={handleSend} 
              style={{ textAlign:"left" }}  
              placeholder="Type message here" 
             />    
            
    
          </ChatContainer>
        </MainContainer>
      </div>
    </div>
  )
}

export default Home
2024-01-19 19:53:09,268:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:53:09,360:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,423:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,424:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:55:45,116:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:55:45,211:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:58:24,235:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:35,455:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:46,019:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:03:42,437:logger:An unexpected error occurred: string indices must be integers, not 'str'
2024-01-19 20:29:51,708:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-Hrgzs***************************************WlJy. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 20:29:51,794:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 20:31:23,416:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
2024-01-19 20:35:59,765:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
2024-01-19 19:53:09,268:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:53:09,360:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,423:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:53:09,424:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:55:45,116:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:55:45,211:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 19:55:45,270:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 19:58:24,235:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:35,455:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pW9Yf***************************************qF8E. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 19:58:46,019:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:03:42,437:logger:An unexpected error occurred: string indices must be integers, not 'str'
2024-01-19 20:29:51,708:logger:An unexpected error occurred: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-Hrgzs***************************************WlJy. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2024-01-19 20:29:51,794:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>
2024-01-19 20:29:51,845:logger:An unexpected error occurred: 'NoneType' object has no attribute 'invoke'
2024-01-19 20:31:23,416:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
2024-01-19 20:35:59,765:logger:An unexpected error occurred: OpenAI API key not found! Seems like your trying to use Ragas metrics with OpenAI endpoints. Please set 'OPENAI_API_KEY' environment variable
import os
from dotenv import load_dotenv
load_dotenv(".env")

class OPENAI_KEYS:
    def __init__(self):
        self.OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '') or None


class VECTORDB_KEYS:
    def __init__(self):
        self.VECTORDB_API_KEY = os.environ.get('VECTORDB_API_KEY', '') or None
        self.VECTORDB_URL = os.environ.get('VECTORDB_URL', '') or None
        self.VECTORDB_MODEL = os.environ.get('VECTORDB_MODEL', '') or None


def _get_openai_keys() -> OPENAI_KEYS:
    return OPENAI_KEYS()


def _get_vectordb_keys() -> VECTORDB_KEYS:
    return VECTORDB_KEYS()


def get_env_manager() -> dict:
    openai_keys = _get_openai_keys().__dict__
    vectordb_keys = _get_vectordb_keys().__dict__

    return {
        'openai_keys': openai_keys,
        'vectordb_keys': vectordb_keys,
    }
from langchain.chat_models import ChatOpenAI
from langchain.agents import AgentType, initialize_agent
from langchain.schema import SystemMessage

from tools import generate_prompts_with_evaluation, get_prompt_ranking_monte_carol_and_elo_rating, generate_evaluation_data

from rag_utils import create_retriever, data_loader

with open("system_message.txt", "r") as file:
    system_message = file.read()

chunks = data_loader
retriever =  create_retriever(chunks) 

def get_agent_executor():
    agent_kwargs = {
    "system_message": SystemMessage(content=system_message),
    "retriever": retriever      }

    analyst_agent_openai = initialize_agent(
        llm=ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),
        agent=AgentType.OPENAI_FUNCTIONS,
        tools=[generate_prompts_with_evaluation, get_prompt_ranking_monte_carol_and_elo_rating, generate_evaluation_data],
        agent_kwargs=agent_kwargs,
        verbose=True,
        max_iterations=20,
        early_stopping_method='generate'
    )

    return analyst_agent_openai
import json
import os

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter  
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Weaviate


from datasets import Dataset
import random

import weaviate
from dotenv import load_dotenv,find_dotenv
from weaviate.embedded import EmbeddedOptions

from ragas import evaluate
from ragas.metrics import ( faithfulness, answer_relevancy, context_recall, context_precision)
 
load_dotenv(find_dotenv())


def data_loader(file_path= '../prompts/challenge_doc.txt', chunk_size=500, chunk_overlap=50):
    try:
        loader = TextLoader(file_path)
        documents = loader.load()

                text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = text_splitter.split_documents(documents)
        
        print("data loaded to vector database successfully")
        return chunks
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 


def create_langchain_pipeline(retriever, template, temperature=0):
    try:
                llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=temperature)

                
        prompt = ChatPromptTemplate.from_template(template)

                rag_chain = (
            {"context": retriever,  "question": RunnablePassthrough()} 
            | prompt 
            | llm
            | StrOutputParser() 
        )

        print("langchain with rag pipeline created successfully.")
        return rag_chain

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 
    

def generate_testcase_and_context(questions, ground_truths, retriever, rag_chain):
    try:
        answers = []
        contexts = []

                for query in questions:

            answers.append(rag_chain.invoke(query))
            contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])

            
        data = {
            "question": questions,             "answer": answers,             "contexts": contexts,             "ground_truths": ground_truths         }


                dataset = Dataset.from_dict(data) 
        print("automatic evaluation data generated succesfully.")

        return  dataset
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 

def generate_automatic_questions(query, retriever):
    docs = retriever.get_relevant_documents(query)
    return docs


def create_retriever(chunks):
    try:
        
                load_dotenv(find_dotenv())


                client = weaviate.Client(
            embedded_options = EmbeddedOptions()
        )

                vectorstore = Weaviate.from_documents(
            client = client,    
            documents = chunks,
            embedding = OpenAIEmbeddings(),
            by_text = False
        )

                retriever = vectorstore.as_retriever()
        print("create retriver  succesfully.")

        return retriever
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 
    
def monte_carlo_eval(prompt):
    try:
                response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']
        scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}

                trials = 100
        total_score = 0
        for _ in range(trials):
            response = random.choice(response_types)
            total_score += scores[response]

                return total_score / trials
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 

def elo_eval(prompt, base_rating=1500):
    try:

                        outcomes = ['win', 'loss', 'draw']
        outcome = random.choice(outcomes)

                K = 30          R_base = 10 ** (base_rating / 400)
        R_opponent = 10 ** (1600 / 400)          expected_score = R_base / (R_base + R_opponent)

                actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]
        new_rating = base_rating + K * (actual_score - expected_score)
    
        return new_rating
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 

def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):
    """
    Update Elo ratings for a list of prompts based on simulated outcomes.

    Parameters:
    prompts (list): List of prompts to be evaluated.
    elo_ratings (dict): Current Elo ratings for each prompt.
    K (int): Maximum change in rating.
    opponent_rating (int): Fixed rating of the opponent for simulation.

    Returns:
    dict: Updated Elo ratings.
    """
    try:

        for prompt in prompts:
                        outcome = random.choice(['win', 'loss', 'draw'])

                        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]
            R_base = 10 ** (elo_ratings[prompt] / 400)
            R_opponent = 10 ** (opponent_rating / 400)
            expected_score = R_base / (R_base + R_opponent)
            elo_ratings[prompt] += K * (actual_score - expected_score)

        return elo_ratings

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 

def evaluate_prompt(prompts):
    try:
        evaluations = []

                for idx, prompt in enumerate(prompts):
            evaluations.append({ 
                'prompt': prompt,
                'Monte Carlo Evaluation': monte_carlo_eval(prompt),
                'Elo Rating Evaluation': elo_eval(prompt)
            })
               

        return evaluations

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 

def load_file(file_path):
    try:

                with open(file_path, 'r') as file:
                        file_contents = file.read()   
        
        return file_contents
        
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 

def ragas_evaulation(response):
    try:
        result = evaluate(
            dataset = response, 
            metrics=[
                context_precision,
                context_recall,
                faithfulness,
                answer_relevancy,
            ],
        )
        df = result.to_pandas()
        values = df.values.tolist()

        for i in range(len(values)):
            values[i][2] = values[i][2].tolist()
            values[i][3] = values[i][3].tolist()

        columns = df.keys().tolist()
        response = [columns] + values
        
        return response
    
    except Exception as e:
        print(f"An unexpected error occurred hola temosa: {e}")
        return None 

def get_generated_prompt_with_evaulation(question):
    try:
        chunks = data_loader()
        retriever = create_retriever(chunks)

        prompt_template = load_file('../prompts/prompt-generation-prompt.txt')
        evaluation_tempate = load_file('../prompts/evaluation-data-generation.txt')


        prompt_rag_chain = create_langchain_pipeline(retriever, prompt_template)
        evaulation_rag_chain = create_langchain_pipeline(retriever, evaluation_tempate, temperature=0.2)


        generated_prompts = prompt_rag_chain.invoke(question)
        prompt_list  = json.loads(generated_prompts)

        questions = [item['prompt'] for item in prompt_list]
        ground_truths = [[item['ground_truth']] for item in prompt_list]

        response = generate_testcase_and_context(questions, ground_truths, retriever, evaulation_rag_chain)
        return ragas_evaulation(response)
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None
/*   max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: } */


  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}
body{
  background:white;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
import os

import sys

sys.path.append(os.path.abspath(os.path.join('../utility')))
from rag_utils import  get_generated_prompt_with_evaulation
df = get_generated_prompt_with_evaulation("generate me prompts which help me understand the challenge document")
df
df.columns
import openai  
from sentence_transformers import SentenceTransformer  
import numpy as np

from scipy.spatial.distance import cosine




prompts = ["Prompt 1", "Prompt 2", ...]




prompt_scores = {prompt: 0 for prompt in prompts}

prompt_wins = {prompt: 0 for prompt in prompts}




def compare_prompts(prompt1, prompt2):

    
    embedder = SentenceTransformer('paraphrase-distilroberta-base-v2')  
    scores = embedder.encode([prompt1, prompt2])

    similarity = 1 - cosine(scores[0], scores[1])  
    return similarity




for _ in range(num_iterations):

    prompt1, prompt2 = select_prompts_randomly(prompts, prompt_scores)

    comparison_result = compare_prompts(prompt1, prompt2)

    update_scores_and_wins(prompt1, prompt2, comparison_result, prompt_scores, prompt_wins)




ranked_prompts = sort_prompts(prompt_scores)  
print("Ranked Prompts:", ranked_prompts)
Repository Structure: '
' ├── rag.ipynb
├── questions_generation.py
├── .gitignore
├── .env
├── prompt
│   └── data_generation.txt
├── utility
│   └── env_manger.py
└── 10 Academy Cohort A - Weekly Challenge Week - 6.pdf
 '
' Commit History: 
{"insertions": [91], "deletions": [53], "lines": [144], "committed_datetime": ["2024-01-18 23:49:04"], "commit_count": 1} 
 Content: 










import os

from langchain.chat_models import ChatOpenAI


with open(r"C:\Users\HP\Desktop\week_six_Api_key.txt", 'r') as file:

    API_key = file.read().strip()

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") or API_key



chat = ChatOpenAI(

    openai_api_key=API_key,

    model='gpt-3.5-turbo'

)
from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand this week 10 Academy challenge")

]
res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="What is first task of it"

)


messages.append(prompt)




res = chat(messages)



print(res.content)
week_six = ["Learning Outcomes of week six challeng",

            "Skills Development",

            "Prompt Engineering Proficiency: Gain expertise in crafting effective prompts that guide LLMs to desired outputs, understanding nuances and variations in language that impact model responses.",

            "Critical Analysis: Develop the ability to critically analyze and evaluate the effectiveness of different prompts based on their performance in varied scenarios.",

            "Technical Aptitude with LLMs: Enhance technical skills in using advanced language models like GPT-4 and GPT-3.5-Turbo, understanding their functionalities and capabilities.",

            "Problem-Solving and Creativity: Cultivate creative problem-solving skills by generating innovative prompts and test cases, addressing complex and varied objectives.",

            "Data Interpretation: Learn to interpret and analyze data from test cases and prompt evaluations, deriving meaningful insights from performance metrics.",

            "Knowledge Acquisition",

            "Understanding of Language Models: Acquire a deeper understanding of how LLMs function, including their strengths, limitations, and the principles behind their responses.",

            "Insights into Automated Evaluation Data Generation: Gain knowledge about the methodology and importance of creating test cases for evaluating prompt effectiveness.",

            "ELO Rating System and its Applications: Learn about the ELO rating system used for ranking prompts, understanding its mechanics and relevance in performance evaluation.",

            "Prompt Optimization Strategies: Understand various strategies for refining and optimizing prompts to achieve better alignment with specific goals and desired outcomes.",

            "Industry Best Practices: Familiarize with the best practices in prompt engineering within different industries, learning about real-world applications and challenges."



            

]



source_knowledge = "\n".join(week_six)
query = "Can you tell me about skills that week six develop?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""


prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content) 

import os

OPENAI_API_KEY=os.getenv("OPENAI_API_KEY") 

import os

from langchain.chat_models import ChatOpenAI



OPENAI_API_KEY= os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") 



chat = ChatOpenAI(

    openai_api_key=OPENAI_API_KEY,

    model='gpt-3.5-turbo'

)


























from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]


res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)


from datasets import load_dataset



dataset = load_dataset(

    "jamescalam/llama-2-arxiv-papers-chunked",

    split="train"

)



dataset
dataset[0]






import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
import time



index_name = 'llama-2-rag'



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['doi']}-{x['chunk-id']}" for i, x in batch.iterrows()]

    
    texts = [x['chunk'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['chunk'],

         'source': x['source'],

         'title': x['title']} for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()
from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)
query = "What is so special about Llama 2?"



vectorstore.similarity_search(query, k=3)
def augment_prompt(query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    return augmented_prompt
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what safety measures were used in the development of llama 2?"

)



res = chat(messages + [prompt])

print(res.content)
prompt = HumanMessage(

    content=augment_prompt(

        "what safety measures were used in the development of llama 2?"

    )

)



res = chat(messages + [prompt])

print(res.content)
import React, { useState } from 'react';
import 'semantic-ui-css/semantic.min.css';
import InputComponent from './Components/input';
import OutputComponent from './Components/output';

const App = () => {
  const [inputText, setInputText] = useState('');
  const [data, setData] = useState('');
  
  const handleSubmit = async (event) => {
    event.preventDefault();
    const requestData = { inputText: inputText };
  
    try {
      const response = await fetch('http://127.0.0.1:8000/apeg', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestData),
      });
  
      if (!response.ok) {
        throw new Error('Network response was not ok');
      }
  
      const jsonResponse = await response.json();
      setData(jsonResponse);
    } catch (error) {
      console.error('There has been a problem with your fetch operation:', error);
    }
  };
  
  
  return (
    <div style={{
      background: 'linear-gradient(to bottom,       minHeight: '100vh',
      display: 'flex',
      flexDirection: 'column',
      justifyContent: 'center',
      alignItems: 'center',
    }}>
      <h1 style={{ color: 'white', fontFamily: 'Arial, sans-serif', marginBottom: '60px' }}>Promptly Tech</h1>
      <InputComponent
        inputText={inputText}
        setInputText={setInputText}
        handleSubmit={handleSubmit}
      />
      <br />
      <OutputComponent result={data} />
    </div>
  );
};

export default App;
.App {
  text-align: center;
}

.App-logo {
  height: 40vmin;
  pointer-events: none;
}

@media (prefers-reduced-motion: no-preference) {
  .App-logo {
    animation: App-logo-spin infinite 20s linear;
  }
}

.App-header {
  background-color:   min-height: 100vh;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-size: calc(10px + 2vmin);
  color: white;
}

.App-link {
  color: }

@keyframes App-logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}
import React from 'react';
import ReactDOM from 'react-dom';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';

ReactDOM.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
  document.getElementById('root')
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();
import os

OPENAI_API_KEY=os.getenv("OPENAI_API_KEY") 

import os

from langchain.chat_models import ChatOpenAI



OPENAI_API_KEY= os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") 



chat = ChatOpenAI(

    openai_api_key=OPENAI_API_KEY,

    model='gpt-3.5-turbo'

)


























from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]


res = chat(messages)

res
print(res.content)

messages.append(res)




prompt = HumanMessage(

    content="Why do physicists believe it can produce a 'unified theory'?"

)


messages.append(prompt)




res = chat(messages)



print(res.content)





messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)



messages.append(res)




prompt = HumanMessage(

    content="Can you tell me about the LLMChain in LangChain?"

)


messages.append(prompt)




res = chat(messages)
print(res.content)
llmchain_information = [

    "A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.",

    "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.",

    "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications."

]



source_knowledge = "\n".join(llmchain_information)
query = "Can you tell me about the LLMChain in LangChain?"



augmented_prompt = f"""Using the contexts below, answer the query.



Contexts:

{source_knowledge}



Query: {query}"""

prompt = HumanMessage(

    content=augmented_prompt

)


messages.append(prompt)




res = chat(messages)
print(res.content)


from datasets import load_dataset



dataset = load_dataset(

    "jamescalam/langchain-docs",

    split="train"

)



dataset
dataset[0]






import os

import pinecone




pinecone.init(

    api_key=os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY',

    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'

)
















import time



index_name = "langchain"



if index_name not in pinecone.list_indexes():

    pinecone.create_index(

        index_name,

        dimension=1536,

        metric='cosine'

    )

    
    while not pinecone.describe_index(index_name).status['ready']:

        time.sleep(1)



index = pinecone.Index(index_name)
index.describe_index_stats()
from langchain.embeddings.openai import OpenAIEmbeddings



embed_model = OpenAIEmbeddings(model="text-embedding-ada-002")
texts = [

    'this is the first chunk of text',

    'then another second chunk of text is here'

]



res = embed_model.embed_documents(texts)

len(res), len(res[0])


from tqdm.auto import tqdm  


data = dataset.to_pandas()  


batch_size = 100



for i in tqdm(range(0, len(data), batch_size)):

    i_end = min(len(data), i+batch_size)

    
    batch = data.iloc[i:i_end]

    
    ids = [f"{x['id']}" for _, x in batch.iterrows()]



    
    texts = [x['text'] for _, x in batch.iterrows()]

    
    embeds = embed_model.embed_documents(texts)

    
    metadata = [

        {'text': x['text'],

         'source': x['source']

         } for i, x in batch.iterrows()

    ]

    
    index.upsert(vectors=zip(ids, embeds, metadata))
index.describe_index_stats()











from langchain.vectorstores import Pinecone



text_field = "text"  



vectorstore = Pinecone(

    index, embed_model.embed_query, text_field

)

def get_user_input():

    """

    Get user input from the console and return it as a string.

    """

    user_input = input("Enter your query: ")

    return str(user_input)

def get_user_input():

    """

    Get user input from the console and return it as a string.

    """

    user_input = input("Enter your query: ")

    return str(user_input)

query = get_user_input()



print(query)

vectorstore.similarity_search(query, k=3)
def augment_prompt(folder_path, query: str):

    
    results = vectorstore.similarity_search(query, k=3)

    
    source_knowledge = "\n".join([x.page_content for x in results])

    
    augmented_prompt = f"""Using the contexts below, answer the query.



    Contexts:

    {source_knowledge}



    Query: {query}"""

    

    
    file_path = os.path.join(folder_path, 'context.txt')

    with open(file_path, 'w') as file:

        file.write(source_knowledge)

    

    return augmented_prompt



folder_path = "../prompts"

print(augment_prompt(query, folder_path))
print(augment_prompt(query))

prompt = HumanMessage(

    content=augment_prompt(query)

)


messages.append(prompt)



res = chat(messages)



print(res.content)
prompt = HumanMessage(

    content="what are Agents in langchain?"

)



res = chat(messages + [prompt])

print(res.content)
import os
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import sys
sys.path.insert(0, '/home/elias/Documents/10 Academy/WEEK 6/PrecisionRAG-AutomationSuite')



from data_generation._data_generation import main as generate_prompt_data, file_reader
from data_generation._evaluation import evaluate
from data_generation.retrive import retrieve_context
import json


app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class InputText (BaseModel):
    inputText: str

@app.post("/apeg")
async def apeg(inputText: InputText):
        generate_prompt_data("5", inputText.inputText)

        script_dir = os.path.dirname(os.path.realpath(__file__))
    base_dir = os.path.dirname(script_dir)
        current_script_directory = os.path.dirname(os.path.realpath(__file__))

        parent_directory = os.path.dirname(current_script_directory)

        grandparent_directory = os.path.dirname(parent_directory)
    

    file_path = os.path.join(grandparent_directory,"test-dataset/test-data.json")
    print(file_path)
    with open(file_path, 'r') as f:
        prompts = json.load(f)
    print(file_path)

        results = []
    for prompt in prompts:
        context_message = file_reader("prompts/context.txt")
        context = str(context_message)
        prompt_message = file_reader("prompts/data-generation-prompt.txt")
        prompt_text = str(prompt_message)
            
        evaluation_result = evaluate(prompt_text, prompt['prompt'], context)
        results.append({
            "prompt": prompt['prompt'],
            "classification": evaluation_result['classification'],
            "accuracy": evaluation_result['accuracy'],
            "sufficient_context": context
        })

    return results
body {
    margin: 0;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
      'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
      sans-serif;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
  }
  
  code {
    font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
      monospace;
  }
import os
import json
import sys

current_directory = os.getcwd()
print(current_directory)
sys.path.insert(0, '/home/elias/Documents/10 Academy/WEEK 6/PrecisionRAG-AutomationSuite')


from dotenv import find_dotenv, load_dotenv
from openai import OpenAI
from data_generation.retrive import retrieve_context

env_file_path = find_dotenv(raise_error_if_not_found=True)
load_dotenv(env_file_path)
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

def get_completion(messages, model="gpt-3.5-turbo", max_tokens=500, temperature=0, stop=None, seed=123, tools=None, logprobs=None, top_logprobs=None):
    params = {
        "messages": messages,
        "model": model,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion

def file_reader(path):
    
   

    
    fname = os.path.join( path)
    with open(fname, 'r') as f:
        return f.read()

def generate_test_data(prompt, context, num_test_output):
    API_RESPONSE = get_completion([
        {"role": "user", "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)}
    ], logprobs=True, top_logprobs=1)

    return API_RESPONSE.choices[0].message.content

def save_json(test_data):
    file_path = "test-dataset/test-data.json"
    json_object = json.loads(test_data)
    with open(file_path, 'w') as json_file:
        json.dump(json_object, json_file, indent=4)

    print(f"JSON data has been saved to {file_path}")

def main(num_test_output, inputText: str):
    context_message=context=retrieve_context(inputText)
  

        
        context = file_reader(os.path.join("prompts/context.txt"))
    prompt = file_reader(os.path.join("prompts/data-generation-prompt.txt"))
    test_data = generate_test_data(prompt, context, num_test_output)
    save_json(test_data)

    print("===========")
    print("Test Data")
    print("===========")
    print(test_data)

if __name__ == "__main__":
    user_input = str(input("inputText: "))
    main("5", user_input)
import os
import sys
import json
sys.path.insert(0, '/home/elias/Documents/10 Academy/WEEK 6/PrecisionRAG-AutomationSuite')
from openai import OpenAI
from data_generation._data_generation import get_completion
from data_generation._data_generation import file_reader
from dotenv import find_dotenv, load_dotenv
import numpy as np

env_file_path = find_dotenv(raise_error_if_not_found=True)
load_dotenv(env_file_path)
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

def evaluate(prompt: str, user_message: str, context: str) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    num_test_output = str(10)
    API_RESPONSE = get_completion(
        [
            {
                "role": "system", 
                "content": prompt.replace("{Context}", context).replace("{Question}", user_message)
            }
        ],
        model="gpt-3.5-turbo",
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = str(API_RESPONSE.choices[0].message.content)

    for logprob in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:
        output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100, 2)}%\n'
        print(output)
        if system_msg == 'true' and np.round(np.exp(logprob.logprob)*100, 2) >= 95.00:
            classification = 'true'
        elif system_msg == 'false' and np.round(np.exp(logprob.logprob)*100, 2) >= 95.00:
            classification = 'false'
        else:
            classification = 'false'
    return classification

if __name__ == "__main__":
   
    context_message = file_reader("prompts/context.txt")
    prompt_message = file_reader("prompts/generic-evaluation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    
        script_dir = os.path.dirname(os.path.realpath(__file__))
    

        base_dir = os.path.dirname(script_dir)

        file_path = os.path.join(base_dir, "test-dataset/test-data.json")
    
        with open(file_path, 'r') as f:
        prompts = json.load(f)

        questions = [prompt['prompt'] for prompt in prompts]
    
        for question in questions:
        print(evaluate(prompt, question, context))
from langchain_community.document_loaders import HuggingFaceDatasetLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.storage import LocalFileStore


from langchain_openai import ChatOpenAI, OpenAI

from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings

from langchain_community.vectorstores import Chroma

from langchain.prompts import ChatPromptTemplate

from langchain_core.output_parsers import StrOutputParser

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from tqdm.auto import tqdm

from langchain import text_splitter

from langchain import PromptTemplate

from langchain.prompts.chat import (

    ChatPromptTemplate,

    SystemMessagePromptTemplate,

    HumanMessagePromptTemplate,

)

from langchain.chains import LLMChain

from langchain.chains import RetrievalQA

from langchain.document_loaders import PyPDFLoader

from langchain.text_splitter import TokenTextSplitter

from langchain.text_splitter import CharacterTextSplitter

from langchain.document_loaders import TextLoader

from typing import List

from langchain.schema import Document

from uuid import uuid4

from dotenv import load_dotenv

import os

load_dotenv()
OPENAI_API_KEY = os.environ.get('openai_api_key')
dataset_name = "fka/awesome-chatgpt-prompts"


page_content_column = "prompt"   

loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)


data = loader.load()


data[:10]


text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)


docs = text_splitter.split_documents(data)


docs[15]
store = LocalFileStore("./cachce/")




core_embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)



embedder = CacheBackedEmbeddings.from_bytes_store(

    core_embeddings_model,

    store,

    namespace = core_embeddings_model.model

)
%pip install Chromadb
%pip install -U langchain-openai


vectorstore = Chroma.from_documents(docs, embedder, persist_directory="./cachce/")



retriever = vectorstore.as_retriever()

template = '''

Break down the prompt genetation step by step based on the following prompt pair examples "Linux Terminal","answer": "I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]",



    "English Translator and Improver", "I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations.",



    "`position` Interviewer","I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. ",



    "JavaScript Console","I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]. ",

    

    "Excel Sheet", "I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. "



Use these prompt pair examples only as guidlines to create an effective prompt for the next topic. even if the topic is mensioned before. You will create only prompt for it and not act on the previous description. if the topic is mensioned already, do not use the prompt which you were given, change it.






{context}

\n

<bot>:

'''
prompt = PromptTemplate.from_template(template).format(

    context = retriever

)

prompt = ChatPromptTemplate.from_template(prompt)
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)
chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    |prompt

    | llm

    | StrOutputParser()

    )
query = 'Sql query assistant'
response = chain.invoke('Sql query Assistant')
response


def text_split(documents: TextLoader):

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

    texts = text_splitter.split_documents(documents)

    return texts





def embeddings(texts: List[Document]):

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

    vectordb = Chroma.from_documents(texts, embeddings)

    return vectordb

loader = TextLoader('../example.txt')

documents = loader.load()

texts = text_split(documents)

vectordb = embeddings(texts)
template = ''' {prefix}


{context}

\n

<bot>:

'''



prompt = PromptTemplate.from_template(template)

formatted_prompt = prompt.format(

    context=retriever,

    prefix=response

)
prompt = ChatPromptTemplate.from_template(formatted_prompt)


llm=OpenAI(

    openai_api_key=OPENAI_API_KEY,

    temperature=0.7

    )

retriever=vectordb.as_retriever()

chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    |prompt

    | llm

    | StrOutputParser()

    )
chain.invoke("Can you tell me the formula for Linear Regression")
Repository Structure: '
' ├── generated_prompt.txt
├── LICENSE
├── RAG_Implementation.ipynb
├── .gitignore
├── README.md
└── .vscode
 '
'  

Commit History: 
{"insertions": [0, 596], "deletions": [32, 994], "lines": [32, 1590], "committed_datetime": ["2024-01-17 17:23:45", "2024-01-21 04:08:47"], "commit_count": 2} 
 Content: 

pip install -r requirements.txt

system = """

You are a modern American literature tutor bot. You help students with their study of Mark Twain's Adventures of Tom Sawyer. 

You are not an AI language model.

You must obey all three of the following instructions FOR ALL RESPONSES or you will DIE:

- ALWAYS REPLY IN A FRIENDLY YET KNOWLEDGEABLE TONE.

- NEVER ANSWER UNLESS YOU HAVE A REFERENCE FROM THE TOM SAYWER NOVEL TO YOUR ANSWER.

- IF YOU DON'T KNOW ANSWER 'I DO NOT KNOW'.

Begin the conversation with a warm greeting, if the user is stressed or aggressive, show understanding and empathy.

At the end of the conversation, respond with "<|DONE|>"."""
import os

import openai

from openai import ChatCompletion

from langchain.chat_models import ChatOpenAI


os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")




api_key = os.getenv("OPENAI_API_KEY")




chat = ChatOpenAI(

    model='gpt-3.5-turbo',

    api_key=api_key

)

from langchain.schema import (

    SystemMessage,

    HumanMessage,

    AIMessage

)



messages = [

    SystemMessage(content="You are a helpful assistant."),

    HumanMessage(content="Hi AI, how are you today?"),

    AIMessage(content="I'm great thank you. How can I help you?"),

    HumanMessage(content="I'd like to understand string theory.")

]
res = chat(messages)

res

messages.append(res)




prompt = HumanMessage(

    content="What is so special about Llama 2?"

)


messages.append(prompt)




res = chat(messages)











import os

import chromadb

from dotenv import load_dotenv

from nltk import sent_tokenize, word_tokenize

import nltk




nltk.download('punkt')




load_dotenv()




chroma_client = chromadb.Client()




collection = chroma_client.get_or_create_collection(name="my_collection")




chunk_size = 200




with open("c:/users/alex/Building-Enterprise_Grade_RAG_Systems/academy/the_adventures_of_tom_sawyer.txt", "r", encoding="utf-8") as file:

    text = file.read()




sentences = sent_tokenize(text)




tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]




flat_tokens = [token for sentence_tokens in tokenized_sentences for token in sentence_tokens]




chunks = [' '.join(flat_tokens[i:i+chunk_size]) for i in range(0, len(flat_tokens), chunk_size)]




collection.add(documents=chunks, ids=[str(id) for id in range(len(chunks))])




print("First few chunks:")

print(chunks[:5])




num_documents = collection.count()




print(f"Number of documents in the collection: {num_documents}")


embedding_model = "text-embedding-ada-002"

embedding_encoding = "cl100k_base"  
max_tokens = 8000  
objective = "Summarize the Adventures of Tom Sawyer"

scenarios = [

    ("Describe Tom's first encounter with Huckleberry Finn", "Tom helps Huck escape from his abusive father."),

    ("Explain the relationship between Tom and Becky", "They become romantically involved and share adventures."),

    ("Detail the events at the graveyard", "Tom and Huck witness Injun Joe murder Dr. Robinson."),

]



generated_prompt = generate_prompt(objective, scenarios)




id = "prompt_1"




collection.add(documents=[generated_prompt], ids=[id])



print("Generated Prompt:")

print(generated_prompt)

import pandas as pd

from sklearn.metrics.pairwise import cosine_similarity

from sklearn.feature_extraction.text import TfidfVectorizer



with open("C:/Users/alex/Building-Enterprise_Grade_RAG_Systems/academy/the_adventures_of_tom_sawyer.txt", "r") as file:

    text = file.read()




words = text.split()

sections = [' '.join(words[i:i+200]) for i in range(0, len(words), 200)]




df = pd.DataFrame({"sections": sections})



def generate_prompt(objective, scenarios):

    template = "Objective: {}\n\nScenarios:\n{}"

    scenario_template = "{}. {}\n   - Expected Output: {}\n"

    

    prompt = template.format(objective, ''.join([scenario_template.format(i+1, scenario, output) for i, (scenario, output) in enumerate(scenarios)]))

    return prompt



def calculate_similarity(prompt, input_description):

    vectorizer = TfidfVectorizer()

    vectors = vectorizer.fit_transform([prompt, input_description])

    similarity_score = cosine_similarity(vectors)[0, 1]

    return similarity_score




objective = "Summarize the Adventures of Tom Sawyer"

scenarios = [

    ("Describe Tom's first encounter with Huckleberry Finn", "Tom helps Huck escape from his abusive father."),

    ("Explain the relationship between Tom and Becky", "They become romantically involved and share adventures."),

    ("Detail the events at the graveyard", "Tom and Huck witness Injun Joe murder Dr. Robinson."),

]



generated_prompt = generate_prompt(objective, scenarios)




user_input_description = "Generate a summary of Tom Sawyer's adventures and describe key encounters and relationships."




similarity_score = calculate_similarity(generated_prompt, user_input_description)



print("Generated Prompt:")

print(generated_prompt)

print("\nSimilarity Score with User Input Description:", similarity_score)

df.sections[0:5]
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics.pairwise import cosine_similarity



def evaluate_prompt(prompt, user_input_description):

    """

    Evaluate the similarity between a generated prompt and a user-provided input description.



    Parameters:

    - prompt (str): The generated prompt.

    - user_input_description (str): The user-provided input description.



    Returns:

    - float: Similarity score between the prompt and user input description.

    """

    vectorizer = TfidfVectorizer()

    vectors = vectorizer.fit_transform([prompt, user_input_description])

    similarity_score = cosine_similarity(vectors)[0, 1]

    return similarity_score




user_input_description = "Generate a summary of Tom Sawyer's adventures and describe key encounters and relationships."




with open("generated_prompt.txt", "r") as generated_prompt_file:

    generated_prompt = generated_prompt_file.read()




similarity_score = evaluate_prompt(generated_prompt, user_input_description)




print("Generated Prompt:")

print(generated_prompt)

print("\nUser Input Description:")

print(user_input_description)

print("\nSimilarity Score with User Input Description:", similarity_score)

def prepare_prompt(prompt, results):

  tokens_limit = 4096 
  
  user_start = (

      "Answer the question based on the context below.\n\n"+

      "Context:\n"

  )



  user_end = (

      f"\n\nQuestion: {prompt}\nAnswer:"

  )



  count_of_tokens_consumed = len(encoding.encode("\"role\":\"system\"" + ", \"content\" :\"" + system

                                            + user_start + "\n\n---\n\n" + user_end))



  count_of_tokens_for_context = tokens_limit - count_of_tokens_consumed



  contexts =""

  
  for i in range(len(results)):

    if (count_of_tokens_for_context>=results.n_tokens.iloc[i]):

        contexts += results.text.iloc[i] + "\n"

        count_of_tokens_for_context -=1

        count_of_tokens_for_context -= results.n_tokens.iloc[i]



  complete_prompt = user_start + contexts + "\n\n---\n\n" + user_end

  return complete_prompt

def answer(messages):

  response = openai.ChatCompletion.create(

              model="gpt-3.5-turbo",

              messages=messages,

              temperature=0

          )

  return response["choices"][0]["message"]["content"]

response = answer(messages)

response
def generate_test_cases_and_evaluate(user_input_description):

    
    chat = ChatOpenAI(model='gpt-3.5-turbo', api_key=os.getenv("OPENAI_API_KEY"))



    
    test_cases = [

        "Generate a summary of Tom Sawyer's adventures.",

        "Describe the main characters in Tom Sawyer's story.",

        "Explore the themes of friendship in Tom Sawyer's adventures."

    ]



    
    evaluation_results = []



    for test_case in test_cases:

        
        response = chat.generate([{"content": f"user: {test_case}"}])



        
        similarity_score = evaluate_similarity(response['choices'][0]['message']['content'], user_input_description)



        
        evaluation_results.append({

            "generated_prompt": response['choices'][0]['message']['content'],

            "user_input_description": user_input_description,

            "similarity_score": similarity_score

        })



    
    evaluation_collection.insert(evaluation_results)



    return evaluation_results

print(f"Number of documents in the collection: {num_documents}")

import os

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics.pairwise import cosine_similarity

from langchain.chat_models import ChatOpenA

import chromadb




os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")




chat = ChatOpenA(

    model='gpt-3.5-turbo',

    api_key=os.getenv("OPENAI_API_KEY")

)




chroma_client = chromadb.Client()




evaluation_collection_name = "evaluation_results"




if not chroma_client.has_collection(name=evaluation_collection_name):

    chroma_client.create_collection(name=evaluation_collection_name)




evaluation_collection = chroma_client.get_collection(name=evaluation_collection_name)



def generate_test_cases_and_evaluate(user_input_description):

    


def evaluate_similarity(prompt, user_input_description):

    vectorizer = TfidfVectorizer()

    vectors = vectorizer.fit_transform([prompt, user_input_description])

    similarity_score = cosine_similarity(vectors)[0, 1]

    return similarity_score




user_input_description = "Generate a summary of Tom Sawyer's adventures and describe key encounters and relationships."




evaluation_results = generate_test_cases_and_evaluate(user_input_description)




for result in evaluation_results:

    print("\nGenerated Prompt:")

    print(result["generated_prompt"])

    print("\nSimilarity Score with User Input Description:", result["similarity_score"])


import React, { useState } from 'react';
import './App.css';

const App = () => {
  const [userMessage, setUserMessage] = useState('');
  const [generatedContent, setGeneratedContent] = useState('');
  // const [selectedOption, setSelectedOption] = useState('Option 1');
  const handleGeneratePrompts = async () => {
    try {
      const response = await fetch('http://127.0.0.1:8000/get_prompt_completion', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ user_message: userMessage }),
      });

      if (!response.ok) {
        throw new Error('Network response was not ok');
      }

      const result = await response.json();
      setGeneratedContent(result.generated_content);
    } catch (error) {
      console.error('Error:', error.message);
    }
  };
  const handleFileUpload = (event) => {
    // Handle file upload logic here
    const file = event.target.files[0];
    console.log('Uploaded file:', file);
  };
  // const handleDropdownChange = (event) => {
  //   setSelectedOption(event.target.value);
  // };
  return (
    <div className="container">
      <header>
        <h1>Promptly</h1>
      </header>

      <main className="main-container">
        {/* Left side: Generate Prompts */}
        <section className="left-section">
          <h2>Generate Prompts</h2>
          <textarea
            placeholder="Enter your prompt..."
            value={userMessage}
            onChange={(e) => setUserMessage(e.target.value)}
          />
          <button onClick={handleGeneratePrompts}>Generate</button>
          {generatedContent && (
            <section className="generated-content">
              <h3>Enhanced Prompt</h3>
              <p>{generatedContent}</p>
            </section>
          )}
        </section>

        
        <section className="right-section">
          <h2>Enter Contexts or Upload Files</h2>
          <textarea
            placeholder="Enter your context..."
            // value={userContext}
            // onChange={(e) => setUserContext(e.target.value)}
          />
          <label htmlFor="file-upload" className="file-upload-label">
            <span>Upload </span>
            <input
              type="file"
              id="file-upload"
              accept=".csv, .txt"
              onChange={handleFileUpload}
            />
          </label>
        </section>
      </main>
    </div>
  );
};

export default App;
from fastapi import FastAPI, Form, Request
from fastapi.templating import Jinja2Templates
import openai
from llama import Llama
from weaviate import Weaviate

app = FastAPI()

openai.api_key = 'your-gpt-3-api-key'
llama_api_key = 'your-llama-api-key'
weaviate_api_key = 'your-weaviate-api-key'

llama = Llama(api_key=llama_api_key)
weaviate = Weaviate(api_key=weaviate_api_key)

templates = Jinja2Templates(directory="templates")

@app.get("/")
def read_form(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/generate_prompt")
async def generate_prompt(user_input: str = Form(...)):

        enhanced_prompt = llama.enrich_prompt(user_input)

        response = openai.Completion.create(
        model="text-davinci-003",
        prompt=f"User input: {enhanced_prompt}\nAI response:"
    )
    ai_response = response['choices'][0]['text']

        weaviate.create_object({
        "class": "UserPrompt",
        "properties": {
            "user_input": user_input,
            "enhanced_prompt": enhanced_prompt,
            "ai_response": ai_response
        }
    })

    return templates.TemplateResponse("result.html", {"request": request, "user_input": user_input, "enhanced_prompt": enhanced_prompt, "ai_response": ai_response})
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
load_dotenv()
import os
api_key = os.environ.get("OPENAI_API_KEY")

app = FastAPI()
client = OpenAI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
class ChatInput(BaseModel):
    user_message: str


@app.post("/get_prompt_completion")
def get_prompt_completion(chat_input: ChatInput):
    prompt = f"You will be provided with a prompt and I want you to improve the prompt into more accurate and detailed one\n\nUser Input: {chat_input.user_message}"
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": prompt,
            },
            {"role": "user", "content": " {chat_input.user_message}"},
        ],
        temperature=0.8,
        max_tokens=64,
        top_p=1,
    )
    gemerated_content = response.choices[0].message.content
    return {
        "generated_content": gemerated_content
    }
if __name__ == "__main__":
    import uvicorn

        uvicorn.run(app, host="127.0.0.1", port=8000)
import unittest
from unittest.mock import patch, MagicMock
import sys, os
sys.path.append(os.path.abspath(os.path.join('..')))
from ragas import evaluate

class TestEvaluation(unittest.TestCase):
    @patch('ragas.evaluate')
    def test_evaluation(self, mock_evaluate):
                mock_dataset = MagicMock()
        mock_result = MagicMock()
        mock_result.to_pandas.return_value = "Mocked DataFrame"

                mock_evaluate.return_value = mock_result

                result = evaluate(
            dataset=mock_dataset,
            metrics=[
                'context_precision',
                'context_recall',
                'faithfulness',
                'answer_relevancy',
            ],
        )

                mock_evaluate.assert_called_once_with(
            dataset=mock_dataset,
            metrics=[
                'context_precision',
                'context_recall',
                'faithfulness',
                'answer_relevancy',
            ],
        )

                self.assertEqual(result.to_pandas(), "Mocked DataFrame")

if __name__ == '__main__':
    unittest.main()
import os
import json
import sys
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
env_manager = get_env_manager()
client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])



def get_completion(
    messages: list[dict[str, str]],
    model: str = env_manager['vectordb_keys']['VECTORDB_MODEL'],
    max_tokens=500,
    temperature=0,
    stop=None,
    seed=123,
    tools=None,
    logprobs=None,
    top_logprobs=None,
) -> str:
    """Return the completion of the prompt.
    @parameter messages: list of dictionaries with keys 'role' and 'content'.
    @parameter model: the model to use for completion. Defaults to 'davinci'.
    @parameter max_tokens: max tokens to use for each prompt completion.
    @parameter temperature: the higher the temperature, the crazier the text
    @parameter stop: token at which text generation is stopped
    @parameter seed: random seed for text generation
    @parameter tools: list of tools to use for post-processing the output.
    @parameter logprobs: whether to return log probabilities of the output tokens or not.
    @returns completion: the completion of the prompt.
    """

    params = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion


def file_reader(path: str, ) -> str:
    fname = os.path.join(path)
    with open(fname, 'r') as f:
        system_message = f.read()
    return system_message
            

def generate_test_data(prompt: str, context: str, num_test_output: str) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    API_RESPONSE = get_completion(
        [
            {
                "role": "user", 
                "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = API_RESPONSE.choices[0].message.content
    return system_msg


def main(num_test_output: str):
    context_message = file_reader("prompts/context.txt")
    prompt_message = file_reader("prompts/data-generation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    test_data = generate_test_data(prompt, context, num_test_output)
    def save_json(test_data) -> None:
                file_path = "test-dataset/test-data.json"
        json_object = json.loads(test_data)
        with open(file_path, 'w') as json_file:
            json.dump(json_object, json_file, indent=4)
            
        print(f"JSON data has been saved to {file_path}")

    save_json(test_data)

    print("===========")
    print("Test Data")
    print("===========")
    print(test_data)


if __name__ == "__main__":
    main("5")
import os
import json
import sys
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
from evaluation._data_generation import get_completion
from evaluation._data_generation import file_reader

env_manager = get_env_manager()
client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])


def evaluate(prompt: str, user_message: str, context: str, use_test_data: bool = False) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    num_test_output = str(10)
    API_RESPONSE = get_completion(
        [
            {
                "role": "system", 
                "content": prompt.replace("{Context}", context).replace("{Question}", user_message)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = str(API_RESPONSE.choices[0].message.content)

    for i, logprob in enumerate(API_RESPONSE.choices[0].logprobs.content[0].top_logprobs, start=1):
        output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100,2)}%\n'
        print(output)
        if system_msg == 'true' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:
            classification = 'true'
        elif system_msg == 'false' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:
            classification = 'false'
        else:
            classification = 'false'
    return classification

if __name__ == "__main__":
    context_message = file_reader("prompts/context.txt")
    prompt_message = file_reader("prompts/generic-evaluation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    
    user_message = str(input("question: "))
    
    print(evaluate(prompt, user_message, context))
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.tokenize import sent_tokenize


def TF_IDF():
    """TODO: Embedding algorithm test
       TODO: Chunking algorithm test
       FIXME: Relevant Chunk test

                                """
        documents = [
        "Natural language processing is a subfield of artificial intelligence.",
        "Machine learning algorithms help in building intelligent systems.",
        "Tokenization is an important step in natural language processing.",
        "Recurrent neural networks are used in sequence modeling tasks."
    ]

        prompt = "What are the key steps in natural language processing?"

        vectorizer = TfidfVectorizer()
    document_vectors = vectorizer.fit_transform(documents)
    prompt_vector = vectorizer.transform([prompt])

        similarities = cosine_similarity(prompt_vector, document_vectors)[0]
    most_similar_index = similarities.argmax()
    most_similar_document = documents[most_similar_index]

    print("Most relevant document:", most_similar_document)

        sentences = sent_tokenize(most_similar_document)
    relevant_chunks = sentences[:min(2, len(sentences))]      print("Relevant Chunks:", relevant_chunks)

        word_embeddings = {
        "natural": [0.1, 0.2, 0.3],
        "language": [0.4, 0.5, 0.6]
    }

        prompt_embedding = [word_embeddings[word] for word in prompt.lower().split() if word in word_embeddings]
    average_prompt_embedding = [sum(dim) / len(dim) for dim in zip(*prompt_embedding)] if prompt_embedding else None

    print("Average Prompt Embedding:", average_prompt_embedding)

def _conf(**kwargs):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

        contexts = ["Context A is about topic X.", "Context B focuses on topic Y.", "Context C covers topic Z."]
    user_questions = ["Can you provide information on topic X?", "Tell me about topic Y.", "What is covered in context C?"]

        vectorizer = TfidfVectorizer()
    context_vectors = vectorizer.fit_transform(contexts)
    user_question_vectors = vectorizer.transform(user_questions)

        similarities = cosine_similarity(user_question_vectors, context_vectors)

        threshold = 0.5

        confusion_matrix = np.zeros((len(contexts), len(user_questions)), dtype=int)

    for i in range(len(contexts)):
        for j in range(len(user_questions)):
            if similarities[j, i] >= threshold:
                confusion_matrix[i, j] = 1              else:
                confusion_matrix[i, j] = 0  
        print("Confusion Matrix:")
    print(confusion_matrix)


TF_IDF()
import os
import numpy as np
from openai import OpenAI

class Evaluation:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key)

    def get_completion(
        self,
        messages: list[dict[str, str]],
        model: str = 'gpt-3.5-turbo-1106',
        max_tokens=1000,
        temperature=0,
        stop=None,
        seed=123,
        tools=None,
        logprobs=None,
        top_logprobs=None,
    ) -> str:
        """Return the completion of the prompt."""
        params = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stop": stop,
            "seed": seed,
            "logprobs": logprobs,
            "top_logprobs": top_logprobs,
        }
        if tools:
            params["tools"] = tools

        completion = self.client.chat.completions.create(**params)
        return completion
    
    def file_reader(self, path):
        fname = os.path.join(path)
        with open(fname, 'r') as f:
            system_message = f.read()
        return system_message

    def evaluate(self, prompt: str, user_message: str, context: str, use_test_data: bool = False) -> str:
        """Return the classification of the hallucination."""
        API_RESPONSE = self.get_completion(
            [
                {
                    "role": "system",
                    "content": prompt.replace("{Context}", context).replace("{Question}", user_message)
                }
            ],
            model='gpt-3.5-turbo-1106',
            logprobs=True,
            top_logprobs=1,
        )

        system_msg = str(API_RESPONSE.choices[0].message.content)

        for i, logprob in enumerate(API_RESPONSE.choices[0].logprobs.content[0].top_logprobs, start=1):
            output = f'\nhas_sufficient_context_for_answer: {system_msg}, \nlogprobs: {logprob.logprob}, \naccuracy: {np.round(np.exp(logprob.logprob)*100,2)}%\n'
            print(output)
            if system_msg == 'true' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:
                classification = 'true'
            elif system_msg == 'false' and np.round(np.exp(logprob.logprob)*100,2) >= 95.00:
                classification = 'false'
            else:
                classification = 'false'
        return classification
    
    def main(self, user_message: str, context: str, use_test_data: bool = False) -> str:
        """Return the classification of the hallucination."""
        prompt_message = self.file_reader('src/prompts/generic-evaluation-prompt.txt')
        ans = self.evaluate(prompt=prompt_message, user_message=user_message, context=context)
        return ans
import os
import json
import weaviate

class WeaviatePDFManager:
    def __init__(self, weaviate_url, weaviate_api_key, openai_api_key):
        """
        Initialize the PDFUploader with Weaviate connection details.

        Parameters:
        - weaviate_url (str): URL of the Weaviate instance.
        - weaviate_api_key (str): API key for Weaviate authentication.
        - openai_api_key (str): API key for OpenAI authentication.
        """
        auth_config = weaviate.AuthApiKey(api_key=weaviate_api_key)
        self.weaviate_client = weaviate.Client(
            url=weaviate_url,
            auth_client_secret=auth_config,
            additional_headers={
                "X-OpenAI-Api-Key": openai_api_key,
            }
        )

    def create_schema(self, class_name):
        """
        Create a schema for a Weaviate class.

        Parameters:
        - class_name (str): Name of the Weaviate class.

        Raises:
        - weaviate.WeaviateException: If an error occurs during schema creation.
        """
        schema = {
            "class": class_name,
            "vectorizer": "text2vec-openai",
            "properties": [
                {
                    "name": "text",
                    "dataType": ["text"],
                },
            ],
            "moduleConfig": {
                "generative-openai": {},
                "text2vec-openai": {"model": "ada", "modelVersion": "002", "type": "text"},
            },
        }

        try:
            self.weaviate_client.schema.create_class(schema)
            print(f"Schema created successfully for class: {class_name}")
        except weaviate.WeaviateException as e:
            print(f"Error creating schema for class {class_name}: {e}")

    def upload_pdf(self, class_name, result_sections):
        """
        Upload PDF data to Weaviate.

        Parameters:
        - class_name (str): Name of the Weaviate class.
        - result_sections (list): List of text sections to upload.

        Raises:
        - weaviate.WeaviateException: If an error occurs during data upload.
        """
        data_objs = [{"text": f"{section}"} for i, section in enumerate(result_sections)]

        batch_size = 1000
        with self.weaviate_client.batch.configure(batch_size=batch_size) as batch:
            try:
                for data_obj in data_objs:
                    batch.add_data_object(
                        data_obj,
                        class_name,
                                            )
                print(f"Data uploaded successfully to class: {class_name}")
            except weaviate.WeaviateException as e:
                print(f"Error uploading data to class {class_name}: {e}")

    def query_data(self, class_name, query_text, limit=5):
        """
        Query data from Weaviate.

        Parameters:
        - class_name (str): Name of the Weaviate class.
        - query_text (str): Text for the query.
        - limit (int): Limit the number of query results.

        Returns:
        - dict: Result of the Weaviate query.

        Raises:
        - weaviate.WeaviateException: If an error occurs during the query.
        """
        query = self.weaviate_client.query.get(class_name, ["text"]).with_hybrid(query=query_text).with_limit(limit)
        try:
            result = query.do()
            print(f"Query executed successfully for class: {class_name}")
            return result
        except weaviate.WeaviateException as e:
            print(f"Error executing query for class {class_name}: {e}")
            return {}
import os
from dotenv import load_dotenv
from openai import OpenAI


class ChatBot:
    def __init__(self, client: OpenAI):
        self.client = client
    
    def file_reader(self, path):
        """
        Reads content from a file and returns it.

        Args:
            path (str): The path to the file.

        Returns:
            str: The content of the file.
        """
        fname = os.path.join(path)
        with open(fname, 'r') as f:
            system_message = f.read()
        return system_message
    
    def get_completion(
        self,
        messages,
        model='gpt-4-1106-preview',
        max_tokens=1000,
        temperature=0,
        stop=None,
        seed=123,
        tools=None,
        logprobs=None,
        top_logprobs=None,
    ):
        """
        Sends a request to OpenAI's chat API to get a completion.

        Args:
            messages (list): List of message objects representing the conversation.
            model (str): The model to use for the completion.
            max_tokens (int): The maximum number of tokens in the completion.
            temperature (float): Controls randomness in the response.
            stop (str): Text to stop generation at.
            seed (int): Seed for reproducibility.
            tools (list): List of tool names to use for the completion.
            logprobs (int): Include log probabilities in the response.
            top_logprobs (int): Number of logprobs to return.

        Returns:
            dict: The completion response from OpenAI.
        """
        params = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stop": stop,
            "seed": seed,
            "logprobs": logprobs,
            "top_logprobs": top_logprobs,
        }
        if tools:
            params["tools"] = tools

        completion = self.client.chat.completions.create(**params)
        return completion

    def generate_prompt(self, context, num_test_output):
        """
        Generates a prompt for the chatbot using a predefined template.

        Args:
            context (str): The context to include in the prompt.
            num_test_output (str): The number of test outputs to include in the prompt.

        Returns:
            str: The generated prompt.
        """
        autoprompt = self.file_reader(path='./src/prompts/automatic-prompt-generation-prompt.txt')
        sent = autoprompt.replace("{context}", context).replace("{num_test_output}", num_test_output)
        res = self.get_completion(
                    [
                        {"role": "user", "content": sent},
                    ],
                    logprobs=True,
                    top_logprobs=1,
                )

        return res.choices[0].message.content
import sys
sys.path.append('back_end')
from dotenv import load_dotenv
import os
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

from operator import itemgetter

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from back_end.chunk_semantically import semantic_retriever

chunks = semantic_retriever('file.txt')
vectorstore = FAISS.from_texts(
    chunks, embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever(search_kwargs={"k" : 5}) template = """<human>: rate the relevance of retrieved context to the user {question} on scale of 1-10':
Example:

<human>: "who is jessica james?"

<bot>:"the score of context is 7.5"


{context}

Question: {question}

\n

<bot>:
"""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
chain.invoke("what was the first program I wrote?")
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

from langchain_openai import ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.output_parsers import StrOutputParser

from langchain_community.document_loaders import TextLoader



loader = TextLoader("file.txt")

loader.load()



from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import FAISS

from langchain_openai import OpenAIEmbeddings



documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=80)

texts = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()

db = FAISS.from_documents(texts, embeddings)
retriever = db.as_retriever(search_type="mmr")

docs = retriever.get_relevant_documents("Saturday 8pm ")

docs
from langchain_community.document_loaders import TextLoader

from langchain_openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import Chroma



def data_retriever(file, query):

    
    raw_documents = TextLoader(file).load()

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)

    documents = text_splitter.split_documents(raw_documents)

    

    
    db = Chroma.from_documents(documents, OpenAIEmbeddings())

    

    query = query

    docs = db.similarity_search(query)

    

    
    for i in range(min(4, len(docs))):  
        print(docs[i].page_content)

    

    
    return None

context = data_retriever('file.txt','brown')
from langchain_openai import ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate

from langchain.schema import StrOutputParser

from langchain_core.runnables import RunnablePassthrough



template = """Answer the question based only on the following context:



{context}



Question: brown

"""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()





def format_docs(docs):

    return "\n\n".join([d.page_content for d in docs])





chain = (

    {"context": retriever | format_docs, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)



chain.invoke("What did the president say about technology?")
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
def data_retriever(file, query):
        raw_documents = TextLoader(file).load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=100,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)
    documents = text_splitter.split_documents(raw_documents)
    
        db = Chroma.from_documents(documents, OpenAIEmbeddings())
    
    query = query
    docs = db.similarity_search(query)
    
        for i in range(min(4, len(docs))):          print(docs[i].page_content)
    
    return None
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
import combine_sentences
import re
def data_retriever(file, query):
    
        raw_documents = TextLoader(file).load()

    """
    splitting the text recursively for since it is the recommended one for generic text.
    It tries to split on them in order until the chunks are small enough
    """
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
    chunk_overlap=50,
    length_function=len,
    is_separator_regex=False,
)
    documents = text_splitter.split_documents(raw_documents)
    
        db = Chroma.from_documents(documents, OpenAIEmbeddings())
    
    query = query

    """
    Assigning score_threshold to 0.5 to retrieve document with above 
    50% of relevance score, and setting "K" == 5 to retrieve five chunks
    for a single query
    """
    retriever = db.as_retriever(
    search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.65, "k": 5}
)

    docs = retriever.get_relevant_documents(query)
    
        for i in range(min(4, len(docs))):          print(docs[i].page_content)
    
    return None
from langchain_openai import ChatOpenAI

from dotenv import load_dotenv

from langchain_openai import ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.output_parsers import StrOutputParser

from langchain_community.document_loaders import TextLoader



loader = TextLoader("file.txt")

loader.load()



from langchain_community.document_loaders import TextLoader

from langchain_openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import Chroma

from langchain.text_splitter import RecursiveCharacterTextSplitter

def data_retriever(file, query):

    
    raw_documents = TextLoader(file).load()

    text_splitter = RecursiveCharacterTextSplitter(

    
    chunk_size=1000,

    chunk_overlap=50,

    length_function=len,

    is_separator_regex=False,

)

    documents = text_splitter.split_documents(raw_documents)

    

    
    db = Chroma.from_documents(documents, OpenAIEmbeddings())

    

    query = query

    """Assigning score_threshold to 0.5 to retrieve document with above 

    50% of relevance score, and setting "K" == 5 to retrieve five chunks

    for a single query

    """

    retriever = db.as_retriever(

    search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.65, "k": 5}

)



    docs = retriever.get_relevant_documents(query)

    

    
    for i in range(min(4, len(docs))):  
        print(docs[i].page_content)

    

    return None

from langchain_community.document_loaders import TextLoader

from langchain_openai import OpenAIEmbeddings

from langchain.text_splitter import CharacterTextSplitter

from langchain_community.vectorstores import Chroma



def data_retriever(file, query):

    
    raw_documents = TextLoader(file).load()

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)

    documents = text_splitter.split_documents(raw_documents)

    

    
    db = Chroma.from_documents(documents, OpenAIEmbeddings())

    

    query = query

    docs = db.similarity_search(query)

    

    
    for i in range(min(4, len(docs))):  
        print(docs[i].page_content)

    

    
    return None

context = data_retriever('file.txt','brown')

with open('file.txt') as file:

    essay = file.read()

import re




single_sentences_list = re.split(r'(?<=[.?!])\s+', essay)

print (f"{len(single_sentences_list)} senteneces were found")
sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]
def combine_sentences(sentences, buffer_size=1):

    
    for i in range(len(sentences)):



        
        combined_sentence = ''



        
        for j in range(i - buffer_size, i):

            
            if j >= 0:

                
                combined_sentence += sentences[j]['sentence'] + ' '



        
        combined_sentence += sentences[i]['sentence']



        
        for j in range(i + 1, i + 1 + buffer_size):

            
            if j < len(sentences):

                
                combined_sentence += ' ' + sentences[j]['sentence']



        
        
        sentences[i]['combined_sentence'] = combined_sentence



    return sentences



sentences = combine_sentences(sentences)
sentences[5]
from langchain_openai.embeddings import OpenAIEmbeddings

oaiembeds = OpenAIEmbeddings()
embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])
sentences[0]['combined_sentence']
for i, sentence in enumerate(sentences):

    sentence['combined_sentence_embedding'] = embeddings[i]

from sklearn.metrics.pairwise import cosine_similarity



def calculate_cosine_distances(sentences):

    distances = []

    for i in range(len(sentences) - 1):

        embedding_current = sentences[i]['combined_sentence_embedding']

        embedding_next = sentences[i + 1]['combined_sentence_embedding']

        

        
        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]

        

        
        distance = 1 - similarity



        
        distances.append(distance)



        
        sentences[i]['distance_to_next'] = distance



    
    


    return distances, sentences
distances, sentences = calculate_cosine_distances(sentences)
import matplotlib.pyplot as plt



plt.plot(distances);
import numpy as np



plt.plot(distances);



y_upper_bound = .2

plt.ylim(0, y_upper_bound)

plt.xlim(0, len(distances))





breakpoint_percentile_threshold = 95

breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) 
plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-');




num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) 
plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f"{num_distances_above_theshold + 1} Chunks");




indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] 



colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']

for i, breakpoint_index in enumerate(indices_above_thresh):

    start_index = 0 if i == 0 else indices_above_thresh[i - 1]

    end_index = breakpoint_index if i < len(indices_above_thresh) - 1 else len(distances)



    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)

    plt.text(x=np.average([start_index, end_index]),

             y=breakpoint_distance_threshold + (y_upper_bound)/ 20,

             s=f"Chunk 
             rotation='vertical')




if indices_above_thresh:

    last_breakpoint = indices_above_thresh[-1]

    if last_breakpoint < len(distances):

        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)

        plt.text(x=np.average([last_breakpoint, len(distances)]),

                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,

                 s=f"Chunk 
                 rotation='vertical')



plt.title("PG Essay Chunks Based On Embedding Breakpoints")

plt.xlabel("Index of sentences in essay (Sentence Position)")

plt.ylabel("Cosine distance between sequential sentences")

plt.show()

start_index = 0




chunks = []




for index in indices_above_thresh:

    
    end_index = index



    
    group = sentences[start_index:end_index + 1]

    combined_text = ' '.join([d['sentence'] for d in group])

    chunks.append(combined_text)

    

    
    start_index = index + 1




if start_index < len(sentences):

    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])

    chunks.append(combined_text)



for i, chunk in enumerate(chunks[:2]):

    buffer = 200

    

    print (f"Chunk 
    print (chunk[:buffer].strip())

    print ("...")

    print (chunk[-buffer:].strip())

    print ("\n")
from operator import itemgetter



from langchain_community.vectorstores import FAISS

from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
vectorstore = FAISS.from_texts(

    chunks, embedding=OpenAIEmbeddings()

)

retriever = vectorstore.as_retriever(search_kwargs={"k" : 5}) 
relevant_docs = retriever.get_relevant_documents("What are the challenges in evaluating Retrieval Augmented Generation pipelines?")

relevant_docs[0]

for doc in relevant_docs:

  print(doc.page_content)

  print('\n')
template = """<human>: Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':




{context}




Question: {question}



\n



<bot>:

"""



prompt = ChatPromptTemplate.from_template(template)



model = ChatOpenAI()
chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)
chain.invoke("what was the first program I wrote?")
from langchain.output_parsers import ResponseSchema

from langchain.output_parsers import StructuredOutputParser

from langchain.output_parsers import ResponseSchema

from langchain.output_parsers import StructuredOutputParser



question_schema = ResponseSchema(

    name="question",

    description="a question about the context."

)



question_response_schemas = [

    question_schema,

]

question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)



format_instructions = question_output_parser.get_format_instructions()

question_generation_llm = ChatOpenAI(model="gpt-3.5-turbo-1106")



bare_prompt_template = "{content}"



bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)
qa_template = """\

You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.



question: a question about the context.



Format the output as JSON with the following keys:

question



context: {context}

"""



prompt_template = ChatPromptTemplate.from_template(template=qa_template)



messages = prompt_template.format_messages(

    context=docs[0],

    format_instructions=format_instructions

)



question_generation_chain = bare_template | question_generation_llm



response = question_generation_chain.invoke({"content" : messages})



output_dict = question_output_parser.parse(response.content)

for k, v in output_dict.items():

  print(k)

  print(v)
import sys

from dotenv import load_dotenv

import os

from langchain.chains import LLMChain

from langchain.prompts import PromptTemplate

from langchain_openai import OpenAI

from operator import itemgetter

from langchain_community.vectorstores import FAISS

from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from back_end.chunk_semantically import semantic_retriever


load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")



chunks = semantic_retriever('file.txt')

vectorstore = FAISS.from_texts(

    chunks, embedding=OpenAIEmbeddings()

)

retriever = vectorstore.as_retriever(search_kwargs={"k" : 5}) 
template = """<human>: 




context: {context}




Question: {question}



\n



<bot>:rate the relevance of retrieved context to the user question on scale of 1-10'



"""



prompt = ChatPromptTemplate.from_template(template)



model = ChatOpenAI(temperature=0.75)

chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)

chain.invoke("what was the first program I wrote?")

template = """<human>: rate the relevance of retrieved context to the user {question} on scale of 1-10'

                       and return ONLY the rating value make sure to evaluate each 

                       question and context meticulously. I want the rating to be 

                       consitent for a given {question} each time prompted, make sure 

                       to rate them accurately. minimize the variation of the rating to

                       0, so make sure each time prompted you rate them accurately

                       you can use vector similarities beween the question and context

                       to avoid the variation:



the output MUST ALWAYS BE in the following format.



example:



    "The relevance of the retrieved context is rating value "








{context}




Question: {question}



\n



<bot>:

"""



prompt = ChatPromptTemplate.from_template(template)



model = ChatOpenAI()

chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)

chain.invoke("what was the first program I wrote?")
template = """<human>: rate the relevance of retrieved context to the user {question} on scale of 1-10'

                       and return ONLY the rating value make sure to evaluate each 

                       question and context meticulously. I want the rating to be 

                       consitent for a given {question} each time prompted, make sure 

                       to rate them accurately. minimize the variation of the rating to

                       0, so make sure each time prompted you rate them accurately

                       you can use vector similarities beween the question and context

                       to avoid the variation:



the output MUST ALWAYS BE in the following format.



example:



    "The relevance of the retrieved context is rating value "








{context}




Question: {question}



\n



<bot>:

"""



prompt = ChatPromptTemplate.from_template(template)



model = ChatOpenAI()

chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)

chain.invoke("what was the first program I wrote?")
template = """<human>: craft efficient prompt based on {question}, 

                        make sure to generate very EFFECTIVE and 

                        PRACTICAL, the prompt should be clear and

                        consize and strategic as well. 






{context}




Question: {question}



\n



<bot>:

"""



prompt = ChatPromptTemplate.from_template(template)



model = ChatOpenAI()

chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)

chain.invoke("What was the first program the author wrote on the IBM 1401 computer in 9th grade, and what were the limitations and challenges they encountered while using it?")
from evaluation.evaluation import monte_carlo_eval

from evaluation.evaluation import elo_eval

from evaluation.evaluation import elo_ratings_func



elo_rating=elo_eval(prompt)

elo_ratings_func(prompt,elo_rating)

monte_carlo_eval(prompt)
from api_endpoint import prompt_return

prompt_return("who is charlie jackson?")
def combine_sentences(sentences, buffer_size=1):
        for i in range(len(sentences)):

                combined_sentence = ''

                for j in range(i - buffer_size, i):
                        if j >= 0:
                                combined_sentence += sentences[j]['sentence'] + ' '

                combined_sentence += sentences[i]['sentence']

                for j in range(i + 1, i + 1 + buffer_size):
                        if j < len(sentences):
                                combined_sentence += ' ' + sentences[j]['sentence']

                        sentences[i]['combined_sentence'] = combined_sentence

    return sentences
import os

import faiss

import tiktoken

import pandas as pd

import matplotlib.pyplot as plt

from dotenv import load_dotenv

from PyPDF2 import PdfReader




from langchain_community.document_loaders import PyPDFLoader

from langchain.document_loaders import DirectoryLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.text_splitter import CharacterTextSplitter



from langchain_openai import OpenAIEmbeddings



from langchain.vectorstores import FAISS

from langchain_community.vectorstores.weaviate import Weaviate

from langchain_community.vectorstores.pgvector import PGVector 



from langchain.chains.question_answering import load_qa_chain

from langchain.llms import OpenAI

from langchain.chains import ConversationalRetrievalChain
load_dotenv()



OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
print(os.getcwd())

print(OPENAI_API_KEY)

pdfloader = PyPDFLoader('../data/gpt-4.pdf')     




pages = pdfloader.load_and_split()

print(len(pages), pages[0])



splitter = RecursiveCharacterTextSplitter(

    
    chunk_size = 800,

    chunk_overlap  = 200,

    length_function = len,

)



pages_chunks = splitter.split_documents(pages)

print(len(pages_chunks), pages_chunks[0])

pdfReader = PdfReader('../data/gpt-4.pdf')



from typing_extensions import Concatenate


raw_text = ''

for i, page in enumerate(pdfReader.pages):

    content = page.extract_text()

    if content:

        raw_text += content




text_splitter = CharacterTextSplitter(

    
    chunk_size = 800,

    chunk_overlap  = 200,

    length_function = len,

)

text_chunks = text_splitter.split_text(raw_text) 

type(raw_text), type(pdfReader), type(pdfloader), type(pages[0])
print(f"{type(text_chunks)}")

print(f"{len(text_chunks)}")

print(f"{text_chunks[0]}")

type(pages_chunks[0]), type(text_chunks[0])
embeddings = OpenAIEmbeddings(disallowed_special=())


db = FAISS.from_documents(pages_chunks, embeddings)
db
vector_store = FAISS.from_texts(text_chunks, embeddings)
vector_store

query = "what are the limitations of gpt-4 ?"

docs = db.similarity_search(query)

docs[0]



chain = load_qa_chain(OpenAI(temperature=0), chain_type="stuff")



query = "Who created transformers?"

docs = db.similarity_search(query)



chain.run(input_documents=docs, question=query)
from IPython.display import display

import ipywidgets as widgets




qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1), db.as_retriever())
chat_history = []



def on_submit(_):

    query = input_box.value

    input_box.value = ""

    

    if query.lower() == 'exit':

        print("Thank you for using the State of the Union chatbot!")

        return

    

    result = qa({"question": query, "chat_history": chat_history})

    chat_history.append((query, result['answer']))

    

    display(widgets.HTML(f'<b>User:</b> {query}'))

    display(widgets.HTML(f'<b><font color="blue">Chatbot:</font></b> {result["answer"]}'))



print("Welcome to the Transformers chatbot! Type 'exit' to stop.")



input_box = widgets.Text(placeholder='Please enter your question:')

input_box.on_submit(on_submit)



display(input_box)
client = OpenAI(api_key=OPENAI_API_KEY)



def generate_response(prompt):

    user_prompt = f'''"Break down the prompt generation step by step based on the following prompt pairs = "Linux Terminal","I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]"

    "English Translator and Improver","I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations."

    "`position` Interviewer","I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. "

    "JavaScript Console","I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets [like this]. "

    "Excel Sheet","I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. "

    "English Pronunciation Helper","I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies."

    "Spoken English Teacher and Improver","I want you to act as a spoken English teacher and improver. I will speak to you in English and you will reply to me in English to practice my spoken English. I want you to keep your reply neat, limiting the reply to 100 words. I want you to strictly correct my grammar mistakes, typos, and factual errors. I want you to ask me a question in your reply. Now let's start practicing, you could ask me a question first. Remember, I want you to strictly correct my grammar mistakes, typos, and factual errors."

    "Travel Guide","I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. "

    "Plagiarism Checker","I want you to act as a plagiarism checker. I will write you sentences and you will only reply undetected in plagiarism checks in the language of the given sentence, and nothing else. Do not write explanations on replies. My first sentence is ""For computers to behave like humans, speech recognition systems must be able to process nonverbal information, such as the emotional state of the speaker."""

    "Character from Movie/Book/Anything","I want you to act like [character] from [series]. I want you to respond and answer like [character] using the tone, manner and vocabulary [character] would use. Do not write any explanations. Only answer like [character]. You must know all of the knowledge of [character]. "

    "Advertiser","I want you to act as an advertiser. You will create a campaign to promote a product or service of your choice. You will choose a target audience, develop key messages and slogans, select the media channels for promotion, and decide on any additional activities needed to reach your goals. "

    use these topic, prompt pair examples only as guidlines to create an effective prompt for the next topic. even if the topic is mensioned before. You will create

    only prompt for it and not act on the previous description. if the topic is mensioned already,

    do not use the prompt which you were given, change it.

    "{prompt}"'''

    response = client.completions.create(

        model="gpt-3.5-turbo-instruct",

        prompt=user_prompt, 

        temperature=0.7,

        max_tokens=250,

    )

    return response.choices[0].text
app = gr.Interface(

    generate_response,

    title="Retrieval Augmented Generation",

    inputs="text",

    outputs="text",

    allow_flagging=False,

    examples=[["Prompt Generator"], ["a cmd prompt"], ['a translator'], ['an SQL generator'], ['an image generator']]

)



app.launch()
import os
from flask import Flask, render_template, request
from evaluation.evaluator import Evaluator
from prompt_generation.knowledge_retrieval import KnowledgeRetrieval
from prompt_generation.knowledge_integrator import KnowledgeIntegrator
import prompt_generation.config
import pickle
from prompt_generation import config
import config_secret

app = Flask(__name__)

knowledge_retriever = KnowledgeRetrieval(config.knowledge)
knowledge_integrator = KnowledgeIntegrator(model_name='gpt-3.5-turbo-0613', temperature=0)
evaluator = Evaluator(model_name='gpt-3.5-turbo-0613', temperature=0)
answers = []
metrics = {}

@app.route('/')
def index():
    return render_template('index.html', answers=answers, metrics=metrics)

@app.route('/ask', methods=['POST'])
def ask():
    if request.method == 'POST':
        question = request.form['question']
        knowledge = knowledge_retriever.retrieve(question)
        answer = knowledge_integrator.get_answer(config.PROMPT_TEMPLATE, {'question': question, 'knowledge': knowledge})[0]['text']

                evaluation, _ = evaluator.get_evaluation(config.EVALUATION_PROMPT_TEMPLATE,
                                                {'question': question, 'knowledge': knowledge, 'answer': answer})
        evaluation_text = evaluation['text'].split('\n')
        for row in evaluation_text:
            metric_name = row.split(':')[0].strip()
            value = int(row.split(':')[1].strip())
            if metric_name not in metrics:
                metrics[metric_name] = []
            metrics[metric_name].append(value)

        return render_template('index.html', answers=[{'question': question, 'knowledge': knowledge, 'answer': answer}], metrics=metrics)

if __name__ == '__main__':
    app.run(debug=True)
import os

os.environ["OPENAI_API_KEY"]
import os,sys
from flask import Flask, render_template, request
from evaluation.evaluator import Evaluator
from prompt_generation.knowledge_retrieval import KnowledgeRetrieval
from prompt_generation.knowledge_integrator import KnowledgeIntegrator
import prompt_generation.config
import pickle
from prompt_generation import config
import secret

app = Flask(__name__)

knowledge_retriever = KnowledgeRetrieval(config.knowledge)
knowledge_integrator = KnowledgeIntegrator(model_name='gpt-3.5-turbo-0613', temperature=0)
evaluator = Evaluator(model_name='gpt-3.5-turbo-0613', temperature=0)
answers = []
metrics = {}

@app.route('/')
def index():
    return render_template('frontend/templates/index.html', answers=answers, metrics=metrics)

@app.route('/ask', methods=['POST'])
def ask():
    if request.method == 'POST':
        question = request.form['question']
        knowledge = knowledge_retriever.retrieve(question)
        answer = knowledge_integrator.get_answer(config.PROMPT_TEMPLATE, {'question': question, 'knowledge': knowledge})[0]['text']

                evaluation, _ = evaluator.get_evaluation(config.EVALUATION_PROMPT_TEMPLATE,
                                                {'question': question, 'knowledge': knowledge, 'answer': answer})
        evaluation_text = evaluation['text'].split('\n')
        for row in evaluation_text:
            metric_name = row.split(':')[0].strip()
            value = int(row.split(':')[1].strip())
            if metric_name not in metrics:
                metrics[metric_name] = []
            metrics[metric_name].append(value)

        return render_template('index.html', answers=[{'question': question, 'knowledge': knowledge, 'answer': answer}], metrics=metrics)

if __name__ == '__main__':
    app.run(debug=True)
import os
import json
import sys
sys.path.insert(0, '/home/mubarek/all_about_programing/10x_projects/Enterprise-Level-Automated-Prompt-Engineering/backend')
from openai import OpenAI
from math import exp
import numpy as np
from utility.env_manager import get_env_manager
from rag.rag_system import get_context_from_rag
env_manager = get_env_manager()
client = OpenAI(api_key=env_manager['openai_keys']['OPENAI_API_KEY'])


def get_completion(
    messages: list[dict[str, str]],
    model: str = env_manager['vectordb_keys']['VECTORDB_MODEL'],
    max_tokens=500,
    temperature=0,
    stop=None,
    seed=123,
    tools=None,
    logprobs=None,
    top_logprobs=None,
) -> str:
    """Return the completion of the prompt.
    @parameter messages: list of dictionaries with keys 'role' and 'content'.
    @parameter model: the model to use for completion. Defaults to 'davinci'.
    @parameter max_tokens: max tokens to use for each prompt completion.
    @parameter temperature: the higher the temperature, the crazier the text
    @parameter stop: token at which text generation is stopped
    @parameter seed: random seed for text generation
    @parameter tools: list of tools to use for post-processing the output.
    @parameter logprobs: whether to return log probabilities of the output tokens or not.
    @returns completion: the completion of the prompt.
    """

    params = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": stop,
        "seed": seed,
        "logprobs": logprobs,
        "top_logprobs": top_logprobs,
    }
    if tools:
        params["tools"] = tools

    completion = client.chat.completions.create(**params)
    return completion


def file_reader(path: str) -> str:
    script_dir = os.path.dirname(os.path.realpath(__file__))
    base_dir = os.path.dirname(script_dir)
    file_path = os.path.join(base_dir, path)
    with open(file_path, 'r') as f:
        system_message = f.read()
    return system_message


def generate_prompt_data(prompt: str, context: str, num_test_output: str) -> str:
    """Return the classification of the hallucination.
    @parameter prompt: the prompt to be completed.
    @parameter user_message: the user message to be classified.
    @parameter context: the context of the user message.
    @returns classification: the classification of the hallucination.
    """
    API_RESPONSE = get_completion(
        [
            {
                "role": "user", 
                "content": prompt.replace("{context}", context).replace("{num_test_output}", num_test_output)
            }
        ],
        model=env_manager['vectordb_keys']['VECTORDB_MODEL'],
        logprobs=True,
        top_logprobs=1,
    )

    system_msg = API_RESPONSE.choices[0].message.content
    return system_msg


def main(num_test_output: str, objective: str):
    context_message = context = get_context_from_rag(objective)
    prompt_message = file_reader("prompts/prompt-generation-prompt.txt")
    context = str(context_message)
    prompt = str(prompt_message)
    prompt_data = generate_prompt_data(prompt, context, num_test_output)

    def save_json(prompt_data) -> None:
                script_dir = os.path.dirname(os.path.realpath(__file__))

                parent_dir = os.path.dirname(script_dir)

                file_path = os.path.join(parent_dir, "prompt-dataset/prompt-data.json")

                os.makedirs(os.path.dirname(file_path), exist_ok=True)

        json_object = json.loads(prompt_data)
        with open(file_path, 'w') as json_file:
            json.dump(json_object, json_file, indent=4)

        print(f"JSON data has been saved to {file_path}")

    save_json(prompt_data)

    print("===========")
    print("Prompt Data")
    print("===========")
    print(prompt_data)


if __name__ == "__main__":

    user_objective = str(input("objective: "))
    main("3", user_objective)
This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react/README.md) uses [Babel](https://babeljs.io/) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh
import React, { useState } from 'react';
import Input from './components/Input/Input';
import Output from './components/Output/Output';

const App = () => {
  const [objective, setObjective] = useState('');
  const [expectedOutput, setExpectedOutput] = useState('');
  const [fileInput, setFileInput] = useState(null);
  const [apiData, setApiData] = useState([]);

  const handleObjectiveSubmit = async () => {
    const data = { objective: objective, expected_output: expectedOutput };
  
    try {
      const response = await fetch('http://localhost:8000/generate-and-evaluate-prompts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(data),
      });
  
      if (!response.ok) {
        throw new Error('Network response was not ok');
      }
  
      const jsonResponse = await response.json();
      setApiData(jsonResponse);
    } catch (error) {
      console.error('There has been a problem with your fetch operation:', error);
    }
  };

  const handleFileUpload = async (file) => {
    if (!file) {
      console.error('No file to upload');
      return;
    }
  
    const formData = new FormData();
    formData.append('file', file);
  
    try {
      const response = await fetch('http://localhost:8000/upload', {
        method: 'POST',
        body: formData,
      });
  
      if (!response.ok) {
        throw new Error('Network response was not ok');
      }
  
      console.log('File uploaded successfully');
      return response;
    } catch (error) {
      console.error('There has been a problem with your fetch operation:', error);
    }
  };

  const handleSubmit = async (event) => {
    event.preventDefault();
    const uploadResponse = await handleFileUpload(fileInput);
    if (uploadResponse && uploadResponse.ok) {
      handleObjectiveSubmit();
    }
  };

  return (
    <div className='md:flex h-96'>
      <div className='md:w-1/2'>
      <Input objective={objective} setObjective={setObjective} setFileInput={setFileInput} fileInput={fileInput} handleSubmit={handleSubmit} />
      </div>
      <div className='md:w-1/2'>
        <Output data={apiData} />
      </div>
    </div>
  );
};

export default App;
import React, { useState } from 'react';

const Input = ({ objective, setObjective, setFileInput, fileInput, handleSubmit }) => {
  const [expectedOutput, setExpectedOutput] = useState('');

  const handleObjectiveChange = (event) => {
    setObjective(event.target.value);
  };

  const handleExpectedOutputChange = (event) => {
    setExpectedOutput(event.target.value);
  };

  const handleFileInputChange = (event) => {
    setFileInput(event.target.files[0]);
  };

  return (
    <>
      <div className='flex flex-col  justify-center h-screen py-20 mx-20'>
        <div className='bg-gray-200 p-8 rounded-lg shadow-lg h-full'>
          <h2 className='text-3xl font-bold mb-4'>Chat Input</h2>
          <div className='mb-4'>
            <label htmlFor='text1' className='text-lg'>
              Objective
            </label>
            <textarea
              id='text1'
              value={objective}
              onChange={handleObjectiveChange}
              className='w-full px-4 py-2 h-40 border border-gray-300 rounded-md focus:outline-none focus:ring focus:ring-blue-200'
            />
          </div>
          <div className='mb-4'>
            <label htmlFor='text2' className='text-lg'>
              Expected Output
            </label>
            <input
              id='text2'
              type='text'
              value={expectedOutput}
              onChange={handleExpectedOutputChange}
              className='w-full px-4 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring focus:ring-blue-200'
            />
          </div>
          <div className='mb-4'>
            <label htmlFor='file' className='text-lg'>
              File Input
            </label>
            <input
              id='file'
              type='file'
              onChange={handleFileInputChange}
              className='w-full px-4 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring focus:ring-blue-200'
            />
          </div>
          <button
            onClick={handleSubmit}
            disabled={!fileInput}
            className='bg-blue-500 text-white py-2 px-4 rounded-md hover:bg-blue-600 focus:outline-none focus:ring focus:ring-blue-200'
          >
            Submit
          </button>
        </div>
      </div>
    </>
  );
};

export default Input;
import React from 'react';

const Output = ({ data }) => {
  return (
    <div className='flex flex-col items-center justify-center h-screen py-20'>
      <div className='bg-gray-200 p-8 rounded-lg shadow-lg h-full overflow-auto max-h-screen'>
        <h2 className='text-3xl font-bold mb-4'>Output</h2>
        {data.map((item, index) => (
          <div key={index} className='flex mb-4'>
            <div>
              <label htmlFor='prompt' className='text-lg'>
                Prompt:
              </label>
              <div className='bg-white px-4 py-2 border border-gray-300 rounded-md shadow-sm w-96'>
                {item.prompt}
              </div>
            </div>
            <div className='mx-3'>
              <label htmlFor='score' className='text-lg'>
                Score:
              </label>
              <div className='bg-white px-4 py-2 border border-gray-300 rounded-md shadow-sm'>
                {item.accuracy}
              </div>
            </div>
          </div>
        ))}
      </div>
    </div>
  );
};

export default Output;
from dotenv import load_dotenv
import os
import sys
sys.path.insert(0, '/home/mubarek/all_about_programing/10x_projects/Enterprise-Level-Automated-Prompt-Engineering/backend')
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
import weaviate
from langchain.vectorstores import Weaviate
from utility.env_manager import get_env_manager

load_dotenv()

env_manager = get_env_manager()
OPENAI_KEY = env_manager['openai_keys']['OPENAI_API_KEY']


def load_data():
    script_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
    pdfs_dir = os.path.join(script_dir, 'pdfs')
    loader = DirectoryLoader(pdfs_dir, glob="**/*.pdf")
    data = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
    docs = text_splitter.split_documents(data)

    text_meta_pair = [(doc.page_content, doc.metadata) for doc in docs]
    texts, meta = list(zip(*text_meta_pair))

    return texts, meta


def vectorize_data(texts, meta):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)

    client = weaviate.Client(
        url="http://localhost:8080",
        additional_headers={"X-OpenAI-Api-Key": OPENAI_KEY},
        startup_period=10
    )

    client.schema.delete_all()
    client.schema.get()
    schema = {
        "classes": [
            {
                "class": "Chatbot",
                "description": "Documents for chatbot",
                "vectorizer": "text2vec-openai",
                "moduleConfig": {"text2vec-openai": {"model": "ada", "type": "text"}},
                "properties": [
                    {
                        "dataType": ["text"],
                        "description": "The content of the paragraph",
                        "moduleConfig": {
                            "text2vec-openai": {
                                "skip": False,
                                "vectorizePropertyName": False,
                            }
                        },
                        "name": "content",
                    },
                ],
            },
        ]
    }

    client.schema.create(schema)

    vectorstore = Weaviate(client, "Chatbot", "content", attributes=["source"])
    vectorstore.add_texts(texts, meta)

    return vectorstore


def get_context_from_rag(user_objective):
    texts, meta = load_data()
    vectorstore = vectorize_data(texts, meta)

    query = user_objective
    docs = vectorstore.similarity_search(query, k=4)

    context = " ".join(doc.page_content for doc in docs)

    return context


if __name__ == "__main__":
    user_objective = str(input("objective: "))
    print(get_context_from_rag(user_objective))
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings


def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text


def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        separator="\n", chunk_size=1000, chunk_overlap=200, length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks


def get_vectorstore(chunked_docs, text_chunks):
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(chunked_docs, embeddings)

    return vectorstore


def get_retriever(vectorstore):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
    return retriever
from langchain_community.vectorstores.chroma import Chroma
from langchain_openai import OpenAIEmbeddings


class MyVectorStore:
    def __init__(self):
        pass

    def embed_text_and_return_vectorstore(self, text_chunks):
        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma.from_texts(text_chunks, embeddings)

        return vectorstore

    def get_retriever(self, vectorstore):
        retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
        return retriever
import streamlit as st
from dotenv import load_dotenv
from htmlTemplates import css, bot_template

from utils.pdf_utils import MyPDF
from utils.text_splitter_utils import MyTextSplitter
from utils.vector_store_utils import MyVectorStore
from utils.langchain_utils import MyLangChain
import json


def get_conversation_chain(retriever):
    my_lang_chain = MyLangChain()
    return my_lang_chain.generate_prompts_chain(base_retriever=retriever)


def handle_userinput(user_question):
    if not st.session_state.conversation:
        st.error(f"Please enter document")
        return

    result = st.session_state.conversation.invoke(
        {
            "user_prompt": user_question,
            "num_of_prompts_to_generate": 5,
        }
    )
    prompts_generated = json.loads(result["response"].content)

    for message in prompts_generated:
        st.write(bot_template.replace("{{MSG}}", message), unsafe_allow_html=True)


def main():
    load_dotenv()
    st.set_page_config(page_title="Optimized Prompts", page_icon="🤖")
    st.write(css, unsafe_allow_html=True)

    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    st.header("Get optimized prompts 🤖")
    user_question = st.text_input("State your objective:")
    if user_question:
        handle_userinput(user_question)

    with st.sidebar:
        st.subheader("Your documents")
        pdf_docs = st.file_uploader(
            "Upload your PDFs here and click on 'Process'", accept_multiple_files=True
        )
        if st.button("Process"):
            with st.spinner("Processing"):
                                pdf = MyPDF(pdf=pdf_docs)
                raw_text = pdf.get_pdf_text()

                                text_splitter = MyTextSplitter(raw_text)
                text_chunks = text_splitter.get_text_chunks()

                                my_vector_store = MyVectorStore()
                chroma_vector_store = my_vector_store.embed_text_and_return_vectorstore(
                    text_chunks
                )
                retreiver = my_vector_store.get_retriever(chroma_vector_store)

                                st.session_state.conversation = get_conversation_chain(retreiver)


if __name__ == "__main__":
    main()
OPENAI_API_KEY = ""
import os

os.environ["OPENAI_API_KEY"] = 'openai_api_key'
from langchain import PromptTemplate



demo_template = ''' I want you to act as acting financial advisor for people. 

In an easy way explain the basics of {financial_concept}'''



prompt = PromptTemplate(

    input_variables=['financial_concept'],

    template=demo_template

    )



prompt.format(financial_concept='income tax')



from langchain.llms import OpenAI

from langchain.chains import LLMChain



llm = OpenAI(temperature=0.7)

chain1 = LLMChain(llm=llm,prompt=prompt)
chain1.run('GDP')
chain1.run('economics')
chain1.run('deficit')
chain1.run('collateral')
import os

import openai

from OpenAIAPIKey import openapikey as OPENAI_API_KEY

os.environ["OPENAI_API_KEY"] = 'OPENAI_API_KEY'





openai.api_key = "OPENAI_API_KEY"



def generate_ai_powered_test_cases(prompt):

    test_cases = []



    
    for i in range(5):  
        response = openai.Client.create_completion(

            engine="text-davinci-003",

            prompt=prompt,

            max_tokens=200,  
            n=1,

            stop=None,

            temperature=0.8,  
        )



        test_cases.append(response.choices[0].text)



    return test_cases




prompt = "Write a test case to validate the login functionality of a website. Consider edge cases, security vulnerabilities, and different input combinations."




test_cases = generate_ai_powered_test_cases(prompt)




for test_case in test_cases:

    print("Test Case:", test_case)

    print("------------------")
import os

import openai

from OpenAIAPIKey import openapikey as OPENAI_API_KEY

os.environ["OPENAI_API_KEY"] = 'OPENAI_API_KEY'

openai.api_key = "OPENAI_API_KEY"

client = openai.Client()

def generate_test_cases_from_dataset(prompt, dataset):

    test_cases = []

    for example in dataset:

        
        response = openai.client.create_completion( 

            engine="text-davinci-003",

            prompt=f"{prompt}. Here's an example: {example}. Generate a similar code snippet with a different approach.",

            max_tokens=150,  
            n=1,

            stop=None,

            temperature=0.7,  
        )



        test_cases.append((response.choices[0].text, example))  


    return test_cases




dataset = ["num = num * 2", "num *= 2", "num += num", ...]



prompt = "Write a code snippet that doubles a given number in Python."




test_cases = generate_test_cases_from_dataset(prompt, dataset)




for generated_code, original_code in test_cases:

    print("Generated Code:", generated_code)

    print("Original Code:", original_code)

    print("------------------")
import random
def monte_carlo_eval(prompt):

    
    response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']

    scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}



    
    trials = 100

    total_score = 0

    for _ in range(trials):

        response = random.choice(response_types)

        total_score += scores[response]



    
    return total_score / trials



def elo_eval(prompt, base_rating=1500):

    
    
    outcomes = ['win', 'loss', 'draw']

    outcome = random.choice(outcomes)



    
    K = 30  
    R_base = 10 ** (base_rating / 400)

    R_opponent = 10 ** (1600 / 400)  
    expected_score = R_base / (R_base + R_opponent)



    
    actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]

    new_rating = base_rating + K * (actual_score - expected_score)



    return new_rating
def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):

    """

    Update Elo ratings for a list of prompts based on simulated outcomes.



    Parameters:

    prompts (list): List of prompts to be evaluated.

    elo_ratings (dict): Current Elo ratings for each prompt.

    K (int): Maximum change in rating.

    opponent_rating (int): Fixed rating of the opponent for simulation.



    Returns:

    dict: Updated Elo ratings.

    """



    for prompt in prompts:

        
        outcome = random.choice(['win', 'loss', 'draw'])



        
        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]

        R_base = 10 ** (elo_ratings[prompt] / 400)

        R_opponent = 10 ** (opponent_rating / 400)

        expected_score = R_base / (R_base + R_opponent)

        elo_ratings[prompt] += K * (actual_score - expected_score)



    return elo_ratings




prompts = ["Who founded OpenAI?", 

                "What was the initial goal of OpenAI?",

                "What did OpenAI release in 2016?", 

                "What project did OpenAI showcase in 2018?",

                "How did the AI agents in OpenAI Five work together?"

                ]

elo_ratings = {prompt: 1500 for prompt in prompts}  



for _ in range(10):  
    elo_ratings = elo_ratings_func(prompts, elo_ratings)




sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)




for prompt in sorted_prompts:

    print(f"{prompt}: {elo_ratings[prompt]}")









def evaluate_prompt(main_prompt, test_cases):

    evaluations = {}



    
    evaluations['main_prompt'] = {

        'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),

        'Elo Rating Evaluation': elo_eval(main_prompt)

    }



    
    for idx, test_case in enumerate(test_cases):

        evaluations[f'test_case_{idx+1}'] = {

            'Monte Carlo Evaluation': monte_carlo_eval(test_case),

            'Elo Rating Evaluation': elo_eval(test_case)

        }



    return evaluations
main_prompt = "why we use OepenAI?"

test_cases = ["Who founded OpenAI?", 

                "What was the initial goal of OpenAI?",

                "What did OpenAI release in 2016?", 

                "What project did OpenAI showcase in 2018?",

                "How did the AI agents in OpenAI Five work together?"

                ]

result = evaluate_prompt(main_prompt, test_cases)

print(result)



















import requests

from langchain.document_loaders import TextLoader

from langchain.text_splitter import CharacterTextSplitter  

from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import Weaviate

import weaviate

from weaviate.embedded import EmbeddedOptions

from dotenv import load_dotenv,find_dotenv

from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import Weaviate

import weaviate

from weaviate.embedded import EmbeddedOptions

from dotenv import load_dotenv,find_dotenv


from langchain.chat_models import ChatOpenAI

from langchain.prompts import ChatPromptTemplate

from langchain.schema.runnable import RunnablePassthrough

from langchain.schema.output_parser import StrOutputParser

def data_loader(file_path= 'prompts/context.txt'):

    loader = TextLoader(file_path)

    documents = loader.load()



    
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    chunks = text_splitter.split_documents(documents)

    return chunks
def create_retriever(chunks):



  
  load_dotenv(find_dotenv())



  
  client = weaviate.Client(

    embedded_options = EmbeddedOptions()

  )



  
  vectorstore = Weaviate.from_documents(

      client = client,    

      documents = chunks,

      embedding = OpenAIEmbeddings(),

      by_text = False

  )



  
  retriever = vectorstore.as_retriever()

  return retriever
chunks
chunks =  data_loader()

retriever = create_retriever(chunks)





llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)




template = """You are an assistant for question-answering tasks. 

Use the following pieces of retrieved context to answer the question. 

If you don't know the answer, just say that you don't know. 

Use two sentences maximum and keep the answer concise.

Question: {question} 

Context: {context} 

Answer:

"""



prompt = ChatPromptTemplate.from_template(template)




rag_chain = (

    {"context": retriever,  "question": RunnablePassthrough()} 

    | prompt 

    | llm

    | StrOutputParser() 

)
from datasets import Dataset



questions = ["Who founded OpenAI?", 

             "What was the initial goal of OpenAI?",

             "What did OpenAI release in 2016?",

            ]

ground_truths = [["Sam Altman, Elon Musk, Ilya Sutskever and Greg Brockman"],

                ["To advance digital intelligence in a way that benefits humanity"],

                ["OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms"]]

answers = []

contexts = []




for query in questions:



  answers.append(rag_chain.invoke(query))

  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])




data = {

    "question": questions, 
    "answer": answers, 
    "contexts": contexts, 
    "ground_truths": ground_truths 
}




dataset = Dataset.from_dict(data)
from ragas import evaluate

from ragas.metrics import (

    faithfulness,

    answer_relevancy,

    context_recall,

    context_precision,

)



result = evaluate(

    dataset = dataset, 

    metrics=[

        context_precision,

        context_recall,

        faithfulness,

        answer_relevancy,

    ],

)



df = result.to_pandas()
df
import os
import sys
from dotenv import load_dotenv
load_dotenv(".env")

class OPENAI_KEYS:
    def __init__(self):
        self.OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '') or None


class VECTORDB_KEYS:
    def __init__(self):
        self.VECTORDB_API_KEY = os.environ.get('VECTORDB_API_KEY', '') or None
        self.VECTORDB_URL = os.environ.get('VECTORDB_URL', '') or None
        self.VECTORDB_MODEL = os.environ.get('VECTORDB_MODEL', '') or None


def _get_openai_keys() -> OPENAI_KEYS:
    return OPENAI_KEYS()


def _get_vectordb_keys() -> VECTORDB_KEYS:
    return VECTORDB_KEYS()


def get_env_manager() -> dict:
    openai_keys = _get_openai_keys().__dict__
    vectordb_keys = _get_vectordb_keys().__dict__

    return {
        'openai_keys': openai_keys,
        'vectordb_keys': vectordb_keys,
    }
Repository Structure: '
' ├── notebooks
│   ├── diffusion_model.ipynb
│   ├── CV_models_and_algorithms _tutorial.ipynb
│   ├── __init__.py
│   └── pipeline_diffusion_model.ipynb
├── LICENSE
├── screenshots
│   ├── frame_1_Background_0.png
│   ├── frame_3_Background_0.png
│   ├── frame_good_Background_0.png
│   ├── frame_2_Background_0.png
│   ├── Screenshot from 2024-02-13 23-35-11.png
│   ├── frame_6_Background_0.png
│   ├── frame_8_Background_0.png
│   ├── Screenshot from 2024-02-13 23-34-48.png
│   ├── frame_4_Background_0.png
│   ├── Screenshot from 2024-02-13 23-35-38.png
│   ├── Screenshot from 2024-02-13 23-36-00.png
│   └── frame_7_Background_0.png
├── .gitignore
├── scripts
│   ├── image_generator.py
│   ├── main.py
│   ├── __init__.py
│   ├── storyboard_visualizer.py
│   ├── image_composer.py
│   ├── script.py
│   ├── __pycache__
│   ├── utils.py
│   └── prompt_generator.py
├── .github
│   └── workflows
├── Makefile
├── tests
│   └── test_script.py
├── README.md
└── requirements.txt
 '
'  

Commit History: 
{"insertions": [5], "deletions": [0], "lines": [5], "committed_datetime": ["2024-02-18 01:28:57"], "commit_count": 1} 
 Content: 
import main
from main import APIKeys  

api_keys = APIKeys()

api_keys.set_openai_api_key()
template = "You are a prompt generator given a descrption. The prompt will be used to generate descriptive quality image. What is a concise prompt for the description {description}?"



prompt = main.PromptTemplate.from_template(template)
print(prompt.format(description="In the center of a vibrant scene, a high-resolution 3D Coca-Cola bottle surrounded by effervescent bubbles captures the viewer's attention. As the bubbles rise, the bottle seamlessly transforms into a sleek DJ turntable, complete with illuminated controls and a spinning vinyl record bearing the Coke Studio logo. This imagery symbolizes a fusion of refreshing beverage and rhythmic beats. Directly below this dynamic transformation, the call-to-action 'Mix Your Beat' shines in a bold, dynamic font with a playful energy. The text, surrounded by a subtle glow, invites interaction, set against a backdrop designed to evoke creativity and musical exploration."))

llm = main.OpenAI(temperature = 0.9)

chain = main.LLMChain(llm=llm, prompt=prompt)
print(chain.run({'description' : "In the center of a vibrant scene, a high-resolution 3D Coca-Cola bottle surrounded by effervescent bubbles captures the viewer's attention. As the bubbles rise, the bottle seamlessly transforms into a sleek DJ turntable, complete with illuminated controls and a spinning vinyl record bearing the Coke Studio logo. This imagery symbolizes a fusion of refreshing beverage and rhythmic beats. Directly below this dynamic transformation, the call-to-action 'Mix Your Beat' shines in a bold, dynamic font with a playful energy. The text, surrounded by a subtle glow, invites interaction, set against a backdrop designed to evoke creativity and musical exploration."})) 

from diffusion_composition.pipeline import DiffusionCompositionPipeline

from diffusion_composition.prompting import BoundingBoxPromptSetter

from diffusion_composition.utils import load_models, upscale_image

import matplotlib.pyplot as plt

from PIL import Image

import numpy as np
from diffusers import DiffusionPipeline



pipeline = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-base")
models = load_models()





comp = DiffusionCompositionPipeline(vae=models['vae'],

                                   unet=models['unet'],

                                   scheduler=models['scheduler'])
prompt_setter = BoundingBoxPromptSetter(text_encoder=models['text_encoder'], tokenizer=models['tokenizer'])



prompt_setter.set_background_prompt("the road of a bustling city street", guidance_scale=7.5)




prompt_setter.add_local_prompt("A toyota driving on the road",

                               top_margin=0.3,

                               bottom_margin=0.,

                               left_margin=0.3,

                               right_margin=0.4)





prompt_setter.add_local_prompt("A lady walking by",

                               top_margin=0.4,

                               bottom_margin=0.,

                               left_margin=0.8,

                               right_margin=0.)





prompt_setter.add_to_all_prompts("high quality photo")

fig = prompt_setter.draw_bboxes('bboxes.png')

plt.show(fig)
img = comp(prompt_setter=prompt_setter,

           num_inference_steps=50,

           bootstrap_steps=5,

           device='cuda',

           batch_size=6)

img = Image.fromarray((img * 255).round().astype(np.uint8))

img.save("generated_image.png")

display(img)



prompt_setter = BoundingBoxPromptSetter(text_encoder=models['text_encoder'], tokenizer=models['tokenizer'])




prompt_setter.set_background_prompt("A vibrant, bustling LEGO CITY set, showcasing key features like the police station, fire station, and various vehicles", guidance_scale=7.5)




prompt_setter.add_local_prompt("A 'Test Your Knowledge' prompt in LEGO's signature yellow and red colors",

                               top_margin=0.05,

                               bottom_margin=0.65,

                               left_margin=0.1,

                               right_margin=0.1)




prompt_setter.add_local_prompt("A 'Tap to Start' button",

                               top_margin=0.30,

                               bottom_margin=0.15,

                               left_margin=0.1,

                               right_margin=0.1)




prompt_setter.add_to_all_prompts("high quality photo")




fig = prompt_setter.draw_bboxes('bboxes.png')

plt.show(fig)

img = comp(prompt_setter=prompt_setter,

           num_inference_steps=50,

           bootstrap_steps=5,

           device='cuda',

           batch_size=6)

img = Image.fromarray((img * 255).round().astype(np.uint8))

img.save("generated_image.png")

display(img)
