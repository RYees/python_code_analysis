from sqlalchemy import create_engine
import pandas as pd

def create_conn():
  engine = None
  try:
  engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
  print("Connection successful")
  except Exception as error:
  print(error)
  return engine

def fetch_data(engine, table_name):
  df = None
  try:
  df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)
  except Exception as error:
  print(error)
  return df
import  script

import pandas as pd
engine=script.create_conn()

data=script.fetch_data(engine, "xdr_data")
data
missing_fields = data.isnull().sum()

print(missing_fields)
column_data_types = data.dtypes

print(column_data_types)
float_columns = data.select_dtypes(include=['float64'])

float_columns
filled_data = float_columns.fillna(float_columns.mean())

print(filled_data)
missing_fields = filled_data.isnull().sum()

print(missing_fields)
import pandas as pd
 
non_float_columns = data.select_dtypes(exclude=['float64'])
 
cleand_data = pd.concat([filled_data, non_float_columns], axis=1)
 
print(cleand_data)
from sqlalchemy import create_engine
 
engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
 
cleand_data.to_sql('xdr_cleaned_data', engine, if_exists='replace', index=False)
from sqlalchemy import create_engine
 
engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
 
float_columns.to_sql('xdr_float_data', engine, if_exists='replace', index=False)
import  script
engine=script.create_conn()

data=script.fetch_data(engine, "xdr_data")
data
column_names_list = data.columns.tolist()

print(column_names_list)
print(data['Handset Type'].head(20))
top_10_handsets = data['Handset Type'].value_counts().head(10)

print(top_10_handsets)
top_3_manufactures = data['Handset Manufacturer'].value_counts().head(3)

print(top_3_manufactures)
 
top_3_manufacturers = data['Handset Manufacturer'].value_counts().head(3).index

filtered_data = data[data['Handset Manufacturer'].isin(top_3_manufacturers)]
 
top_5_handsets_per_manufacturer = filtered_data.groupby('Handset Manufacturer')['Handset Type'].value_counts().groupby(level=0, group_keys=False).nlargest(5)

print(top_5_handsets_per_manufacturer)
import  script
engine=script.create_conn()

cleaned_data=script.fetch_data(engine, "xdr_cleaned_data")

sessions_per_user = cleaned_data.groupby('MSISDN/Number')['Bearer Id'].nunique()
sessions_per_user
import pandas as pd

cleaned_data['Start'] = pd.to_datetime(cleaned_data['Start'])

cleaned_data['End'] = pd.to_datetime(cleaned_data['End'])

cleaned_data['Session Duration'] = (cleaned_data['End'] - cleaned_data['Start']).dt.total_seconds()

total_session_duration = cleaned_data.groupby('MSISDN/Number')['Session Duration'].sum()
total_session_duration
import pandas as pd
 
total_download_per_user = cleaned_data.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()

total_upload_per_user = cleaned_data.groupby('MSISDN/Number')['Total UL (Bytes)'].sum()
 
total_data_per_user = pd.DataFrame({
  'Total Download (Bytes)': total_download_per_user,
  'Total Upload (Bytes)': total_upload_per_user

}).reset_index()
total_data_per_user
column_headers = cleaned_data.columns

column_headers
import pandas as pd
 
download_columns = ['Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

upload_columns = ['Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']
 
total_data_per_user_per_app = cleaned_data.groupby('MSISDN/Number')[download_columns + upload_columns].sum().reset_index()

total_data_per_user_per_app
import  script
engine=script.create_conn()

float_data=script.fetch_data(engine, "xdr_float_data")
float_data
import pandas as pd
 
selected_columns =float_data.drop(['Bearer Id', 'IMSI', 'IMEI', 'MSISDN/Number', 'Start ms', 'End ms'], axis=1)
 
results = pd.DataFrame(index=['mean', 'median', 'mode', 'variance', 'std_dev', 'range', 'skewness', 'kurtosis'])
 
for column in selected_columns:
  mean = selected_columns[column].mean()
  median = selected_columns[column].median()
  mode = selected_columns[column].mode()[0]
  variance = selected_columns[column].var()
  std_dev = selected_columns[column].std()
  data_range = selected_columns[column].max() - selected_columns[column].min()
  skewness = selected_columns[column].skew()
  kurtosis = selected_columns[column].kurtosis()
  results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]
 
results
float_data.head(10)
import matplotlib.pyplot as plt
 
columns_for_analysis = ["Dur. (ms)", "Avg RTT DL (ms)", "Avg RTT UL (ms)", "Avg Bearer TP DL (kbps)", "Total UL (Bytes)"]
 
for column in columns_for_analysis:
  plt.figure(figsize=(8, 5))
  plt.hist(float_data[column].dropna(), bins=30, edgecolor='black', color='skyblue')
  plt.title(f'Histogram of {column}')
  plt.xlabel(column)
  plt.ylabel('Frequency')
  plt.show()

import seaborn as sns
 
columns_for_analysis = ["Total UL (Bytes)"]
 
for column in columns_for_analysis:
  plt.figure(figsize=(8, 5))
  sns.boxplot(y=column, data=float_data, color='skyblue')
  plt.title(f'Box Plot of {column}')
  plt.show()

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt
 
columns_for_analysis = ["Social Media DL (Bytes)", "Social Media UL (Bytes)", "Google DL (Bytes)", "Google UL (Bytes)",
  "Email DL (Bytes)", "Email UL (Bytes)", "Youtube DL (Bytes)", "Youtube UL (Bytes)",
  "Netflix DL (Bytes)", "Netflix UL (Bytes)", "Gaming DL (Bytes)", "Gaming UL (Bytes)",
  "Other DL (Bytes)", "Other UL (Bytes)", "Total DL (Bytes)", "Total UL (Bytes)"]
 
selected_data = float_data[columns_for_analysis]
 
correlation_matrix = selected_data.corr()
 
plt.figure(figsize=(12, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

plt.title('Correlation Heatmap - Applications vs Total DL+UL Data')

plt.show()
import  script
engine=script.create_conn()

float_data=script.fetch_data(engine, "xdr_float_data")
float_data
float_data
import pandas as pd
 
selected_columns =float_data.drop(['Bearer Id', 'IMSI', 'IMEI', 'MSISDN/Number', 'Start ms', 'End ms'], axis=1)
 
results = pd.DataFrame(index=['mean', 'median', 'mode', 'variance', 'std_dev', 'range', 'skewness', 'kurtosis'])
 
for column in selected_columns:
  mean = selected_columns[column].mean()
  median = selected_columns[column].median()
  mode = selected_columns[column].mode()[0]
  variance = selected_columns[column].var()
  std_dev = selected_columns[column].std()
  data_range = selected_columns[column].max() - selected_columns[column].min()
  skewness = selected_columns[column].skew()
  kurtosis = selected_columns[column].kurtosis()
  results[column] = [mean, median, mode, variance, std_dev, data_range, skewness, kurtosis]
 
results
float_data.head(10)
import matplotlib.pyplot as plt
 
columns_for_analysis = ["Dur. (ms)", "Avg RTT DL (ms)", "Avg RTT UL (ms)", "Avg Bearer TP DL (kbps)", "Total UL (Bytes)"]
 
for column in columns_for_analysis:
  plt.figure(figsize=(8, 5))
  plt.hist(float_data[column].dropna(), bins=50, edgecolor='black', color='skyblue')
  plt.title(f'Histogram of {column}')
  plt.xlabel(column)
  plt.ylabel('Frequency')
  plt.show()

import numpy as np
 
dur_percentile_12 = np.percentile(float_data["Dur. (ms)"].dropna(), 12)

print(f"The value below which 12% of observations fall is: {dur_percentile_12}")

import seaborn as sns
 
columns_for_analysis = ["Total UL (Bytes)"]
 
for column in columns_for_analysis:
  plt.figure(figsize=(8, 5))
  sns.boxplot(y=column, data=float_data, color='skyblue')
  plt.title(f'Box Plot of {column}')
  plt.show()

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt
 
columns_for_analysis = ["Social Media DL (Bytes)", "Social Media UL (Bytes)", "Google DL (Bytes)", "Google UL (Bytes)",
  "Email DL (Bytes)", "Email UL (Bytes)", "Youtube DL (Bytes)", "Youtube UL (Bytes)",
  "Netflix DL (Bytes)", "Netflix UL (Bytes)", "Gaming DL (Bytes)", "Gaming UL (Bytes)",
  "Other DL (Bytes)", "Other UL (Bytes)", "Total DL (Bytes)", "Total UL (Bytes)"]
 
selected_data = float_data[columns_for_analysis]
 
correlation_matrix = selected_data.corr()
 
plt.figure(figsize=(12, 10))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

plt.title('Correlation Heatmap - Applications vs Total DL+UL Data')

plt.show()
import  script

import pandas as pd
engine=script.create_conn()

data=script.fetch_data(engine, "xdr_data")
data
missing_fields = data.isnull().sum()

print(missing_fields)
column_data_types = data.dtypes

print(column_data_types)
float_columns = data.select_dtypes(include=['float64'])

float_columns
filled_data = float_columns.fillna(float_columns.mean())

print(filled_data)
missing_fields = filled_data.isnull().sum()

print(missing_fields)
import pandas as pd
 
non_float_columns = data.select_dtypes(exclude=['float64'])
 
cleand_data = pd.concat([filled_data, non_float_columns], axis=1)
 
print(cleand_data)
from sqlalchemy import create_engine
 
engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
 
cleand_data.to_sql('xdr_cleaned_data', engine, if_exists='replace', index=False)
from sqlalchemy import create_engine
 
engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
 
float_columns.to_sql('xdr_float_data', engine, if_exists='replace', index=False)
from sqlalchemy import create_engine
import pandas as pd
"""
a function that connect to the local database
"""
def create_conn():
  engine = None
  try:
  engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
  print("Connection successful")
  except Exception as error:
  print(error)
  return engine

"""
a function that that accept engine, and table_name as an argument and return pandas data fream
"""
def fetch_data(engine, table_name):
  df = None
  try:
  df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)
  except Exception as error:
  print(error)
  return df
import  script
engine=script.create_conn()

cleaned_data=script.fetch_data(engine, "xdr_cleaned_data")

sessions_per_user = cleaned_data.groupby('MSISDN/Number')['Bearer Id'].nunique()
sessions_per_user
import pandas as pd

cleaned_data['Start'] = pd.to_datetime(cleaned_data['Start'])

cleaned_data['End'] = pd.to_datetime(cleaned_data['End'])

cleaned_data['Session Duration'] = (cleaned_data['End'] - cleaned_data['Start']).dt.total_seconds()

total_session_duration = cleaned_data.groupby('MSISDN/Number')['Session Duration'].sum()
total_session_duration
import pandas as pd
 
total_download_per_user = cleaned_data.groupby('MSISDN/Number')['Total DL (Bytes)'].sum()

total_upload_per_user = cleaned_data.groupby('MSISDN/Number')['Total UL (Bytes)'].sum()
 
total_data_per_user = pd.DataFrame({
  'Total Download (Bytes)': total_download_per_user,
  'Total Upload (Bytes)': total_upload_per_user

}).reset_index()
total_data_per_user
column_headers = cleaned_data.columns

column_headers
import pandas as pd
 
download_columns = ['Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']

upload_columns = ['Email UL (Bytes)', 'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']
 
total_data_per_user_per_app = cleaned_data.groupby('MSISDN/Number')[download_columns + upload_columns].sum().reset_index()

total_data_per_user_per_app
import  script
engine=script.create_conn()

cleaned_data=script.fetch_data(engine, "xdr_cleaned_data")
cleaned_data.columns

import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt
 
engagement_metrics = cleaned_data[['MSISDN/Number', 'Dur. (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)']]
 
agg_engagement = engagement_metrics.groupby('MSISDN/Number').agg({
  'Dur. (ms)': 'sum',
  'Total UL (Bytes)': 'sum',
  'Total DL (Bytes)': 'sum'

}).reset_index()
 
agg_engagement['Total Engagement'] = agg_engagement['Dur. (ms)'] + agg_engagement['Total UL (Bytes)'] + agg_engagement['Total DL (Bytes)']
 
top_10_customers = agg_engagement.sort_values('Total Engagement', ascending=False).head(10)
 
top_10_customers
import pandas as pd

from sklearn.cluster import KMeans

from sklearn.preprocessing import MinMaxScaler
 
cluster_data = agg_engagement[['Dur. (ms)', 'Total UL (Bytes)', 'Total DL (Bytes)']]
 
scaler = MinMaxScaler()

normalized_data = scaler.fit_transform(cluster_data)
 
kmeans = KMeans(n_clusters=3, random_state=42)

agg_engagement['Cluster'] = kmeans.fit_predict(normalized_data)
 
cluster_summary = agg_engagement.groupby('Cluster').agg({
  'Dur. (ms)': ['min', 'max', 'mean', 'sum'],
  'Total UL (Bytes)': ['min', 'max', 'mean', 'sum'],
  'Total DL (Bytes)': ['min', 'max', 'mean', 'sum']

}).reset_index()
 
cluster_summary
import pandas as pd
 
cleaned_data['Social Media Traffic'] = cleaned_data['Social Media DL (Bytes)'] + cleaned_data['Social Media UL (Bytes)']

cleaned_data['Google Traffic'] = cleaned_data['Google DL (Bytes)'] + cleaned_data['Google UL (Bytes)']

cleaned_data['Email Traffic'] = cleaned_data['Email DL (Bytes)'] + cleaned_data['Email UL (Bytes)']

cleaned_data['Youtube Traffic'] = cleaned_data['Youtube DL (Bytes)'] + cleaned_data['Youtube UL (Bytes)']

cleaned_data['Netflix Traffic'] = cleaned_data['Netflix DL (Bytes)'] + cleaned_data['Netflix UL (Bytes)']

cleaned_data['Gaming Traffic'] = cleaned_data['Gaming DL (Bytes)'] + cleaned_data['Gaming UL (Bytes)']

cleaned_data['Other Traffic'] = cleaned_data['Other DL (Bytes)'] + cleaned_data['Other UL (Bytes)']
 
agg_user_app_traffic = cleaned_data.groupby('MSISDN/Number')[['Social Media Traffic', 'Google Traffic', 'Email Traffic',
  'Youtube Traffic', 'Netflix Traffic', 'Gaming Traffic', 'Other Traffic']].sum().reset_index()
 
top_10_social_media_users = agg_user_app_traffic.nlargest(10, 'Social Media Traffic')

top_10_google_users = agg_user_app_traffic.nlargest(10, 'Google Traffic')

top_10_email_users = agg_user_app_traffic.nlargest(10, 'Email Traffic')

top_10_youtube_users = agg_user_app_traffic.nlargest(10, 'Youtube Traffic')

top_10_netflix_users = agg_user_app_traffic.nlargest(10, 'Netflix Traffic')

top_10_gaming_users = agg_user_app_traffic.nlargest(10, 'Gaming Traffic')

top_10_other_users = agg_user_app_traffic.nlargest(10, 'Other Traffic')
 
top_10_social_media_users
 
top_3_apps = agg_user_app_traffic[['Social Media Traffic', 'Google Traffic', 'Email Traffic',
  'Youtube Traffic', 'Netflix Traffic', 'Gaming Traffic', 'Other Traffic']].sum().nlargest(3)
 
top_3_apps.plot(kind='bar', rot=0, color='skyblue')

plt.title('Top 3 Most Used Applications')

plt.xlabel('Application')

plt.ylabel('Total Traffic')

plt.show()
import streamlit as st
from over_view import over_view
from user_engagment import engagement
from experience_analytics import experiencd
from satisfaction_analytics import satisfaction

page_options = ["Over view", "User Engagment", "Experience Analytics", "Satisfaction Analytics"]
selected_page = st.sidebar.selectbox("Select a page", page_options)

if selected_page == "Over view":
  over_view()
elif selected_page == "User Engagment":
  engagement()
elif selected_page == "Experience Analytics":
  experiencd()
elif selected_page == "Satisfaction Analytics":
  satisfaction()
from sqlalchemy import create_engine
import pandas as pd
"""
a function that connect to the local database
"""
def create_conn():
  engine = None
  try:
  engine = create_engine('postgresql://postgres:telecom@localhost:5432/telecom')
  print("Connection successful")
  except Exception as error:
  print(error)
  return engine

"""
a function that that accept engine, and table_name as an argument and return pandas data fream
"""
def fetch_data(engine, table_name):
  df = None
  try:
  df = pd.read_sql_query(f"SELECT * FROM {table_name};", engine)
  except Exception as error:
  print(error)
  return df
import streamlit as st
from other_module import fetch_data, create_conn
from visualization import create_top_10_handsets_chart, create_top_3_manufacturers_chart, create_top_5_handsets_per_manufacturer_chart

def over_view():
  st.title("Over view")
  st.write("This is Over view analysis TellCo's company")
  engine = create_conn()
  data = fetch_data(engine, "xdr_data")
  cleaned_data = fetch_data(engine, "xdr_cleaned_data")
  st.write("   create_top_10_handsets_chart(data)
  st.write("   create_top_3_manufacturers_chart(data)
  st.write("   create_top_5_handsets_per_manufacturer_chart(data)
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from other_module import fetch_data, create_conn
"""
  function that plot top 10 handsets type and it's count
"""
def create_top_10_handsets_chart(data):
  top_10_handsets = data['Handset Type'].value_counts().head(10)
  fig = px.bar(top_10_handsets, x=top_10_handsets.index, y=top_10_handsets.values, labels={'x': 'Handset Type', 'y': 'Count'})
  fig.update_layout(title='Top 10 Handsets')
  st.plotly_chart(fig)
"""
  function that plot top 3 manufacturers with it's count 
"""
def create_top_3_manufacturers_chart(data):
  top_3_manufactures = data['Handset Manufacturer'].value_counts().head(3)
  fig = px.bar(top_3_manufactures, x=top_3_manufactures.index, y=top_3_manufactures.values, labels={'x': 'Manufacturer', 'y': 'Count'})
  fig.update_layout(title='Top 3 Handset Manufacturers')
  st.plotly_chart(fig)
"""
  function that plot top 5 handsets per manufacturer with it's type
"""
def create_top_5_handsets_per_manufacturer_chart(data):
  top_3_manufacturers = data['Handset Manufacturer'].value_counts().head(3).index
  filtered_data = data[data['Handset Manufacturer'].isin(top_3_manufacturers)]
  top_5_handsets_per_manufacturer = filtered_data.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')
  fig = go.Figure(data=go.Heatmap(
  z=top_5_handsets_per_manufacturer['Count'],
  x=top_5_handsets_per_manufacturer['Handset Manufacturer'],
  y=top_5_handsets_per_manufacturer['Handset Type'],
  colorscale='Viridis',
  colorbar=dict(title='Count'),
  ))
  fig.update_layout(title='Top 5 Handsets per Manufacturer',
  xaxis=dict(title='Manufacturer'),
  yaxis=dict(title='Handset Type'))
  st.plotly_chart(fig)
from sqlalchemy import create_engine
 
print(__name__)

def connect_to_postgres():
  database_name = 'telecom'
  table_name= 'xdr_data'
  connection_params = { "host": "localhost", "user": "postgres", "password": "postgres",
  "port": "5432", "database": database_name}
  engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
  return engine
 
if __name__ == "__main__":
  connect_to_postgres()
import pandas as pd
from postgres_connection import connect_to_postgres
 
def get_df():
  df = pd.read_csv('data.csv')
  return df
import pandas as pd

from process_data import  get_df

import numpy as np

import seaborn as sns

pd.set_option('display.float_format', lambda x: '%.0f' % x)

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler
 
df = get_df()
 
Handset_counts = df['Handset Type'].value_counts()

Handset_counts = Handset_counts .reset_index()

Handset_counts.columns = ['Handset Type', 'Count']

Handset_counts.head(10)

manufacturers_counts = df['Handset Manufacturer'].value_counts()

manufacturers_counts = manufacturers_counts .reset_index()

manufacturers_counts.columns = ['top 3 handset manufacturers', 'Count']

manufacturers_counts.head(3)
df.columns
 
filtered_df = df[df['Handset Manufacturer'].isin(['Apple', 'Samsung', 'Huawei'])]

filtered_df.iloc[0]

Handset_counts = filtered_df['Handset Type'].value_counts()

Handset_counts = Handset_counts .reset_index()

Handset_counts.columns = ['top 5 handsets', 'Count']

Handset_counts.head(5)
 
df['Start'] = pd.to_datetime(df['Start'])

df['End'] = pd.to_datetime(df['End'])
 
df['Session Duration (s)'] = (df['End'] - df['Start']).dt.total_seconds()
 
applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']
 
user_aggregated = df.groupby('MSISDN/Number').agg({
  'Bearer Id': 'count',   'Session Duration (s)': 'sum',   'Total DL (Bytes)': 'sum',   'Total UL (Bytes)': 'sum',   **{f'{app} DL (Bytes)': 'sum' for app in applications},   **{f'{app} UL (Bytes)': 'sum' for app in applications}  
})
 
for app in applications:
  user_aggregated[f'{app} (Total Bytes)'] = user_aggregated[f'{app} DL (Bytes)'] + user_aggregated[f'{app} UL (Bytes)']
  user_aggregated.drop([f'{app} DL (Bytes)', f'{app} UL (Bytes)'], axis=1, inplace=True)
 
user_aggregated.rename(columns={'Bearer Id': 'Number of xDR sessions'}, inplace=True)
 
user_aggregated.head(10)
missing_values = df.isnull()

missing_values

missing_values_count = df.isnull().sum()

missing_values_count
 
non_numeric_columns = df.select_dtypes(exclude=['number']).columns
 
df_numeric = df.drop(non_numeric_columns, axis=1)
 
cleaned_data = df_numeric.fillna(df_numeric.mean())
 
selected_columns = [
  "Dur. (ms)",
  "Avg Bearer TP DL (kbps)",
  "Avg Bearer TP UL (kbps)",
  "Social Media DL (Bytes)",
  "Social Media UL (Bytes)",
  "Google DL (Bytes)",
  "Google UL (Bytes)",
  "Email DL (Bytes)",
  "Email UL (Bytes)",
  "Youtube DL (Bytes)",
  "Youtube UL (Bytes)",
  "Netflix DL (Bytes)",
  "Netflix UL (Bytes)",
  "Gaming DL (Bytes)",
  "Gaming UL (Bytes)",
  "Total UL (Bytes)",
  "Total DL (Bytes)",
  "Other DL (Bytes)",
  "Other UL (Bytes)"

]
 
displayed_data = cleaned_data[selected_columns]
 
displayed_data.describe()
 
quantitative_columns = displayed_data.select_dtypes(include=['number']).columns
 
dispersion_data = pd.DataFrame(index=quantitative_columns, columns=['Range', 'Variance', 'Standard Deviation', 'IQR'])
 
for column in quantitative_columns:
  data = displayed_data[column].dropna()   data_range = data.max() - data.min()
  data_variance = data.var()
  data_std_dev = data.std()
  data_iqr = data.quantile(0.75) - data.quantile(0.25)
  dispersion_data.loc[column] = [data_range, data_variance, data_std_dev, data_iqr]
 
dispersion_data
import matplotlib.pyplot as plt
 
numeric_columns = displayed_data.select_dtypes(include='number').columns
 
for column in numeric_columns:
  plt.figure(figsize=(8, 5))
  plt.hist(df[column].dropna(), bins=30, color='skyblue', edgecolor='black')
  plt.title(f'Histogram of {column}')
  plt.xlabel(column)
  plt.ylabel('Frequency')
  plt.show()
 
import seaborn as sns

import matplotlib.pyplot as plt
 
applications = [
  'Social Media',
  'Google',
  'Email',
  'Youtube',
  'Netflix',
  'Gaming',
  'Other',

]
 
cleaned_data['Total Data (DL+UL)'] = cleaned_data['Total UL (Bytes)'] + cleaned_data['Total DL (Bytes)']
 
plt.figure(figsize=(14, 10))

for app in applications:
  sns.scatterplot(x=cleaned_data[app + ' DL (Bytes)'], y=cleaned_data[app + ' UL (Bytes)'], label=app)
 
plt.xlabel('DL Data (Bytes)')

plt.ylabel('UL Data (Bytes)')

plt.title('Scatter Plot of DL vs. UL Data for Each Application')

plt.legend()

plt.show()
 
df['Start'] = pd.to_datetime(df['Start'])

df['End'] = pd.to_datetime(df['End'])
 
df['Session Duration (s)'] = (df['End'] - df['Start']).dt.total_seconds()
 
df['Total Data (DL+UL)'] = df['Total UL (Bytes)'] + df['Total DL (Bytes)']
 
df['Duration Decile'] = pd.qcut(df.groupby('MSISDN/Number')['Session Duration (s)'].transform('sum'), q=10, labels=False, duplicates='drop')
 
decile_data = df.groupby('Duration Decile')['Total Data (DL+UL)'].sum().reset_index()
 
decile_data = decile_data.sort_values(by='Total Data (DL+UL)', ascending=False)
 
print(decile_data)
 
columns_of_interest = [
  'Social Media DL (Bytes)',
  'Google DL (Bytes)',
  'Email DL (Bytes)',
  'Youtube DL (Bytes)',
  'Netflix DL (Bytes)',
  'Gaming DL (Bytes)',
  'Other DL (Bytes)',
  'Social Media UL (Bytes)',
  'Google UL (Bytes)',
  'Email UL (Bytes)',
  'Youtube UL (Bytes)',
  'Netflix UL (Bytes)',
  'Gaming UL (Bytes)',
  'Other UL (Bytes)',

]
 
correlation_data = cleaned_data[columns_of_interest]
 
correlation_matrix = correlation_data.corr()
 
correlation_matrix

app_columns = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',
  'Google DL (Bytes)', 'Google UL (Bytes)',
  'Email DL (Bytes)', 'Email UL (Bytes)',
  'Youtube DL (Bytes)', 'Youtube UL (Bytes)',
  'Netflix DL (Bytes)', 'Netflix UL (Bytes)',
  'Gaming DL (Bytes)', 'Gaming UL (Bytes)',
  'Other DL (Bytes)', 'Other UL (Bytes)']
 
for app in ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other', 'Total']:
  cleaned_data[f'{app} Total Bytes'] = cleaned_data[f'{app} DL (Bytes)'] + cleaned_data[f'{app} UL (Bytes)']
 
total_bytes_columns = [f'{app} Total Bytes' for app in ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']]

total_bytes_data = cleaned_data[total_bytes_columns]
 
corr_matrix_total_bytes = total_bytes_data.corr()
 
plt.figure(figsize=(12, 10))

sns.heatmap(corr_matrix_total_bytes, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

plt.title('Correlation Matrix for Total Bytes')

plt.show()
 
all_columns_for_pca = ['Social Media DL (Bytes)', 'Social Media UL (Bytes)',
  'Google DL (Bytes)', 'Google UL (Bytes)',
  'Email DL (Bytes)', 'Email UL (Bytes)',
  'Youtube DL (Bytes)', 'Youtube UL (Bytes)',
  'Netflix DL (Bytes)', 'Netflix UL (Bytes)',
  'Gaming DL (Bytes)', 'Gaming UL (Bytes)',
  'Other DL (Bytes)', 'Other UL (Bytes)',
  'Total DL (Bytes)', 'Total UL (Bytes)',
  'Avg RTT DL (ms)', 'Avg RTT UL (ms)',
  'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',
  'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)',
  'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)',
  '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)',
  'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)',
  '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)',
  'HTTP DL (Bytes)', 'HTTP UL (Bytes)',
  'Activity Duration DL (ms)', 'Activity Duration UL (ms)',
  'Nb of sec with 125000B < Vol DL', 'Nb of sec with 1250B < Vol UL < 6250B',
  'Nb of sec with 31250B < Vol DL < 125000B', 'Nb of sec with 37500B < Vol UL',
  'Nb of sec with 6250B < Vol DL < 31250B', 'Nb of sec with 6250B < Vol UL < 37500B',
  'Nb of sec with Vol DL < 6250B', 'Nb of sec with Vol UL < 1250B']
 
data_for_pca = cleaned_data[all_columns_for_pca]
 
scaler = StandardScaler()

scaled_data = scaler.fit_transform(data_for_pca)
 
pca = PCA(10)

principal_components = pca.fit_transform(scaled_data)
 
explained_variance_ratio = pca.explained_variance_ratio_
 
plt.figure(figsize=(12, 6))

plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.8, align='center')

plt.step(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), where='mid')

plt.title('Explained Variance vs. Number of Principal Components')

plt.xlabel('Number of Principal Components')

plt.ylabel('Explained Variance Ratio')

plt.show()
 
principal_components  = pd.DataFrame(principal_components)

principal_components
! pip install scikit-learn
import streamlit as st
from streamlit.logger import get_logger
from process_data import get_df
import matplotlib.pyplot as plt
import seaborn as sns  
LOGGER = get_logger(__name__)
df = get_df()

df.to_csv('data.csv')
def run():
  st.set_page_config(
  page_title="10Academy",
  page_icon="👋",
  )
  st.write("
  Handset_counts = df['Handset Type'].value_counts()
  Handset_counts = Handset_counts .reset_index()
  Handset_counts.columns = ['Handset Type', 'Count']
  manufacturers_counts = df['Handset Manufacturer'].value_counts()
  manufacturers_counts = manufacturers_counts .reset_index()
  manufacturers_counts.columns = ['top 3 handset manufacturers', 'Count']
  filtered_df = df[df['Handset Manufacturer'].isin(['Apple', 'Samsung', 'Huawei'])]
  Handset_counts = filtered_df['Handset Type'].value_counts()
  Handset_counts = Handset_counts .reset_index()
  Handset_counts.columns = ['top 5 handsets', 'Count']
  numeric_columns = df.select_dtypes(include=['number']).columns
  for column in numeric_columns:
  fig, ax = plt.subplots(figsize=(8, 5))
  ax.hist(df[column].dropna(), bins=30, color='skyblue', edgecolor='black')
  ax.set_title(f'Histogram of {column}')
  ax.set_xlabel(column)
  ax.set_ylabel('Frequency')
  st.pyplot(fig)
 
if __name__ == "__main__":
  run()
import pandas as pd

from process_data import  get_df

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans
 
pd.set_option('display.float_format', lambda x: '%.0f' % x)
 
df = get_df()

df
 
analytics_columns = [
  'MSISDN/Number',
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)',
  'Handset Type',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',

]
 
num_columns = [
  'Avg RTT DL (ms)',
  'Avg RTT UL (ms)',
  'TCP DL Retrans. Vol (Bytes)',
  'TCP UL Retrans. Vol (Bytes)',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',

]
 
df = df[analytics_columns]
 
df_cleaned_handset=df.dropna(subset=['Handset Type'])
 
df_cleaned_handset  
lower_percentile = 1

upper_percentile = 99
 
lower_bounds = df_cleaned_handset[num_columns].quantile(lower_percentile / 100)

upper_bounds = df_cleaned_handset[num_columns].quantile(upper_percentile / 100)
 
for col in num_columns:
  outliers = (df_cleaned_handset[col] < lower_bounds[col]) | (df_cleaned_handset[col] > upper_bounds[col])
  if col  == 'Avg Bearer TP DL (kbps)':
  df_cleaned_handset[col] = df_cleaned_handset[col].mask(outliers, df_cleaned_handset[col].mean())
 
df_cleaned_handset
 
df_clean_nan = df_cleaned_handset.copy()

df_clean_nan[num_columns] = df_clean_nan[num_columns].fillna(df_clean_nan[num_columns].mean())

df_clean_nan
 
df_clean_nan['Sum RTT'] = df_clean_nan['Avg RTT DL (ms)'] + df_clean_nan['Avg RTT UL (ms)']

df_clean_nan['Sum TCP Retrans. Vol (Bytes)'] = df_clean_nan['TCP DL Retrans. Vol (Bytes)'] + df_clean_nan['TCP UL Retrans. Vol (Bytes)']

df_clean_nan['Sum Bearer TP'] = df_clean_nan['Avg Bearer TP DL (kbps)'] + df_clean_nan['Avg Bearer TP UL (kbps)']
 
df_clean_nan = df_clean_nan.rename(columns={
  'Avg RTT DL (ms)': 'Avg RTT DL',
  'Avg RTT UL (ms)': 'Avg RTT UL',
  'TCP DL Retrans. Vol (Bytes)': 'TCP Retrans. Vol DL',
  'TCP UL Retrans. Vol (Bytes)': 'TCP Retrans. Vol UL',
  'Avg Bearer TP DL (kbps)': 'Avg Bearer TP DL',
  'Avg Bearer TP UL (kbps)': 'Avg Bearer TP UL',

})
 
df_clean_nan

important_columns = [
  'MSISDN/Number',
  'Handset Type',
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP'

]
 
num_important_columns  = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP'

]
 
df_important = df_clean_nan[important_columns]

df_important
 
num_important_columns = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP'

]
 
grouped_df = df_important.groupby('MSISDN/Number')[num_important_columns].sum().reset_index()
 
grouped_df.head(10)
 
top_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].nlargest(10)

bottom_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].nsmallest(10)

most_frequent_tcp_values = df_important['Sum TCP Retrans. Vol (Bytes)'].mode()
 
print("Top TCP Values:")

print(top_tcp_values)
 
print("\nBottom TCP Values:")

print(bottom_tcp_values)
 
print("\nMost Frequent TCP Values:")

print(most_frequent_tcp_values)
 
top_rtt_values = df_important['Sum RTT'].nlargest(10)

bottom_rtt_values = df_important['Sum RTT'].nsmallest(10)

most_frequent_rtt_values = df_important['Sum RTT'].mode()
 
print("Top RTT Values:")

print(top_rtt_values)
 
print("\nBottom RTT Values:")

print(bottom_rtt_values)
 
print("\nMost Frequent RTT Values:")

print(most_frequent_rtt_values)
 
top_throughput_values = df_important['Sum Bearer TP'].nlargest(10)

bottom_throughput_values = df_important['Sum Bearer TP'].nsmallest(10)

most_frequent_throughput_values = df_important['Sum Bearer TP'].mode()
 
print("Top Throughput Values:")

print(top_throughput_values)
 
print("\nBottom Throughput Values:")

print(bottom_throughput_values)
 
print("\nMost Frequent Throughput Values:")

print(most_frequent_throughput_values)
 
average_throughput_per_handset = df_important.groupby('Handset Type')['Sum Bearer TP'].mean().reset_index()

print("Distribution of Average Throughput per Handset Type:")

print(average_throughput_per_handset)
 
avg_tcp_r_per_handset = df_important.groupby('Handset Type')['Sum TCP Retrans. Vol (Bytes)'].mean().reset_index()

print("Distribution of Average TCP Retransmission per Handset Type:")

avg_tcp_r_per_handset
 
experience_metrics = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP',

]
 
df_cluster = grouped_df[experience_metrics]
 
scaler = StandardScaler()

scaled_data = scaler.fit_transform(df_cluster)
 
kmeans = KMeans(n_clusters=3, random_state=42)

df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)
 
cluster_means = df_cluster.groupby('Cluster').mean()

print(cluster_means)
 
experience_metrics = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP',

]
 
df_cluster = grouped_df[experience_metrics].dropna()
 
scaler = StandardScaler()

scaled_data = scaler.fit_transform(df_cluster)
 
kmeans = KMeans(n_clusters=3, random_state=42)

df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)
 
sns.set(style="whitegrid")
 
sns.pairplot(df_cluster, hue="Cluster", palette="Set1", height=3, diag_kind="kde")

plt.show()
 
experience_metrics = [
  'Sum RTT',
  'Sum TCP Retrans. Vol (Bytes)',
  'Sum Bearer TP',

]
 
df_cluster = grouped_df[experience_metrics].dropna()
 
scaler = StandardScaler()

scaled_data = scaler.fit_transform(df_cluster)
 
kmeans = KMeans(n_clusters=3, random_state=42)

df_cluster['Cluster'] = kmeans.fit_predict(scaled_data)
 
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
scatter = ax.scatter(
  df_cluster['Sum RTT'],
  df_cluster['Sum TCP Retrans. Vol (Bytes)'],
  df_cluster['Sum Bearer TP'],
  c=df_cluster['Cluster'],
  cmap='viridis',
  s=50,
  alpha=0.6,
  edgecolors='w'

)
 
ax.set_xlabel('Sum RTT')

ax.set_ylabel('Sum TCP Retrans. Vol (Bytes)')

ax.set_zlabel('Sum Bearer TP')

ax.set_title('K-Means Clustering of User Experiences')
 
legend1 = ax.legend(*scatter.legend_elements(), title='Clusters')

ax.add_artist(legend1)
 
plt.show()
import pandas as pd

from sqlalchemy import create_engine, MetaData, Table

import psycopg2

databse_name = 'telecome'

table_name = 'xdr_data'

con_param ={"host": "localhost","user": "postgres", "password":"1272", "port":"5432", "database":databse_name}

print(databse_name)

engin  = create_engine(f"postgresql://{con_param['user']}:{con_param['password']}@{con_param['host']}/{con_param['port']}/{con_param['database']}")
 
sql_query = 'SELECT * FROM xdr_data'
 
conn = psycopg2.connect(
  host="localhost",
  port=5432,
  database="telecome",
  user="postgres",
  password="1272"

)

cursor =conn.cursor()

cursor.execute(sql_query)
 
import os
from abc import ABC, abstractmethod
from sqlalchemy.engine.base import Engine
from typing import Generic, Optional, TypeVar
from dotenv import load_dotenv

RawConnectionT = TypeVar("RawConnectionT")

class ConnectionBase(ABC, Generic[RawConnectionT]):
  """The abstract base class that all connections must inherit from
  This base class provides connection authors with a way to set up   database parameters like database name, database password, port, and also   instance of database engine.
  """
  def __init__(self, **kwargs):
  """ create a BaseConnection
  Parameters
  ----------
  **kwargs: dict
  dictionary of parameters of any length to pass tho the connection class
  Returns
  -------
  None
  """
  self._kwargs = kwargs if kwargs else {
  'user': os.getenv('DB_USER'),
  'password': os.getenv('DB_PASSWORD'),
  'host': os.getenv('DB_HOST'),
  'port': os.getenv('DB_PORT'),
  'database': os.getenv('DB_NAME'),
  }
  self._raw_instance: Optional[Engine] = self._connect()
  @property
  def _instance(self) -> RawConnectionT:
  """Get an instance of the underlying connection, creating a new one if needed."""
  if self._raw_instance is None:
  self._raw_instance = self._connect()
  return self._raw_instance
  @abstractmethod
  def _connect(self) -> RawConnectionT:
  """Create an instance of an underlying connection object.
  This abstract method is the one method that we require subclasses of
  BaseConnection to provide an implementation for. It is called when first
  creating a connection and when reconnecting after a connection is reset.
  Returns
  -------
  RawConnectionT
  The underlying connection object.
  """
  raise NotImplementedError
  def __enter__(self):
  self._raw_instance = self._connect()
  return self._raw_instance
  def __exit__(self, exc_type, exc_value, traceback):
  if self._raw_instance is not None:
  self._raw_instance.dispose()
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)

import logging
from sqlalchemy import create_engine
from connections.connection_base import ConnectionBase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PostgresConnection(ConnectionBase):
  """A class representing a connection to a PostgreSQL database.
  This class inherits from ConnectionBase, providing a standardized way to manage
  the connection to the PostgreSQL database.
  Parameters
  ----------
  **kwargs : dict
  Keyword arguments containing the database connection parameters.
  Attributes
  ----------
  _kwargs : dict
  Dictionary containing the database connection parameters.
  _raw_instance : sqlalchemy.engine.base.Engine
  """
  def __init__(self, **kwargs):
  super().__init__(**kwargs)
  def _connect(self):
  """Create an instance of the underlying SQLAlchemy Engine for PostgreSQL.
  Returns
  -------
  sqlalchemy.engine.base.Engine
  The SQLAlchemy Engine instance representing the PostgreSQL database connection.
  Raises
  ------
  Exception
  If there is an error connecting to the database.
  """
  try:
  engine = create_engine(
  f"postgresql://{self._kwargs['user']}:{self._kwargs['password']}@{self._kwargs['host']}:{self._kwargs['port']}/{self._kwargs['database']}"
  )
  engine.connect()
  return engine
  except Exception as e:
  logger.error(f"Error connecting to the database: {e}")
  raise
import os, sys
 
rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

df = rd.read_data()
df.shape
df.head()
df.info()
df.isnull().sum()
df.duplicated().sum()
df.describe()
for col in df.columns:
  print(df[col].value_counts())
import os, sys

import pandas as pd

rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

import scripts.write_to_db as wd

import scripts.data_cleaning as sd  
df = rd.read_data()
df = sd.drop_high_missing_columns(df, 0.7)

df.columns

def remove_missing_values():
  columns_to_check = [
  'Start',   'Start ms',   'End',   'End ms',   'Dur. (ms)',
  'Avg Bearer TP DL (kbps)',
  'Avg Bearer TP UL (kbps)',
  'Activity Duration DL (ms)',   'Activity Duration UL (ms)',   'Dur. (ms).1',
  'Total UL (Bytes)',   'Total DL (Bytes)'
  ]
  return sd.remove_rows_with_missing_values(df, columns_to_check)
 
cleaned_df = remove_missing_values()
df.shape
cleaned_df.isnull().sum()

def impute_columns():
  columns_to_impute = [
  "Avg RTT DL (ms)",   "Avg RTT UL (ms)",   "TCP DL Retrans. Vol (Bytes)",   "TCP UL Retrans. Vol (Bytes)",   "DL TP < 50 Kbps (%)",   "50 Kbps < DL TP < 250 Kbps (%)",   "250 Kbps < DL TP < 1 Mbps (%)",   "DL TP > 1 Mbps (%)",   "UL TP < 10 Kbps (%)",   "10 Kbps < UL TP < 50 Kbps (%)",   "50 Kbps < UL TP < 300 Kbps (%)",   "UL TP > 300 Kbps (%)",   "HTTP DL (Bytes)",   "HTTP UL (Bytes)",   "Nb of sec with 125000B < Vol DL",   "Nb of sec with 1250B < Vol UL < 6250B",   "Nb of sec with 31250B < Vol DL < 125000B",   "Nb of sec with 6250B < Vol DL < 31250B",   "Nb of sec with Vol DL < 6250B",   "Nb of sec with Vol UL < 1250B"   ]
  return sd.impute_numeric_missing(cleaned_df, columns_to_impute)
 
imputed_df = impute_columns()
imputed_df.isnull().sum()
imputed_df.shape
def replace_with_mode():
  columns_to_replace = [
  'Bearer Id',
  'IMSI',
  'MSISDN/Number',
  'IMEI',   'Last Location Name',
  'Handset Manufacturer',
  'Handset Type'   ]
  return sd.replace_column_with_mode(imputed_df, columns_to_replace)
 
cleaned_df = replace_with_mode()
cleaned_df.isnull().sum()
cleaned_df.shape
def handle_outliers():
  columns = [
  "Avg RTT DL (ms)",   "Avg RTT UL (ms)",   "TCP DL Retrans. Vol (Bytes)",   "TCP UL Retrans. Vol (Bytes)",   "DL TP < 50 Kbps (%)",   "50 Kbps < DL TP < 250 Kbps (%)",   "250 Kbps < DL TP < 1 Mbps (%)",   "DL TP > 1 Mbps (%)",   "UL TP < 10 Kbps (%)",   "10 Kbps < UL TP < 50 Kbps (%)",   "50 Kbps < UL TP < 300 Kbps (%)",   "UL TP > 300 Kbps (%)",   "HTTP DL (Bytes)",   "HTTP UL (Bytes)",   "Nb of sec with 125000B < Vol DL",   "Nb of sec with 1250B < Vol UL < 6250B",   "Nb of sec with 31250B < Vol DL < 125000B",   "Nb of sec with 6250B < Vol DL < 31250B",   "Nb of sec with Vol DL < 6250B",   "Nb of sec with Vol UL < 1250B"   ]
  return sd.handle_outliers(cleaned_df, columns)
 
processed_df = handle_outliers()

processed_df.head()
processed_df.shape
wd.write_data(processed_df, 'processed_data')
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

def drop_high_missing_columns(
  df: pd.DataFrame,   threshold=0.8
  ) -> pd.DataFrame:
  """Drop columns with missing values exceeding a specified threshold.
  Parameters
  ----------
  - df: DataFrame
  The dataframe of raw data set.
  - threshold: float, default 0.8
  The threshold for the percentage of missing values in a column.
  Returns
  -------
  - Dataframe object
  where columns specified in the columns_to_drop list,   which have missing values, dropped.
  """
  missing_percentage = df.isnull().mean()
  columns_to_drop = missing_percentage[missing_percentage > threshold].index
  df_cleaned = df.drop(columns=columns_to_drop)
  return df_cleaned

def impute_numeric_missing(
  df:pd.DataFrame,   columns_list:list[str],
  strategy:str='mean'
  ) -> pd.DataFrame:
  """Impute missing values for numerical columns.
  Parameters
  ----------
  - df: DataFrame
  - columns_list
  list of columnst to Impute the missing values
  - strategy: str, default 'mean'
  Imputation strategy, options: 'mean', 'zero'.
  Returns
  -------
  - Dataframe object
  with imputed missing columns from columns_list
  """
  numeric_columns = df[columns_list].select_dtypes(include='number').columns
  if strategy == 'zero':
  df_imputed = df.copy()   df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(0)
  else:
  imputation_values = df[numeric_columns].mean() if strategy == 'mean' else df[numeric_columns].median()
  df_imputed = df.copy()   df_imputed[numeric_columns] = df_imputed[numeric_columns].fillna(imputation_values)
  return df_imputed

def remove_rows_with_missing_values(
  df: pd.DataFrame,   columns_to_check: list[str]
  ):
  """
  Remove rows from a DataFrame where any of the specified columns have missing values.
  Parameters
  ----------
  df : pandas.DataFrame
  The input DataFrame.
  columns_to_check : list of str
  A list of column names to check for missing values. Rows will be dropped
  if any of these columns have missing values.
  Returns
  -------
  pandas.DataFrame
  A new DataFrame with rows removed where any of the specified columns have missing values.
  """
  cleaned_df = df.dropna(subset=columns_to_check, how='any')
  return cleaned_df

import pandas as pd

def replace_column_with_mode(
  df: pd.DataFrame,   column_names: list[str]
  ) -> pd.DataFrame:
  """Replace missing values in a column with its mode.
  Parameters
  ----------
  - df: DataFrame
  - column_list: list[str]
  Name of the list of columns to replace missing values.
  Returns
  -------
  - DataFrame object
  with missing values in the specified column replaced by its mode.
  """
  df_mode = df.copy()   for column_name in column_names:
  mode_value = df_mode[column_name].mode().iloc[0]
  df_mode[column_name] = df_mode[column_name].fillna(mode_value)
  return df_mode

def handle_outliers(
  df: pd.DataFrame,   columns: list[str],   method:str='mean'
  ) -> pd.DataFrame:
  """Handle outliers in specified columns using a specified method.
  Parameters
  ----------
  - df: DataFrame
  - columns: list
  List of columns to handle outliers.
  - method: str, default 'clip'
  Outlier handling method, options: 'clip', 'remove', 'mean'
  Returns
  -------
  - Dataframe object
  where outliers are handled
  """
  if method == 'clip':
  for col in columns:
  df[col] = np.clip(df[col], df[col].quantile(0.05), df[col].quantile(0.95))
  elif method == 'remove':
  for col in columns:
  q1 = df[col].quantile(0.25)
  q3 = df[col].quantile(0.75)
  iqr = q3 - q1
  df = df[(df[col] >= q1 - 1.5 * iqr) & (df[col] <= q3 + 1.5 * iqr)]
  elif method == 'mean':
  for col in columns:
  mean_val = df[col].mean()
  df[col] = np.where(
  (df[col] < df[col].quantile(0.05)) | (df[col] > df[col].quantile(0.95)),
  mean_val,
  df[col]
  )
  return df
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)

import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def read_data(table_name='xdr_data'):
  """Read data from a PostgreSQL database table.
  Parameters
  ----------
  table_name : str, optional
  The name of the table from which to fetch data. Default is 'xdr_data'.
  Returns
  -------
  pandas.DataFrame
  A DataFrame containing the data retrieved from the specified table.
  Raises
  ------
  Exception
  If there is an error during the data retrieval process.
  Notes
  -----
  This function uses the `PostgresConnection` class to establish a connection
  to the PostgreSQL database. Ensure that the necessary environment variables
  (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
  this function.
  """
  try:
  with PostgresConnection() as postgres_conn:
  df = pd.read_sql_query(f"SELECT * FROM {table_name};", postgres_conn)
  logger.info('Data fetched succesfully')
  return df
  except Exception as e:
  logger.error(f"Error in the main block: {e}")
import os, sys

rpath = os.path.abspath('..')
if rpath not in sys.path:
  sys.path.insert(0, rpath)

import logging
import pandas as pd
from connections.postegresql_connection import PostgresConnection

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def write_data(
  df: pd.DataFrame,
  table_name : str
  ) -> None:
  """Write DataFrame to a PostgreSQL database table.
  Parameters
  ----------
  df : pandas.DataFrame
  The DataFrame containing the data to be written.
  table_name : str
  The name of the table where the data will be written.
  Returns
  -------
  None
  Raises
  ------
  Exception
  If there is an error during the data writing process.
  Notes
  -----
  This function uses the `PostgresConnection` class to establish a connection
  to the PostgreSQL database. Ensure that the necessary environment variables
  (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME) are set before calling
  this function.
  """
  try:
  conn = PostgresConnection()
  df.to_sql(name=table_name, con=conn._instance, index=False, if_exists='replace')
  logger.info('Data written successfully')
  except Exception as e:
  logger.error(f"Error in the main block: {e}")
import os, sys

import pandas as pd

from pandasql import sqldf
 
rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
df.shape
pysqldf = lambda q: sqldf(q, globals())
df.columns
query = '''   SELECT DISTINCT   "Handset Type",   COUNT(*) as UsageCount
  FROM df
  GROUP BY "Handset Type"   ORDER BY UsageCount DESC
  limit 10

'''
 
result_df = pysqldf(query)

result_df

query = '''   SELECT DISTINCT   "Handset Manufacturer",   COUNT(*) as "Number of Users"
  FROM df
  GROUP BY "Handset Manufacturer"   ORDER BY "Number of Users" DESC
  limit 3

'''
 
result_df = pysqldf(query)

result_df
query = '''   SELECT "Handset Manufacturer", AVG("Dur. (ms)") AS "Avg Session Duration (ms)"
  FROM df
  WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')
  GROUP BY "Handset Manufacturer";
 
'''

result_df = pysqldf(query)

result_df

query = '''   SELECT
  "Handset Manufacturer",
  SUM("Total UL (Bytes)" + "Total DL (Bytes)") AS "Total Data Volume (Bytes)"
  FROM df
  WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')
  GROUP BY "Handset Manufacturer";

'''

result_df = sqldf(query)

result_df
query = '''   WITH RankedHandsets AS (
  SELECT
  "Handset Manufacturer",
  "Handset Type",
  RANK() OVER (PARTITION BY "Handset Manufacturer" ORDER BY COUNT(*) DESC) AS "Rank"
  FROM df
  WHERE "Handset Manufacturer" IN ('Apple', 'Samsung', 'Huawei')
  GROUP BY "Handset Manufacturer", "Handset Type"
  )
  SELECT
  "Handset Manufacturer",
  "Handset Type",
  "Rank"
  FROM RankedHandsets
  WHERE "Rank" <= 5;

'''
 
result_df = sqldf(query)

result_df
query = '''   SELECT "MSISDN/Number" AS UserIdentifer,
  COUNT(*) AS NumberOfXDRSessions
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY NumberOfXDRSessions DESc;

'''
 
result_df = pysqldf(query)

result_df
query = '''
  SELECT COUNT(*) AS num_users
  FROM (
  SELECT COUNT(*) AS NumberOfXDRSessions
  FROM df
  GROUP BY "MSISDN/Number"
  HAVING COUNT(*) >= 10
  ) AS subquery

'''
 
result_df = pysqldf(query)

result_df
query = '''   SELECT
  "MSISDN/Number" AS UserIdentifier,
  SUM("Dur. (ms)") / 1000 AS TotalSessionDurationInSeconds
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalSessionDurationInSeconds DESC;
 
'''
 
result_df = pysqldf(query)

result_df
query = '''   SELECT
  CAST(SUBSTRING(CAST("Start" AS TEXT), INSTR(CAST("Start" AS TEXT), ' ') + 1, INSTR(CAST("Start" AS TEXT), ':') - INSTR(CAST("Start" AS TEXT), ' ') - 1) AS INTEGER) AS HourOfDay,
  COUNT(*) AS NumberOfSessions
  FROM df
  WHERE "Start" IS NOT NULL
  GROUP BY HourOfDay
  ORDER BY NumberOfSessions DESC;
 
'''
 
result_df = pysqldf(query)  
result_df
query = '''   SELECT
  "MSISDN/Number" AS User,
  COUNT(*) AS SessionCount,
  SUM("Dur. (ms)") AS TotalSessionDuration
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalSessionDuration DESC;
 
'''
 
result_df = pysqldf(query)

result_df
result_df = util.get_total_download_for_each_app(df, "Total DL (Bytes)", "Total UL (Bytes)")
query = '''SELECT COUNT(*) AS num_users
  FROM result_df
  WHERE Total >= 100000000;
  '''
 
pysqldf(query)

util.get_total_download_for_each_app(df, "Social Media DL (Bytes)", "Social Media UL (Bytes)")
util.get_total_download_for_each_app(df, "YouTube DL (Bytes)", "YouTube UL (Bytes)")
util.get_total_download_for_each_app(df, "Netflix DL (Bytes)", "Netflix UL (Bytes)")
util.get_total_download_for_each_app(df, "Google DL (Bytes)", "Google UL (Bytes)")
util.get_total_download_for_each_app(df, "Email DL (Bytes)", "Email UL (Bytes)")
util.get_total_download_for_each_app(df, "Gaming DL (Bytes)", "Gaming UL (Bytes)")
util.get_total_download_for_each_app(df, "Other DL", "Other UL")
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf
 
rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
quantitative_columns = [
  "Avg RTT DL (ms)",   "Avg RTT UL (ms)",   "Avg Bearer TP DL (kbps)",
  "Avg Bearer TP UL (kbps)",   "TCP DL Retrans. Vol (Bytes)",   "TCP UL Retrans. Vol (Bytes)",   "DL TP < 50 Kbps (%)",   "50 Kbps < DL TP < 250 Kbps (%)",   "250 Kbps < DL TP < 1 Mbps (%)",   "DL TP > 1 Mbps (%)",   "UL TP < 10 Kbps (%)",   "10 Kbps < UL TP < 50 Kbps (%)",   "50 Kbps < UL TP < 300 Kbps (%)",   "UL TP > 300 Kbps (%)",
  "Activity Duration DL (ms)",
  "Activity Duration UL (ms)",   "HTTP DL (Bytes)",   "HTTP UL (Bytes)",   "Dur. (ms).1",   "Nb of sec with 125000B < Vol DL",   "Nb of sec with 1250B < Vol UL < 6250B",   "Nb of sec with 31250B < Vol DL < 125000B",   "Nb of sec with 6250B < Vol DL < 31250B",   "Nb of sec with Vol DL < 6250B",   "Nb of sec with Vol UL < 1250B",   "Social Media DL (Bytes)",
  "Social Media UL (Bytes)",
  "Youtube DL (Bytes)",
  "Youtube UL (Bytes)",
  "Netflix DL (Bytes)",
  "Netflix UL (Bytes)",
  "Google DL (Bytes)",
  "Google UL (Bytes)",
  "Email DL (Bytes)",
  "Email UL (Bytes)",
  "Gaming DL (Bytes)",
  "Gaming UL (Bytes)",
  "Other DL (Bytes)",
  "Other UL (Bytes)",
  "Total DL (Bytes)",
  "Total UL (Bytes)"

]

df = dc.handle_outliers(df, quantitative_columns)
df.shape
descriptive_stats = df[quantitative_columns].describe()

descriptive_stats
plt.hist(df['Avg RTT UL (ms)'], bins=20, color='blue', alpha=0.7)

plt.xlabel('Average RTT UL (ms)')

plt.ylabel('Frequency')

plt.title('Histogram of Avg RTT DL (ms)')

plt.show()
 
sns.boxplot(x=df['Avg Bearer TP DL (kbps)'])

plt.xlabel('Average Bearer TP DL (kbps)')

plt.title('Boxplot of Avg Bearer TP DL (kbps)')

plt.show()

plt.figure(figsize=(8, 5))

df['Handset Type'].value_counts().head(10).plot(kind='bar', color='green')

plt.xlabel('Handset Type')

plt.ylabel('Count')

plt.title('Distribution of Handset Types')

plt.show()

correlation_matrix = df[
  [
  "Social Media DL (Bytes)",
  "Social Media UL (Bytes)",
  "Youtube DL (Bytes)",
  "Youtube UL (Bytes)",
  "Netflix DL (Bytes)",
  "Netflix UL (Bytes)",
  "Google DL (Bytes)",
  "Google UL (Bytes)",
  "Email DL (Bytes)",
  "Email UL (Bytes)",
  "Gaming DL (Bytes)",
  "Gaming UL (Bytes)",
  "Other DL (Bytes)",
  "Other UL (Bytes)",
  "Total DL (Bytes)",
  "Total UL (Bytes)"

]].corr()

correlation_matrix
 
sns.scatterplot(x='Netflix DL (Bytes)', y='Total DL (Bytes)', data=df)

plt.title('Youtube download vs Total Download')

plt.show()
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = '''   SELECT
  "MSISDN/Number" AS MSISDN,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY SessionFrequency DESC
  LIMIT 10

'''
 
result_df = pysqldf(query)

result_df
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration
  FROM df
  GROUP BY "MSISDN/Number"   ORDER BY SessionDuration DESC
  LIMIT 10;
  '''  
result_df = pysqldf(query)

result_df

query = '''   SELECT   "MSISDN/Number",
  SUM("Total DL (Bytes)") AS TotalDownload,
  SUM("Total UL (Bytes)") AS TotalUpload,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalTraffic DESC
  LIMIT 10;

'''
 
pysqldf(query)
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  '''  
aggregated_df = pysqldf(query)

aggregated_df.tail()
scaler = MinMaxScaler()

columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]

customer_id = aggregated_df['MSISDN/Number']

transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)

df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')
 
ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')
 
plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
processed_df = dc.handle_outliers(normalized_data, columns_to_normalize)
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(processed_df['SessionDuration'], processed_df['SessionFrequency'], processed_df['TotalTraffic'], c='blue', marker='o')
 
ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')
 
plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
selected_columns = columns_to_normalize

X = processed_df[selected_columns]

k = 3
 
kmeans = KMeans(n_clusters=3, random_state=0)  
kmeans.fit(X)
 
processed_df['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = processed_df[processed_df['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['SessionDuration'],   cluster_data['SessionFrequency'],   cluster_data['TotalTraffic'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.write_to_db as wd

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
query = '''   SELECT
  "MSISDN/Number" AS MSISDN,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY SessionFrequency DESC
  LIMIT 10

'''
 
result_df = pysqldf(query)

result_df
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration
  FROM df
  GROUP BY "MSISDN/Number"   ORDER BY SessionDuration DESC
  LIMIT 10;
  '''  
result_df = pysqldf(query)

result_df

query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration
  FROM df
  GROUP BY "MSISDN/Number"   ORDER BY SessionDuration DESC;
  '''  
result_df = pysqldf(query)

result_df['SessionDuration'].median()
query = '''   SELECT   "MSISDN/Number",
  SUM("Total DL (Bytes)") AS TotalDownload,
  SUM("Total UL (Bytes)") AS TotalUpload,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalTraffic DESC
  LIMIT 10;

'''
 
pysqldf(query)
query = '''   SELECT   "MSISDN/Number",
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  ORDER BY TotalTraffic DESC

'''
 
pysqldf(query)['TotalTraffic'].median()
query = '''
  SELECT   "MSISDN/Number",
  SUM("Dur. (ms)") AS SessionDuration,
  COUNT(DISTINCT "Bearer Id") AS SessionFrequency,
  (SUM("Total DL (Bytes)") + SUM("Total UL (Bytes)")) AS TotalTraffic
  FROM df
  GROUP BY "MSISDN/Number"
  '''  
aggregated_df = pysqldf(query)

aggregated_df.tail()
aggregated_df = dc.handle_outliers(aggregated_df, ["SessionDuration","SessionFrequency","TotalTraffic"])
scaler = MinMaxScaler()

columns_to_normalize = ["SessionDuration", "SessionFrequency","TotalTraffic"]

customer_id = aggregated_df['MSISDN/Number']

transformed_data = scaler.fit_transform(aggregated_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)

df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(df_normalized['SessionDuration'], df_normalized['SessionFrequency'], df_normalized['TotalTraffic'], c='blue', marker='o')
 
ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')cases = 707,443
 
Total deaths = 3,891

ax.set_zlabel('TotalTraffic')
 
plt.title('3D Scatter Plot of SessionDuration, SessionFrequency, and TotalTraffic')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 3
 
kmeans = KMeans(n_clusters=3, random_state=0, n_init=25)  
kmeans.fit(X)
 
df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['SessionDuration'],   cluster_data['SessionFrequency'],   cluster_data['TotalTraffic'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
grouped_df = (df_normalized
  .groupby('Cluster')[["SessionDuration", "SessionFrequency", "TotalTraffic"]]
  .agg(['min', 'max', 'mean', 'sum']))
 
grouped_df

wcss = []

X = df_normalized.iloc[:, [1, 3]]
 
for i in range(1, 11):
  kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)
 
plt.plot(range(1, 11), wcss)

plt.xlabel('Number of clusters')

plt.ylabel('WCSS')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 7
 
kmeans = KMeans(n_clusters=k, random_state=0, n_init=25)  
kmeans.fit(X)
 
df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green', 3:'black', 4:'yellow', 5:'brown', 6:'purple'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['SessionDuration'],   cluster_data['SessionFrequency'],   cluster_data['TotalTraffic'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('SessionDuration')

ax.set_ylabel('SessionFrequency')

ax.set_zlabel('TotalTraffic')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
wd.write_data(df_normalized, 'user_engagement')
import os, sys

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import KMeans

from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

import scripts.write_to_db as wd

import scripts.data_cleaning as dc

import scripts.utils as util
df = rd.read_data(table_name='processed_data')
pysqldf = lambda q: sqldf(q, globals())
def helper(field1 : str, field2 : str) -> pd.DataFrame:
  query = f'''   SELECT
  "MSISDN/Number" AS CustomerID,
  AVG("{field1}") AS AvgDL,
  AVG("{field2}") AS AvgUL,
  (
  AVG("{field1}") +   AVG("{field2}")
  ) / 2 AS Avg
  FROM df
  GROUP BY "MSISDN/Number";
  '''
  return pysqldf(query)
avg_retransmission = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")

avg_retransmission
avg_rtt = helper("Avg RTT DL (ms)", "Avg RTT UL (ms)")

avg_rtt
query = '''   SELECT
  "MSISDN/Number" AS User_MSISDN,
  "Handset Type" AS HandsetType,
  COUNT(*) AS HandsetTypeCount
  FROM df
  GROUP BY "MSISDN/Number", "Handset Type";
 
'''
 
count_handset = pysqldf(query)  
count_handset

avg_throughput = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")

avg_throughput
def helper(field, order):
  query = f'''   SELECT   "{field}",
  COUNT("{field}") AS "Frequency"
  FROM df
  GROUP BY "{field}"
  ORDER BY "Frequency" {order}   LIMIT 10;
  '''
  return pysqldf(query)
top_tcp = helper("TCP DL Retrans. Vol (Bytes)", "DESC")

top_tcp
bottom_tcp = helper("TCP DL Retrans. Vol (Bytes)", "ASC")

bottom_tcp
top_rtt = helper("Avg RTT DL (ms)", "DESC")

top_rtt
bottom_rtt = helper("Avg RTT DL (ms)", "ASC")

bottom_rtt
top_throughput = helper("Avg Bearer TP DL (kbps)", "Desc")

top_throughput
bottom_throughput = helper("Avg Bearer TP DL (kbps)", "ASC")

bottom_throughput
def helper(field1, field2):
  query = f'''
  SELECT
  "Handset Type" AS HandsetType,
  AVG("{field1}" + "{field2}") / 2 AS Average
  FROM df
  GROUP BY "Handset Type"
  ORDER BY Average DESC;
  '''
  return pysqldf(query)
avg_throughput_per_handset = helper("Avg Bearer TP DL (kbps)", "Avg Bearer TP UL (kbps)")

avg_throughput_per_handset
average_tcp_per_handset = helper("TCP DL Retrans. Vol (Bytes)", "TCP UL Retrans. Vol (Bytes)")

average_tcp_per_handset
query = '''   SELECT   "MSISDN/Number" AS CustomerID,
  (AVG("TCP DL Retrans. Vol (Bytes)") + AVG("TCP UL Retrans. Vol (Bytes)")) / 2 AS AvgTCP,
  (AVG("Avg RTT DL (ms)") + AVG("Avg RTT UL (ms)")) / 2 AS AvgRTT,
  (AVG("Avg Bearer TP DL (kbps)") + AVG("Avg Bearer TP UL (kbps)")) / 2 AS AvgThroughput
  FROM df
  GROUP BY CustomerID

'''
 
agg_df = pysqldf(query)

agg_df.head()
agg_df = dc.handle_outliers(agg_df, ["AvgTCP", "AvgRTT","AvgThroughput"])
scaler = MinMaxScaler()

columns_to_normalize = ["AvgTCP", "AvgRTT","AvgThroughput"]
 
customer_id = agg_df['CustomerID']

transformed_data = scaler.fit_transform(agg_df[columns_to_normalize])

normalized_data = pd.DataFrame(transformed_data, columns=columns_to_normalize)
 
df_normalized = pd.concat([customer_id, normalized_data], axis=1)

df_normalized
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
ax.scatter(df_normalized["AvgTCP"], df_normalized["AvgRTT"], df_normalized["AvgThroughput"], c='blue', marker='o')
 
ax.set_xlabel("AvgTCP")

ax.set_ylabel("AvgRTT")

ax.set_zlabel("AvgThroughput")
 
plt.title('3D Scatter Plot of AvgTCP, AvgRTT, and AvgThroughput')

plt.show()
wcss = []

X = df_normalized.iloc[:, [1, 3]]
 
for i in range(1, 11):
  kmeans = KMeans(n_clusters = i, random_state=42, n_init=10)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)
 
plt.plot(range(1, 11), wcss)

plt.xlabel('Number of clusters')

plt.ylabel('WCSS')

plt.show()
selected_columns = columns_to_normalize

X = df_normalized[selected_columns]

k = 3
 
kmeans = KMeans(n_clusters=k, random_state=0, n_init=15)  
kmeans.fit(X)
 
df_normalized['Cluster'] = kmeans.labels_
fig = plt.figure(figsize=(10, 8))

ax = fig.add_subplot(111, projection='3d')
 
cluster_colors = {0: 'red', 1: 'blue', 2:'green'}
 
for cluster_label, color in cluster_colors.items():
  cluster_data = df_normalized[df_normalized['Cluster'] == cluster_label]
  ax.scatter(
  cluster_data['AvgTCP'],   cluster_data['AvgRTT'],   cluster_data['AvgThroughput'],   label=f'Cluster {cluster_label}',
  color=color
  )
 
ax.set_xlabel('AvgTCP')

ax.set_ylabel('AvgRTT')

ax.set_zlabel('AvgThroughput')
 
plt.title(f'3D Scatter plot with k-Means clustering (K={k})')

plt.show()
grouped_df = (df_normalized
  .groupby('Cluster')[["AvgTCP", "AvgRTT", "AvgThroughput"]]
  .agg(['min', 'max', 'mean', 'sum']))
 
grouped_df
wd.write_data(df_normalized, 'user_experience')
import os, sys

import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from pandasql import sqldf

from sklearn.preprocessing import MinMaxScaler

from sklearn.linear_model import LinearRegression

from sklearn.cluster import KMeans

from sklearn.model_selection import train_test_split

from sklearn.metrics import euclidean_distances

from mpl_toolkits.mplot3d import Axes3D
 
rpath = os.path.abspath('..')

if rpath not in sys.path:
  sys.path.insert(0, rpath)
 
import scripts.read_data_from_db as rd

import scripts.data_cleaning as dc

import scripts.write_to_db as wd

import scripts.utils as util
experience_df = rd.read_data(table_name='user_experience')

engagement_df = rd.read_data(table_name='user_engagement')
pysqldf = lambda q: sqldf(q, globals())
experience_df.head()
engagement_df.head()
merged_df = engagement_df.merge(experience_df, left_on='MSISDN/Number', right_on="CustomerID", how='inner')

merged_df.head()
experience_columns = ["AvgTCP","AvgRTT","AvgThroughput"]

engagement_columns = ["SessionDuration","SessionFrequency","TotalTraffic"]
k = 3

X_eng = merged_df[engagement_columns]

X_exp = merged_df[experience_columns]
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)

cluster_labels = kmeans.fit_predict(X_eng)

merged_df['cluster'] = cluster_labels
query = '''   SELECT   cluster,
  AVG(("SessionDuration"+"SessionFrequency"+"TotalTraffic") / 3) AS AVG   FROM merged_df
  GROUP BY cluster;

'''
 
grouped = pysqldf(query)

less_engaged_cluster = grouped['AVG'].idxmin()

less_engaged_cluster
less_engaged_cluster_center = kmeans.cluster_centers_[less_engaged_cluster]
 
distances = euclidean_distances(X_eng, [less_engaged_cluster_center]).flatten()

merged_df['EngagementScore'] =  distances

merged_df.head()
query = '''   SELECT   cluster,
  AVG(("AvgTCP" + "AvgRTT" + "AvgThroughput") / 3) AS AVG   FROM merged_df
  GROUP BY cluster;

'''
 
grouped = pysqldf(query)

worst_experience_cluster = grouped['AVG'].idxmin()

worst_experience_cluster
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)

cluster_labels = kmeans.fit_predict(X_exp)
 
worst_experience_cluster_cluster_center = kmeans.cluster_centers_[worst_experience_cluster]
 
distances = euclidean_distances(X_eng, [worst_experience_cluster_cluster_center]).flatten()

merged_df['ExperienceScore'] =  distances

merged_df.head()
merged_df['SatisfactionScore'] = (merged_df['EngagementScore'] + merged_df['ExperienceScore']) / 2

merged_df.head()
query = '''   SELECT   "MSISDN/Number",
  SatisfactionScore
  FROM merged_df
  ORDER BY SatisfactionScore DESC
  LIMIT 10

'''
 
pysqldf(query)
merged_df['SatisfactionScore'].describe()

bins = np.arange(0, 1.4142, 0.35355)

labels = ["Low Satisfaction", "Medium Satisfaction", "High Satisfaction"]
 
merged_df['SatisfactionGroup'] = pd.cut(merged_df['SatisfactionScore'], bins=bins, labels=labels, include_lowest=True)
 
group_counts = merged_df['SatisfactionGroup'].value_counts()
 
plt.pie(group_counts, labels=group_counts.index, autopct='%1.1f%%', startangle=90)

plt.title('Satisfaction Score Distribution')
 
plt.show()
bins
features = engagement_columns + experience_columns

X = merged_df[features]

y = merged_df.SatisfactionScore
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()  
model.fit(X_train, y_train)
 
y_pred  = model.predict(X_test)

selected_columns = ["EngagementScore", "SatisfactionScore"]

X = merged_df[selected_columns]

k = 2
 
kmeans = KMeans(n_clusters=k, random_state=0, n_init=15)  
kmeans.fit(X)
 
merged_df['cluster'] = kmeans.labels_
cluster_colors = {0: 'red', 1: 'blue'}

for cluster_label, color in cluster_colors.items():
  cluster_df = merged_df[merged_df["cluster"] == cluster_label]
  plt.scatter(cluster_df["EngagementScore"], cluster_df["ExperienceScore"], c=color)
 
plt.xlabel("EngagementScore")

plt.ylabel("ExperienceScore")

plt.title("'2D Scatter Plot'of EngagementScore vs ExperienceScore")
 
plt.show()

query = '''   SELECT   cluster,
  AVG("SatisfactionScore") As "Average Satisfaction"
  FROM merged_df   GROUP BY cluster;

'''

avg_satsifaction = pysqldf(query)

avg_satsifaction
query = '''   SELECT   cluster,
  AVG("ExperienceScore") AS "Average Experience"   FROM merged_df   GROUP BY cluster  
'''
 
avg_experience = pysqldf(query)

avg_experience
df = pd.DataFrame({
  "UserID": merged_df["CustomerID"],
  "SatisfactionScore": merged_df["SatisfactionScore"],
  "ExperienceScore": merged_df["ExperienceScore"],
  "EngagementScore": merged_df["EngagementScore"]

})
 
wd.write_data(df, "satisfaction_score")
from doctest import debug_script
from json import load
from pydoc import describe
from quopri import decodestring
import pandas as pd
import psycopg2

def loadData():
  conn = psycopg2.connect(
  host="localhost",
  port="5432",
  database="tellco",
  user="postgres",
  password="root"
  )
  cursor = conn.cursor('my_cursor_name', withhold=True)
  cursor.execute("SELECT * FROM xdr_data")
  batch_size = 1000
  rows = cursor.fetchmany(batch_size)
  data = []   while rows:
  data += rows
  rows = cursor.fetchmany(batch_size)
  if not rows:
  break
  if len(data) >= 50000:
  columns = [desc[0] for desc in cursor.description]
  df = pd.DataFrame(data, columns=columns)
  return df
  columns = [desc[0] for desc in cursor.description]
  cursor.close()
  conn.close()   df = pd.DataFrame(data, columns=columns)   print(len(data))
  return (df)
import pandas.io.sql as sqlio
import psycopg2
from psycopg2 import sql
from sqlalchemy import create_engine
import pandas as pd

def db_connection_sqlalchemy():
  database_name='telecom'
  table_name='xdr_data'
  connection_params={
  "host":"localhost",
  "user":"postgres",
  "password":"admin",
  "port":"5432",
  "database":database_name
  }
  engine = create_engine(f"postgresql+psycopg2://{connection_params['user']}:{connection_params['password']}@{connection_params['host']}:{connection_params['port']}/{connection_params['database']}")
  return engine

def db_read_table_sqlalchemy(engine,table_name):
  sql_query=f'SELECT * FROM {table_name}'
  df = pd.read_sql(sql_query, con= engine)
  return df  
def db_connection_psycopg():
  pgconn=psycopg2.connect(
  dbname="telecom",
  user="postgres",
  password="admin",
  host="localhost",
  port="5432"
  )
  return pgconn  
def db_read_table_psycopg(pgconn,table_name):
  sql_query=f'SELECT * FROM {table_name}'
  df=sqlio.read_sql_query(sql_query,pgconn)
  return df
import pandas as pd

from script import db_connection
 
alchemyConn=db_connection.db_connection_sqlalchemy()
 
pgconn=db_connection.db_connection_psycopg()

print(alchemyConn)

print(pgconn)

dfa = db_connection.db_read_table_sqlalchemy(alchemyConn, 'xdr_data')

dfa.head()

dfa = db_connection.db_read_table_psycopg(pgconn, 'xdr_data')

dfa.head()

df.info()
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import sys

sys.path.append('../')
 
import warnings

warnings.filterwarnings('ignore')

from script import db_connection
 
alchemyConn=db_connection.db_connection_sqlalchemy()
 
pgconn=db_connection.db_connection_psycopg()

print(alchemyConn)

print(pgconn)

dfa = db_connection.db_read_table_sqlalchemy(alchemyConn, 'xdr_data')

dfa.head()

dfg = db_connection.db_read_table_psycopg(pgconn, 'xdr_data')

dfg.head()

dfa.info()

dfa.nunique()

dfa.Start.unique()

dfa.Start.nunique()

dfa.isnull().sum()

(dfa.isnull().sum()/(len(dfa)))*100

dfa.describe().T

dfa['Total UL (Bytes)'].describe().T

dfa.describe(include='all').T
 
def percent_missing(dfa):
  totalCells = np.product(dfa.shape)
  missingCount = dfa.isnull().sum()
  totalMissing = missingCount.sum()
  print("The telecom dataset contains", round(((totalMissing/totalCells) * 100), 2), "%", "missing values.")
 
percent_missing(dfa)
 
sessions = len(dfa.columns)

print("Number of xDR sessions:", sessions)

dfa['Google DL (Bytes)'].hist()
 
num_cols=dfa.select_dtypes(include=np.number).columns.tolist()
 
num_cols.skew()

session_durations = dfa['Dur. (ms)'].count()
 
print("Number of session durations:", session_durations)

total_dl_data = dfa['Total DL (Bytes)'].sum()

total_ul_data = dfa['Total UL (Bytes)'].sum()
 
print("Total Download Data:", total_dl_data, "bytes")

print("Total Upload Data:", total_ul_data, "bytes")

total_social_media_dl = dfa['Social Media DL (Bytes)'].sum()

total_social_media_ul = dfa['Social Media UL (Bytes)'].sum()

total_google_dl = dfa['Google DL (Bytes)'].sum()

total_google_ul = dfa['Google UL (Bytes)'].sum()

total_email_dl = dfa['Email DL (Bytes)'].sum()

total_email_ul = dfa['Email UL (Bytes)'].sum()

total_youtube_dl = dfa['Youtube DL (Bytes)'].sum()

total_youtube_ul = dfa['Youtube UL (Bytes)'].sum()

total_netflix_dl = dfa['Netflix DL (Bytes)'].sum()

total_netflix_ul = dfa['Netflix UL (Bytes)'].sum()

total_gaming_dl = dfa['Gaming DL (Bytes)'].sum()

total_gaming_ul = dfa['Gaming UL (Bytes)'].sum()

total_other_dl = dfa['Other DL (Bytes)'].sum()

total_other_ul = dfa['Other UL (Bytes)'].sum()
 
print("Total Social Media Data (DL):", total_social_media_dl, "bytes")

print("Total Social Media Data (UL):", total_social_media_ul, "bytes")

print("Total Google Data (DL):", total_google_dl, "bytes")

print("Total Google Data (UL):", total_google_ul, "bytes")

print("Total Email Data (DL):", total_email_dl, "bytes")

print("Total Email Data (UL):", total_email_ul, "bytes")

print("Total YouTube Data (DL):", total_youtube_dl, "bytes")

print("Total YouTube Data (UL):", total_youtube_ul, "bytes")

print("Total Netflix Data (DL):", total_netflix_dl, "bytes")

print("Total Netflix Data (UL):", total_netflix_ul, "bytes")

print("Total Gaming Data (DL):", total_gaming_dl, "bytes")

print("Total Gaming Data (UL):", total_gaming_ul, "bytes")

print("Total Other Data (DL):", total_other_dl, "bytes")

print("Total Other Data (UL):", total_other_ul, "bytes")
 
num_cols = dfa.select_dtypes(include=np.number).columns.tolist()

cat_cols=dfa.select_dtypes(include=['object']).columns

for col in num_cols:
  print(col)
  print('Skew :', round(dfa[col].skew(), 2))
  plt.figure(figsize = (15, 4))
  plt.subplot(1, 2, 1)
  dfa[col].hist(grid=False)
  plt.ylabel('count')
  plt.subplot(1, 2, 2)
  sns.boxplot(x=dfa[col])
  plt.show()
 
application_vars = ['Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)', 'Youtube UL (Bytes)',
  'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)','Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)',
  'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']
 
dfa['DL+UL (Bytes)']=dfa['Total UL (Bytes)']+dfa['Total DL (Bytes)']
 
for var in application_vars:
  subset = dfa[[var, 'DL+UL (Bytes)']]
  plt.figure(figsize=(13, 17))
  sns.pairplot(subset)
  plt.title(f'Pair Plot for {var} with Total UL')
  plt.show()

dfa['Total Duration'] = dfa['Dur. (ms)'] + dfa['Dur. (ms).1'] + dfa['Activity Duration DL (ms)'] + dfa['Activity Duration UL (ms)']
 
sorted_dfa = dfa.sort_values('Total Duration', ascending=False)
 
decile_size = len(sorted_dfa) // 5
 
sorted_dfa['Decile Class'] = pd.qcut(sorted_dfa.index, 5, labels=False)

sorted_dfa['Decile Class'] = sorted_dfa['Decile Class'].apply(lambda x: x + 1)
 
decile_data = sorted_dfa.groupby('Decile Class')[['DL+UL (Bytes)']].sum()
 
print(decile_data)
 
data_list = ['Social Media UL (Bytes)', 'Social Media DL (Bytes)',
  'Google UL (Bytes)', 'Google DL (Bytes)',
  'Email UL (Bytes)', 'Email DL (Bytes)',
  'Youtube UL (Bytes)', 'Youtube DL (Bytes)',
  'Netflix UL (Bytes)', 'Netflix DL (Bytes)',
  'Gaming UL (Bytes)', 'Gaming DL (Bytes)',
  'Other UL (Bytes)', 'Other DL (Bytes)']
 
subset = dfa[data_list]
 
correlation_matrix = subset.corr()
 
print(correlation_matrix)
 
session_duration = dfa.groupby('MSISDN/Number')['Dur. (ms)'].sum()

top_10_session_duration = session_duration.nlargest(10)
 
print("\nTop 10 Customers by Session Duration:")

print(top_10_session_duration)
dfa_clean=dfa[['Bearer Id','Start']]

dfa_clean.info()

dfa_clean.to_sql('clean_data',alchemyConn)
