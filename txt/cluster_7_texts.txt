import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub('', text)

    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import unittest
import sys, os
sys.path.append(os.path.abspath(os.path.join('..')))

from scripts.util import find_average, count_occurence


class TestCases(unittest.TestCase):
    def test_find_average(self):
        """
        Test that it retunrs the average of a given list
        """
        data = [1, 2, 3]
        result = find_average(data)
        self.assertEqual(result, 2.0)

    def test_input_value(self):
        """
        Provide an assertion level for arg input
        """
        
        self.assertRaises(TypeError, find_average, True)

class TestCountOccurence(unittest.TestCase):
    def test_count_occurence(self):
        """
        Test that it returns the count of each unique values in the given list
        """
        data = [0,0,9,0,8,9,0,7]
        result = count_occurence(data)
        output = {0: 4, 9: 2, 8: 1, 7: 1}
        self.assertAlmostEqual(result, output)

    def test_input_value(self):
        """
        Provide an assertion level for arg input
        """
        self.assertRaises(TypeError, count_occurence, True)

if __name__ == '__main__':
    unittest.main()
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub('', text)

    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
parsed_dir = "../data/raw/"

cleaned_dir = "../data/parsed/"

file_name = "TIKVAH-data"

util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")

df.head()

df = pd.DataFrame(df['messages'])

df_messages = pd.DataFrame(df['messages'].tolist())

df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))

df_messages = df_messages[['id', 'text', 'date']]

df_messages
df_filtered = df_messages[(df_messages['id'] >= 60000) & (df_messages['id'] <= 75000)]

df_filtered.head()
data_null_removed = df_filtered.dropna()

data_null_removed.head()
tigvah_data = data_null_removed.replace('\n', ' ', regex=True)

tigvah_data.head()

tigvah_data['hashtags'] = tigvah_data['text'].apply(lambda x: util.extract_hashtags(x))

tigvah_data.head()

tigvah_data['text'] = tigvah_data['text'].str.replace(r'\
tigvah_data.head()

tigvah_data['emojis'] = tigvah_data['text'].apply(util.extract_emojis)

tigvah_data.tail()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_emojis)

tigvah_data.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    tigvah_data['text'] = tigvah_data['text'].str.replace(letter[0][i], letter[1][i])
tigvah_data.head()
tigvah_data['symbols'] = tigvah_data['text'].apply(util.extract_symbols)

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_symbols)

tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)

tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)

tigvah_data.head()
tigvah_data['mentions'] = tigvah_data['text'].apply(util.extract_mentions)

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace('\s+', ' ', regex=True).str.strip()

tigvah_data['text'] = tigvah_data['text'].replace(r'!+', '!', regex=True)

tigvah_data['text'] = tigvah_data['text'].replace(r'\.+', '', regex=True)
tigvah_data.head()
tigvah_data = tigvah_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
tigvah_data
tigvah_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
parsed_dir = "../data/raw/"

cleaned_dir = "../data/parsed/"

file_name = "YeneTube"

util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")

df.head()

df = pd.DataFrame(df['messages'])

df_messages = pd.DataFrame(df['messages'].tolist())

df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))

df_messages = df_messages[['id', 'text', 'date']]

df_messages
data_null_removed = df_messages.dropna()

data_null_removed.head()
yenetube_data = data_null_removed.replace('\n', ' ', regex=True)

yenetube_data.head()

yenetube_data['hashtags'] = yenetube_data['text'].apply(lambda x: util.extract_hashtags(x))

yenetube_data.head()

yenetube_data['text'] = yenetube_data['text'].str.replace(r'\
yenetube_data.head()

yenetube_data['emojis'] = yenetube_data['text'].apply(util.extract_emojis)

yenetube_data.tail()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_emojis)

yenetube_data.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    yenetube_data['text'] = yenetube_data['text'].str.replace(letter[0][i], letter[1][i])
yenetube_data.head()
yenetube_data['symbols'] = yenetube_data['text'].apply(util.extract_symbols)

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_symbols)

yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)

yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)

yenetube_data.head()
yenetube_data['mentions'] = yenetube_data['text'].apply(util.extract_mentions)

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace('\s+', ' ', regex=True).str.strip()

yenetube_data['text'] = yenetube_data['text'].replace(r'!+', '!', regex=True)

yenetube_data['text'] = yenetube_data['text'].replace(r'\.+', '', regex=True)
yenetube_data.head()
yenetube_data = yenetube_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
yenetube_data
yenetube_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
def combine_sentences(sentences, buffer_size=1):
        for i in range(len(sentences)):

                combined_sentence = ''

                for j in range(i - buffer_size, i):
                        if j >= 0:
                                combined_sentence += sentences[j]['sentence'] + ' '

                combined_sentence += sentences[i]['sentence']

                for j in range(i + 1, i + 1 + buffer_size):
                        if j < len(sentences):
                                combined_sentence += ' ' + sentences[j]['sentence']


                sentences[i]['combined_sentence'] = combined_sentence

    return sentences
import json




text_values = []




with open('manchester.json', 'r') as file:

    data = json.load(file)

    messages = data.get('messages', [])



    for message in messages:

        text_content = message.get('text', None)



        
        if text_content and isinstance(text_content, list):

            
            first_part_of_text = text_content[0] if text_content and isinstance(text_content[0], str) else None

            if first_part_of_text:

                text_values.append(first_part_of_text)




concatenated_text = '\n'.join(text_values)




print(len(concatenated_text))
def combine_sentences(sentences, buffer_size=1):

    
    for i in range(len(sentences)):



        
        combined_sentence = ''



        
        for j in range(i - buffer_size, i):

            
            if j >= 0:

                
                combined_sentence += sentences[j]['sentence'] + ' '



        
        combined_sentence += sentences[i]['sentence']



        
        for j in range(i + 1, i + 1 + buffer_size):

            
            if j < len(sentences):

                
                combined_sentence += ' ' + sentences[j]['sentence']



        
        
        sentences[i]['combined_sentence'] = combined_sentence



    return sentences





def pretty_print_docs(docs):

    print(

        f"\n{'-' * 100}\n".join(

            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]

        )

    )
from langchain.text_splitter import CharacterTextSplitter

from langchain_community.document_loaders import TextLoader

from langchain_community.vectorstores import FAISS

from langchain_openai import OpenAIEmbeddings



documents = TextLoader(concatenated_text).load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

texts = text_splitter.split_documents(documents)

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()



docs = retriever.get_relevant_documents(

    "What did the president say about Ketanji Brown Jackson"

)

pretty_print_docs(docs)
from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_distances(sentences):
    distances = []
    for i in range(len(sentences) - 1):
        embedding_current = sentences[i]['combined_sentence_embedding']
        embedding_next = sentences[i + 1]['combined_sentence_embedding']
        
                similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
        
                distance = 1 - similarity

                distances.append(distance)

                sentences[i]['distance_to_next'] = distance

        
    return distances, sentences
import re
import numpy as np
from combine_sentences import combine_sentences
import calculate_cosine_distance
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from calculate_cosine_distance import calculate_cosine_distances
from dotenv import load_dotenv
import os
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
def semantic_retriever(essay):
    

    try:
        single_sentences_list = re.split(r'(?<=[.?!])\s+', essay)
        sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]
        
        try:
            sentences = combine_sentences(sentences)
        except Exception as e:
            print(f"Error in combine_sentences: {e}")
            return []

        try:
            oaiembeds = OpenAIEmbeddings()
            embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])
        except Exception as e:
            print(f"Error in OpenAI embeddings: {e}")
            return []

        for i, sentence in enumerate(sentences):
            sentence['combined_sentence_embedding'] = embeddings[i]

        try:
            distances, sentences = calculate_cosine_distances(sentences)
        except Exception as e:
            print(f"Error in calculate_cosine_distance: {e}")
            return []

        breakpoint_percentile_threshold = 95
        breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)

        indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]

        start_index = 0
        chunks = []

        for index in indices_above_thresh:
            end_index = index
            group = sentences[start_index:end_index + 1]
            combined_text = ' '.join([d['sentence'] for d in group])
            chunks.append(combined_text)
            start_index = index + 1

        if start_index < len(sentences):
            combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])
            chunks.append(combined_text)

        return chunks

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return []
from extract_json import extract_json

a= extract_json('ብስራት ስፖርት.json')
from semantic_chuncking import semantic_retriever

chunks = semantic_retriever(a)
from operator import itemgetter



from langchain_community.vectorstores import FAISS

from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(

    chunks, embedding=OpenAIEmbeddings()

)

retriever = vectorstore.as_retriever(search_kwargs={"k" : 1}) relevant_docs = retriever.get_relevant_documents("ቤቲንግ")

relevant_docs
for doc in relevant_docs:

  print(doc.page_content)

  print('\n')
template = """<human>: Create a compelling Amharic or Amharic mixed with English advertisement for {question}.

Highlight the key features,unique selling points, and the exceptional services offered by the brand. 

Craft a persuasive narrative that resonates with the target audience, emphasizing the brand's values 

and commitment to customer satisfaction. Use vibrant and engaging language to evoke a positive emotional

response and encourage potential customers to explore and choose {question}. 

Ensure the ad reflects the brand's identity and leaves a lasting impression on the audience.



Make it a highly personalized promotional text that resonates with {context}, drawing insights from their 

contextual data. Leverage historical interactions, discussions, or online activities to tailor the ad 

closely to {context} unique preferences and interests. Highlight features and services that align with 

their needs based on the available contextual information. Tailor the language to address specific topics,

preferences, or issues derived from {context} historical interactions. Ensure the personalized content feels

natural and aligns with [User's] communication style, fostering a deep connection and encouraging meaningful

engagement. The goal is to create an ad that demonstrates an understanding of {context} context, providing

value and relevance in a way that feels organic.


{context}




Question: {question}



\n



<bot>:

"""



prompt = ChatPromptTemplate.from_template(template)



model = Mistral_7B
chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)
import json
import re

def extract_json(json_file):
    last_25_percent_text = ""

        with open(json_file, 'r') as file:
        data = json.load(file)
        messages = data.get('messages', [])

        last_25_percent_length = int(len(messages) * 0.99)
        for message in messages[-last_25_percent_length:]:
                        text_content = message.get('text', [])

                        if isinstance(text_content, list):
                for item in text_content:
                    if isinstance(item, str):
                        last_25_percent_text += item

        cleaned_text = re.sub(r'[!@
    return last_25_percent_text
import json
import re


class Util:
    def __init__(self) -> None:
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"              "\U0001F300-\U0001F5FF"              "\U0001F680-\U0001F6FF"              "\U0001F700-\U0001F77F"              "\U0001F780-\U0001F7FF"              "\U0001F800-\U0001F8FF"              "\U0001F900-\U0001F9FF"              "\U0001FA00-\U0001FA6F"              "\U0001FA70-\U0001FAFF"              "\u2600-\u26FF"              "\u2700-\u27BF"              "\u2B50"              "\U0001F1E6-\U0001F1FF"              "]+",
            flags=re.UNICODE,
        )
        self.symbols = re.compile(
            "[" '"' "\“" '"' "'" "\-" "\*" "\•" "\ℹ" "\﻿" "\_" "]+"
        )
        self.url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        self.mention_pattern = r"@(\w+)"

    def read_file(self, file_path: str) -> dict:
                with open(file_path, "r") as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, "w") as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item["text"])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {"id": [], "text": [], "date": []}
        for message in messages:
            if message["type"] != "message" or len(message["text"]) == 0:
                continue
            parsed_messages["id"].append(message["id"])
            message_content = self.parse_text(message["text"])
            parsed_messages["text"].append(message_content)
            parsed_messages["date"].append(message["date"])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith("
    def extract_emojis(self, text):
        return "".join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub("", text)

    def extract_symbols(self, text):
        return "".join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(" ", text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def remove_links(self, text):
        return re.sub(self.url_pattern, " ", text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils import utils
letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

util = utils.Util()
def clean_parsed_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return

    

    
    if not os.path.exists(cleaned_dir):

            os.makedirs(cleaned_dir)



    
    for file_name in os.listdir(folder_path):

        base_name, extension = os.path.splitext(file_name)

        print(base_name,extension)

        if extension =='.csv':

            df = pd.read_csv(f"{folder_path}/{file_name}", index_col='id')

            df = df.dropna()

            df = df.replace('\n', ' ', regex=True)

            
            df['hashtags'] = df['text'].apply(lambda x: util.extract_hashtags(x))

            df['text'] = df['text'].str.replace(r'\
            

            
            df['emojis'] = df['text'].apply(util.extract_emojis)

            


            for letter in letters:

                for i in range(len(letter[0])):

                    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

            
            df['symbols'] = df['text'].apply(util.extract_symbols)

            df['text'] = df['text'].apply(util.remove_symbols)

            
            df['links'] = df['text'].apply(util.extract_urls)

            df['text'] = df['text'].apply(util.remove_links)



            df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

            df['text'] = df['text'].replace(r'!+', '!', regex=True)

            df['text'] = df['text'].replace(r'\.+', '', regex=True)

            base_name, extension = os.path.splitext(file_name)

            df.to_csv(f"{cleaned_dir}/{base_name}.csv")

            df['text'].to_csv(f"{cleaned_dir}/{base_name}.txt", index=False, header=False)





        
clean_parsed_data(parsed_dir)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils.utils import Util
util = Util()
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"
def process_raw_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return



    
    for filename in os.listdir(folder_path):

        print(filename)

        data = util.read_file(f'{folder_path}/{filename}')

        parsed_message = util.parse_messages(data['messages'])



        
        df = pd.DataFrame(parsed_message)

        df.set_index('id', inplace=True)

        base_name, extension = os.path.splitext(filename)

        
        if not os.path.exists(parsed_dir):

            os.makedirs(parsed_dir)

        df.to_csv(f'{parsed_dir}/{base_name}.csv')

        
process_raw_data(raw_dir)
!pip install transformers



from transformers import AutoModelForSequenceClassification, AutoTokenizer

sys.path.append(os.path.abspath(os.path.join('../data/')))










input_text = "እኔ አማርኛ መናገር እረዳለሁ"




input_ids = tokenizer.encode(input_text, return_tensors="pt")




output_ids = model.generate(input_ids)




output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Amharic text:", output_text)
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)

from peft import LoraConfig

from trl import SFTTrainer
base_model ="Samuael/llama-2-7b-tebot-amharic"


tokenizer = AutoTokenizer.from_pretrained(base_model, add_prefix_space =True)

tokenizer

def tokenize_function(data):

    text = data['message']

    tokenizer.truncation_side= "left"

    tokenized_input =tokenizer(text, return_tensors="np",truncation=True, max_length=512)

    return tokenized_input

    

    
if tokenizer.pad_token is None:

    tokenizer.add_special_tokens({'pad_token':'[PAD]'})

    model.resize

    

    
data_set= "data/cleaned"

def load_dataset(data_set_path):

    dataset=
import argparse
from dataclasses import dataclass
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
import os
from langchain_openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

CHROMA_PATH = './chromadb/'

client = OpenAI(
    api_key=OPENAI_API_KEY
)

core_embeddings_model = None
def get_context():
        vectorstore = Chroma(persist_directory="./cachce",embedding_function=core_embeddings_model)
    
    retriever = vectorstore.as_retriever()
        return retriever

def generate_add(user_input, context):
    template = f'''
    Generate an advertisement given the following context.    
    You must use the following context:
    {context}
    '''   
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": template},{"role": "user", "content": user_input}],
        n=3,
    )

    return response
from peft import PeftModel
from transformers import LlamaForCausalLM, LlamaConfig

def load_model(model_name, quantization):
    model = LlamaForCausalLM.from_pretrained(
        model_name,
        return_dict=True,
        load_in_8bit=quantization,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    return model


def load_peft_model(model, peft_model):
    peft_model = PeftModel.from_pretrained(model, peft_model)
    return peft_model

def load_llama_from_config(config_path):
    model_config = LlamaConfig.from_pretrained(config_path) 
    model = LlamaForCausalLM(config=model_config)
    return model
import fire
import torch
import os
import sys
import time
import json 
from typing import List

from transformers import LlamaTokenizer, LlamaForCausalLM
from model_utils import load_model, load_peft_model

BASE_PROMPT = """Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers.
Human: {}
Assistant [Amharic] : """

def main(
    model_name: str="",
    peft_model: str=None,
    quantization: bool=False,
    max_new_tokens =400,     prompt_file: str=None,
    seed: int=42,     do_sample: bool=True,     min_length: int=None,     use_cache: bool=True,      top_p: float=1.0,     temperature: float=1.0,     top_k: int=1,     repetition_penalty: float=1.0,     length_penalty: int=1,     enable_azure_content_safety: bool=False,     enable_sensitive_topics: bool=False,     enable_saleforce_content_safety: bool=False,     **kwargs
):    
    
    print("***Note: model is not set up for chat use case, history is reset after each response.")
    print("***Ensure that you have replaced the default LLAMA2 tokenizer with the Amharic tokenizer")
    
        torch.cuda.manual_seed(seed)
    torch.manual_seed(seed)
    
    MAIN_PATH = '/model/Llama-2-7b-hf'
        peft_model = '/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'
    model_name = MAIN_PATH
    quantization = True
    model = load_model(model_name, quantization)

    tokenizer = LlamaTokenizer.from_pretrained(model_name)
    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))

    if peft_model:
        model = load_peft_model(model, peft_model)

    model.eval()

    while True:

        
        user_query = input('Type question in Amharic or English: ')
        user_prompt = BASE_PROMPT.format(user_query)
        batch = tokenizer(user_prompt, return_tensors="pt")
        batch = {k: v.to("cuda") for k, v in batch.items()}
        start = time.perf_counter()
        with torch.no_grad():
            outputs = model.generate(
                **batch,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                top_p=top_p,
                temperature=temperature,
                min_length=min_length,
                use_cache=use_cache,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                length_penalty=length_penalty,
                **kwargs 
            )
        e2e_inference_time = (time.perf_counter()-start)*1000
        print(f"the inference time is {e2e_inference_time} ms")
        
        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("MODEL_OUTPUT: {}".format(output_text))
        
if __name__ == "__main__":
    fire.Fire(main)
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from transformers import AutoTokenizer, AutoModel
import os
import torch
import shutil
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

model_name = 'Davlan/bert-base-multilingual-cased-finetuned-amharic'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

data_path = '../data/'
CHROMA_PATH = '../RAG/chromadb'

def embed_doc(document):
    
        encoded_input = tokenizer(document, padding=True, truncation=True, return_tensors='pt')
    
        with torch.no_grad():
        outputs1 = model(**encoded_input)

    

        embeddings1 = outputs1.last_hidden_state.squeeze(dim=0)
    

        
    return embeddings1

def load_documents(data_path):    
    try:
        loader = DirectoryLoader(data_path)
        documents = loader.load()       
        print("data loaded sucessfully")
        return documents[0].page_content
    except:
        print("document not found!")
        return None
    

def split_text(documents:list[Document]):
    try:
        text_spliter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=500,
            length_function=len,
            add_start_index = True
        )
        chunk = text_spliter.split_documents(documents)
        print("data splited successfuly!")
        return chunk
    except:
        print("document not found")

def save_chunks_to_chroma(chunks):
        if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
    try:
        db = Chroma.from_documents(chunks,embed_doc(),\
                                persist_directory=CHROMA_PATH)
        db.persist()
        print("Vectorstore created successfully!")
    except:
        print("Couldn't create the vectore database")

def generate_data_store():
    documents = load_documents(data_path)
    chunks = split_text(documents)
    embeding1 = embed_doc(chunks)
    print(embeding1)
    save_chunks_to_chroma(embeding1) 

def main():
    generate_data_store()      


if __name__ == "__main__":
    main()
import torch
from contextlib import nullcontext
from transformers import (
    LlamaForCausalLM, 
    LlamaTokenizer, 
    TrainerCallback, 
    default_data_collator, 
    Trainer, 
    TrainingArguments
)
from peft import (
    LoraConfig,
    TaskType,
    prepare_model_for_int8_training,
    PeftModel
)

from pathlib import Path
from utils.dataset_utils import get_preprocessed_dataset
from configs.datasets import amharic_dataset

def print_trainable_parameters(model):
    print("Trainable Parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f" - {name}")


def finetune():
    LLAMA_DIR = '/model/Llama-2-7b-hf'
    PT_DIR = '/model/llama-2-amharic-3784m'
    OUTPUT_DIR = "/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output"

    tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)

    model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)


    train_dataset = get_preprocessed_dataset(tokenizer, amharic_dataset, 'train')


    model.train()



    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))


    print('loading the pretrained model from config')

    model = prepare_model_for_int8_training(model)
    model = PeftModel.from_pretrained(model, PT_DIR)
    model.print_trainable_parameters()
    lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            modules_to_save = ["embed_tokens","lm_head"]
        )

    enable_profiler = False


    config = {
        'lora_config': lora_config,
        'learning_rate': 1e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 1,
        'per_device_train_batch_size': 2,
        'gradient_checkpointing': False,
    }

        if enable_profiler:
        wait, warmup, active, repeat = 1, 1, 2, 1
        total_steps = (wait + warmup + active) * (1 + repeat)
        schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
        profiler = torch.profiler.profile(
            schedule=schedule,
            on_trace_ready=torch.profiler.tensorboard_trace_handler(f"{OUTPUT_DIR}/logs/tensorboard"),
            record_shapes=True,
            profile_memory=True,
            with_stack=True)

        class ProfilerCallback(TrainerCallback):
            def __init__(self, profiler):
                self.profiler = profiler

            def on_step_end(self, *args, **kwargs):
                self.profiler.step()

        profiler_callback = ProfilerCallback(profiler)
    else:
        profiler = nullcontext()


        training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        bf16=True,                  logging_dir=f"{OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=10,
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=1,
        warmup_ratio=0.03,
        optim="adamw_torch_fused",
        max_steps=total_steps if enable_profiler else -1,
        **{k:v for k,v in config.items() if k != 'lora_config'}
    )

    with profiler:
                trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=default_data_collator,
            callbacks=[profiler_callback] if enable_profiler else [],
        )

        print_trainable_parameters(model)

                trainer.train()

    model.save_pretrained(OUTPUT_DIR)


finetune()
from dataclasses import dataclass

@dataclass
class amharic_dataset:
    dataset: str = "amharic_dataset"
    train_split: str = "train"
    test_split: str = "val"
    data_path: str = "/data/fine_tun_data6.json"
import copy
import json
import torch


from torch.utils.data import Dataset



class InstructionDataset(Dataset):
    def __init__(self, dataset_config, tokenizer, partition="train", max_words=50):
        self.ann = json.load(open(dataset_config.data_path))

        if partition == "train":
            self.ann = self.ann
        else:
            self.ann = self.ann[:200]

        self.max_words = max_words
    
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.ann)

    def __getitem__(self, index):

        ann = self.ann[index]
        data = self.create_prompt_formats(ann)
        prompt = data['prompt']
        example = data['text']  
        prompt = torch.tensor(
            self.tokenizer.encode(prompt), dtype=torch.int64
        )

        example = self.tokenizer.encode(example)

        example.append(self.tokenizer.eos_token_id)
        example = torch.tensor(
            example, dtype=torch.int64
        )
        padding = self.max_words - example.shape[0]

        if padding > 0:
            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - 1))
        elif padding < 0:
            example = example[: self.max_words]

        labels = copy.deepcopy(example)

        labels[: len(prompt)] = -1

        example_mask = example.ge(0)
        label_mask = labels.ge(0)
        example[~example_mask] = 0
        labels[~label_mask] = 0
        example_mask = example_mask.float()
        label_mask = label_mask.float()

        return {
            "input_ids": example,
            "labels": labels,
            "attention_mask":example_mask,
        }
    
    def create_prompt_formats(self,sample):
        """
        Format various fields of the sample ('text', 'label',)
        Then concatenate them using two newline characters
        :param sample: Sample dictionnary
        """

        INTRO_BLURB = "Generate an advertisement given a category"
        INSTRUCTION_KEY = "        RESPONSE_KEY = "Response:"
        END_KEY = "
        blurb = f"{INTRO_BLURB}"
        text = f"{INSTRUCTION_KEY}\n{sample['input']}"
        response = f"{RESPONSE_KEY}\n{sample['output']}"
        end = f"{END_KEY}"

        parts = [part for part in [blurb, text, response, end] if part]

        formatted_prompt = "\n\n".join(parts)

        sample["text"] = formatted_prompt
        parts = [part for part in [blurb, text,] if part]
        formatted_prompt = "\n\n".join(parts)

        sample["prompt"]= formatted_prompt

        return sample
"""
Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.

Here is the full list of checkpoints on the hub that can be fine-tuned by this script:
https://huggingface.co/models?filter=text-generation
"""

import logging
import numpy as np
import math
import os
import sys
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Dict, Any, Mapping
from pathlib import Path
import datasets
import torch
from datasets import load_dataset, concatenate_datasets

import transformers
from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    is_torch_tpu_available,
    set_seed,
)
from transformers.testing_utils import CaptureLogger
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version

from sklearn.metrics import accuracy_score
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora_model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")

        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)


def accuracy(predictions, references, normalize=True, sample_weight=None):
        return {
            "accuracy": float(
                accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
            )
        }


def compute_metrics(eval_preds):
    preds, labels = eval_preds
            labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)


def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
                        logits = logits[0]
    return logits.argmax(dim=-1)


def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}

                if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["labels"] = torch.tensor([f["label"] for f in features], dtype=dtype)
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)

        
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else:
                    batch[k] = torch.tensor([f[k] for f in features])
    except ValueError:         for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else:
                    batch[k] = torch.tensor([features[0][k]] * len(features))

    return batch


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The tokenizer for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides can't be used in combination with --config_name or --model_name_or_path"
            )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    dataset_dir: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional input sequence length after tokenization. "
                "The training dataset will be truncated in block of this size for training. "
                "Default to the model max input length for single sentence inputs (take into account special tokens)."
            )
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep line breaks when using TXT files or not."}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed stored"})

    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")


@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj,v_proj")
    lora_rank : Optional[int] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.1)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[bool] = field(default=False)
    peft_path : Optional[str] = field(default=None)


logger = logging.getLogger(__name__)


def main():

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, MyTrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

            send_example_telemetry("run_clm", model_args, data_args)

        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,          handlers=[logging.StreamHandler(sys.stdout)],)


    if training_args.should_log:
                transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
        logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )

        last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

        set_seed(training_args.seed)

    config_kwargs = {
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else:
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("You are instantiating a new config instance from scratch.")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")

    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script."
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )

                tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            output = tokenizer(examples["text"])
                if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return output
    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)

        def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
                        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
                result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    with training_args.main_process_first(desc="dataset map tokenization and grouping"):
        lm_datasets = []
        path = Path(data_args.dataset_dir)
        files = [file.name for file in path.glob("*.txt")]
        if training_args.debug_mode is True:
            files = [files[0]]
        print("printing files")
        print(files)    
        for idx, file in enumerate(files):
            data_file = os.path.join(path, file)
            filename = ''.join(file.split(".")[:-1])
            cache_path = os.path.join(data_args.data_cache_dir, filename)
            os.makedirs(cache_path, exist_ok=True)
            try:
                processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=False)
                logger.info(f'training datasets-{filename} has been loaded from disk')
            except Exception:
                cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                os.makedirs(cache_dir, exist_ok=True)
                raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                logger.info(f"{file} has been loaded")
                tokenized_dataset = raw_dataset.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns="text",
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'tokenized.arrow') for k in raw_dataset},
                    desc="Running tokenizer on dataset",
                )
                grouped_datasets = tokenized_dataset.map(
                    group_texts,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'grouped.arrow') for k in tokenized_dataset},
                    desc=f"Grouping texts in chunks of {block_size}",
                )
                processed_dataset = grouped_datasets
                processed_dataset.save_to_disk(cache_path)
            if idx == 0:
                lm_datasets = processed_dataset['train']
            else:
                assert lm_datasets.features.type == processed_dataset["train"].features.type
                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset["train"]])

        lm_datasets = lm_datasets.train_test_split(test_size = data_args.validation_split_percentage)

    if training_args.do_train:
        train_dataset = lm_datasets['train']
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))
        logger.info(f"Num train_samples  {len(train_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(train_dataset[0]['input_ids']))
    if training_args.do_eval:
        eval_dataset = lm_datasets["test"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        logger.info(f"Num eval_samples  {len(eval_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(eval_dataset[0]['input_ids']))



    if model_args.model_name_or_path:
        torch_dtype = (
            model_args.torch_dtype
            if model_args.torch_dtype in ["auto", None]
            else getattr(torch, model_args.torch_dtype)
        )
        model = LlamaForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool(".ckpt" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            use_auth_token=True if model_args.use_auth_token else None,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
        )
    else:
        model = AutoModelForCausalLM.from_config(config)
        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")

    model_vocab_size = model.get_output_embeddings().weight.size(0)
    if not (
       (model_vocab_size==32000 and len(tokenizer)==51008) or \
       (model_vocab_size==32000 and len(tokenizer)==32000) or \
       (model_vocab_size==51008 and len(tokenizer)==51008) or \
       (model_vocab_size==49954 and len(tokenizer)==49954)
       
    ):
        raise ValueError(
            f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
            "Valid configurations (base model / tokenizer):\n"
            "- Continue pre-training original LLaMA: 32000 / 32000 \n"
            "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
            "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
            "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n")

    model.resize_token_embeddings(len(tokenizer))
    if training_args.peft_path is not None:
        logger.info("Peft from pre-trained model")
        model = PeftModel.from_pretrained(model, training_args.peft_path)
    else:
        logger.info("Init new peft model")
        target_modules = training_args.trainable.split(',')
        modules_to_save = training_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')
        lora_rank = training_args.lora_rank
        lora_dropout = training_args.lora_dropout
        lora_alpha = training_args.lora_alpha
        logger.info(f"target_modules: {target_modules}")
        logger.info(f"lora_rank: {lora_rank}")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=lora_rank, lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            modules_to_save=modules_to_save)
        model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

                                

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
        modules_to_save = ["embed_tokens","lm_head"]
    )

    """
        config = {
        'lora_config': lora_config,
        'learning_rate': 2e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 2,
        'per_device_train_batch_size': 1,
        'per_device_eval_batch_size': 1,
        'gradient_checkpointing': True,
    }
    """


    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=fault_tolerance_data_collator,
        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
        if training_args.do_eval and not is_torch_tpu_available()
        else None,
    )

    """
    output_dir=training_args.output_dir,
    overwrite_output_dir=True,
    bf16=True,          logging_dir=f"{training_args.output_dir}/logs",
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="steps",
    save_steps=5000,
    optim="adamw_torch_fused",
    max_steps=-1,     **{k:v for k,v in config.items() if k != 'lora_config'}
    """

    trainer.add_callback(SavePeftModelCallback)
        if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)

        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
    model.save_pretrained(training_args.output_dir)
        if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()

        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
        try:
            perplexity = math.exp(metrics["eval_loss"])
        except OverflowError:
            perplexity = float("inf")
        metrics["perplexity"] = perplexity

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)


if __name__ == "__main__":
    main()
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=/model/Llama-2-7b-hf
amharic_tokenizer_path=/model/Llama-2-7b-hf
dataset_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/data/cleaned
data_cache=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/cache
per_device_train_batch_size=32
per_device_eval_batch_size=1
gradient_accumulation_steps=1
output_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output

python pretrain.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${amharic_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 1 \
    --save_steps 7528 \
    --evaluation_strategy steps \
    --eval_steps 3000 \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --bf16 \
    --overwrite_output_dir \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --gradient_checkpointing \
import pandas as pd

df = pd.read_json("/data/fine_tun_data2.json")

df.tail()
df[df['output']=='not advertisement'].iloc[1]['input']
'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: ለኢሬቻ በኣል ወደቢሾፍቱ የተጓዙት የሲዳማ ወጣቶች(ኤጄቶዎች) ከሰኣታት በፊት ቢሾፍቱ ገብተዋል። @tsegabwolde @tikvahethiopia''
import pandas as pd

df = pd.read_csv("/data/wasu_mohammed_labeled.csv")

df.head()
df.shape
df['label'] = df['label'].fillna("Not Advertisement")

df.tail(5)
df
from datasets import Dataset




data_dict = {"text": df['text'].tolist()}




dataset = Dataset.from_dict(data_dict)

dataset.save_to_disk("../data/train")




from peft import PeftModel

from transformers import LlamaForCausalLM, LlamaConfig




def load_model(model_name, quantization):

    model = LlamaForCausalLM.from_pretrained(

        model_name,

        return_dict=True,

        load_in_8bit=quantization,

        device_map="auto",

        low_cpu_mem_usage=True,

    )

    return model






def load_peft_model(model, peft_model):

    peft_model = PeftModel.from_pretrained(model, peft_model)

    return peft_model




def load_llama_from_config(config_path):

    model_config = LlamaConfig.from_pretrained(config_path) 

    model = LlamaForCausalLM(config=model_config)

    return model
from transformers import LlamaTokenizer





MAIN_PATH = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)



example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685





tokens = tokenizer.tokenize(example)

print(tokens)
print(len(tokenizer))
example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685




tokens = tokenizer.tokenize(example)

print(tokens)
df['text'][0]
df = pd.read_csv("/data/wasu_mohammed_labeled.csv")




total_word_count = 0

total_tokens = 0


for index, row in df.iterrows():

    
    text = row['text']

    if not isinstance(text, str): 

        continue



    

    
    word_count = len(text.split())



    
    total_word_count += word_count

    tokens = tokenizer.tokenize(text)

    total_tokens+=tokens

    print(tokens)

    






print("Total Word Count:", total_word_count)

print("Total tokens count: ",total_tokens)
total_tokens
df.shape
from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    HfArgumentParser,

    TrainingArguments,

    pipeline,

    LlamaForCausalLM, 

    LlamaTokenizer,

    logging,

)

from peft import (

    LoraConfig,

    TaskType,

    prepare_model_for_int8_training,

    PeftModel

)

import torch

LLAMA_DIR = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)



model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)

embedding_size = model.get_input_embeddings().weight.shape[0]



if len(tokenizer) != embedding_size:

    print("resize the embedding size by the size of the tokenizer")

    model.resize_token_embeddings(len(tokenizer))





new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'

model = PeftModel.from_pretrained(model, new_model)




prompt = "Who is Leonardo Da Vinci?"

pipe = pipeline(task="text-generation", model=model, tokenizer=model, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
ቨርቹዋል ረዳቶች እንደ Amazon&
run-20240203_155644-4hm9i4tp
! pip install transformers bitsandbytes peft trl accelerate
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
base_model = "NousResearch/Llama-2-7b-chat-hf"

guanaco_dataset = "mlabonne/guanaco-llama2-1k"

new_model = "LLama-2-7b-chat-prac"
print(peft.__version__)
dataset = load_dataset(guanaco_dataset, split="train")
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=compute_dtype,

    bnb_4bit_use_double_quant=False,

)
model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=quant_config,

    device_map={"": 0}

)



model.config.use_cache = False

model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_size= "right"
peft_params = LoraConfig(

    lora_alpha = 16,

    lora_dropout = 0.1,

    r=64,

    bias="none",

    task_type= "CAUSAL_LM"

)
training_params = TrainingArguments(

    output_dir = "./results",

    num_train_epochs=1,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim="paged_adamw_32bit",

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-4,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=-1,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type="constant",

    report_to="tensorboard"

)
!pip install --upgrade peft
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
import pandas as pd

import numpy as np

import sys, os

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util

from concurrent.futures import ThreadPoolExecutor
util = Util()

cleaned_dir = "../cleaned"

json_file_path = '../raw/dilela_page.json'




df = pd.read_json(json_file_path)

df.head()
df.shape

columns = ["id", "channel_name", "type", "message_id", "message_type","text", "label", "created_at", "updated_at", ]

new_df = pd.DataFrame(columns=columns)

new_df

telegeram_channel_id  = df["id"][0]

telegram_channel_name = df["name"][0]

telegeram_channel_type = df["type"][0]

message_df = df["messages"]

data = [{

       'telegeram_channel_id': telegeram_channel_id,

       'telegram_channel_name': telegram_channel_name,

       'telegeram_channel_type': telegeram_channel_type,

       'message_id': message.get('id', np.nan),

        'message_type': message.get('type', np.nan),

        'text': message['text_entities'][0]['text'] if message.get('text_entities') and message['text_entities'] else np.nan,

        'created_at': message.get('date', np.nan),

        'update_at': message.get('edited', np.nan),

        }for message in message_df]

message_df = pd.DataFrame(data)

message_df = message_df.sort_values(by='message_id')

message_df.head(20)
message_df.shape

nan_rows_count = message_df.isna().any(axis=1).sum()

nan_rows_count

message_df = message_df.dropna()

message_df.head()
message_df.shape
message_df = message_df.replace('\n', ' ', regex=True)

message_df.head()

message_df["hashtags"] = message_df['text'].apply(lambda text: util.extract_hashtags(text))

message_df.head()

message_df["text"] = message_df["text"].str.replace(r'\
message_df.head()
message_df["emojis"] = message_df["text"].apply(util.extract_emojis)

message_df.head()

message_df['text'] = message_df['text'].apply(util.remove_emojis_using_emoji_pattern)

message_df.tail()
def remove_emojis_parallel(text):

    return util.remove_emojis(text)





with ThreadPoolExecutor() as executor:

    message_df['text'] = list(executor.map(remove_emojis_parallel, message_df['text']))
message_df.head()

message_df.replace('', pd.NA, inplace=True)

nan_rows_count = message_df.isna().any(axis=1).sum()


message_df = message_df.dropna()

message_df.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    message_df['text'] = message_df['text'].str.replace(letter[0][i], letter[1][i])
message_df['symbols'] = message_df['text'].apply(util.extract_symbols)

message_df.head()
message_df['text'] = message_df['text'].apply(util.remove_symbols)

message_df.tail()
message_df['links'] = message_df['text'].apply(util.extract_urls)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

message_df.head()
message_df['mentions'] = message_df['text'].apply(util.extract_mentions)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

message_df.tail()

message_df['text'] = message_df['text'].str.replace('\s+', ' ', regex=True).str.strip()
message_df['text'] = message_df['text'].replace(r'!+', '!', regex=True)

message_df['text'] = message_df['text'].replace(r'\.+', '', regex=True)
message_df.head()

nan_rows_count = message_df['text'].isna().sum()

nan_rows_count


message_df = message_df.dropna(subset='text')

message_df.tail()


message_df = message_df[message_df['text'].str.len() >= 20]
message_df.to_csv(f"{cleaned_dir}/dilela_page.csv")
message_df['text'].to_csv(f"{cleaned_dir}/dilela_page.txt", index=False, header=False)
df = pd.read_csv(f"{cleaned_dir}/dilela_page.csv")

df.head()
df['word_count'] = df['text'].str.split().str.len()

df.columns


df_labeled = df.drop(['Unnamed: 0','telegram_channel_name','telegeram_channel_type','message_id','message_type','hashtags', 'emojis', 'created_at','symbols', 'links','mentions'],axis=1)

df_labeled.rename(columns={'update_at':'date','telegeram_channel_id':'channel_id'},inplace=True)

df_labeled.to_csv(f"{cleaned_dir}/dilela_page_labeled.csv")

len = df_labeled['word_count'].sum()

len
from fastapi import FastAPI, HTTPException, Depends
from typing import Annotated, List
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from utils import simple_rag
from utils import hugging_face_hub

app = FastAPI()

origins = ["http://localhost:5173"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
)


class RagResponseBase(BaseModel):
    question: str
    answer: str


class HugResponseBase(BaseModel):
    question: str
    answer: str


class AmharicModelWithRAGBase(BaseModel):
    question: str
    answer: str


@app.get("/getanswer", response_model=RagResponseBase)
async def return_answer(question: str):
    result = simple_rag.test_RAG(question)
    return result


@app.get("/getHuggingFaceAnswer", response_model=HugResponseBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.invoke_current_hugging_face_model(model, prompt)
    return result


@app.get("/getAmharicModelWithRAGAnswer", response_model=AmharicModelWithRAGBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.use_amharic_model(model, prompt)
    return result
from langchain import OpenAI
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from models import simple_rag_response
import os
from dotenv import load_dotenv

load_dotenv()


def load_data():
    loader = TextLoader("/week_6_challenge_doc.txt")
    documents = loader.load()
    return documents


def return_chunks(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)
    texts = text_splitter.split_documents(documents)
    return texts


def return_chain(texts):
    embeddings = OpenAIEmbeddings()
    store = Chroma.from_documents(
        texts, embeddings, collection_name="challenge_document"
    )
    llm = OpenAI(temperature=0)
    return RetrievalQA.from_chain_type(llm, retriever=store.as_retriever())


def test_RAG(question):
    documents = load_data()
    chunks = return_chunks(documents)
    chain = return_chain(chunks)
    response = chain.run(question)
    return simple_rag_response.RagResponse(question, response)
import React, { useState , useRef} from 'react'
import 'bootstrap/dist/css/bootstrap.css'
import FileInput from  './components/FileInput'
import TextInputWithLable from  './components/TextInputWithLable'
import Dropdown from './components/Dropdown'
import NavBarComp from './components/Navbar'
import SpinnerWithText from './components/SpinnerWithText'
// import './App.css'
import api from './api/api'
function App() {
  const [answer,setAnswer] = useState([]);
  const [isShow,setShow] = useState(false);

  const fetchResponse = async () =>{
    console.log(ref.current.value);
    const question = ref.current.value;
    setShow(true)
    const response = await api.get('/getanswer?question='+question);
    console.log(response.data);
    setAnswer(response.data)
    setShow(false)
  }
  const ref = useRef(null);

  return (
    <React.Fragment>
      
      <NavBarComp />
      <main className='container'>
        <form className="row g-3" >
          
            <div>
              <label htmlFor="inputLable" className="form-label">Input Ad description to be generated</label>
              <textarea className="form-control" id="inputTextarea" rows="7" ref={ref}/>
            </div>

            {isShow && <SpinnerWithText />}

            <button type="button" className="btn btn-primary mb-4" onClick={fetchResponse}>Get Ad</button> 

            <div>
              <TextInputWithLable value= {answer}/>
            </div>

        </form>
      </main>

    </React.Fragment>
  )
}

export default App
import pandas as pd

import json
df = pd.read_json('sheger.json')

df.head()
df.info()
df.messages.iloc[0]
df.columns
message_df = pd.json_normalize(df.messages)
message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)

    return ''.join(extracted_text)




message_df['extracted_text'] = message_df['text'].apply(extract_text_from_data)
message_df = message_df[['id','type','date','extracted_text']]

message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)



    
    full_text = ''.join(extracted_text)



    
    substrings_to_remove = ['\n\n', '@sheger_press\n@sheger_press', '❗️❗️❗️']

    for substring in substrings_to_remove:

        full_text = full_text.replace(substring, '')



    
    full_text = ''.join(char for char in full_text if char.isalnum() or char.isspace())



    return full_text.strip()




message_df['cleaned_text'] = message_df['extracted_text'].apply(extract_text_from_data)

message_df.head()
df = df[['name','type','id']]

df.rename(columns={'name':'channel_name',

                  'type':'channel_type',

                  'id':'channel_id'}, inplace=True)
df.head()
message_df=message_df[['id','type','date','cleaned_text']]

message_df.rename(columns = {'id':'message_id',

                  'type':'message_type',

                  'date':'message_date',

                  'cleaned_text':'text'}, inplace = True)

message_df.head()
sheger_df= pd.concat([df,message_df], axis =1)
sheger_df.head()
press_df = press_df[press_df.text != '']

press_df.head()
sheger_df.to_csv('sheger_press.csv', index = None)
import pandas as pd

import json

import os

from pprint import pprint

import bitsandbytes as bnb

import torch

import torch.nn as nn

import transformers

from datasets import load_dataset, Dataset

from huggingface_hub import notebook_login



from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipelines, logging
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa' 
dataset = load_dataset(dataset_name, split="train")
MODEL_NAME = "mistralai/Mistral-7B-v0.1"

new_model = "amharic-mistral-7b"

config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, load_in_4bit=True)






bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_use_double_quant=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.bfloat16

)





model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    device_map="auto",

    trust_remote_code=True,

    quantization_config=bnb_config,

)



tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tokenizer.pad_token = tokenizer.eos_token
model = prepare_model_for_kbit_training(model)
use_4bit = True




bnb_4bit_compute_dtype = "float16"




bnb_4bit_quant_type = "nf4"




use_nested_quant = False

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

if compute_dtype == torch.float16 and use_4bit:

    major, _ = torch.cuda.get_device_capability()

    if major >= 8:

        print("=" * 80)

        print("Your GPU supports bfloat16: accelerate training with bf16=True")

        print("=" * 80)
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=2,

    lora_alpha=32,

    target_modules=get_last_layer_linears(model),

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM"

)



model = get_peft_model(model, config)





output_dir = "./results"




num_train_epochs = 1




fp16 = False

bf16 = False




per_device_train_batch_size = 4




per_device_eval_batch_size = 4




gradient_accumulation_steps = 1




max_grad_norm = 0.3




learning_rate = 2e-4




weight_decay = 0.001




optim = "paged_adamw_32bit"




lr_scheduler_type = "constant"




warmup_ratio = 0.03





group_by_length = True




save_steps = 25




logging_steps = 25


base_model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(base_model, new_model)

model = model.merge_and_unload()

training_arguments = TrainingArguments(

    output_dir=output_dir,

    num_train_epochs=num_train_epochs,

    per_device_train_batch_size=per_device_train_batch_size,

    gradient_accumulation_steps=gradient_accumulation_steps,

    optim=optim,

    save_steps=save_steps,

    logging_steps=logging_steps,

    learning_rate=learning_rate,

    weight_decay=weight_decay,

    fp16=fp16,

    bf16=bf16,

    max_grad_norm=max_grad_norm,

    max_steps=25,

    warmup_ratio=warmup_ratio,

    group_by_length=group_by_length,

    lr_scheduler_type=lr_scheduler_type,

    report_to="tensorboard"

)
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
class RagResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class HugResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class AmharicModelResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass
from dotenv import load_dotenv
from models import simple_rag_response

load_dotenv()

from langchain import HuggingFaceHub


def invoke_current_hugging_face_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.HugResponse(prompt, response)


def use_amharic_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.AmharicModelResponse(prompt, response)
import time

import sentencepiece as spm
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=m --vocab_size=100000')

sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))




print(sp.encode_as_pieces('ሃይ ሰላም ናችሁ?'))



spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=am-word --model_type=word  --vocab_size=100000')



sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.encode_as_ids('ፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.decode_ids([47914, 1024, 33, 7716, 29922, 26700]))
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U00002049 \U0000FE0F"                                         u"\U0000203C"                                         u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'
        print(self.emoji_pattern.pattern)

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub(' ', text)
        
    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import pandas as pd

import csv, os, sys

from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import
"""
Convert .txt to csv

"""

import csv
from sklearn.model_selection import train_test_split

def convert_txt_to_csv(input_txt, output_csv):
    with open(input_txt, 'r', encoding='utf-8') as infile, open(output_csv, 'w', encoding='utf-8', newline='') as outfile:
       
        reader = infile.readlines()
        data = [line.strip().split() for line in reader]
        csv_writer = csv.writer(outfile)
        csv_writer.writerows(data)

def split_data(input_csv, output_train_csv, output_test_csv, output_val_csv, test_size=0.2, val_size=0.1, random_seed=42):
    with open(input_csv, 'r', encoding='utf-8') as file:
        csv_reader = csv.reader(file)
        data = list(csv_reader)
        
    train_data, test_val_data = train_test_split(data, test_size=(test_size + val_size), random_state=random_seed)
    test_data, val_data = train_test_split(test_val_data, test_size=(val_size / (test_size + val_size)), random_state=random_seed)

    with open(output_train_csv, 'w', encoding='utf-8', newline='') as train_file:
        csv_writer = csv.writer(train_file)
        csv_writer.writerows(train_data)

    with open(output_test_csv, 'w', encoding='utf-8', newline='') as test_file:
        csv_writer = csv.writer(test_file)
        csv_writer.writerows(test_data)

    with open(output_val_csv, 'w', encoding='utf-8', newline='') as val_file:
        csv_writer = csv.writer(val_file)
        csv_writer.writerows(val_data)

if __name__ == "__main__":
    input_txt_file = '/home/biniyam_ajaw/finetuning/data/dataset.txt'
    output_csv_file = '/home/biniyam_ajaw/finetuning/data/output_data.csv'
    output_train_csv = '/home/biniyam_ajaw/finetuning/data/train_data.csv'
    output_test_csv = '/home/biniyam_ajaw/finetuning/data/test_data.csv'
    output_val_csv = '/home/biniyam_ajaw/finetuning/data/val_data.csv'

    convert_txt_to_csv(input_txt_file, output_csv_file)
    split_data(output_csv_file, output_train_csv, output_test_csv, output_val_csv)
    print("Conversion to CSV and data split completed.")
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=100000)

import tokenizers

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

folder = 'data'
files = [f"/home/biniyam_ajaw/finetuning/{folder}/{split}.csv" for split in ["test_data", "train_data", "valid_data"]]
tokenizer.train(files, trainer)

from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")

from transformers import PreTrainedTokenizerFast

custom_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
custom_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
custom_tokenizer.save_pretrained("amharic_tokenizer")

custom_tokenizer.push_to_hub("amharic_tokenizer")
max-width: 100%;
  width: 100%;
  height: 100%;
  margin: 5 auto;
  padding: 2rem;
  text-align: start;
  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function FileInput (){
    return(
        <div>
            <div className="input-group mb-3">
                <input type="file" className="form-control" id="inputGroupFile02"/>
                <label clclassNameass="input-group-text" for="inputGroupFile02">Upload</label>
            </div>
        </div>
      
    );
}


export default FileInput;
/* :root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;
  width: 100%;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}
a {
  font-weight: 500;
  color:   text-decoration: inherit;
}
a:hover {
  color: }

body {
  margin: 0;
  display: flex;
  place-items: center;
  min-width: 320px;
  min-height: 100vh;
}

h1 {
  font-size: 3.2em;
  line-height: 1.1;
}

button {
  border-radius: 8px;
  border: 1px solid transparent;
  padding: 0.6em 1.2em;
  font-size: 1em;
  font-weight: 500;
  font-family: inherit;
  background-color:   cursor: pointer;
  transition: border-color 0.25s;
}
button:hover {
  border-color: }
button:focus,
button:focus-visible {
  outline: 4px auto -webkit-focus-ring-color;
}

@media (prefers-color-scheme: light) {
  :root {
    color:     background-color:   }
  a:hover {
    color:   }
  button {
    background-color:   }
} */
import Container from 'react-bootstrap/Container';
import Nav from 'react-bootstrap/Nav';
import Navbar from 'react-bootstrap/Navbar';
import NavDropdown from 'react-bootstrap/NavDropdown';

function NavBarComp() {
  return (
    <Navbar expand="lg" className="bg-body-tertiary container-fluid">
      <Container >
        <Navbar.Brand href="        <Navbar.Toggle aria-controls="basic-navbar-nav" />
        <Navbar.Collapse id="basic-navbar-nav">
          <Nav className="me-auto">
            <NavDropdown title="Select Model" id="basic-nav-dropdown">
                <NavDropdown.Item href="                <NavDropdown.Item href="                <NavDropdown.Item href="            </NavDropdown>
          </Nav>
        </Navbar.Collapse>
      </Container>
    </Navbar>
  );
}

export default NavBarComp;
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function TextInputWithLabel (props) {
    return(
        <div>
            <div className="mb-3">
                <label htmlFor="exampleFormControlTextarea1" className="form-label">Generated Ad</label>
                <textarea className="form-control" id="exampleFormControlTextarea1" rows="7"  value={props.value.answer}/>
            </div>
        </div>
      
    );
}


export default TextInputWithLabel;
import torch

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging

from datasets import load_dataset

import os, sys

from huggingface_hub import notebook_login

import torch.nn as nn

import getpass

from trl import SFTTrainer

from peft import PeftConfig, LoraConfig
os.environ["HUGGING_FACE_HUB_TOKEN"] = getpass.getpass("Token:")

assert os.environ["HUGGING_FACE_HUB_TOKEN"]
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")

double_quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model_id = "microsoft/phi-2"

new_model = 'amharic-phi'

model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cuda:0', quantization_config=nf4_config)
tokenizer = AutoTokenizer.from_pretrained("dagim/amharic_tokenizer")



tokenizer.tokenize("ከአሜሪካ ወደ አዲስ አበባለመጓዝምንያህልጊዜይወስዳል??")
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa'

dataset = load_dataset(dataset_name, split="train")
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=4,

    lora_alpha=32,

    
    lora_dropout=0.03,

    bias='none',

    task_type="CAUSAL_LM"

)

training_arguments = TrainingArguments(

    output_dir="./results",

    num_train_epochs=2,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim='paged_adamw_32bit',

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-8,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=25,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type='constant',

    report_to="tensorboard",

    gradient_checkpointing=True

)
trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=config,

    dataset_text_field='inputs',

    max_seq_length=None,

    tokenizer=tokenizer,

    args=training_arguments,

    packing=False

)
trainer.train()
trainer.model.save_pretrained(new_model)
logging.set_verbosity(logging.CRITICAL)



prompt = "የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?"

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="/home/biniyam_ajaw/llama-2-amharic-3784m/tokenizer.json")
print(len(tokenizer.encode('የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?')))
import logging
import numpy as np
import math
import os, sys
import torch
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Tuple, Dict, Any, Mapping
from pathlib import Path
import datasets
from datasets import Dataset, DatasetDict, load_dataset, load_metric, concatenate_datasets

from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,  
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
    is_torch_gpu_available,
)

from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.testing_utils import CaptureLogger
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version_core

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora-Model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")
            
        
        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)
        
    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

def accuracy(predictions, references, normalize=True, sample_weight=None):
    return {
        "accuracy": float(
            accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
        )
    }

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)

def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
        logits = logits[0]
    return logits.argmax(dim=-1)

def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}
    
    if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["label"] = torch.tensor([f["label"] for f in features], dtype=dtype)
        
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
            
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)
            
    
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else: batch[k] = torch.tensor([f[k] for f in features])
                
    except ValueError:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                                        batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else: 
                    batch[k] = torch.tensor([features[0][k]] * len(features))
                    
    return batch

MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={"help": ("The tokenizer for weights initialization.")},
    )
    
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    
    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides cannot be used with --config_name or --model_name_or_path. To override some of "
            )
            
@dataclass
class DataTrainingArguments:
    '''
    Arguments pertaining to what data we are going to input our model for training and eval.
    '''
    
    dataset_dir: Optional[str] = field(
        default=None, metadata={"the name of the dataset to use"}
    )

    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name opf the dataset to use"}
    )
    
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training file"}
    )
    
    validation_file: Optional[str] = field(
        default=None, metadata={"help": "This is optional but recommended if you want to use early stopping"}
    )
    
    max_training_sample: Optional[int] = field(
        default=None,
        metadata={
            "help": "Debugging purposes"
        },
    )
    
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging"
        },
    )
    
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    
        block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional"
                "Training dataset will be truncated into a block of this size for training"
                "Default to the model max input sequence"
            )
        }
    )
    
            
    cache_dir: bool = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    
            validation_strategy: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "Percentage of the validation set used at the end of each epoch"
        }
        
    )
        preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes to use for preprocessing"}
    )
    
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep the linebreaks when using txt files or not"}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed store"})
    
    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets >= 2.0.0`")
            
            
@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj, v_proj")
    lora_rank : Optional[str] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.03)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[str] = field(default=False)
    peft_path : Optional[str] = field(default=None)
    
logger = logging.getLogger(__name__)

def main():
    
    parser = HfArgumentParser(ModelArguments, DataTrainingArguments, MyTrainingArguments)
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                model_args, data_args, training_args = parser.parse_parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_to_dataclasses()
        
    send-example_telemetry("run_clm", model_args, data_args)
    
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", datefmt="%m/%d/%Y %H:%M:%S",
                        level=logging.INFO,                         handlers=[logging.StreamHandler(sys.stdout)],)
    
    if training_args.should_log:
        transformers.utils.logging.set_verbosity_info()
        
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
    logger.warning(
        f"Process rank: {training_args.output_dir}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"Distributed training: {bool(training_args.local_rank != -1)}, 16-bits-training: {training_args.fp16}"
    )
    
        
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError (
                f"Outpur dir {training_args.output_dir} already exists and is not mt"
                "Use --overwrite_output_dir to overcome"
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is not None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this, change "
                "the --output-dir or --overwrite_output_dir to train from scratch"
            )
            
        set_seed(training_args.seed)
    
    config_kwargs = {
        "cache_dir": model.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model-args.use_auth_token else None
    }
    
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else: 
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("This is a new config from scratch")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")
            
            
    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None
    }
    
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "Instantiating a tokenizer from scratch"
        )
        
            
    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            return tokenizer(examples[text])
        
        if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^ PLease ignore the warning above ^^^^^^^"
            )
            
        return output

    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024  
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                "Override with `--block_size xxx`"
            
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)
        
        
    def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        
        if total_length >= block_size:
                        total_length = {total_length // block_size} *  block_size
                        result = {
                k: [t[i: i + block_size] for i in range(0, total_length, block_size)]
                for k, t in concatenated_examples.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result
        with training_args.main_process_first(desc="dataset map tokenizer"):
            lm_datasets = []
            path = Path(data_args.dataset_dir)
            filename = [file.name for file in path.glob("*.txt")]
            
            if training_args.debug_mode:
                files = [files[0]]
            for idx, file in enumerate(files):
                data_file = os.path.join(path, file)
                filename = ''.join(file.split('.')[:-1])
                cache_path = os.path.join(data_args.data_cache_dir, filename)
                os.makedirs(cache_path, exist_ok=True)
                try:
                    processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=True)
                    logger.info(f'Training datasets-{filename} has been loaded from disk')
                except Exception:
                    cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                    os.makedirs(cache_dir, exist_ok=True)
                    raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                    logger.info(f"{file} has been loaded")
                    tokenized_dataset = raw_dataset.map(
                        tokenize_function,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        remove_columns="text",
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "tokenized.arrow") for k in raw_dataset},
                        desc="Running tokenizer on the dataset",
                    )
                    
                    grouped_datasets = tokenized_dataset.map(
                        group_texts,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "grouped.arrow") for k in tokenized_dataset},
                        desc=f'Grouping texts in chunks of {block_size}',
            
                    )
                    
                    processed_dataset = grouped_datasets
                    processed_dataset.save_to_disk(cache_path)
                    
                if idx == 0:
                    lm_datasets = processed_dataset['train']
                else:
                    assert lm_datasets.features.type == processed_dataset['train'].features.type
                    lm_dataset = concatenate_datasets([lm_datasets, processed_dataset['train']])
                    
            lm_datasets = lm_datasets.train_test_split(test_size= data_args.validation_split_percentage())
            
        if training_args.do_train:
            train_dataset = lm_datasets["train"]
            
            
            if data_args.max_train_samples is not None:
                max_train_samples = min(len(train_dataset), data_args.max_train_samples)
                train_dataset = train_dataset.select(range(max_train_samples))
                logger.info(f"Num train samples {len(train_dataset)}")
                logger.info("Training example: ")
                logger.info(tokenizer.decode(train_dataset[0]["input_ids"]))
                
                
        if model_args.model_name_or_path:
            torch_dtype = (
                model_args.torch_dtype
                if model_args.torch_dtype in ["auto", None]
                else getattr(torch, model_args.torch_dtype)
            )
            
            model = LlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                from_tf=bool(".cpkt" in model_args.model_name_or_path),
                config=config,
                cache_dir=model_args.cache_dir,
                revision=model_args.model_revision,
                use_auth_token=True if model_args.use_auth_token else None,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
            )
            
        else:
            model = AutoModelForCausalLM.from_config(config)
            n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
            logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M parameters")
        
        model_vocab_size = model.get_output_embeddings().weight.size(0)
        
        if not (
            (model_vocab_size==32000 and len(tokenizer)==51008) or \
            (model_vocab_size==32000 and len(tokenizer)==32000) or \
            (model_vocab_size==51008 and len(tokenizer)==51008) or \
            (model_vocab_size==49954 and len(tokenizer)==49954)
        ):
            raise ValueError(
                f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
                "Valid configurations (base model / tokenizer):\n"
                "- Continue pre-training original LLaMA: 32000 / 32000 \n"
                "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
                "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
                "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n"
            )
            
                model.resize_token_embeddings(len(tokenizer))
        
        if training_args.peft_path is not None:
            logger.info("PEFT from pretrained model")
            model = PeftModel.from_pretrained(model, training_args.peft_path)
        else:
            logger.info("Init new peft model")
            target_modules = training_args.trainable.split(",")
            modules_to_save = training_args.modules_to_save
            if modules_to_save is not None:
                modules_to_save = modules_to_save.split(",")
            lora_rank = training_args.lora_rank
            lora_dropout = training_args.lora_dropout
            lora_alpha = training_args.lora_alpha
            logger.info(f"Target modules: {target_modules}")
            logger.info(f"LoRA Rank: {lora_rank}")
            peft_config = LoraConfig(
                task_type = TaskType.CAUSAL_LM,
                targert_modules = target_modules,
                inference_mode=False,
                r = lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                modules_to_save=modules_to_save,
            )    
            
            model= get_peft_model(model, peft_config)
        model.print_trainable_parameters()
!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb
import os

from dotenv import load_dotenv




load_dotenv()




hf_token = os.getenv("hf_token")



import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
import pandas as pd

file_path = '../../merged.csv'




df = pd.read_csv(file_path)

df
dataset=df[['Text']]

dataset
dataset_2=dataset.copy()










dataset_2
!pip install scikit-learn



from sklearn.model_selection import train_test_split





train_val_data, test_data = train_test_split(dataset_2, test_size=0.20, random_state=42)

train_data, evaluation_data = train_test_split(train_val_data, test_size=0.10, random_state=42)



print('Training dataset shape:', len(train_data))

print('evaluation dataset shape:', len(evaluation_data))

print('Testing dataset shape:', len(test_data))
evaluation_data
import numpy as np


msk = np.random.rand(len(dataset_2)) < 0.8

train_dataset = dataset_2[msk]

test_dataset = dataset_2[~msk]

from datasets import Dataset



test_dataset=Dataset.from_pandas(test_dataset)



train_dataset=Dataset.from_pandas(train_dataset)



evaluation_dataset=Dataset.from_pandas(evaluation_data)
test_dataset



test_dataset=test_dataset.remove_columns("__index_level_0__")

train_dataset=train_dataset.remove_columns("__index_level_0__")

evaluation_dataset=evaluation_dataset.remove_columns("__index_level_0__")


import datasets


main_dataset= datasets.DatasetDict({

    'train': train_dataset,

    'test': test_dataset,

    'evaluate': evaluation_dataset

})
main_dataset
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    AutoTokenizer,

    TrainingArguments,

    pipeline,

)

from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training

from trl import SFTTrainer

base_model = "NousResearch/Llama-2-7b-hf"

new_model = "llama-2-7b-Amh"






tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)

tokenizer.pad_token = tokenizer.unk_token

tokenizer.padding_side = "right"

bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.float16,

    bnb_4bit_use_double_quant=True,

)




peft_config = LoraConfig(

    r=16,

    lora_alpha=32,

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM",

    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']

)

def load_model(model_name, bnb_config):

    n_gpus = torch.cuda.device_count()

    max_memory = f'{23000}MB'



load_model(base_model,bnb_config)
import torch

torch.cuda.empty_cache()


model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=bnb_config,

    device_map={"": 0}

)




model = prepare_model_for_kbit_training(model)
training_dataset=main_dataset
model
import torch




device = torch.device("cuda:0")




torch.cuda.empty_cache()




print(torch.cuda.memory_summary(device))

import torch

torch.cuda.empty_cache()

import torch




n_gpus = torch.cuda.device_count()

print(f"Number of available GPUs: {n_gpus}")




for i in range(n_gpus):

    gpu_memory = torch.cuda.get_device_properties(i).total_memory

    print(f"GPU {i}: Total memory: {gpu_memory / (1024**3)} GB")

from transformers import Trainer, TrainingArguments, BitsAndBytesConfig


training_arguments = TrainingArguments(

        output_dir="../results",

        num_train_epochs=1,

        per_device_train_batch_size=10,

        per_device_eval_batch_size=1,

        gradient_accumulation_steps=1,

        gradient_checkpointing=True,

        fp16=True,

        evaluation_strategy="steps",

        eval_steps=1000,

        logging_steps=1,

        optim="paged_adamw_8bit",

        learning_rate=2e-4,

        lr_scheduler_type="linear",

        warmup_steps=10,

        
        max_steps=10, 
)




trainer = SFTTrainer(

    model=model,

    train_dataset=main_dataset["train"],

    eval_dataset=main_dataset["evaluate"],

    peft_config=peft_config,

    dataset_text_field="Text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_arguments,

)


model.config.use_cache = False  



trainer.train()




trainer.model.save_pretrained(new_model)


prompt = "የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "how can i treat flu, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "tell me about ethiopian politics, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "who the prime minister of ethiopia, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "3 Ethiopian premier league club, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

del model

del pipe

del trainer

import gc

gc.collect()

gc.collect()
import torch

torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(

    base_model,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(model, new_model)

model = model.merge_and_unload()




tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_side = "right"
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils import utils
letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

util = utils.Util()
def clean_parsed_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return

    

    
    if not os.path.exists(cleaned_dir):

            os.makedirs(cleaned_dir)



    
    for file_name in os.listdir(folder_path):

        base_name, extension = os.path.splitext(file_name)

        print(base_name,extension)

        if extension =='.csv':

            df = pd.read_csv(f"{folder_path}/{file_name}", index_col='id')

            df = df.dropna()

            df = df.replace('\n', ' ', regex=True)

            
            df['hashtags'] = df['text'].apply(lambda x: util.extract_hashtags(x))

            df['text'] = df['text'].str.replace(r'\
            

            
            df['emojis'] = df['text'].apply(util.extract_emojis)

            


            for letter in letters:

                for i in range(len(letter[0])):

                    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

            
            df['symbols'] = df['text'].apply(util.extract_symbols)

            df['text'] = df['text'].apply(util.remove_symbols)

            
            df['links'] = df['text'].apply(util.extract_urls)

            df['text'] = df['text'].apply(util.remove_links)



            df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

            df['text'] = df['text'].replace(r'!+', '!', regex=True)

            df['text'] = df['text'].replace(r'\.+', '', regex=True)

            base_name, extension = os.path.splitext(file_name)

            df.to_csv(f"{cleaned_dir}/{base_name}.csv")

            df['text'].to_csv(f"{cleaned_dir}/{base_name}.txt", index=False, header=False)





        
clean_parsed_data(parsed_dir)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils.utils import Util
util = Util()
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"
def process_raw_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return



    
    for filename in os.listdir(folder_path):

        print(filename)

        data = util.read_file(f'{folder_path}/{filename}')

        parsed_message = util.parse_messages(data['messages'])



        
        df = pd.DataFrame(parsed_message)

        df.set_index('id', inplace=True)

        base_name, extension = os.path.splitext(filename)

        
        if not os.path.exists(parsed_dir):

            os.makedirs(parsed_dir)

        df.to_csv(f'{parsed_dir}/{base_name}.csv')

        
process_raw_data(raw_dir)
!pip install transformers


from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM



model_name = "Samuael/llama-2-7b-tebot-amharic"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(model_name)


input_text = "እኔ አማርኛ መናገር እረዳለሁ"




input_ids = tokenizer.encode(input_text, return_tensors="pt")




output_ids = model.generate(input_ids)




output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Amharic text:", output_text)
import json
import re


class Util:
    def __init__(self) -> None:
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"              "\U0001F300-\U0001F5FF"              "\U0001F680-\U0001F6FF"              "\U0001F700-\U0001F77F"              "\U0001F780-\U0001F7FF"              "\U0001F800-\U0001F8FF"              "\U0001F900-\U0001F9FF"              "\U0001FA00-\U0001FA6F"              "\U0001FA70-\U0001FAFF"              "\u2600-\u26FF"              "\u2700-\u27BF"              "\u2B50"              "\U0001F1E6-\U0001F1FF"              "]+",
            flags=re.UNICODE,
        )
        self.symbols = re.compile(
            "[" '"' "\“" '"' "'" "\-" "\*" "\•" "\ℹ" "\﻿" "\_" "]+"
        )
        self.url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        self.mention_pattern = r"@(\w+)"

    def read_file(self, file_path: str) -> dict:
                with open(file_path, "r") as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, "w") as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item["text"])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {"id": [], "text": [], "date": []}
        for message in messages:
            if message["type"] != "message" or len(message["text"]) == 0:
                continue
            parsed_messages["id"].append(message["id"])
            message_content = self.parse_text(message["text"])
            parsed_messages["text"].append(message_content)
            parsed_messages["date"].append(message["date"])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith("
    def extract_emojis(self, text):
        return "".join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub("", text)

    def extract_symbols(self, text):
        return "".join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(" ", text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def remove_links(self, text):
        return re.sub(self.url_pattern, " ", text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import argparse
from dataclasses import dataclass
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
import os
from langchain_openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

CHROMA_PATH = './chromadb/'

client = OpenAI(
    api_key=OPENAI_API_KEY
)

core_embeddings_model = None
def get_context():
        vectorstore = Chroma(persist_directory="./cachce",embedding_function=core_embeddings_model)
    
    retriever = vectorstore.as_retriever()
        return retriever

def generate_add(user_input, context):
    template = f'''
    Generate an advertisement given the following context.    
    You must use the following context:
    {context}
    '''   
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": template},{"role": "user", "content": user_input}],
        n=3,
    )

    return response
from peft import PeftModel
from transformers import LlamaForCausalLM, LlamaConfig

def load_model(model_name, quantization):
    model = LlamaForCausalLM.from_pretrained(
        model_name,
        return_dict=True,
        load_in_8bit=quantization,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    return model


def load_peft_model(model, peft_model):
    peft_model = PeftModel.from_pretrained(model, peft_model)
    return peft_model

def load_llama_from_config(config_path):
    model_config = LlamaConfig.from_pretrained(config_path) 
    model = LlamaForCausalLM(config=model_config)
    return model
import fire
import torch
import os
import sys
import time
import json 
from typing import List

from transformers import LlamaTokenizer, LlamaForCausalLM
from model_utils import load_model, load_peft_model

BASE_PROMPT = """Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers.
Human: {}
Assistant [Amharic] : """

def main(
    model_name: str="",
    peft_model: str=None,
    quantization: bool=False,
    max_new_tokens =400,     prompt_file: str=None,
    seed: int=42,     do_sample: bool=True,     min_length: int=None,     use_cache: bool=True,      top_p: float=1.0,     temperature: float=1.0,     top_k: int=1,     repetition_penalty: float=1.0,     length_penalty: int=1,     enable_azure_content_safety: bool=False,     enable_sensitive_topics: bool=False,     enable_saleforce_content_safety: bool=False,     **kwargs
):    
    
    print("***Note: model is not set up for chat use case, history is reset after each response.")
    print("***Ensure that you have replaced the default LLAMA2 tokenizer with the Amharic tokenizer")
    
        torch.cuda.manual_seed(seed)
    torch.manual_seed(seed)
    
    MAIN_PATH = '/model/Llama-2-7b-hf'
        peft_model = '/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'
    model_name = MAIN_PATH
    quantization = True
    model = load_model(model_name, quantization)

    tokenizer = LlamaTokenizer.from_pretrained(model_name)
    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))

    if peft_model:
        model = load_peft_model(model, peft_model)

    model.eval()

    while True:

        
        user_query = input('Type question in Amharic or English: ')
        user_prompt = BASE_PROMPT.format(user_query)
        batch = tokenizer(user_prompt, return_tensors="pt")
        batch = {k: v.to("cuda") for k, v in batch.items()}
        start = time.perf_counter()
        with torch.no_grad():
            outputs = model.generate(
                **batch,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                top_p=top_p,
                temperature=temperature,
                min_length=min_length,
                use_cache=use_cache,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                length_penalty=length_penalty,
                **kwargs 
            )
        e2e_inference_time = (time.perf_counter()-start)*1000
        print(f"the inference time is {e2e_inference_time} ms")
        
        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("MODEL_OUTPUT: {}".format(output_text))
        
if __name__ == "__main__":
    fire.Fire(main)
import torch
from contextlib import nullcontext
from transformers import (
    LlamaForCausalLM, 
    LlamaTokenizer, 
    TrainerCallback, 
    default_data_collator, 
    Trainer, 
    TrainingArguments
)
from peft import (
    LoraConfig,
    TaskType,
    prepare_model_for_int8_training,
    PeftModel
)

from pathlib import Path
from utils.dataset_utils import get_preprocessed_dataset
from configs.datasets import amharic_dataset

def print_trainable_parameters(model):
    print("Trainable Parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f" - {name}")


def finetune():
    LLAMA_DIR = '/model/Llama-2-7b-hf'
    PT_DIR = '/model/llama-2-amharic-3784m'
    OUTPUT_DIR = "/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output"

    tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)

    model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)


    train_dataset = get_preprocessed_dataset(tokenizer, amharic_dataset, 'train')


    model.train()



    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))


    print('loading the pretrained model from config')

    model = prepare_model_for_int8_training(model)
    model = PeftModel.from_pretrained(model, PT_DIR)
    model.print_trainable_parameters()
    lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            modules_to_save = ["embed_tokens","lm_head"]
        )

    enable_profiler = False


    config = {
        'lora_config': lora_config,
        'learning_rate': 1e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 1,
        'per_device_train_batch_size': 2,
        'gradient_checkpointing': False,
    }

        if enable_profiler:
        wait, warmup, active, repeat = 1, 1, 2, 1
        total_steps = (wait + warmup + active) * (1 + repeat)
        schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
        profiler = torch.profiler.profile(
            schedule=schedule,
            on_trace_ready=torch.profiler.tensorboard_trace_handler(f"{OUTPUT_DIR}/logs/tensorboard"),
            record_shapes=True,
            profile_memory=True,
            with_stack=True)

        class ProfilerCallback(TrainerCallback):
            def __init__(self, profiler):
                self.profiler = profiler

            def on_step_end(self, *args, **kwargs):
                self.profiler.step()

        profiler_callback = ProfilerCallback(profiler)
    else:
        profiler = nullcontext()


        training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        bf16=True,                  logging_dir=f"{OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=10,
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=1,
        warmup_ratio=0.03,
        optim="adamw_torch_fused",
        max_steps=total_steps if enable_profiler else -1,
        **{k:v for k,v in config.items() if k != 'lora_config'}
    )

    with profiler:
                trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=default_data_collator,
            callbacks=[profiler_callback] if enable_profiler else [],
        )

        print_trainable_parameters(model)

                trainer.train()

    model.save_pretrained(OUTPUT_DIR)


finetune()
from dataclasses import dataclass

@dataclass
class amharic_dataset:
    dataset: str = "amharic_dataset"
    train_split: str = "train"
    test_split: str = "val"
    data_path: str = "/data/fine_tun_data6.json"
import copy
import json
import torch


from torch.utils.data import Dataset



class InstructionDataset(Dataset):
    def __init__(self, dataset_config, tokenizer, partition="train", max_words=50):
        self.ann = json.load(open(dataset_config.data_path))

        if partition == "train":
            self.ann = self.ann
        else:
            self.ann = self.ann[:200]

        self.max_words = max_words
    
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.ann)

    def __getitem__(self, index):

        ann = self.ann[index]
        data = self.create_prompt_formats(ann)
        prompt = data['prompt']
        example = data['text']  
        prompt = torch.tensor(
            self.tokenizer.encode(prompt), dtype=torch.int64
        )

        example = self.tokenizer.encode(example)

        example.append(self.tokenizer.eos_token_id)
        example = torch.tensor(
            example, dtype=torch.int64
        )
        padding = self.max_words - example.shape[0]

        if padding > 0:
            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - 1))
        elif padding < 0:
            example = example[: self.max_words]

        labels = copy.deepcopy(example)

        labels[: len(prompt)] = -1

        example_mask = example.ge(0)
        label_mask = labels.ge(0)
        example[~example_mask] = 0
        labels[~label_mask] = 0
        example_mask = example_mask.float()
        label_mask = label_mask.float()

        return {
            "input_ids": example,
            "labels": labels,
            "attention_mask":example_mask,
        }
    
    def create_prompt_formats(self,sample):
        """
        Format various fields of the sample ('text', 'label',)
        Then concatenate them using two newline characters
        :param sample: Sample dictionnary
        """

        INTRO_BLURB = "Generate an advertisement given a category"
        INSTRUCTION_KEY = "        RESPONSE_KEY = "Response:"
        END_KEY = "
        blurb = f"{INTRO_BLURB}"
        text = f"{INSTRUCTION_KEY}\n{sample['input']}"
        response = f"{RESPONSE_KEY}\n{sample['output']}"
        end = f"{END_KEY}"

        parts = [part for part in [blurb, text, response, end] if part]

        formatted_prompt = "\n\n".join(parts)

        sample["text"] = formatted_prompt
        parts = [part for part in [blurb, text,] if part]
        formatted_prompt = "\n\n".join(parts)

        sample["prompt"]= formatted_prompt

        return sample
"""
Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.

Here is the full list of checkpoints on the hub that can be fine-tuned by this script:
https://huggingface.co/models?filter=text-generation
"""

import logging
import numpy as np
import math
import os
import sys
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Dict, Any, Mapping
from pathlib import Path
import datasets
import torch
from datasets import load_dataset, concatenate_datasets

import transformers
from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    is_torch_tpu_available,
    set_seed,
)
from transformers.testing_utils import CaptureLogger
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version

from sklearn.metrics import accuracy_score
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora_model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")

        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)


def accuracy(predictions, references, normalize=True, sample_weight=None):
        return {
            "accuracy": float(
                accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
            )
        }


def compute_metrics(eval_preds):
    preds, labels = eval_preds
            labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)


def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
                        logits = logits[0]
    return logits.argmax(dim=-1)


def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}

                if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["labels"] = torch.tensor([f["label"] for f in features], dtype=dtype)
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)

        
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else:
                    batch[k] = torch.tensor([f[k] for f in features])
    except ValueError:         for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else:
                    batch[k] = torch.tensor([features[0][k]] * len(features))

    return batch


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The tokenizer for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides can't be used in combination with --config_name or --model_name_or_path"
            )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    dataset_dir: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional input sequence length after tokenization. "
                "The training dataset will be truncated in block of this size for training. "
                "Default to the model max input length for single sentence inputs (take into account special tokens)."
            )
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep line breaks when using TXT files or not."}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed stored"})

    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")


@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj,v_proj")
    lora_rank : Optional[int] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.1)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[bool] = field(default=False)
    peft_path : Optional[str] = field(default=None)


logger = logging.getLogger(__name__)


def main():

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, MyTrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

            send_example_telemetry("run_clm", model_args, data_args)

        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,          handlers=[logging.StreamHandler(sys.stdout)],)


    if training_args.should_log:
                transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
        logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )

        last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

        set_seed(training_args.seed)

    config_kwargs = {
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else:
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("You are instantiating a new config instance from scratch.")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")

    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script."
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )

                tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            output = tokenizer(examples["text"])
                if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return output
    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)

        def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
                        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
                result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    with training_args.main_process_first(desc="dataset map tokenization and grouping"):
        lm_datasets = []
        path = Path(data_args.dataset_dir)
        files = [file.name for file in path.glob("*.txt")]
        if training_args.debug_mode is True:
            files = [files[0]]
        print("printing files")
        print(files)    
        for idx, file in enumerate(files):
            data_file = os.path.join(path, file)
            filename = ''.join(file.split(".")[:-1])
            cache_path = os.path.join(data_args.data_cache_dir, filename)
            os.makedirs(cache_path, exist_ok=True)
            try:
                processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=False)
                logger.info(f'training datasets-{filename} has been loaded from disk')
            except Exception:
                cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                os.makedirs(cache_dir, exist_ok=True)
                raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                logger.info(f"{file} has been loaded")
                tokenized_dataset = raw_dataset.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns="text",
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'tokenized.arrow') for k in raw_dataset},
                    desc="Running tokenizer on dataset",
                )
                grouped_datasets = tokenized_dataset.map(
                    group_texts,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'grouped.arrow') for k in tokenized_dataset},
                    desc=f"Grouping texts in chunks of {block_size}",
                )
                processed_dataset = grouped_datasets
                processed_dataset.save_to_disk(cache_path)
            if idx == 0:
                lm_datasets = processed_dataset['train']
            else:
                assert lm_datasets.features.type == processed_dataset["train"].features.type
                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset["train"]])

        lm_datasets = lm_datasets.train_test_split(test_size = data_args.validation_split_percentage)

    if training_args.do_train:
        train_dataset = lm_datasets['train']
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))
        logger.info(f"Num train_samples  {len(train_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(train_dataset[0]['input_ids']))
    if training_args.do_eval:
        eval_dataset = lm_datasets["test"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        logger.info(f"Num eval_samples  {len(eval_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(eval_dataset[0]['input_ids']))



    if model_args.model_name_or_path:
        torch_dtype = (
            model_args.torch_dtype
            if model_args.torch_dtype in ["auto", None]
            else getattr(torch, model_args.torch_dtype)
        )
        model = LlamaForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool(".ckpt" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            use_auth_token=True if model_args.use_auth_token else None,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
        )
    else:
        model = AutoModelForCausalLM.from_config(config)
        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")

    model_vocab_size = model.get_output_embeddings().weight.size(0)
    if not (
       (model_vocab_size==32000 and len(tokenizer)==51008) or \
       (model_vocab_size==32000 and len(tokenizer)==32000) or \
       (model_vocab_size==51008 and len(tokenizer)==51008) or \
       (model_vocab_size==49954 and len(tokenizer)==49954)
       
    ):
        raise ValueError(
            f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
            "Valid configurations (base model / tokenizer):\n"
            "- Continue pre-training original LLaMA: 32000 / 32000 \n"
            "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
            "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
            "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n")

    model.resize_token_embeddings(len(tokenizer))
    if training_args.peft_path is not None:
        logger.info("Peft from pre-trained model")
        model = PeftModel.from_pretrained(model, training_args.peft_path)
    else:
        logger.info("Init new peft model")
        target_modules = training_args.trainable.split(',')
        modules_to_save = training_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')
        lora_rank = training_args.lora_rank
        lora_dropout = training_args.lora_dropout
        lora_alpha = training_args.lora_alpha
        logger.info(f"target_modules: {target_modules}")
        logger.info(f"lora_rank: {lora_rank}")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=lora_rank, lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            modules_to_save=modules_to_save)
        model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

                                

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
        modules_to_save = ["embed_tokens","lm_head"]
    )

    """
        config = {
        'lora_config': lora_config,
        'learning_rate': 2e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 2,
        'per_device_train_batch_size': 1,
        'per_device_eval_batch_size': 1,
        'gradient_checkpointing': True,
    }
    """


    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=fault_tolerance_data_collator,
        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
        if training_args.do_eval and not is_torch_tpu_available()
        else None,
    )

    """
    output_dir=training_args.output_dir,
    overwrite_output_dir=True,
    bf16=True,          logging_dir=f"{training_args.output_dir}/logs",
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="steps",
    save_steps=5000,
    optim="adamw_torch_fused",
    max_steps=-1,     **{k:v for k,v in config.items() if k != 'lora_config'}
    """

    trainer.add_callback(SavePeftModelCallback)
        if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)

        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
    model.save_pretrained(training_args.output_dir)
        if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()

        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
        try:
            perplexity = math.exp(metrics["eval_loss"])
        except OverflowError:
            perplexity = float("inf")
        metrics["perplexity"] = perplexity

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)


if __name__ == "__main__":
    main()
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=/model/Llama-2-7b-hf
amharic_tokenizer_path=/model/Llama-2-7b-hf
dataset_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/data/cleaned
data_cache=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/cache
per_device_train_batch_size=32
per_device_eval_batch_size=1
gradient_accumulation_steps=1
output_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output

python pretrain.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${amharic_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 1 \
    --save_steps 7528 \
    --evaluation_strategy steps \
    --eval_steps 3000 \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --bf16 \
    --overwrite_output_dir \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --gradient_checkpointing \
import pandas as pd

df = pd.read_json("/data/fine_tun_data2.json")

df.tail()
df[df['output']=='not advertisement'].iloc[1]['input']
'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: ለኢሬቻ በኣል ወደቢሾፍቱ የተጓዙት የሲዳማ ወጣቶች(ኤጄቶዎች) ከሰኣታት በፊት ቢሾፍቱ ገብተዋል። @tsegabwolde @tikvahethiopia''
import pandas as pd

df = pd.read_csv("/data/wasu_mohammed_labeled.csv")

df.head()
df.shape
df['label'] = df['label'].fillna("Not Advertisement")

df.tail(5)
df
from datasets import Dataset




data_dict = {"text": df['text'].tolist()}




dataset = Dataset.from_dict(data_dict)

dataset.save_to_disk("../data/train")




from peft import PeftModel

from transformers import LlamaForCausalLM, LlamaConfig




def load_model(model_name, quantization):

    model = LlamaForCausalLM.from_pretrained(

        model_name,

        return_dict=True,

        load_in_8bit=quantization,

        device_map="auto",

        low_cpu_mem_usage=True,

    )

    return model






def load_peft_model(model, peft_model):

    peft_model = PeftModel.from_pretrained(model, peft_model)

    return peft_model




def load_llama_from_config(config_path):

    model_config = LlamaConfig.from_pretrained(config_path) 

    model = LlamaForCausalLM(config=model_config)

    return model
from transformers import LlamaTokenizer





MAIN_PATH = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)



example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685





tokens = tokenizer.tokenize(example)

print(tokens)
print(len(tokenizer))
example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685




tokens = tokenizer.tokenize(example)

print(tokens)
df['text'][0]
df = pd.read_csv("/data/wasu_mohammed_labeled.csv")




total_word_count = 0

total_tokens = 0


for index, row in df.iterrows():

    
    text = row['text']

    if not isinstance(text, str): 

        continue



    

    
    word_count = len(text.split())



    
    total_word_count += word_count

    tokens = tokenizer.tokenize(text)

    total_tokens+=tokens

    print(tokens)

    






print("Total Word Count:", total_word_count)

print("Total tokens count: ",total_tokens)
total_tokens
df.shape
from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    HfArgumentParser,

    TrainingArguments,

    pipeline,

    LlamaForCausalLM, 

    LlamaTokenizer,

    logging,

)

from peft import (

    LoraConfig,

    TaskType,

    prepare_model_for_int8_training,

    PeftModel

)

import torch

LLAMA_DIR = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)



model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)

embedding_size = model.get_input_embeddings().weight.shape[0]



if len(tokenizer) != embedding_size:

    print("resize the embedding size by the size of the tokenizer")

    model.resize_token_embeddings(len(tokenizer))





new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'

model = PeftModel.from_pretrained(model, new_model)




prompt = "Who is Leonardo Da Vinci?"

pipe = pipeline(task="text-generation", model=model, tokenizer=model, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
ቨርቹዋል ረዳቶች እንደ Amazon&
run-20240203_155644-4hm9i4tp
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from transformers import AutoTokenizer, AutoModel
import os
import torch
import shutil
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

model_name = 'Davlan/bert-base-multilingual-cased-finetuned-amharic'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

data_path = '../data/'
CHROMA_PATH = '../RAG/chromadb'

def embed_doc(document):
    
        encoded_input = tokenizer(document, padding=True, truncation=True, return_tensors='pt')
    
        with torch.no_grad():
        outputs1 = model(**encoded_input)

    

        embeddings1 = outputs1.last_hidden_state.squeeze(dim=0)
    

        
    return embeddings1

def load_documents(data_path):    
    try:
        loader = DirectoryLoader(data_path)
        documents = loader.load()       
        print("data loaded sucessfully")
        return documents[0].page_content
    except:
        print("document not found!")
        return None
    

def split_text(documents:list[Document]):
    try:
        text_spliter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=500,
            length_function=len,
            add_start_index = True
        )
        chunk = text_spliter.split_documents(documents)
        print("data splited successfuly!")
        return chunk
    except:
        print("document not found")

def save_chunks_to_chroma(chunks):
        if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
    try:
        db = Chroma.from_documents(chunks,embed_doc(),\
                                persist_directory=CHROMA_PATH)
        db.persist()
        print("Vectorstore created successfully!")
    except:
        print("Couldn't create the vectore database")

def generate_data_store():
    documents = load_documents(data_path)
    chunks = split_text(documents)
    embeding1 = embed_doc(chunks)
    print(embeding1)
    save_chunks_to_chroma(embeding1) 

def main():
    generate_data_store()      


if __name__ == "__main__":
    main()
! pip install transformers bitsandbytes peft trl accelerate
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
base_model = "NousResearch/Llama-2-7b-chat-hf"

guanaco_dataset = "mlabonne/guanaco-llama2-1k"

new_model = "LLama-2-7b-chat-prac"
print(peft.__version__)
dataset = load_dataset(guanaco_dataset, split="train")
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=compute_dtype,

    bnb_4bit_use_double_quant=False,

)
model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=quant_config,

    device_map={"": 0}

)



model.config.use_cache = False

model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_size= "right"
peft_params = LoraConfig(

    lora_alpha = 16,

    lora_dropout = 0.1,

    r=64,

    bias="none",

    task_type= "CAUSAL_LM"

)
training_params = TrainingArguments(

    output_dir = "./results",

    num_train_epochs=1,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim="paged_adamw_32bit",

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-4,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=-1,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type="constant",

    report_to="tensorboard"

)
!pip install --upgrade peft
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
import pandas as pd

import numpy as np

import sys, os

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util

from concurrent.futures import ThreadPoolExecutor
util = Util()

cleaned_dir = "../cleaned"

json_file_path = '../raw/dilela_page.json'




df = pd.read_json(json_file_path)

df.head()
df.shape

columns = ["id", "channel_name", "type", "message_id", "message_type","text", "label", "created_at", "updated_at", ]

new_df = pd.DataFrame(columns=columns)

new_df

telegeram_channel_id  = df["id"][0]

telegram_channel_name = df["name"][0]

telegeram_channel_type = df["type"][0]

message_df = df["messages"]

data = [{

       'telegeram_channel_id': telegeram_channel_id,

       'telegram_channel_name': telegram_channel_name,

       'telegeram_channel_type': telegeram_channel_type,

       'message_id': message.get('id', np.nan),

        'message_type': message.get('type', np.nan),

        'text': message['text_entities'][0]['text'] if message.get('text_entities') and message['text_entities'] else np.nan,

        'created_at': message.get('date', np.nan),

        'update_at': message.get('edited', np.nan),

        }for message in message_df]

message_df = pd.DataFrame(data)

message_df = message_df.sort_values(by='message_id')

message_df.head(20)
message_df.shape

nan_rows_count = message_df.isna().any(axis=1).sum()

nan_rows_count

message_df = message_df.dropna()

message_df.head()
message_df.shape
message_df = message_df.replace('\n', ' ', regex=True)

message_df.head()

message_df["hashtags"] = message_df['text'].apply(lambda text: util.extract_hashtags(text))

message_df.head()

message_df["text"] = message_df["text"].str.replace(r'\
message_df.head()
message_df["emojis"] = message_df["text"].apply(util.extract_emojis)

message_df.head()

message_df['text'] = message_df['text'].apply(util.remove_emojis_using_emoji_pattern)

message_df.tail()
def remove_emojis_parallel(text):

    return util.remove_emojis(text)





with ThreadPoolExecutor() as executor:

    message_df['text'] = list(executor.map(remove_emojis_parallel, message_df['text']))
message_df.head()

message_df.replace('', pd.NA, inplace=True)

nan_rows_count = message_df.isna().any(axis=1).sum()


message_df = message_df.dropna()

message_df.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    message_df['text'] = message_df['text'].str.replace(letter[0][i], letter[1][i])
message_df['symbols'] = message_df['text'].apply(util.extract_symbols)

message_df.head()
message_df['text'] = message_df['text'].apply(util.remove_symbols)

message_df.tail()
message_df['links'] = message_df['text'].apply(util.extract_urls)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

message_df.head()
message_df['mentions'] = message_df['text'].apply(util.extract_mentions)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

message_df.tail()

message_df['text'] = message_df['text'].str.replace('\s+', ' ', regex=True).str.strip()
message_df['text'] = message_df['text'].replace(r'!+', '!', regex=True)

message_df['text'] = message_df['text'].replace(r'\.+', '', regex=True)
message_df.head()

nan_rows_count = message_df['text'].isna().sum()

nan_rows_count


message_df = message_df.dropna(subset='text')

message_df.tail()


message_df = message_df[message_df['text'].str.len() >= 20]
message_df.to_csv(f"{cleaned_dir}/dilela_page.csv")
message_df['text'].to_csv(f"{cleaned_dir}/dilela_page.txt", index=False, header=False)
df = pd.read_csv(f"{cleaned_dir}/dilela_page.csv")

df.head()
df['word_count'] = df['text'].str.split().str.len()

df.columns


df_labeled = df.drop(['Unnamed: 0','telegram_channel_name','telegeram_channel_type','message_id','message_type','hashtags', 'emojis', 'created_at','symbols', 'links','mentions'],axis=1)

df_labeled.rename(columns={'update_at':'date','telegeram_channel_id':'channel_id'},inplace=True)

df_labeled.to_csv(f"{cleaned_dir}/dilela_page_labeled.csv")

len = df_labeled['word_count'].sum()

len
from fastapi import FastAPI, HTTPException, Depends
from typing import Annotated, List
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from utils import simple_rag
from utils import hugging_face_hub

app = FastAPI()

origins = ["http://localhost:5173"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
)


class RagResponseBase(BaseModel):
    question: str
    answer: str


class HugResponseBase(BaseModel):
    question: str
    answer: str


class AmharicModelWithRAGBase(BaseModel):
    question: str
    answer: str


@app.get("/getanswer", response_model=RagResponseBase)
async def return_answer(question: str):
    result = simple_rag.test_RAG(question)
    return result


@app.get("/getHuggingFaceAnswer", response_model=HugResponseBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.invoke_current_hugging_face_model(model, prompt)
    return result


@app.get("/getAmharicModelWithRAGAnswer", response_model=AmharicModelWithRAGBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.use_amharic_model(model, prompt)
    return result
from langchain import OpenAI
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from models import simple_rag_response
import os
from dotenv import load_dotenv

load_dotenv()


def load_data():
    loader = TextLoader("/week_6_challenge_doc.txt")
    documents = loader.load()
    return documents


def return_chunks(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)
    texts = text_splitter.split_documents(documents)
    return texts


def return_chain(texts):
    embeddings = OpenAIEmbeddings()
    store = Chroma.from_documents(
        texts, embeddings, collection_name="challenge_document"
    )
    llm = OpenAI(temperature=0)
    return RetrievalQA.from_chain_type(llm, retriever=store.as_retriever())


def test_RAG(question):
    documents = load_data()
    chunks = return_chunks(documents)
    chain = return_chain(chunks)
    response = chain.run(question)
    return simple_rag_response.RagResponse(question, response)
import React, { useState , useRef} from 'react'
import 'bootstrap/dist/css/bootstrap.css'
import FileInput from  './components/FileInput'
import TextInputWithLable from  './components/TextInputWithLable'
import Dropdown from './components/Dropdown'
import NavBarComp from './components/Navbar'
import SpinnerWithText from './components/SpinnerWithText'
// import './App.css'
import api from './api/api'
function App() {
  const [answer,setAnswer] = useState([]);
  const [isShow,setShow] = useState(false);

  const fetchResponse = async () =>{
    console.log(ref.current.value);
    const question = ref.current.value;
    setShow(true)
    const response = await api.get('/getanswer?question='+question);
    console.log(response.data);
    setAnswer(response.data)
    setShow(false)
  }
  const ref = useRef(null);

  return (
    <React.Fragment>
      
      <NavBarComp />
      <main className='container'>
        <form className="row g-3" >
          
            <div>
              <label htmlFor="inputLable" className="form-label">Input Ad description to be generated</label>
              <textarea className="form-control" id="inputTextarea" rows="7" ref={ref}/>
            </div>

            {isShow && <SpinnerWithText />}

            <button type="button" className="btn btn-primary mb-4" onClick={fetchResponse}>Get Ad</button> 

            <div>
              <TextInputWithLable value= {answer}/>
            </div>

        </form>
      </main>

    </React.Fragment>
  )
}

export default App
import pandas as pd

import json
df = pd.read_json('sheger.json')

df.head()
df.info()
df.messages.iloc[0]
df.columns
message_df = pd.json_normalize(df.messages)
message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)

    return ''.join(extracted_text)




message_df['extracted_text'] = message_df['text'].apply(extract_text_from_data)
message_df = message_df[['id','type','date','extracted_text']]

message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)



    
    full_text = ''.join(extracted_text)



    
    substrings_to_remove = ['\n\n', '@sheger_press\n@sheger_press', '❗️❗️❗️']

    for substring in substrings_to_remove:

        full_text = full_text.replace(substring, '')



    
    full_text = ''.join(char for char in full_text if char.isalnum() or char.isspace())



    return full_text.strip()




message_df['cleaned_text'] = message_df['extracted_text'].apply(extract_text_from_data)

message_df.head()
df = df[['name','type','id']]

df.rename(columns={'name':'channel_name',

                  'type':'channel_type',

                  'id':'channel_id'}, inplace=True)
df.head()
message_df=message_df[['id','type','date','cleaned_text']]

message_df.rename(columns = {'id':'message_id',

                  'type':'message_type',

                  'date':'message_date',

                  'cleaned_text':'text'}, inplace = True)

message_df.head()
sheger_df= pd.concat([df,message_df], axis =1)
sheger_df.head()
press_df = press_df[press_df.text != '']

press_df.head()
sheger_df.to_csv('sheger_press.csv', index = None)
import pandas as pd

import json

import os

from pprint import pprint

import bitsandbytes as bnb

import torch

import torch.nn as nn

import transformers

from datasets import load_dataset, Dataset

from huggingface_hub import notebook_login



from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipelines, logging
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa' 
dataset = load_dataset(dataset_name, split="train")
MODEL_NAME = "mistralai/Mistral-7B-v0.1"

new_model = "amharic-mistral-7b"

config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, load_in_4bit=True)






bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_use_double_quant=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.bfloat16

)





model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    device_map="auto",

    trust_remote_code=True,

    quantization_config=bnb_config,

)



tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tokenizer.pad_token = tokenizer.eos_token
model = prepare_model_for_kbit_training(model)
use_4bit = True




bnb_4bit_compute_dtype = "float16"




bnb_4bit_quant_type = "nf4"




use_nested_quant = False

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

if compute_dtype == torch.float16 and use_4bit:

    major, _ = torch.cuda.get_device_capability()

    if major >= 8:

        print("=" * 80)

        print("Your GPU supports bfloat16: accelerate training with bf16=True")

        print("=" * 80)
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=2,

    lora_alpha=32,

    target_modules=get_last_layer_linears(model),

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM"

)



model = get_peft_model(model, config)





output_dir = "./results"




num_train_epochs = 1




fp16 = False

bf16 = False




per_device_train_batch_size = 4




per_device_eval_batch_size = 4




gradient_accumulation_steps = 1




max_grad_norm = 0.3




learning_rate = 2e-4




weight_decay = 0.001




optim = "paged_adamw_32bit"




lr_scheduler_type = "constant"




warmup_ratio = 0.03





group_by_length = True




save_steps = 25




logging_steps = 25


base_model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(base_model, new_model)

model = model.merge_and_unload()

training_arguments = TrainingArguments(

    output_dir=output_dir,

    num_train_epochs=num_train_epochs,

    per_device_train_batch_size=per_device_train_batch_size,

    gradient_accumulation_steps=gradient_accumulation_steps,

    optim=optim,

    save_steps=save_steps,

    logging_steps=logging_steps,

    learning_rate=learning_rate,

    weight_decay=weight_decay,

    fp16=fp16,

    bf16=bf16,

    max_grad_norm=max_grad_norm,

    max_steps=25,

    warmup_ratio=warmup_ratio,

    group_by_length=group_by_length,

    lr_scheduler_type=lr_scheduler_type,

    report_to="tensorboard"

)
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
class RagResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class HugResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class AmharicModelResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass
from dotenv import load_dotenv
from models import simple_rag_response

load_dotenv()

from langchain import HuggingFaceHub


def invoke_current_hugging_face_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.HugResponse(prompt, response)


def use_amharic_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.AmharicModelResponse(prompt, response)
import time

import sentencepiece as spm
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=m --vocab_size=100000')

sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))




print(sp.encode_as_pieces('ሃይ ሰላም ናችሁ?'))



spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=am-word --model_type=word  --vocab_size=100000')



sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.encode_as_ids('ፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.decode_ids([47914, 1024, 33, 7716, 29922, 26700]))
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U00002049 \U0000FE0F"                                         u"\U0000203C"                                         u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'
        print(self.emoji_pattern.pattern)

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub(' ', text)
        
    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import pandas as pd

import csv, os, sys

from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import
"""
Convert .txt to csv

"""

import csv
from sklearn.model_selection import train_test_split

def convert_txt_to_csv(input_txt, output_csv):
    with open(input_txt, 'r', encoding='utf-8') as infile, open(output_csv, 'w', encoding='utf-8', newline='') as outfile:
       
        reader = infile.readlines()
        data = [line.strip().split() for line in reader]
        csv_writer = csv.writer(outfile)
        csv_writer.writerows(data)

def split_data(input_csv, output_train_csv, output_test_csv, output_val_csv, test_size=0.2, val_size=0.1, random_seed=42):
    with open(input_csv, 'r', encoding='utf-8') as file:
        csv_reader = csv.reader(file)
        data = list(csv_reader)
        
    train_data, test_val_data = train_test_split(data, test_size=(test_size + val_size), random_state=random_seed)
    test_data, val_data = train_test_split(test_val_data, test_size=(val_size / (test_size + val_size)), random_state=random_seed)

    with open(output_train_csv, 'w', encoding='utf-8', newline='') as train_file:
        csv_writer = csv.writer(train_file)
        csv_writer.writerows(train_data)

    with open(output_test_csv, 'w', encoding='utf-8', newline='') as test_file:
        csv_writer = csv.writer(test_file)
        csv_writer.writerows(test_data)

    with open(output_val_csv, 'w', encoding='utf-8', newline='') as val_file:
        csv_writer = csv.writer(val_file)
        csv_writer.writerows(val_data)

if __name__ == "__main__":
    input_txt_file = '/home/biniyam_ajaw/finetuning/data/dataset.txt'
    output_csv_file = '/home/biniyam_ajaw/finetuning/data/output_data.csv'
    output_train_csv = '/home/biniyam_ajaw/finetuning/data/train_data.csv'
    output_test_csv = '/home/biniyam_ajaw/finetuning/data/test_data.csv'
    output_val_csv = '/home/biniyam_ajaw/finetuning/data/val_data.csv'

    convert_txt_to_csv(input_txt_file, output_csv_file)
    split_data(output_csv_file, output_train_csv, output_test_csv, output_val_csv)
    print("Conversion to CSV and data split completed.")
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=100000)

import tokenizers

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

folder = 'data'
files = [f"/home/biniyam_ajaw/finetuning/{folder}/{split}.csv" for split in ["test_data", "train_data", "valid_data"]]
tokenizer.train(files, trainer)

from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")

from transformers import PreTrainedTokenizerFast

custom_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
custom_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
custom_tokenizer.save_pretrained("amharic_tokenizer")

custom_tokenizer.push_to_hub("amharic_tokenizer")
max-width: 100%;
  width: 100%;
  height: 100%;
  margin: 5 auto;
  padding: 2rem;
  text-align: start;
  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function FileInput (){
    return(
        <div>
            <div className="input-group mb-3">
                <input type="file" className="form-control" id="inputGroupFile02"/>
                <label clclassNameass="input-group-text" for="inputGroupFile02">Upload</label>
            </div>
        </div>
      
    );
}


export default FileInput;
/* :root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;
  width: 100%;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}
a {
  font-weight: 500;
  color:   text-decoration: inherit;
}
a:hover {
  color: }

body {
  margin: 0;
  display: flex;
  place-items: center;
  min-width: 320px;
  min-height: 100vh;
}

h1 {
  font-size: 3.2em;
  line-height: 1.1;
}

button {
  border-radius: 8px;
  border: 1px solid transparent;
  padding: 0.6em 1.2em;
  font-size: 1em;
  font-weight: 500;
  font-family: inherit;
  background-color:   cursor: pointer;
  transition: border-color 0.25s;
}
button:hover {
  border-color: }
button:focus,
button:focus-visible {
  outline: 4px auto -webkit-focus-ring-color;
}

@media (prefers-color-scheme: light) {
  :root {
    color:     background-color:   }
  a:hover {
    color:   }
  button {
    background-color:   }
} */
import Container from 'react-bootstrap/Container';
import Nav from 'react-bootstrap/Nav';
import Navbar from 'react-bootstrap/Navbar';
import NavDropdown from 'react-bootstrap/NavDropdown';

function NavBarComp() {
  return (
    <Navbar expand="lg" className="bg-body-tertiary container-fluid">
      <Container >
        <Navbar.Brand href="        <Navbar.Toggle aria-controls="basic-navbar-nav" />
        <Navbar.Collapse id="basic-navbar-nav">
          <Nav className="me-auto">
            <NavDropdown title="Select Model" id="basic-nav-dropdown">
                <NavDropdown.Item href="                <NavDropdown.Item href="                <NavDropdown.Item href="            </NavDropdown>
          </Nav>
        </Navbar.Collapse>
      </Container>
    </Navbar>
  );
}

export default NavBarComp;
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function TextInputWithLabel (props) {
    return(
        <div>
            <div className="mb-3">
                <label htmlFor="exampleFormControlTextarea1" className="form-label">Generated Ad</label>
                <textarea className="form-control" id="exampleFormControlTextarea1" rows="7"  value={props.value.answer}/>
            </div>
        </div>
      
    );
}


export default TextInputWithLabel;
import torch

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging

from datasets import load_dataset

import os, sys

from huggingface_hub import notebook_login

import torch.nn as nn

import getpass

from trl import SFTTrainer

from peft import PeftConfig, LoraConfig
os.environ["HUGGING_FACE_HUB_TOKEN"] = getpass.getpass("Token:")

assert os.environ["HUGGING_FACE_HUB_TOKEN"]
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")

double_quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model_id = "microsoft/phi-2"

new_model = 'amharic-phi'

model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cuda:0', quantization_config=nf4_config)
tokenizer = AutoTokenizer.from_pretrained("dagim/amharic_tokenizer")



tokenizer.tokenize("ከአሜሪካ ወደ አዲስ አበባለመጓዝምንያህልጊዜይወስዳል??")
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa'

dataset = load_dataset(dataset_name, split="train")
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=4,

    lora_alpha=32,

    
    lora_dropout=0.03,

    bias='none',

    task_type="CAUSAL_LM"

)

training_arguments = TrainingArguments(

    output_dir="./results",

    num_train_epochs=2,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim='paged_adamw_32bit',

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-8,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=25,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type='constant',

    report_to="tensorboard",

    gradient_checkpointing=True

)
trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=config,

    dataset_text_field='inputs',

    max_seq_length=None,

    tokenizer=tokenizer,

    args=training_arguments,

    packing=False

)
trainer.train()
trainer.model.save_pretrained(new_model)
logging.set_verbosity(logging.CRITICAL)



prompt = "የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?"

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="/home/biniyam_ajaw/llama-2-amharic-3784m/tokenizer.json")
print(len(tokenizer.encode('የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?')))
import logging
import numpy as np
import math
import os, sys
import torch
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Tuple, Dict, Any, Mapping
from pathlib import Path
import datasets
from datasets import Dataset, DatasetDict, load_dataset, load_metric, concatenate_datasets

from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,  
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
    is_torch_gpu_available,
)

from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.testing_utils import CaptureLogger
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version_core

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora-Model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")
            
        
        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)
        
    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

def accuracy(predictions, references, normalize=True, sample_weight=None):
    return {
        "accuracy": float(
            accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
        )
    }

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)

def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
        logits = logits[0]
    return logits.argmax(dim=-1)

def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}
    
    if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["label"] = torch.tensor([f["label"] for f in features], dtype=dtype)
        
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
            
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)
            
    
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else: batch[k] = torch.tensor([f[k] for f in features])
                
    except ValueError:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                                        batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else: 
                    batch[k] = torch.tensor([features[0][k]] * len(features))
                    
    return batch

MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={"help": ("The tokenizer for weights initialization.")},
    )
    
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    
    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides cannot be used with --config_name or --model_name_or_path. To override some of "
            )
            
@dataclass
class DataTrainingArguments:
    '''
    Arguments pertaining to what data we are going to input our model for training and eval.
    '''
    
    dataset_dir: Optional[str] = field(
        default=None, metadata={"the name of the dataset to use"}
    )

    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name opf the dataset to use"}
    )
    
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training file"}
    )
    
    validation_file: Optional[str] = field(
        default=None, metadata={"help": "This is optional but recommended if you want to use early stopping"}
    )
    
    max_training_sample: Optional[int] = field(
        default=None,
        metadata={
            "help": "Debugging purposes"
        },
    )
    
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging"
        },
    )
    
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    
        block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional"
                "Training dataset will be truncated into a block of this size for training"
                "Default to the model max input sequence"
            )
        }
    )
    
            
    cache_dir: bool = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    
            validation_strategy: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "Percentage of the validation set used at the end of each epoch"
        }
        
    )
        preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes to use for preprocessing"}
    )
    
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep the linebreaks when using txt files or not"}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed store"})
    
    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets >= 2.0.0`")
            
            
@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj, v_proj")
    lora_rank : Optional[str] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.03)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[str] = field(default=False)
    peft_path : Optional[str] = field(default=None)
    
logger = logging.getLogger(__name__)

def main():
    
    parser = HfArgumentParser(ModelArguments, DataTrainingArguments, MyTrainingArguments)
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                model_args, data_args, training_args = parser.parse_parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_to_dataclasses()
        
    send-example_telemetry("run_clm", model_args, data_args)
    
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", datefmt="%m/%d/%Y %H:%M:%S",
                        level=logging.INFO,                         handlers=[logging.StreamHandler(sys.stdout)],)
    
    if training_args.should_log:
        transformers.utils.logging.set_verbosity_info()
        
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
    logger.warning(
        f"Process rank: {training_args.output_dir}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"Distributed training: {bool(training_args.local_rank != -1)}, 16-bits-training: {training_args.fp16}"
    )
    
        
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError (
                f"Outpur dir {training_args.output_dir} already exists and is not mt"
                "Use --overwrite_output_dir to overcome"
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is not None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this, change "
                "the --output-dir or --overwrite_output_dir to train from scratch"
            )
            
        set_seed(training_args.seed)
    
    config_kwargs = {
        "cache_dir": model.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model-args.use_auth_token else None
    }
    
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else: 
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("This is a new config from scratch")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")
            
            
    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None
    }
    
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "Instantiating a tokenizer from scratch"
        )
        
            
    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            return tokenizer(examples[text])
        
        if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^ PLease ignore the warning above ^^^^^^^"
            )
            
        return output

    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024  
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                "Override with `--block_size xxx`"
            
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)
        
        
    def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        
        if total_length >= block_size:
                        total_length = {total_length // block_size} *  block_size
                        result = {
                k: [t[i: i + block_size] for i in range(0, total_length, block_size)]
                for k, t in concatenated_examples.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result
        with training_args.main_process_first(desc="dataset map tokenizer"):
            lm_datasets = []
            path = Path(data_args.dataset_dir)
            filename = [file.name for file in path.glob("*.txt")]
            
            if training_args.debug_mode:
                files = [files[0]]
            for idx, file in enumerate(files):
                data_file = os.path.join(path, file)
                filename = ''.join(file.split('.')[:-1])
                cache_path = os.path.join(data_args.data_cache_dir, filename)
                os.makedirs(cache_path, exist_ok=True)
                try:
                    processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=True)
                    logger.info(f'Training datasets-{filename} has been loaded from disk')
                except Exception:
                    cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                    os.makedirs(cache_dir, exist_ok=True)
                    raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                    logger.info(f"{file} has been loaded")
                    tokenized_dataset = raw_dataset.map(
                        tokenize_function,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        remove_columns="text",
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "tokenized.arrow") for k in raw_dataset},
                        desc="Running tokenizer on the dataset",
                    )
                    
                    grouped_datasets = tokenized_dataset.map(
                        group_texts,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "grouped.arrow") for k in tokenized_dataset},
                        desc=f'Grouping texts in chunks of {block_size}',
            
                    )
                    
                    processed_dataset = grouped_datasets
                    processed_dataset.save_to_disk(cache_path)
                    
                if idx == 0:
                    lm_datasets = processed_dataset['train']
                else:
                    assert lm_datasets.features.type == processed_dataset['train'].features.type
                    lm_dataset = concatenate_datasets([lm_datasets, processed_dataset['train']])
                    
            lm_datasets = lm_datasets.train_test_split(test_size= data_args.validation_split_percentage())
            
        if training_args.do_train:
            train_dataset = lm_datasets["train"]
            
            
            if data_args.max_train_samples is not None:
                max_train_samples = min(len(train_dataset), data_args.max_train_samples)
                train_dataset = train_dataset.select(range(max_train_samples))
                logger.info(f"Num train samples {len(train_dataset)}")
                logger.info("Training example: ")
                logger.info(tokenizer.decode(train_dataset[0]["input_ids"]))
                
                
        if model_args.model_name_or_path:
            torch_dtype = (
                model_args.torch_dtype
                if model_args.torch_dtype in ["auto", None]
                else getattr(torch, model_args.torch_dtype)
            )
            
            model = LlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                from_tf=bool(".cpkt" in model_args.model_name_or_path),
                config=config,
                cache_dir=model_args.cache_dir,
                revision=model_args.model_revision,
                use_auth_token=True if model_args.use_auth_token else None,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
            )
            
        else:
            model = AutoModelForCausalLM.from_config(config)
            n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
            logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M parameters")
        
        model_vocab_size = model.get_output_embeddings().weight.size(0)
        
        if not (
            (model_vocab_size==32000 and len(tokenizer)==51008) or \
            (model_vocab_size==32000 and len(tokenizer)==32000) or \
            (model_vocab_size==51008 and len(tokenizer)==51008) or \
            (model_vocab_size==49954 and len(tokenizer)==49954)
        ):
            raise ValueError(
                f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
                "Valid configurations (base model / tokenizer):\n"
                "- Continue pre-training original LLaMA: 32000 / 32000 \n"
                "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
                "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
                "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n"
            )
            
                model.resize_token_embeddings(len(tokenizer))
        
        if training_args.peft_path is not None:
            logger.info("PEFT from pretrained model")
            model = PeftModel.from_pretrained(model, training_args.peft_path)
        else:
            logger.info("Init new peft model")
            target_modules = training_args.trainable.split(",")
            modules_to_save = training_args.modules_to_save
            if modules_to_save is not None:
                modules_to_save = modules_to_save.split(",")
            lora_rank = training_args.lora_rank
            lora_dropout = training_args.lora_dropout
            lora_alpha = training_args.lora_alpha
            logger.info(f"Target modules: {target_modules}")
            logger.info(f"LoRA Rank: {lora_rank}")
            peft_config = LoraConfig(
                task_type = TaskType.CAUSAL_LM,
                targert_modules = target_modules,
                inference_mode=False,
                r = lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                modules_to_save=modules_to_save,
            )    
            
            model= get_peft_model(model, peft_config)
        model.print_trainable_parameters()
!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb
import os

from dotenv import load_dotenv




load_dotenv()




hf_token = os.getenv("hf_token")



import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
import pandas as pd

file_path = '../../merged.csv'




df = pd.read_csv(file_path)

df
dataset=df[['Text']]

dataset
dataset_2=dataset.copy()










dataset_2
!pip install scikit-learn



from sklearn.model_selection import train_test_split





train_val_data, test_data = train_test_split(dataset_2, test_size=0.20, random_state=42)

train_data, evaluation_data = train_test_split(train_val_data, test_size=0.10, random_state=42)



print('Training dataset shape:', len(train_data))

print('evaluation dataset shape:', len(evaluation_data))

print('Testing dataset shape:', len(test_data))
evaluation_data
import numpy as np


msk = np.random.rand(len(dataset_2)) < 0.8

train_dataset = dataset_2[msk]

test_dataset = dataset_2[~msk]

from datasets import Dataset



test_dataset=Dataset.from_pandas(test_dataset)



train_dataset=Dataset.from_pandas(train_dataset)



evaluation_dataset=Dataset.from_pandas(evaluation_data)
test_dataset



test_dataset=test_dataset.remove_columns("__index_level_0__")

train_dataset=train_dataset.remove_columns("__index_level_0__")

evaluation_dataset=evaluation_dataset.remove_columns("__index_level_0__")


import datasets


main_dataset= datasets.DatasetDict({

    'train': train_dataset,

    'test': test_dataset,

    'evaluate': evaluation_dataset

})
main_dataset
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    AutoTokenizer,

    TrainingArguments,

    pipeline,

)

from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training

from trl import SFTTrainer

base_model = "NousResearch/Llama-2-7b-hf"

new_model = "llama-2-7b-Amh"






tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)

tokenizer.pad_token = tokenizer.unk_token

tokenizer.padding_side = "right"

bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.float16,

    bnb_4bit_use_double_quant=True,

)




peft_config = LoraConfig(

    r=16,

    lora_alpha=32,

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM",

    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']

)

def load_model(model_name, bnb_config):

    n_gpus = torch.cuda.device_count()

    max_memory = f'{23000}MB'



load_model(base_model,bnb_config)
import torch

torch.cuda.empty_cache()


model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=bnb_config,

    device_map={"": 0}

)




model = prepare_model_for_kbit_training(model)
training_dataset=main_dataset
model
import torch




device = torch.device("cuda:0")




torch.cuda.empty_cache()




print(torch.cuda.memory_summary(device))

import torch

torch.cuda.empty_cache()

import torch




n_gpus = torch.cuda.device_count()

print(f"Number of available GPUs: {n_gpus}")




for i in range(n_gpus):

    gpu_memory = torch.cuda.get_device_properties(i).total_memory

    print(f"GPU {i}: Total memory: {gpu_memory / (1024**3)} GB")

from transformers import Trainer, TrainingArguments, BitsAndBytesConfig


training_arguments = TrainingArguments(

        output_dir="../results",

        num_train_epochs=1,

        per_device_train_batch_size=10,

        per_device_eval_batch_size=1,

        gradient_accumulation_steps=1,

        gradient_checkpointing=True,

        fp16=True,

        evaluation_strategy="steps",

        eval_steps=1000,

        logging_steps=1,

        optim="paged_adamw_8bit",

        learning_rate=2e-4,

        lr_scheduler_type="linear",

        warmup_steps=10,

        
        max_steps=10, 
)




trainer = SFTTrainer(

    model=model,

    train_dataset=main_dataset["train"],

    eval_dataset=main_dataset["evaluate"],

    peft_config=peft_config,

    dataset_text_field="Text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_arguments,

)


model.config.use_cache = False  



trainer.train()




trainer.model.save_pretrained(new_model)


prompt = "የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "how can i treat flu, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "tell me about ethiopian politics, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "who the prime minister of ethiopia, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "3 Ethiopian premier league club, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

del model

del pipe

del trainer

import gc

gc.collect()

gc.collect()
import torch

torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(

    base_model,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(model, new_model)

model = model.merge_and_unload()




tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_side = "right"
import unittest
import sys, os
sys.path.append(os.path.abspath(os.path.join('..')))

from scripts.util import find_average, count_occurence


class TestCases(unittest.TestCase):
    def test_find_average(self):
        """
        Test that it retunrs the average of a given list
        """
        data = [1, 2, 3]
        result = find_average(data)
        self.assertEqual(result, 2.0)

    def test_input_value(self):
        """
        Provide an assertion level for arg input
        """
        
        self.assertRaises(TypeError, find_average, True)

class TestCountOccurence(unittest.TestCase):
    def test_count_occurence(self):
        """
        Test that it returns the count of each unique values in the given list
        """
        data = [0,0,9,0,8,9,0,7]
        result = count_occurence(data)
        output = {0: 4, 9: 2, 8: 1, 7: 1}
        self.assertAlmostEqual(result, output)

    def test_input_value(self):
        """
        Provide an assertion level for arg input
        """
        self.assertRaises(TypeError, count_occurence, True)

if __name__ == '__main__':
    unittest.main()
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub('', text)

    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
parsed_dir = "../data/raw/"

cleaned_dir = "../data/parsed/"

file_name = "TIKVAH-data"

util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")

df.head()

df = pd.DataFrame(df['messages'])

df_messages = pd.DataFrame(df['messages'].tolist())

df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))

df_messages = df_messages[['id', 'text', 'date']]

df_messages
df_filtered = df_messages[(df_messages['id'] >= 60000) & (df_messages['id'] <= 75000)]

df_filtered.head()
data_null_removed = df_filtered.dropna()

data_null_removed.head()
tigvah_data = data_null_removed.replace('\n', ' ', regex=True)

tigvah_data.head()

tigvah_data['hashtags'] = tigvah_data['text'].apply(lambda x: util.extract_hashtags(x))

tigvah_data.head()

tigvah_data['text'] = tigvah_data['text'].str.replace(r'\
tigvah_data.head()

tigvah_data['emojis'] = tigvah_data['text'].apply(util.extract_emojis)

tigvah_data.tail()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_emojis)

tigvah_data.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    tigvah_data['text'] = tigvah_data['text'].str.replace(letter[0][i], letter[1][i])
tigvah_data.head()
tigvah_data['symbols'] = tigvah_data['text'].apply(util.extract_symbols)

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_symbols)

tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)

tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)

tigvah_data.head()
tigvah_data['mentions'] = tigvah_data['text'].apply(util.extract_mentions)

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace('\s+', ' ', regex=True).str.strip()

tigvah_data['text'] = tigvah_data['text'].replace(r'!+', '!', regex=True)

tigvah_data['text'] = tigvah_data['text'].replace(r'\.+', '', regex=True)
tigvah_data.head()
tigvah_data = tigvah_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
tigvah_data
tigvah_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
parsed_dir = "../data/raw/"

cleaned_dir = "../data/parsed/"

file_name = "YeneTube"

util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")

df.head()

df = pd.DataFrame(df['messages'])

df_messages = pd.DataFrame(df['messages'].tolist())

df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))

df_messages = df_messages[['id', 'text', 'date']]

df_messages
data_null_removed = df_messages.dropna()

data_null_removed.head()
yenetube_data = data_null_removed.replace('\n', ' ', regex=True)

yenetube_data.head()

yenetube_data['hashtags'] = yenetube_data['text'].apply(lambda x: util.extract_hashtags(x))

yenetube_data.head()

yenetube_data['text'] = yenetube_data['text'].str.replace(r'\
yenetube_data.head()

yenetube_data['emojis'] = yenetube_data['text'].apply(util.extract_emojis)

yenetube_data.tail()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_emojis)

yenetube_data.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    yenetube_data['text'] = yenetube_data['text'].str.replace(letter[0][i], letter[1][i])
yenetube_data.head()
yenetube_data['symbols'] = yenetube_data['text'].apply(util.extract_symbols)

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_symbols)

yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)

yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)

yenetube_data.head()
yenetube_data['mentions'] = yenetube_data['text'].apply(util.extract_mentions)

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace('\s+', ' ', regex=True).str.strip()

yenetube_data['text'] = yenetube_data['text'].replace(r'!+', '!', regex=True)

yenetube_data['text'] = yenetube_data['text'].replace(r'\.+', '', regex=True)
yenetube_data.head()
yenetube_data = yenetube_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
yenetube_data
yenetube_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_distances(sentences):
    distances = []
    for i in range(len(sentences) - 1):
        embedding_current = sentences[i]['combined_sentence_embedding']
        embedding_next = sentences[i + 1]['combined_sentence_embedding']
        
                similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
        
                distance = 1 - similarity

                distances.append(distance)

                sentences[i]['distance_to_next'] = distance

        
    return distances, sentences
import re
import numpy as np
from combine_sentences import combine_sentences
import calculate_cosine_distance
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from calculate_cosine_distance import calculate_cosine_distances
from dotenv import load_dotenv
import os
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
def semantic_retriever(essay):
    

    try:
        single_sentences_list = re.split(r'(?<=[.?!])\s+', essay)
        sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]
        
        try:
            sentences = combine_sentences(sentences)
        except Exception as e:
            print(f"Error in combine_sentences: {e}")
            return []

        try:
            oaiembeds = OpenAIEmbeddings()
            embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])
        except Exception as e:
            print(f"Error in OpenAI embeddings: {e}")
            return []

        for i, sentence in enumerate(sentences):
            sentence['combined_sentence_embedding'] = embeddings[i]

        try:
            distances, sentences = calculate_cosine_distances(sentences)
        except Exception as e:
            print(f"Error in calculate_cosine_distance: {e}")
            return []

        breakpoint_percentile_threshold = 95
        breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)

        indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]

        start_index = 0
        chunks = []

        for index in indices_above_thresh:
            end_index = index
            group = sentences[start_index:end_index + 1]
            combined_text = ' '.join([d['sentence'] for d in group])
            chunks.append(combined_text)
            start_index = index + 1

        if start_index < len(sentences):
            combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])
            chunks.append(combined_text)

        return chunks

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return []
from extract_json import extract_json

a= extract_json('ብስራት ስፖርት.json')
from semantic_chuncking import semantic_retriever

chunks = semantic_retriever(a)
from operator import itemgetter



from langchain_community.vectorstores import FAISS

from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(

    chunks, embedding=OpenAIEmbeddings()

)

retriever = vectorstore.as_retriever(search_kwargs={"k" : 1}) relevant_docs = retriever.get_relevant_documents("ቤቲንግ")

relevant_docs
for doc in relevant_docs:

  print(doc.page_content)

  print('\n')
template = """<human>: Create a compelling Amharic or Amharic mixed with English advertisement for {question}.

Highlight the key features,unique selling points, and the exceptional services offered by the brand. 

Craft a persuasive narrative that resonates with the target audience, emphasizing the brand's values 

and commitment to customer satisfaction. Use vibrant and engaging language to evoke a positive emotional

response and encourage potential customers to explore and choose {question}. 

Ensure the ad reflects the brand's identity and leaves a lasting impression on the audience.



Make it a highly personalized promotional text that resonates with {context}, drawing insights from their 

contextual data. Leverage historical interactions, discussions, or online activities to tailor the ad 

closely to {context} unique preferences and interests. Highlight features and services that align with 

their needs based on the available contextual information. Tailor the language to address specific topics,

preferences, or issues derived from {context} historical interactions. Ensure the personalized content feels

natural and aligns with [User's] communication style, fostering a deep connection and encouraging meaningful

engagement. The goal is to create an ad that demonstrates an understanding of {context} context, providing

value and relevance in a way that feels organic.


{context}




Question: {question}



\n



<bot>:

"""



prompt = ChatPromptTemplate.from_template(template)



model = Mistral_7B
chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)
import json
import re

def extract_json(json_file):
    last_25_percent_text = ""

        with open(json_file, 'r') as file:
        data = json.load(file)
        messages = data.get('messages', [])

        last_25_percent_length = int(len(messages) * 0.99)
        for message in messages[-last_25_percent_length:]:
                        text_content = message.get('text', [])

                        if isinstance(text_content, list):
                for item in text_content:
                    if isinstance(item, str):
                        last_25_percent_text += item

        cleaned_text = re.sub(r'[!@
    return last_25_percent_text
import json




text_values = []




with open('manchester.json', 'r') as file:

    data = json.load(file)

    messages = data.get('messages', [])



    for message in messages:

        text_content = message.get('text', None)



        
        if text_content and isinstance(text_content, list):

            
            first_part_of_text = text_content[0] if text_content and isinstance(text_content[0], str) else None

            if first_part_of_text:

                text_values.append(first_part_of_text)




concatenated_text = '\n'.join(text_values)




print(len(concatenated_text))
def combine_sentences(sentences, buffer_size=1):

    
    for i in range(len(sentences)):



        
        combined_sentence = ''



        
        for j in range(i - buffer_size, i):

            
            if j >= 0:

                
                combined_sentence += sentences[j]['sentence'] + ' '



        
        combined_sentence += sentences[i]['sentence']



        
        for j in range(i + 1, i + 1 + buffer_size):

            
            if j < len(sentences):

                
                combined_sentence += ' ' + sentences[j]['sentence']



        
        
        sentences[i]['combined_sentence'] = combined_sentence



    return sentences





def pretty_print_docs(docs):

    print(

        f"\n{'-' * 100}\n".join(

            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]

        )

    )
from langchain.text_splitter import CharacterTextSplitter

from langchain_community.document_loaders import TextLoader

from langchain_community.vectorstores import FAISS

from langchain_openai import OpenAIEmbeddings



documents = TextLoader(concatenated_text).load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

texts = text_splitter.split_documents(documents)

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()



docs = retriever.get_relevant_documents(

    "What did the president say about Ketanji Brown Jackson"

)

pretty_print_docs(docs)
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=yene-tube.txt --model_prefix=m --vocab_size=2000')




sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))




print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))
print(sp.decode_ids([434, 111, 997, 1507]))

print(sp.get_piece_size())




print(sp.id_to_piece(460))

print(sp.piece_to_id('▁በአዲስ'))




print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))





for id in range(3):

  print(sp.id_to_piece(id), sp.is_control(id))
pip install tensorflow
import tensorflow as tf




serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()



sp = spm.SentencePieceProcessor()

sp.load_from_serialized_proto(serialized_model_proto)



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   


sp = spm.SentencePieceProcessor()

sp.load('m_bos_as_user.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')



print('bos=', sp.bos_id())

print('eos=', sp.eos_id())

print('unk=', sp.unk_id())

print('pad=', sp.pad_id())  




print(sp.encode_as_ids('በአዲስ አበባ'))




print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])
%pip install emoji
import sys, os, json, re, zipfile, csv

import pandas as pd

import emoji

class Util():

    def __init__(self) -> None:

        self.emoji_pattern = re.compile(r"[\U0001F000-\U0001F9FF\U0001FA00-\U0001FFFF\U00020000-\U0002FFFF\U00030000-\U0003FFFF]+", flags=re.UNICODE)



        

        self.symbols = re.compile("["

                                  "\""

                                  "\“"

                                  "\""

                                  "\'"

                                  "\-"

                                  "\*"

                                  "\•"

                                  "\ℹ"

                                  "\﻿"

                                  "\_"

                                  "]+")

        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

        self.mention_pattern = r'@(\w+)'



    def read_file(self, file_path: str) -> dict:

        
        with open(file_path, 'r') as file:

            
            data = json.load(file)

            return data



    def write_file(self, file_path: str, data: dict) -> None:

        
        with open(file_path, 'w') as file:

            
            json.dump(data, file, indent=2)



    def parse_text(self, text: any) -> str:

        if isinstance(text, str):

            return text

        elif isinstance(text, list):

            contents = []

            for item in text:

                if isinstance(item, str):

                    contents.append(item)

                elif isinstance(item, dict):

                    contents.append(item['text'])

            return "".join(contents)

        else:

            return ""



    def parse_messages(self, messages: list) -> dict:

        parsed_messages = {

            'id': [],

            'text': [],

            'date': []

        }

        for message in messages:

            if message['type'] != 'message' or len(message['text']) == 0:

                continue

            parsed_messages['id'].append(message['id'])

            message_content = self.parse_text(message['text'])

            parsed_messages['text'].append(message_content)

            parsed_messages['date'].append(message['date'])

        return parsed_messages



    def extract_hashtags(self, text: str) -> list:

        return [word for word in text.split() if word.startswith('


    def extract_emojis(self, text):

        return ''.join(self.emoji_pattern.findall(text))



    def remove_emojis(self, text):

        return self.emoji_pattern.sub('', text)



    def extract_symbols(self, text):

        return ''.join(self.symbols.findall(text))



    def remove_symbols(self, text):

        return self.symbols.sub(' ', text)



    def extract_urls(self, text):

        return re.findall(self.url_pattern, text)



    def extract_mentions(self, text):

        return re.findall(self.mention_pattern, text)

    

    def extract_fields(self, message):

        """

        Extracts relevant fields from the message.

        Returns a tuple containing (channel_id, text, date, labels).

        """

        text = ' '.join(item['text'] for item in message['text_entities'] if item['type'] in 'plain')

        date = message['date']

        labels = "LABEL"  
        return text, date, labels



    def process_json_file(self, json_file, csv_writer):

        """

        Processes a JSON file, extracts relevant fields, and writes to CSV.

        """

        data = json.load(json_file)



        channel_id = data['id']

        for message in data['messages']:

            text, date, labels = self.extract_fields(message)

            csv_writer.writerow([channel_id, text, date, labels])



    def process_zip(self, zip_file_path, output_csv_path):

        """

        Processes a zip file, extracts data from JSON files, and writes to a CSV file.

        """

        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:

            with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:

                csv_writer = csv.writer(csv_file)

                csv_writer.writerow(['id', 'text', 'date', 'label'])



                for file_info in zip_file.infolist():

                    with zip_file.open(file_info.filename) as json_file:

                        print(json_file)

                        self.process_json_file(json_file, csv_writer)



    def process_zip_files(self, zip_file_path, output_directory):

        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:

            
            for file_info in zip_file.infolist():

                with zip_file.open(file_info.filename) as json_file:

                    
                    data = json.load(json_file)

                    parsed_data = self.parse_json_data(data)



                    
                    df = pd.DataFrame(parsed_data)



                    
                    output_file_name = os.path.splitext(os.path.basename(file_info.filename))[0]

                    output_csv_path = os.path.join(output_directory, f"{output_file_name}_parsed.csv")

                    df.to_csv(output_csv_path, index=False)



    def parse_json_data(self, data):

        
        
        parsed_data = {

            'id': [],

            'text': [],

            'date': [],

            'label': []

        }



        for message in data['messages']:

            
            text, date, labels = self.extract_fields(message)

            parsed_data['id'].append(data['id'])

            parsed_data['text'].append(text)

            parsed_data['date'].append(date)

            parsed_data['label'].append(labels)

            

        return parsed_data

                        

    def file_reader(self, path: str, ) -> str:

        fname = os.path.join(path)

        with open(fname, 'r') as f:

            system_message = f.read()

        return system_message

util = Util()










zip_file_path = "../data/raw/raw.zip"

output_csv_path = "../data/parsed/parsed.csv"



util.process_zip(zip_file_path, output_csv_path)

print("Parsing completed. Output saved to", output_csv_path)























parsed_csv_path = "../data/parsed/parsed.csv"

output_cleaned_csv_path = "../data/parsed/cleaned_parsed.csv"




df = pd.read_csv(parsed_csv_path)

df = df.dropna()

df.head()

df['text'] = df['text'].replace('\n', ' ', regex=True)

df.head()

df['text'] = df['text'].str.replace(r'\
df.head()


df['text'] = df['text'].apply(util.remove_emojis)

df.head()

df['text'] = df['text'].apply(util.remove_symbols)

df.head()

df['text'] = df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

df.head()
df['text'] = df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

df.head()

df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

df['text'] = df['text'].replace(r'!+', '!', regex=True)

df['text'] = df['text'].replace(r'\.+', '', regex=True)

df.head()







letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]



for letter in letters:

  for i in range(len(letter[0])):

    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])



df.head()  


df['text'] = df['text'].str.replace(r'[A-Za-z]+', '', regex=True)

df.head()

cleaned_output_path = "../data/parsed/cleaned_parsed_data.csv"

df.to_csv(cleaned_output_path, index=False)

output_text_path = "../data/cleaned/cleaned.txt"

df['text'] = df['text'].apply(util.remove_emojis)

df['text'].to_csv(output_text_path, index=False, header=False, sep='\t')
zip_file_path = "../data/raw/raw.zip"

output_directory = "../data/parsed/"

util.process_zip_files(zip_file_path, output_directory)
import os

import pandas as pd




parsed_files_directory = "../data/parsed/"

cleaned_files_directory = "../data/cleaned/"




for filename in os.listdir(parsed_files_directory):

    if filename.endswith("_parsed.csv"):

        
        filepath = os.path.join(parsed_files_directory, filename)

        df = pd.read_csv(filepath)



        
        df = df.dropna()



        
        df['text'] = df['text'].replace('\n', ' ', regex=True)



        
        df['text'] = df['text'].str.replace(r'\


        
        df['text'] = df['text'].apply(util.remove_emojis)



        
        df['text'] = df['text'].apply(util.remove_symbols)



        
        df['text'] = df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

        df['text'] = df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()



        
        df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

        df['text'] = df['text'].replace(r'!+', '!', regex=True)

        df['text'] = df['text'].replace(r'\.+', '', regex=True)



        
        letters = [

            [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

            [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

            [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

            [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

            [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

        ]



        for letter in letters:

            for i in range(len(letter[0])):

                df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])



        
        df['text'] = df['text'].str.replace(r'[A-Za-z]+', '', regex=True)



        
        df.to_csv(filepath, index=False)



        
        cleaned_text_path = os.path.join(cleaned_files_directory, f"{os.path.splitext(filename)[0]}.txt")

        df['text'].to_csv(cleaned_text_path, index=False, header=False)
import os
import pandas as pd
from utils import Util

def clean(filepath):
    df = pd.read_csv(filepath)

        df = df.dropna()

        df['text'] = df['text'].replace('\n', ' ', regex=True)

        df['text'] = df['text'].str.replace(r'\
        df['text'] = df['text'].apply(util.remove_emojis)

        df['text'] = df['text'].apply(util.remove_symbols)

        df['text'] = df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()
    df['text'] = df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

        df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()
    df['text'] = df['text'].replace(r'!+', '!', regex=True)
    df['text'] = df['text'].replace(r'\.+', '', regex=True)

        letters = [
        [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
        [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
        [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],
        [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],
        [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]
    ]

    for letter in letters:
        for i in range(len(letter[0])):
            df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

        df['text'] = df['text'].str.replace(r'[A-Za-z]+', '', regex=True)


        cleaned_text_path = os.path.join(cleaned_files_directory, f"{os.path.splitext(filename)[0]}.txt")
    df['text'].to_csv(cleaned_text_path, index=False, header=False)

def clean_all_in_one(input_path, output_path):
        parsed_csv_path = "../data/parsed/parsed.csv"
    output_cleaned_csv_path = "../data/parsed/cleaned_parsed.csv"

    




if __name__ == "__main__":
    util = Util()
import os
import pandas as pd
from utils import Util

def parse_all_in_one():
    zip_file_path = "../data/raw/raw.zip"
    output_csv_path = "../data/parsed/parsed.csv"

    util.process_zip(zip_file_path, output_csv_path)
    print("Parsing completed. Output saved to", output_csv_path)

def parse_cleaned_individual_files():
    zip_file_path = "../data/raw/raw.zip"
    output_directory = "../data/parsed/"
    util.process_zip_files(zip_file_path, output_directory)


if __name__ == "__main__":
    util = Util()
    parse_all_in_one()
    parse_cleaned_individual_files()
Backend code for Amharic Ad generator.


This backend handles prompt generation based on user input.



- Python (version 3.11.7)
- Flask (install via `pip install fastapi`)
- Uvicorn (install via `pip install uvicorn`)
- ...


```bash
git clone https://github.com/group-3-collab-team/Amharic-RAG-Ad-Builder.git
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')




sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))




print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))
print(sp.decode_ids([460, 133, 774, 1276]))


print(sp.get_piece_size())




print(sp.id_to_piece(460))

print(sp.piece_to_id('▁በአዲስ'))




print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))





for id in range(3):

  print(sp.id_to_piece(id), sp.is_control(id))


import tensorflow as tf




serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()



sp = spm.SentencePieceProcessor()

sp.load_from_serialized_proto(serialized_model_proto)



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))










spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')



sp_ctrl = spm.SentencePieceProcessor()

sp_ctrl.load('m_ctrl.model')




print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))

print(sp_ctrl.piece_to_id('<sep>'))  
print(sp_ctrl.piece_to_id('<cls>'))  
print('3=', sp_ctrl.decode_ids([3]))  
print('4=', sp_ctrl.decode_ids([4]))  spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   


sp = spm.SentencePieceProcessor()

sp.load('m_bos_as_user.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   

spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')



print('bos=', sp.bos_id())

print('eos=', sp.eos_id())

print('unk=', sp.unk_id())

print('pad=', sp.pad_id())  




print(sp.encode_as_ids('በአዲስ አበባ'))




print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])


spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')





for n in range(10):

  print(sp.sample_encode_as_pieces('በአዲስ አበባ', -1, 0.1))



for n in range(10):

  print(sp.sample_encode_as_ids('በአዲስ አበባ', -1, 0.1))

print(sp.nbest_encode_as_pieces('በአዲስ አበባ', 10))

print(sp.nbest_encode_as_ids('በአዲስ አበባ', 10))


spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')

sp_bpe = spm.SentencePieceProcessor()

sp_bpe.load('m_bpe.model')



print('*** BPE ***')

print(sp_bpe.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_bpe.nbest_encode_as_pieces('በአዲስ አበባ', 5))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')

sp_unigram = spm.SentencePieceProcessor()

sp_unigram.load('m_unigram.model')



print('*** Unigram ***')

print(sp_unigram.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_unigram.nbest_encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ', 5))
import json

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter  
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Weaviate
import logging
from typing import List, Optional, Union
from langchain.prompts import PromptTemplate
from transformers import pipeline


import transformers
from sentence_transformers import SentenceTransformer



from datasets import Dataset

import weaviate
from dotenv import load_dotenv,find_dotenv
from weaviate.embedded import EmbeddedOptions

 
load_dotenv(find_dotenv())

logger = logging.getLogger(__name__)

def data_loader(file_path: str, chunk_size: int = 500, chunk_overlap: int = 50) -> Union[List[str], None]:
    """
    Load data from a file, split it into chunks, and return the chunks.

    Parameters:
    - file_path (str): The path to the file containing the data.
    - chunk_size (int): The size of each data chunk. Default is 500.
    - database (int): The overlap between consecutive chunks. Default is 50.

    Returns:
    - list: A list of data chunks.
    """
    try:
        loader = TextLoader(file_path)
        documents = loader.load()

                text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = text_splitter.split_documents(documents)
        
        logger.info("Data loaded to vector database successfully")
        return chunks
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        return None 


        





    
    
def create_retriever(chunks, model):
   try:
              load_dotenv(find_dotenv())

    

           client = weaviate.Client(embedded_options=EmbeddedOptions())

              vectorstore = Weaviate.from_documents(
           client=client,
           documents=chunks,
           embedding=model.encode,             by_text=False
       )

              retriever = vectorstore.as_retriever()
       print("Retriever created successfully.")

       return retriever

   except Exception as e:
       print(f"An unexpected error occurred: {e}")
       return None




        



    

def create_langchain_pipeline(retriever, template, temperature=0, model_name="meta-llama/Llama-2-7b-chat-hf"):
    try:
                model_name = "meta-llama/Llama-2-7b-chat-hf"
        token = "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"
        
        
                        llm = pipeline("text-generation", model=model_name, temperature=temperature)

                prompt = PromptTemplate.from_template(template)

                rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        print("Langchain with RAG pipeline created successfully.")
        return rag_chain

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None
    

def generate_testcase_and_context(questions, ground_truths, retriever, rag_chain):
    try:
        answers = []
        contexts = []

                for query in questions:

            answers.append(rag_chain.invoke(query))
            contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])

            
        data = {
            "question": questions,             "answer": answers,             "contexts": contexts,             "ground_truths": ground_truths         }


                dataset = Dataset.from_dict(data) 
        print("automatic evaluation data generated succesfully.")

        return  dataset
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 


    


def load_file(file_path):
    try:

                with open(file_path, 'r') as file:
                        file_contents = file.read()   
        
        return file_contents
        
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 


def get_generated_prompt_with_evaulation(question):
    try:
        chunks = data_loader()
        retriever = create_retriever(chunks)

        prompt_template = load_file('../prompts/prompt-generation-prompt.txt')
        evaluation_tempate = load_file('../prompts/evaluation-data-generation.txt')


        prompt_rag_chain = create_langchain_pipeline(retriever, prompt_template)
        evaulation_rag_chain = create_langchain_pipeline(retriever, evaluation_tempate, temperature=0.2)


        generated_prompts = prompt_rag_chain.invoke(question)
        prompt_list  = json.loads(generated_prompts)

        questions = [item['prompt'] for item in prompt_list]
        ground_truths = [[item['ground_truth']] for item in prompt_list]

        response = generate_testcase_and_context(questions, ground_truths, retriever, evaulation_rag_chain)
        return response
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None
Welcome to the front-end repository of the Enterprise-Grade RAG System, built with React.js.


To run the development server, use one of the following commands:

```bash
npm run dev
yarn dev
npm dev
max-width: 1440px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

nav{
  background-color: }
.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
@tailwind base;
@tailwind components;
@tailwind utilities;

* {
  font-family: "Inter", sans-serif;
}

@media screen and (min-width: 480px) {
  .card:nth-child(7n + 1) {
    grid-column: auto/span 2;
    grid-row: auto/span 2;
  }
}

.prompt::-webkit-scrollbar {
  width: 5px;
}

.prompt::-webkit-scrollbar-thumb {
  background-color:   border-radius: 5px;
}

input {
  color: }

body {
  background-color: theme("colors.hero") ;
  color: theme("colors.grey");
  scroll-behavior: smooth;
  font-size: 14px;
}

.paginate,
.next,
.prev {
  border-radius: 50%;
  background-color: theme("colors.grey");
  padding: 0.2rem 0.6rem;
  font-size: 0.8rem;
  display: flex;
  justify-content: center;
  align-items: center;
  color: theme("colors.dark");
  font-weight: bold;
  margin: 0 0.1rem;
  cursor: pointer;
  transition: all 0.5s ease;
}

.next:hover,
.prev:hover,
.paginate:hover {
  color: theme("colors.brand");
}

header {
  z-index: 100;
}
.paginate.active {
  background-color: theme("colors.accent");
}
.next,
.prev {
  border-radius: 10px;
  background-color: theme("colors.grey");
}

.next:disabled,
.prev:disabled {
  background-color: theme("colors.darkgrey");
  color: theme("colors.grey");
  opacity: 0.3;
  cursor: not-allowed;
}

h1 {
  font-size: 2rem;
  font-weight: 600;
}

.hero {
  display: grid;
  grid-template-columns: 100%;
  align-items: center;
  grid-gap: 10%;
  justify-content: center;
}

.hero__img {
  width: 90%;
  height: 80%;
  position: relative;
  overflow: hidden;
  border-radius: 10px 0 0 300px;
  display: none;
}

.hero__img img {
  transition: all 0.6s ease;
  cursor: pointer;
}

.hero__img img:hover {
  transform: scale(1.2);
}

::placeholder {
  color:   font-size: 0.75rem;
}
.form {
  display: grid;
  grid-template-columns: 1fr;
  grid-gap: 2rem;
}

.form_photo {
  width: 100%;
  height: 100%;
  margin-top: auto;
}
@media screen and (min-width: 768px) {
  .form {
    grid-template-columns: 1fr 1fr;
  }
  .form_photo {
    width: 80%;
    height: 80%;
  }
  .hero {
    grid-template-columns: 40% 50%;
  }
  .hero__img {
    display: block;
  }
  h1 {
    font-size: 2.5rem;
  }
  body{
    font-size: 16px;
  }
}
import React from 'react'
import { BrowserRouter, Route, Routes, Link } from 'react-router-dom'
import { aiqem_logo } from "./assets"
import { Home, CreatePost } from "./pages"


const App = () => {
  return (
    <BrowserRouter>
      <header className="w-full fixed flex justify-between items-center bg-blue-800 sm:px-8 px-4 py-4 border-b border-blue-800">
        <Link to="/">
          <img src={aiqem_logo} alt="Logo" /> 
        </Link>
        <Link to="/" className="font-inter font-medium text-white px-2 ml-auto">Home</Link>
        <Link to="/create" className="font-inter font-bold bg-blue-800 text-white px-2 py-1 rounded-md">Chat</Link>
      </header>
      <main className="py-8 w-full bg-white  min-h-[calc(100vh)]">
        <Routes>
          <Route path="/create" element={<CreatePost />} />
          <Route path="/" element={<Home />} />
        </Routes>
      </main>
    </BrowserRouter>
  )
}

export default App

//106e75
import download from "./download.png";
import aiqem_logo from "./aiqem_logo.svg";
import logo from "./aiqem_logo.svg";
import preview from "./preview.png";
import hero from "./hero.png";
import africa1 from "./africa1.jpg";
import images from "./images.jpeg"
import telegram from "./telegram.jpeg";
export { download, aiqem_logo, logo,  preview, hero, africa1 , images, telegram};
import React from 'react'
import { useState, useRef} from 'react';
import axios from "axios";

const FileUpload = () => {
    const fileInputRef = useRef(null);

    const [text, setText] = useState("");

    const handleButtonClick = () => {
        // Trigger the file input when the button is clicked
        fileInputRef.current.click();
      };

    const handleFileChange = async (event) => {
    const selectedFile = event.target.files[0];

    const formData = new FormData();
    formData.append('file', selectedFile);


    try {
        const response = await axios.post('http://127.0.0.1:5000/extract-text', formData, {
        headers: { 'Content-Type': 'multipart/form-data' },
        });
        setText(response.data.data);
    } catch (error) {
        console.error(error);
    }

    };

      
  return (
    <div>
    <button onClick={handleButtonClick}>
      <span role="img" aria-label="attachment">📎</span>
    </button>
    <input
      ref={fileInputRef}
      type="file"
      accept=".pdf"
      style={{ display: 'none' }}
      onChange={handleFileChange}
    />  
</div>
  )
}

export default FileUpload
import React, { useState } from "react";
import { useNavigate } from "react-router-dom";

import { preview } from "../assets";
import { getRandomPrompt } from "../utils";
import { FormFields, Loader } from "../components";
import FileUpload from "../components/FileUpload";
import { africa1 } from "../assets/index";

const CreatePost = () => {
    const navigate = useNavigate();
    const models_list = [
        {"model_name": "Llama2 Model",
        "model_type": ""
    },
    {"model_name": "Finetuned Llama2 Model",
    "model_type": ""
    },
    {"model_name": "GPT 3.5 TURBO Model",
    "model_type": "gpt-3.5-turbo"
    },
    {"model_name": "GPT 4.0 Model",
    "model_type": "gpt-4-1106-preview"
    }
    ]
    const [selectedModel, setSelectedModel] = useState(models_list[0].model_name); // Set the default selected model
    console.log(selectedModel)
    const [message, setMessage] = useState('');
    const [chatResponse, setchatResponse] = useState('');



    const submitQuery= async () => {
        if (form.scenario) {
            try {
                setGeneratingprompt(true);
                const response = await fetch(
                    "https://192.168.137.236/api/generate",
                    {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                        },
                        body: JSON.stringify({
                            prompt: form.scenario,
                        }),
                    }
                );
                const data = await response.json();
                setForm({ ...form, preview: `data:image/jpeg;base64,${data.photo}` });
                setResult(data.result); // Set the result in the state
            } catch (err) {
                console.log(err);
            } finally {
                setGeneratingprompt(false);
            }
        } else {
            alert("Please provide a proper prompt");
        }
    };

    const handleMessageChange = (event) => {
        setMessage(event.target.value);
    };

    console.log('Submitted input:', message);
    console.log('Submitted input:', chatResponse);

    const submitInput = async () => {
        // Handle submitting the input, you can use the 'message' state here
        // Add your logic for submitting the input to the backend
        try {
        const response = await fetch(
            'https://9bba-196-189-127-38.ngrok-free.app/api/v1/chat', {

            method: 'POST',
            headers: {
            'Content-Type': 'application/json',
            },
            body: JSON.stringify({ "message":message , "model_type":models_list[selectedModel].model_type}),
        });

        if (response.ok) {
            
            setMessage('')
            setchatResponse(response);
        } else {
            console.error('Failed to submit input to the backend');
        }
        } catch (error) {
        console.error('Error during API call:', error);
        }
    };


    const handleSubmit = async (e) => {
        e.preventDefault();
    
        if (form.scenario && form.preview) {
            setLoading(true);
            try {
                const response = await fetch(
                    "https://192.168.137.236/api/generate",
                    {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                        },
                        body: JSON.stringify({ ...form}),
                    }
                );
    
                if (response.ok) {
                    const responseData = await response.json();
                    // Assuming the response has a property named "result"
                    const result = responseData.result;
    
                    // Do something with the result
                    console.log(result);
                    // You can also update your UI or state with the received result
                } else {
                    console.log("Failed to get a successful response from the server");
                }
            } catch (err) {
                console.error(err);
            } finally {
                setLoading(false);
            }
        } else {
            alert("Please generate a prompt with proper details");
        }
    };
    
    return (
        <section className="flex flex-row bg-white min-h-[calc(100vh)]">
                <div className="sm:hidden  md:flex md:flex-col md:w-1/3 md:flex-shrink-0 md:lg:w-[240px] md:h-[calc(100vh-120px)] md:whitespace-nowrap md:fixed bg-white md:overflow-x-hidden md:transition-all md:ease-in-out pt-2">
                    <div className="flex flex-col mt-24 items-start space-y-4  md:h-full ml-[16px]">
                        <label className="text-lg ml-4 font-bold text-black" id="demo-radio-buttons-group-label">
                            Select Model
                        </label>

                        <div className="flex flex-col space-y-2 pl-2">
      {models_list.map((model, index) => (
        <div key={index}>
          <input
            type="radio"
            id={`model-${index}`}
            name="radio-buttons-group"
            value={model.model_name}
            className="mr-2"
            checked={index === selectedModel}
            onChange={() => setSelectedModel(index)}
          />
          <label htmlFor={`model-${index}`} className="text-base text-black">
            {model.model_name}
          </label>
        </div>
      ))}
      
    </div>
                    </div>
                </div>
  {/* Main Content */}
                <div className="flex flex-col h-full md:w-3/4 px-4 py-6 sm:w-full">
                    {/* Texts */}
                    <div className="sm:flex sm:flex-col md:ml-[530px]  sm:ml-[100px] sm:mt-16 sm:font-extrabold sm:text-text sm:text-[42px]">
                        <h1 className="md:ml-[100px] text-black sm:text-[40px] sm:ml-[160px]">አድባር</h1>

                        <div className="flex justify-center space-x-6 mt-8 ml-[-4px]">
                            {/* First Container */}
                            <div className="md:text-3xl text-xl text-black bg-gray-100 rounded-lg p-6 shadow-md sm:max-w-[400px] md:max-w-[1600px]">
                                <h2 className="font-bold ml-4">Retail</h2>
                                <p className="text-gray-600">Generate Telegram Ad</p>
                            </div>

                            {/* Second Container */}
                            <div className="md:text-3xl text-xl text-black bg-gray-100 rounded-lg p-6 shadow-md sm:max-w-[400px] md:max-w-[1600px]">
                                <h2 className="font-bold ml-4">Automotive</h2>
                                <p className="text-gray-600">Generate Telegram Ad</p>
                            </div>

                            {/* Third Container */}
                            <div className="md:text-3xl text-xl text-black bg-gray-100 rounded-lg p-6 shadow-md sm:max-w-[400px] md:max-w-[1600px]">
                                <h2 className="font-bold ml-4">Real Estate</h2>
                                <p className="text-gray-600">Generate Telegram Ad</p>
                            </div>
                            </div>
                        {/* <img src={ africa1 } alt="img" className="ml- -8 sm:w-[400px] sm:ml-[52px] md:ml-[2px] w-[600px]"/> */}
                    </div>
                    

                    <div className="sm:flex sm:flex-col md:ml-32 sm:ml-[-40px] sm:w-3/4">

                        <footer className="flex-row-2 mt-2 mb-2 border-blue-800 p-4 absolute bottom-0 ml-36 w-3/4" onSubmit={handleSubmit}>
                        <label for="chat" class="sr-only">Your message</label>
                            <div class="flex items-center py-2 px-3 bg-blue-800 rounded-lg dark:bg-blue-800">
                            <FileUpload/>
                            <div>

    
                                <textarea
                                    id="chat" 
                                    rows="1" 
                                    
                                    class="block mx-4 p-2.5 w-full text-sm text-gray-900 bg-white rounded-lg border focus:ring-blue-500 focus:border-blue-500 dark:bg-white-800 dark:border-blue-800 dark:placeholder-blue-800 dark:text-black dark:focus:ring-blue-500 dark:focus:border-blue-500"
                                    placeholder="Your message..."
                                    value={message}
                                    onChange={handleMessageChange}
                                />

      {/* Display the value (optional) */}
      

                                <button
                                    type="submit"
                                    onClick={submitInput}
                                    class="inline-flex justify-center p-2 text-blue-600 rounded-full cursor-pointer hover:bg-blue-100 dark:text-blue-500 dark:hover:bg-gray-600">
                                    <svg
                                    className="w-6 h-6 rotate-90"
                                    fill="white"
                                    viewBox="0 0 20 20"
                                    xmlns="http://www.w3.org/2000/svg"
                                    >
                                    <path d="M10.894 2.553a1 1 0 00-1.788 0l-7 14a1 1 0 001.169 1.409l5-1.429A1 1 0 009 15.571V11a1 1 0 112 0v4.571a1 1 0 00.725.962l5 1.428a1 1 0 001.17-1.408l-7-14z"></path>
                                    </svg>
                                </button>
                            </div>

    {chatResponse && (
        <div style={{ border: '1px solid           <p>Response:</p>
          <pre>{JSON.stringify(chatResponse, null, 2)}</pre>
        </div>
      )}
                            </div>
                    </footer>
                                
                    </div>
                </div>
</section>
    );
};

export default CreatePost;
import React, { useState, useEffect } from "react";
import { Loader, FormFields, Card } from "../components";
import { Link } from "react-router-dom";
import { africa1, telegram } from "../assets/index";

const RenderCards = ({ data, title }) => {
    if (data?.length > 0) {
        return data.map((post) => <Card key={post._id} {...post} />);
    } else {
        return <h2 className="text-brand font-bold text-xl">{title}</h2>;
    }
};

const Home = () => {
    const [loading, setLoading] = useState(false);
    const [allPosts, setAllPosts] = useState([]);
    const [searchText, setSearchText] = useState("");
    const [filteredPosts, setFilteredPosts] = useState([]);
    const [searchTimeout, setSearchTimeout] = useState(null);

    useEffect(() => {
        const fetchPosts = async () => {
            setLoading(true);
            try {
                const response = await fetch(
                    "https://dalle-hn3a.onrender.com/api/v1/post",
                    {
                        method: "GET",
                        headers: {
                            "Content-Type": "application/json",
                        },
                    }
                );

                if (response.ok) {
                    const result = await response.json();
                    setAllPosts(result.data.reverse());
                }
            } catch (err) {
                console.log(err);
            } finally {
                setLoading(false);
            }
        };
        fetchPosts();
    }, []);

    const handleSearchChange = async (e) => {
        clearTimeout(searchTimeout);
        setSearchText(e.target.value);

        setSearchTimeout(
            setTimeout(() => {
                const filteredPosts = allPosts.filter((post) =>
                    post.prompt.toLowerCase().includes(searchText.toLowerCase())
                );
                setFilteredPosts(filteredPosts);
                setLoading(false);
            }, 500)
        );
    };

    // set dynamic imgPerPage value according to screen size
    if (window.innerWidth <= 768) {
        var dynamicPerPage = 3;
    } else {
        dynamicPerPage = 6;
    }

    // implement pagination
    const [currentPage, setCurrentPage] = useState(1);
    const [postsPerPage] = useState(dynamicPerPage);
    const indexOfLastPost = currentPage * postsPerPage;
    const indexOfFirstRepo = indexOfLastPost - postsPerPage;
    const currentPosts = allPosts.slice(indexOfFirstRepo, indexOfLastPost);

    const paginate = (pageNumber) => {
        setCurrentPage(pageNumber);
        window.scrollTo({ top: 0, behavior: "smooth" });
    };

    // calculate page numbers
    const pageNumbers = [];
    for (let i = 1; i <= Math.ceil(allPosts.length / postsPerPage); i++) {
        pageNumbers.push(i);
    }

    return (
        <section className="mx-auto">
            <div className="md:grid md:grid-cols-2 md:grid-flow-row md:gap-4 max-w-7xl mt-16 sm:p-8 px-4 py-8 m-auto bg-white">
                <div className="hero__text grid-col-1 flex flex-col"> <br />
                    <h1 className="text-text text-blue-800">አድባር</h1>
                    <p className="mt-2 text-text max-w-[520px] text-hero text-[15px]">
                    Welcome to AIQEM, where innovation meets impact in the heart of African technology! 🌍
                    Unleashing the power of AI and Blockchain, AIQEM proudly presents አድባር – our groundbreaking Telegram Ad solution tailored for Ethiopian businesses.
                    Elevate your advertising strategy with አድባር, our end-to-end AI-based platform designed to optimize ad placements across diverse Telegram channels.
                    Explore the future of marketing with AIQEM's Amharic RAG pipeline, revolutionizing the creation of engaging Amharic text Ad content for unparalleled campaign success.
                    Join us on the forefront of technological innovation as we reshape the landscape of AI and Blockchain solutions for Ethiopian and African businesses. 🚀    
                    </p>
                    <br />
                    <Link
                        to="/create"
                        className="font-inter font-bold bg-blue-800 text-white px-2 py-1 rounded-md w-[60px]"
                    >
                        Chat
                    </Link>
                </div>
            <div className="mt-16]">
                <img src={telegram} style={{ width: 500, height: 400 }} alt="img" className=""/>
            </div>
            </div>
        </section>
    );
};

export default Home;
import unittest
import sys, os
sys.path.append(os.path.abspath(os.path.join('..')))

from scripts.util import find_average, count_occurence


class TestCases(unittest.TestCase):
    def test_find_average(self):
        """
        Test that it retunrs the average of a given list
        """
        data = [1, 2, 3]
        result = find_average(data)
        self.assertEqual(result, 2.0)

    def test_input_value(self):
        """
        Provide an assertion level for arg input
        """
        
        self.assertRaises(TypeError, find_average, True)

class TestCountOccurence(unittest.TestCase):
    def test_count_occurence(self):
        """
        Test that it returns the count of each unique values in the given list
        """
        data = [0,0,9,0,8,9,0,7]
        result = count_occurence(data)
        output = {0: 4, 9: 2, 8: 1, 7: 1}
        self.assertAlmostEqual(result, output)

    def test_input_value(self):
        """
        Provide an assertion level for arg input
        """
        self.assertRaises(TypeError, count_occurence, True)

if __name__ == '__main__':
    unittest.main()
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub('', text)

    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
parsed_dir = "../data/raw/"

cleaned_dir = "../data/parsed/"

file_name = "TIKVAH-data"

util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")

df.head()

df = pd.DataFrame(df['messages'])

df_messages = pd.DataFrame(df['messages'].tolist())

df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))

df_messages = df_messages[['id', 'text', 'date']]

df_messages
df_filtered = df_messages[(df_messages['id'] >= 60000) & (df_messages['id'] <= 75000)]

df_filtered.head()
data_null_removed = df_filtered.dropna()

data_null_removed.head()
tigvah_data = data_null_removed.replace('\n', ' ', regex=True)

tigvah_data.head()

tigvah_data['hashtags'] = tigvah_data['text'].apply(lambda x: util.extract_hashtags(x))

tigvah_data.head()

tigvah_data['text'] = tigvah_data['text'].str.replace(r'\
tigvah_data.head()

tigvah_data['emojis'] = tigvah_data['text'].apply(util.extract_emojis)

tigvah_data.tail()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_emojis)

tigvah_data.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    tigvah_data['text'] = tigvah_data['text'].str.replace(letter[0][i], letter[1][i])
tigvah_data.head()
tigvah_data['symbols'] = tigvah_data['text'].apply(util.extract_symbols)

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_symbols)

tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)

tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)

tigvah_data.head()
tigvah_data['mentions'] = tigvah_data['text'].apply(util.extract_mentions)

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace('\s+', ' ', regex=True).str.strip()

tigvah_data['text'] = tigvah_data['text'].replace(r'!+', '!', regex=True)

tigvah_data['text'] = tigvah_data['text'].replace(r'\.+', '', regex=True)
tigvah_data.head()
tigvah_data = tigvah_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
tigvah_data
tigvah_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
parsed_dir = "../data/raw/"

cleaned_dir = "../data/parsed/"

file_name = "YeneTube"

util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")

df.head()

df = pd.DataFrame(df['messages'])

df_messages = pd.DataFrame(df['messages'].tolist())

df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))

df_messages = df_messages[['id', 'text', 'date']]

df_messages
data_null_removed = df_messages.dropna()

data_null_removed.head()
yenetube_data = data_null_removed.replace('\n', ' ', regex=True)

yenetube_data.head()

yenetube_data['hashtags'] = yenetube_data['text'].apply(lambda x: util.extract_hashtags(x))

yenetube_data.head()

yenetube_data['text'] = yenetube_data['text'].str.replace(r'\
yenetube_data.head()

yenetube_data['emojis'] = yenetube_data['text'].apply(util.extract_emojis)

yenetube_data.tail()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_emojis)

yenetube_data.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    yenetube_data['text'] = yenetube_data['text'].str.replace(letter[0][i], letter[1][i])
yenetube_data.head()
yenetube_data['symbols'] = yenetube_data['text'].apply(util.extract_symbols)

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_symbols)

yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)

yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)

yenetube_data.head()
yenetube_data['mentions'] = yenetube_data['text'].apply(util.extract_mentions)

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace('\s+', ' ', regex=True).str.strip()

yenetube_data['text'] = yenetube_data['text'].replace(r'!+', '!', regex=True)

yenetube_data['text'] = yenetube_data['text'].replace(r'\.+', '', regex=True)
yenetube_data.head()
yenetube_data = yenetube_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
yenetube_data
yenetube_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_distances(sentences):
    distances = []
    for i in range(len(sentences) - 1):
        embedding_current = sentences[i]['combined_sentence_embedding']
        embedding_next = sentences[i + 1]['combined_sentence_embedding']
        
                similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
        
                distance = 1 - similarity

                distances.append(distance)

                sentences[i]['distance_to_next'] = distance

        
    return distances, sentences
import re
import numpy as np
from combine_sentences import combine_sentences
import calculate_cosine_distance
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from calculate_cosine_distance import calculate_cosine_distances
from dotenv import load_dotenv
import os
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
def semantic_retriever(essay):
    

    try:
        single_sentences_list = re.split(r'(?<=[.?!])\s+', essay)
        sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]
        
        try:
            sentences = combine_sentences(sentences)
        except Exception as e:
            print(f"Error in combine_sentences: {e}")
            return []

        try:
            oaiembeds = OpenAIEmbeddings()
            embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])
        except Exception as e:
            print(f"Error in OpenAI embeddings: {e}")
            return []

        for i, sentence in enumerate(sentences):
            sentence['combined_sentence_embedding'] = embeddings[i]

        try:
            distances, sentences = calculate_cosine_distances(sentences)
        except Exception as e:
            print(f"Error in calculate_cosine_distance: {e}")
            return []

        breakpoint_percentile_threshold = 95
        breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)

        indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]

        start_index = 0
        chunks = []

        for index in indices_above_thresh:
            end_index = index
            group = sentences[start_index:end_index + 1]
            combined_text = ' '.join([d['sentence'] for d in group])
            chunks.append(combined_text)
            start_index = index + 1

        if start_index < len(sentences):
            combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])
            chunks.append(combined_text)

        return chunks

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return []
from extract_json import extract_json

a= extract_json('ብስራት ስፖርት.json')
from semantic_chuncking import semantic_retriever

chunks = semantic_retriever(a)
from operator import itemgetter



from langchain_community.vectorstores import FAISS

from langchain_core.output_parsers import StrOutputParser

from langchain_core.prompts import ChatPromptTemplate

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(

    chunks, embedding=OpenAIEmbeddings()

)

retriever = vectorstore.as_retriever(search_kwargs={"k" : 1}) relevant_docs = retriever.get_relevant_documents("ቤቲንግ")

relevant_docs
for doc in relevant_docs:

  print(doc.page_content)

  print('\n')
template = """<human>: Create a compelling Amharic or Amharic mixed with English advertisement for {question}.

Highlight the key features,unique selling points, and the exceptional services offered by the brand. 

Craft a persuasive narrative that resonates with the target audience, emphasizing the brand's values 

and commitment to customer satisfaction. Use vibrant and engaging language to evoke a positive emotional

response and encourage potential customers to explore and choose {question}. 

Ensure the ad reflects the brand's identity and leaves a lasting impression on the audience.



Make it a highly personalized promotional text that resonates with {context}, drawing insights from their 

contextual data. Leverage historical interactions, discussions, or online activities to tailor the ad 

closely to {context} unique preferences and interests. Highlight features and services that align with 

their needs based on the available contextual information. Tailor the language to address specific topics,

preferences, or issues derived from {context} historical interactions. Ensure the personalized content feels

natural and aligns with [User's] communication style, fostering a deep connection and encouraging meaningful

engagement. The goal is to create an ad that demonstrates an understanding of {context} context, providing

value and relevance in a way that feels organic.


{context}




Question: {question}



\n



<bot>:

"""



prompt = ChatPromptTemplate.from_template(template)



model = Mistral_7B
chain = (

    {"context": retriever, "question": RunnablePassthrough()}

    | prompt

    | model

    | StrOutputParser()

)
import json
import re

def extract_json(json_file):
    last_25_percent_text = ""

        with open(json_file, 'r') as file:
        data = json.load(file)
        messages = data.get('messages', [])

        last_25_percent_length = int(len(messages) * 0.99)
        for message in messages[-last_25_percent_length:]:
                        text_content = message.get('text', [])

                        if isinstance(text_content, list):
                for item in text_content:
                    if isinstance(item, str):
                        last_25_percent_text += item

        cleaned_text = re.sub(r'[!@
    return last_25_percent_text
import json




text_values = []




with open('manchester.json', 'r') as file:

    data = json.load(file)

    messages = data.get('messages', [])



    for message in messages:

        text_content = message.get('text', None)



        
        if text_content and isinstance(text_content, list):

            
            first_part_of_text = text_content[0] if text_content and isinstance(text_content[0], str) else None

            if first_part_of_text:

                text_values.append(first_part_of_text)




concatenated_text = '\n'.join(text_values)




print(len(concatenated_text))
def combine_sentences(sentences, buffer_size=1):

    
    for i in range(len(sentences)):



        
        combined_sentence = ''



        
        for j in range(i - buffer_size, i):

            
            if j >= 0:

                
                combined_sentence += sentences[j]['sentence'] + ' '



        
        combined_sentence += sentences[i]['sentence']



        
        for j in range(i + 1, i + 1 + buffer_size):

            
            if j < len(sentences):

                
                combined_sentence += ' ' + sentences[j]['sentence']



        
        
        sentences[i]['combined_sentence'] = combined_sentence



    return sentences





def pretty_print_docs(docs):

    print(

        f"\n{'-' * 100}\n".join(

            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]

        )

    )
from langchain.text_splitter import CharacterTextSplitter

from langchain_community.document_loaders import TextLoader

from langchain_community.vectorstores import FAISS

from langchain_openai import OpenAIEmbeddings



documents = TextLoader(concatenated_text).load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

texts = text_splitter.split_documents(documents)

retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()



docs = retriever.get_relevant_documents(

    "What did the president say about Ketanji Brown Jackson"

)

pretty_print_docs(docs)
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=yene-tube.txt --model_prefix=m --vocab_size=2000')




sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))




print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))
print(sp.decode_ids([434, 111, 997, 1507]))

print(sp.get_piece_size())




print(sp.id_to_piece(460))

print(sp.piece_to_id('▁በአዲስ'))




print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))





for id in range(3):

  print(sp.id_to_piece(id), sp.is_control(id))
pip install tensorflow
import tensorflow as tf




serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()



sp = spm.SentencePieceProcessor()

sp.load_from_serialized_proto(serialized_model_proto)



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   


sp = spm.SentencePieceProcessor()

sp.load('m_bos_as_user.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')



print('bos=', sp.bos_id())

print('eos=', sp.eos_id())

print('unk=', sp.unk_id())

print('pad=', sp.pad_id())  




print(sp.encode_as_ids('በአዲስ አበባ'))




print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
parsed_data_dir = "/home/habte/llm_finetuning_amharic/data/parsed/channel_1.csv"

cleaned_data_dir = "/home/habte/llm_finetuning_amharic/data/cleaned/channel_1_cleaned.csv"

file_name = "TIKVAH"

util = Util()
import pandas as pd

import os




file_name = "/home/habte/llm_finetuning_amharic/data/parsed/channel_1.csv"




file_path = os.path.join(parsed_data_dir, file_name)




df = pd.read_csv(file_path, index_col='id')




filtered_df = df[(df.index > 60000) & (df.index <= 750000)]




print(filtered_df.head())

filtered_df
filtered_df = filtered_df.dropna()

filtered_df.head()
filtered_df.shape
filtered_df = filtered_df.replace('\n', ' ', regex=True)

filtered_df.head()

filtered_df['hashtags'] = filtered_df['text'].apply(lambda x: util.extract_hashtags(x))

filtered_df.head()

filtered_df['text'] = filtered_df['text'].str.replace(r'\
filtered_df.head()

filtered_df['emojis'] = filtered_df['text'].apply(util.extract_emojis)

filtered_df.tail()
df_83826 = filtered_df.loc[83826]

df_83826
filtered_df['text'] = filtered_df['text'].apply(util.remove_emojis)
df_83826 = filtered_df.loc[83826]

df_83826





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    filtered_df['text'] = filtered_df['text'].str.replace(letter[0][i], letter[1][i])

    

filtered_df['symbols'] = filtered_df['text'].apply(util.extract_symbols)

filtered_df.tail()
filtered_df['text'] = filtered_df['text'].apply(util.remove_symbols)

filtered_df.tail()
filtered_df['links'] = filtered_df['text'].apply(util.extract_urls)

filtered_df.tail()
filtered_df['text'] = filtered_df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

filtered_df.tail()
filtered_df['mentions'] = filtered_df['text'].apply(util.extract_mentions)

filtered_df.tail()
filtered_df['text'] = filtered_df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

filtered_df.tail()
filtered_df['text'] = filtered_df['text'].str.replace('\s+', ' ', regex=True).str.strip()
filtered_df['text'] = filtered_df['text'].replace(r'!+', '!', regex=True)

filtered_df['text'] = filtered_df['text'].replace(r'\.+', '', regex=True)
filtered_df.tail()
filtered_df.to_csv(f"{cleaned_data_dir}")
filtered_df['text'].to_csv(f"{cleaned_data_dir}", index=False, header=False)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('/home/habte/llm_finetuning_amharic/scripts/')))

from scripts.util import Util

import sys

import os

import pandas as pd




script_dir = os.path.abspath("/home/habte/llm_finetuning_amharic/scripts/")

sys.path.append(script_dir)




from util import Util




util_instance = Util()

raw_data_dir = "/home/habte/llm_finetuning_amharic/data/raw/TIKVAH.json"

parsed_data_dir = "/home/habte/llm_finetuning_amharic/data/parsed/channel_1.csv"


file_name = "TIKVAH"

util = Util()
data = util.read_file(f'{raw_data_dir}')
import os



def read_file(self, file_path: str) -> dict:

    directory = os.path.dirname(file_path)

    if not os.path.exists(directory):

        print(f"Directory '{directory}' does not exist.")

        return None 



    
    with open(file_path, 'r') as file:

        
        data = json.load(file)

        return data

parsed_message = util.parse_messages(data['messages'])

import pandas as pd





df = pd.DataFrame(parsed_message)




df.set_index('id', inplace=True)




filtered_df = df[(df.index > 60000) & (df.index < 750001)]




print(filtered_df.head())

filtered_df
filtered_df.to_csv(f'{parsed_data_dir}')
filtered_df.info()
import json
import os
import re
import pandas as pd
import sys, os
sys.path.append(os.path.abspath(os.path.join('/home/habte/llm_finetuning_amharic/scripts')))
from scripts.util import Util

raw_data_dir = "/home/habte/llm_finetuning_amharic/data/raw/TIKVAH.json"
parsed_data_dir = "/home/habte/llm_finetuning_amharic/data/parsed/channel_1.csv"
cleaned_data_dir = "/home/habte/llm_finetuning_amharic/data/cleaned/channel_1_cleaned.csv"

os.makedirs(parsed_data_dir, exist_ok=True)
os.makedirs(cleaned_data_dir, exist_ok=True)

def parse_raw_data(raw_file_path):
    with open(raw_file_path, "r", encoding="utf-8") as file:
        raw_data = json.load(file)
    
    parsed_data = []
    for item in raw_data:
        parsed_item = {
            "id": item.get("channel_id", ""),
            "date": item.get("date", ""),
            "message": item.get("message", "")
        }
        parsed_data.append(parsed_item)
    
    return parsed_data

def clean_text(text):
        cleaned_text = re.sub(r"\s+", " ", text.replace("\n", "").strip())
    
        cleaned_text = re.sub(r"    
        cleaned_text = cleaned_text.translate(str.maketrans(
        ''.join(['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ']),
        ''.join(['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ'])
    ))
    cleaned_text = cleaned_text.translate(str.maketrans(
        ''.join(['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ']),
        ''.join(['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ'])
    ))
    cleaned_text = cleaned_text.translate(str.maketrans(
        ''.join(['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ']),
        ''.join(['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ'])
    ))
    cleaned_text = cleaned_text.translate(str.maketrans(
        ''.join(['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ']),
        ''.join(['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ'])
    ))
    cleaned_text = cleaned_text.translate(str.maketrans(
        ''.join(['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ']),
        ''.join(['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ'])
    ))

    return cleaned_text

def process_and_save_data(parsed_data, output_file_path):
    cleaned_data = []
    for item in parsed_data:
        cleaned_item = {
            "id": item["id"],
            "date": item["date"],
            "message": clean_text(item["message"])
        }
        cleaned_data.append(cleaned_item)

    with open(output_file_path, "w", encoding="utf-8") as file:
        json.dump(cleaned_data, file, ensure_ascii=False, indent=2)

for raw_file_name in os.listdir(raw_data_dir):
    raw_file_path = os.path.join(raw_data_dir, raw_file_name)
    
        parsed_data = parse_raw_data(raw_file_path)
    
        parsed_output_file_path = os.path.join(parsed_data_dir, f"parsed_{raw_file_name}")
    with open(parsed_output_file_path, "w", encoding="utf-8") as parsed_file:
        json.dump(parsed_data, parsed_file, ensure_ascii=False, indent=2)

        cleaned_output_file_path = os.path.join(cleaned_data_dir, f"cleaned_{raw_file_name}")
    process_and_save_data(parsed_data, cleaned_output_file_path)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{parsed_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape
df = df.dropna()

df.head()
df.shape
df = df.replace('\n', ' ', regex=True)

df.head()

df['hashtags'] = df['text'].apply(lambda x: preprocess.extract_hashtags(x))

df.head()

df['text'] = df['text'].str.replace(r'\
df.head()

df['emojis'] = df['text'].apply(preprocess.extract_emojis)

df.tail()

df['text'] = df['text'].apply(preprocess.remove_emojis)






letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

    

df['symbols'] = df['text'].apply(preprocess.extract_symbols)

df.tail()
df['text'] = df['text'].apply(preprocess.remove_symbols)

df.tail()
df['links'] = df['text'].apply(preprocess.extract_urls)

df.tail()
df['text'] = df['text'].str.replace(preprocess.url_pattern, '', regex=True).str.strip()

df.tail()
df['mentions'] = df['text'].apply(preprocess.extract_mentions)

df.tail()
df['text'] = df['text'].str.replace(preprocess.mention_pattern, '', regex=True).str.strip()

df.tail()
df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()
df['text'] = df['text'].replace(r'!+', '!', regex=True)

df['text'] = df['text'].replace(r'\.+', '', regex=True)
df.tail()
df.to_csv(f"{cleaned_dir}/{file_name}.csv")
df['text'].to_csv(f"{cleaned_dir}/{file_name}.txt", index=False, header=False)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
data = preprocess.read_file(f'{raw_dir}/{file_name}.json')
parsed_message = preprocess.parse_messages(data['messages'])


df = pd.DataFrame(parsed_message)

df.set_index('id', inplace=True)

df.head()
df.shape
df.to_csv(f'{parsed_dir}/{file_name}.csv')
import sys, os

import pandas as pd

from gensim.models import Word2Vec

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
cleaned_dir = "../data/cleaned"

final_dir = "../data/final"

file_name = "TIKVAH"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{cleaned_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape

tokenized_corpus = [str(sentence).lower().split() for sentence in df['text']]

embedding_size = 100 
window_size = 5 
min_count = 5 
model = Word2Vec(sentences=tokenized_corpus, vector_size=embedding_size, window=window_size, min_count=min_count, workers=4)

vector = model.wv['ኢትዮጵያ'] 
vector

similar_words = model.wv.most_similar('ኢትዮጵያ', topn=5)

similar_words

model.save(f'{final_dir}/{file_name}_word2vec_model.bin')
import os 

from huggingface_hub import hf_hub_download
HUGGING_FACE_API_KEY = os.environ.get("HUGGING_FACE_API_KEY")
model_id = "iocuydi/llama-2-amharic-3784m"

filenames = [

    ".gitattributes", "adapter_config.json", "adapter_model.bin", "config.json", "generation_config.json", 

    "inference_demo.py", "special_tokens_map.json", "tokenizer.json", "tokenizer.model", "tokenizer_config.json"

]
for filename in filenames:

    downloaded_model_path = hf_hub_download(

        repo_id = model_id,

        filename = filename,

        token = HUGGING_FACE_API_KEY

    )

    print(downloaded_model_path)
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM



tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)

model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

pipeline = pipeline ("Text-Generation", model=model, device=-1, tokenizer=tokenizer, max_length=1000 )
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from PyPDF2 import PdfReader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,
)



persist_directory = "db"


class MySpecialFunctions:
    def __init__(self):
        pass
    
    def get_file_text(self, files):
        text = ""
        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    text += content
            except Exception as e:
                print(f"Error reading file {file}: {e}")

        return text
    
    
    def get_pdf_text(self, pdf):
        text = ""
        for doc in pdf:
            reader = PdfReader(doc)
            for page in reader.pages:
                text += page.extract_text()

        return text


    
    
    def get_text_chunks(self, text):
        text_siplitter = RecursiveCharacterTextSplitter(
            chunk_size = 1000, 
            chunk_overlap = 200,
            separators=['\n', '\n\n'],
            length_function = len)
        chunk = text_siplitter.split_text(text)
        return chunk
    

    def get_vectorstore(self, chunks):
                        hf_embedding = HuggingFaceEmbeddings()

                                
                embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        
                vector_db = Chroma.from_documents(
            documents = chunks,
            embedding = hf_embedding,
                    )

        return vector_db
import streamlit as st 
from MyFunctions import MySpecialFunctions
from dotenv import load_dotenv


special_functions_instance = MySpecialFunctions()

def get_text(external_data ):
    return special_functions_instance.get_file_text(external_data )

def get_pdf_text(external_data ):
    return special_functions_instance.get_pdf_text(external_data )

def get_chunks(text):
    return special_functions_instance.get_text_chunks(text)

def get_vectorstore(text_chunks):
    return special_functions_instance.get_vectorstore(text_chunks) 





def main():
    load_dotenv()
    st.set_page_config(page_title="Generation of Telegram Ads in Amharic", page_icon= ":smile")
    

    with st.sidebar:
                pass

    st.markdown("    external_data = st.file_uploader(" Upload the generative text from fune-tuning", accept_multiple_files= True)
    if st.button("Retrieval"):
        with st.spinner("Processing"):
                                    text = get_pdf_text(external_data )
            
                        text_chunks = get_chunks(text)
            st.write(text_chunks)


                        vectorstore_db = get_vectorstore(text_chunks)
                                    




if __name__ == "__main__":
    main()
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')




sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))




print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))
print(sp.decode_ids([460, 133, 774, 1276]))


print(sp.get_piece_size())




print(sp.id_to_piece(460))

print(sp.piece_to_id('▁በአዲስ'))




print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))





for id in range(3):

  print(sp.id_to_piece(id), sp.is_control(id))


import tensorflow as tf




serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()



sp = spm.SentencePieceProcessor()

sp.load_from_serialized_proto(serialized_model_proto)



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))










spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')



sp_ctrl = spm.SentencePieceProcessor()

sp_ctrl.load('m_ctrl.model')




print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))

print(sp_ctrl.piece_to_id('<sep>'))  
print(sp_ctrl.piece_to_id('<cls>'))  
print('3=', sp_ctrl.decode_ids([3]))  
print('4=', sp_ctrl.decode_ids([4]))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   


sp = spm.SentencePieceProcessor()

sp.load('m_bos_as_user.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   

spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')



print('bos=', sp.bos_id())

print('eos=', sp.eos_id())

print('unk=', sp.unk_id())

print('pad=', sp.pad_id())  




print(sp.encode_as_ids('በአዲስ አበባ'))




print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])


spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')





for n in range(10):

  print(sp.sample_encode_as_pieces('በአዲስ አበባ', -1, 0.1))



for n in range(10):

  print(sp.sample_encode_as_ids('በአዲስ አበባ', -1, 0.1))

print(sp.nbest_encode_as_pieces('በአዲስ አበባ', 10))

print(sp.nbest_encode_as_ids('በአዲስ አበባ', 10))


spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')

sp_bpe = spm.SentencePieceProcessor()

sp_bpe.load('m_bpe.model')



print('*** BPE ***')

print(sp_bpe.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_bpe.nbest_encode_as_pieces('በአዲስ አበባ', 5))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')

sp_unigram = spm.SentencePieceProcessor()

sp_unigram.load('m_unigram.model')



print('*** Unigram ***')

print(sp_unigram.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_unigram.nbest_encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ', 5))
import json
import re


class Util:
    def __init__(self) -> None:
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"              "\U0001F300-\U0001F5FF"              "\U0001F680-\U0001F6FF"              "\U0001F700-\U0001F77F"              "\U0001F780-\U0001F7FF"              "\U0001F800-\U0001F8FF"              "\U0001F900-\U0001F9FF"              "\U0001FA00-\U0001FA6F"              "\U0001FA70-\U0001FAFF"              "\u2600-\u26FF"              "\u2700-\u27BF"              "\u2B50"              "\U0001F1E6-\U0001F1FF"              "]+",
            flags=re.UNICODE,
        )
        self.symbols = re.compile(
            "[" '"' "\“" '"' "'" "\-" "\*" "\•" "\ℹ" "\﻿" "\_" "]+"
        )
        self.url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        self.mention_pattern = r"@(\w+)"

    def read_file(self, file_path: str) -> dict:
                with open(file_path, "r") as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, "w") as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item["text"])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {"id": [], "text": [], "date": []}
        for message in messages:
            if message["type"] != "message" or len(message["text"]) == 0:
                continue
            parsed_messages["id"].append(message["id"])
            message_content = self.parse_text(message["text"])
            parsed_messages["text"].append(message_content)
            parsed_messages["date"].append(message["date"])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith("
    def extract_emojis(self, text):
        return "".join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub("", text)

    def extract_symbols(self, text):
        return "".join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(" ", text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def remove_links(self, text):
        return re.sub(self.url_pattern, " ", text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils import utils
letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

util = utils.Util()
def clean_parsed_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return

    

    
    if not os.path.exists(cleaned_dir):

            os.makedirs(cleaned_dir)



    
    for file_name in os.listdir(folder_path):

        base_name, extension = os.path.splitext(file_name)

        print(base_name,extension)

        if extension =='.csv':

            df = pd.read_csv(f"{folder_path}/{file_name}", index_col='id')

            df = df.dropna()

            df = df.replace('\n', ' ', regex=True)

            
            df['hashtags'] = df['text'].apply(lambda x: util.extract_hashtags(x))

            df['text'] = df['text'].str.replace(r'\
            

            
            df['emojis'] = df['text'].apply(util.extract_emojis)

            


            for letter in letters:

                for i in range(len(letter[0])):

                    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

            
            df['symbols'] = df['text'].apply(util.extract_symbols)

            df['text'] = df['text'].apply(util.remove_symbols)

            
            df['links'] = df['text'].apply(util.extract_urls)

            df['text'] = df['text'].apply(util.remove_links)



            df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

            df['text'] = df['text'].replace(r'!+', '!', regex=True)

            df['text'] = df['text'].replace(r'\.+', '', regex=True)

            base_name, extension = os.path.splitext(file_name)

            df.to_csv(f"{cleaned_dir}/{base_name}.csv")

            df['text'].to_csv(f"{cleaned_dir}/{base_name}.txt", index=False, header=False)





        
clean_parsed_data(parsed_dir)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils.utils import Util
util = Util()
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"
def process_raw_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return



    
    for filename in os.listdir(folder_path):

        print(filename)

        data = util.read_file(f'{folder_path}/{filename}')

        parsed_message = util.parse_messages(data['messages'])



        
        df = pd.DataFrame(parsed_message)

        df.set_index('id', inplace=True)

        base_name, extension = os.path.splitext(filename)

        
        if not os.path.exists(parsed_dir):

            os.makedirs(parsed_dir)

        df.to_csv(f'{parsed_dir}/{base_name}.csv')

        
process_raw_data(raw_dir)
!pip install transformers


from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM



model_name = "Samuael/llama-2-7b-tebot-amharic"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(model_name)


input_text = "እኔ አማርኛ መናገር እረዳለሁ"




input_ids = tokenizer.encode(input_text, return_tensors="pt")




output_ids = model.generate(input_ids)




output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Amharic text:", output_text)
import argparse
from dataclasses import dataclass
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
import os
from langchain_openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

CHROMA_PATH = './chromadb/'

client = OpenAI(
    api_key=OPENAI_API_KEY
)

core_embeddings_model = None
def get_context():
        vectorstore = Chroma(persist_directory="./cachce",embedding_function=core_embeddings_model)
    
    retriever = vectorstore.as_retriever()
        return retriever

def generate_add(user_input, context):
    template = f'''
    Generate an advertisement given the following context.    
    You must use the following context:
    {context}
    '''   
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": template},{"role": "user", "content": user_input}],
        n=3,
    )

    return response
from peft import PeftModel
from transformers import LlamaForCausalLM, LlamaConfig

def load_model(model_name, quantization):
    model = LlamaForCausalLM.from_pretrained(
        model_name,
        return_dict=True,
        load_in_8bit=quantization,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    return model


def load_peft_model(model, peft_model):
    peft_model = PeftModel.from_pretrained(model, peft_model)
    return peft_model

def load_llama_from_config(config_path):
    model_config = LlamaConfig.from_pretrained(config_path) 
    model = LlamaForCausalLM(config=model_config)
    return model
import fire
import torch
import os
import sys
import time
import json 
from typing import List

from transformers import LlamaTokenizer, LlamaForCausalLM
from model_utils import load_model, load_peft_model

BASE_PROMPT = """Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers.
Human: {}
Assistant [Amharic] : """

def main(
    model_name: str="",
    peft_model: str=None,
    quantization: bool=False,
    max_new_tokens =400,     prompt_file: str=None,
    seed: int=42,     do_sample: bool=True,     min_length: int=None,     use_cache: bool=True,      top_p: float=1.0,     temperature: float=1.0,     top_k: int=1,     repetition_penalty: float=1.0,     length_penalty: int=1,     enable_azure_content_safety: bool=False,     enable_sensitive_topics: bool=False,     enable_saleforce_content_safety: bool=False,     **kwargs
):    
    
    print("***Note: model is not set up for chat use case, history is reset after each response.")
    print("***Ensure that you have replaced the default LLAMA2 tokenizer with the Amharic tokenizer")
    
        torch.cuda.manual_seed(seed)
    torch.manual_seed(seed)
    
    MAIN_PATH = '/model/Llama-2-7b-hf'
        peft_model = '/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'
    model_name = MAIN_PATH
    quantization = True
    model = load_model(model_name, quantization)

    tokenizer = LlamaTokenizer.from_pretrained(model_name)
    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))

    if peft_model:
        model = load_peft_model(model, peft_model)

    model.eval()

    while True:

        
        user_query = input('Type question in Amharic or English: ')
        user_prompt = BASE_PROMPT.format(user_query)
        batch = tokenizer(user_prompt, return_tensors="pt")
        batch = {k: v.to("cuda") for k, v in batch.items()}
        start = time.perf_counter()
        with torch.no_grad():
            outputs = model.generate(
                **batch,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                top_p=top_p,
                temperature=temperature,
                min_length=min_length,
                use_cache=use_cache,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                length_penalty=length_penalty,
                **kwargs 
            )
        e2e_inference_time = (time.perf_counter()-start)*1000
        print(f"the inference time is {e2e_inference_time} ms")
        
        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("MODEL_OUTPUT: {}".format(output_text))
        
if __name__ == "__main__":
    fire.Fire(main)
import torch
from contextlib import nullcontext
from transformers import (
    LlamaForCausalLM, 
    LlamaTokenizer, 
    TrainerCallback, 
    default_data_collator, 
    Trainer, 
    TrainingArguments
)
from peft import (
    LoraConfig,
    TaskType,
    prepare_model_for_int8_training,
    PeftModel
)

from pathlib import Path
from utils.dataset_utils import get_preprocessed_dataset
from configs.datasets import amharic_dataset

def print_trainable_parameters(model):
    print("Trainable Parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f" - {name}")


def finetune():
    LLAMA_DIR = '/model/Llama-2-7b-hf'
    PT_DIR = '/model/llama-2-amharic-3784m'
    OUTPUT_DIR = "/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output"

    tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)

    model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)


    train_dataset = get_preprocessed_dataset(tokenizer, amharic_dataset, 'train')


    model.train()



    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))


    print('loading the pretrained model from config')

    model = prepare_model_for_int8_training(model)
    model = PeftModel.from_pretrained(model, PT_DIR)
    model.print_trainable_parameters()
    lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            modules_to_save = ["embed_tokens","lm_head"]
        )

    enable_profiler = False


    config = {
        'lora_config': lora_config,
        'learning_rate': 1e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 1,
        'per_device_train_batch_size': 2,
        'gradient_checkpointing': False,
    }

        if enable_profiler:
        wait, warmup, active, repeat = 1, 1, 2, 1
        total_steps = (wait + warmup + active) * (1 + repeat)
        schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
        profiler = torch.profiler.profile(
            schedule=schedule,
            on_trace_ready=torch.profiler.tensorboard_trace_handler(f"{OUTPUT_DIR}/logs/tensorboard"),
            record_shapes=True,
            profile_memory=True,
            with_stack=True)

        class ProfilerCallback(TrainerCallback):
            def __init__(self, profiler):
                self.profiler = profiler

            def on_step_end(self, *args, **kwargs):
                self.profiler.step()

        profiler_callback = ProfilerCallback(profiler)
    else:
        profiler = nullcontext()


        training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        bf16=True,                  logging_dir=f"{OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=10,
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=1,
        warmup_ratio=0.03,
        optim="adamw_torch_fused",
        max_steps=total_steps if enable_profiler else -1,
        **{k:v for k,v in config.items() if k != 'lora_config'}
    )

    with profiler:
                trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=default_data_collator,
            callbacks=[profiler_callback] if enable_profiler else [],
        )

        print_trainable_parameters(model)

                trainer.train()

    model.save_pretrained(OUTPUT_DIR)


finetune()
from dataclasses import dataclass

@dataclass
class amharic_dataset:
    dataset: str = "amharic_dataset"
    train_split: str = "train"
    test_split: str = "val"
    data_path: str = "/data/fine_tun_data6.json"
import copy
import json
import torch


from torch.utils.data import Dataset



class InstructionDataset(Dataset):
    def __init__(self, dataset_config, tokenizer, partition="train", max_words=50):
        self.ann = json.load(open(dataset_config.data_path))

        if partition == "train":
            self.ann = self.ann
        else:
            self.ann = self.ann[:200]

        self.max_words = max_words
    
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.ann)

    def __getitem__(self, index):

        ann = self.ann[index]
        data = self.create_prompt_formats(ann)
        prompt = data['prompt']
        example = data['text']  
        prompt = torch.tensor(
            self.tokenizer.encode(prompt), dtype=torch.int64
        )

        example = self.tokenizer.encode(example)

        example.append(self.tokenizer.eos_token_id)
        example = torch.tensor(
            example, dtype=torch.int64
        )
        padding = self.max_words - example.shape[0]

        if padding > 0:
            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - 1))
        elif padding < 0:
            example = example[: self.max_words]

        labels = copy.deepcopy(example)

        labels[: len(prompt)] = -1

        example_mask = example.ge(0)
        label_mask = labels.ge(0)
        example[~example_mask] = 0
        labels[~label_mask] = 0
        example_mask = example_mask.float()
        label_mask = label_mask.float()

        return {
            "input_ids": example,
            "labels": labels,
            "attention_mask":example_mask,
        }
    
    def create_prompt_formats(self,sample):
        """
        Format various fields of the sample ('text', 'label',)
        Then concatenate them using two newline characters
        :param sample: Sample dictionnary
        """

        INTRO_BLURB = "Generate an advertisement given a category"
        INSTRUCTION_KEY = "        RESPONSE_KEY = "Response:"
        END_KEY = "
        blurb = f"{INTRO_BLURB}"
        text = f"{INSTRUCTION_KEY}\n{sample['input']}"
        response = f"{RESPONSE_KEY}\n{sample['output']}"
        end = f"{END_KEY}"

        parts = [part for part in [blurb, text, response, end] if part]

        formatted_prompt = "\n\n".join(parts)

        sample["text"] = formatted_prompt
        parts = [part for part in [blurb, text,] if part]
        formatted_prompt = "\n\n".join(parts)

        sample["prompt"]= formatted_prompt

        return sample
"""
Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.

Here is the full list of checkpoints on the hub that can be fine-tuned by this script:
https://huggingface.co/models?filter=text-generation
"""

import logging
import numpy as np
import math
import os
import sys
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Dict, Any, Mapping
from pathlib import Path
import datasets
import torch
from datasets import load_dataset, concatenate_datasets

import transformers
from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    is_torch_tpu_available,
    set_seed,
)
from transformers.testing_utils import CaptureLogger
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version

from sklearn.metrics import accuracy_score
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora_model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")

        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)


def accuracy(predictions, references, normalize=True, sample_weight=None):
        return {
            "accuracy": float(
                accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
            )
        }


def compute_metrics(eval_preds):
    preds, labels = eval_preds
            labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)


def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
                        logits = logits[0]
    return logits.argmax(dim=-1)


def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}

                if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["labels"] = torch.tensor([f["label"] for f in features], dtype=dtype)
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)

        
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else:
                    batch[k] = torch.tensor([f[k] for f in features])
    except ValueError:         for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else:
                    batch[k] = torch.tensor([features[0][k]] * len(features))

    return batch


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The tokenizer for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides can't be used in combination with --config_name or --model_name_or_path"
            )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    dataset_dir: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional input sequence length after tokenization. "
                "The training dataset will be truncated in block of this size for training. "
                "Default to the model max input length for single sentence inputs (take into account special tokens)."
            )
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep line breaks when using TXT files or not."}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed stored"})

    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")


@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj,v_proj")
    lora_rank : Optional[int] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.1)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[bool] = field(default=False)
    peft_path : Optional[str] = field(default=None)


logger = logging.getLogger(__name__)


def main():

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, MyTrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

            send_example_telemetry("run_clm", model_args, data_args)

        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,          handlers=[logging.StreamHandler(sys.stdout)],)


    if training_args.should_log:
                transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
        logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )

        last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

        set_seed(training_args.seed)

    config_kwargs = {
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else:
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("You are instantiating a new config instance from scratch.")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")

    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script."
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )

                tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            output = tokenizer(examples["text"])
                if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return output
    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)

        def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
                        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
                result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    with training_args.main_process_first(desc="dataset map tokenization and grouping"):
        lm_datasets = []
        path = Path(data_args.dataset_dir)
        files = [file.name for file in path.glob("*.txt")]
        if training_args.debug_mode is True:
            files = [files[0]]
        print("printing files")
        print(files)    
        for idx, file in enumerate(files):
            data_file = os.path.join(path, file)
            filename = ''.join(file.split(".")[:-1])
            cache_path = os.path.join(data_args.data_cache_dir, filename)
            os.makedirs(cache_path, exist_ok=True)
            try:
                processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=False)
                logger.info(f'training datasets-{filename} has been loaded from disk')
            except Exception:
                cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                os.makedirs(cache_dir, exist_ok=True)
                raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                logger.info(f"{file} has been loaded")
                tokenized_dataset = raw_dataset.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns="text",
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'tokenized.arrow') for k in raw_dataset},
                    desc="Running tokenizer on dataset",
                )
                grouped_datasets = tokenized_dataset.map(
                    group_texts,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'grouped.arrow') for k in tokenized_dataset},
                    desc=f"Grouping texts in chunks of {block_size}",
                )
                processed_dataset = grouped_datasets
                processed_dataset.save_to_disk(cache_path)
            if idx == 0:
                lm_datasets = processed_dataset['train']
            else:
                assert lm_datasets.features.type == processed_dataset["train"].features.type
                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset["train"]])

        lm_datasets = lm_datasets.train_test_split(test_size = data_args.validation_split_percentage)

    if training_args.do_train:
        train_dataset = lm_datasets['train']
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))
        logger.info(f"Num train_samples  {len(train_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(train_dataset[0]['input_ids']))
    if training_args.do_eval:
        eval_dataset = lm_datasets["test"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        logger.info(f"Num eval_samples  {len(eval_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(eval_dataset[0]['input_ids']))



    if model_args.model_name_or_path:
        torch_dtype = (
            model_args.torch_dtype
            if model_args.torch_dtype in ["auto", None]
            else getattr(torch, model_args.torch_dtype)
        )
        model = LlamaForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool(".ckpt" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            use_auth_token=True if model_args.use_auth_token else None,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
        )
    else:
        model = AutoModelForCausalLM.from_config(config)
        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")

    model_vocab_size = model.get_output_embeddings().weight.size(0)
    if not (
       (model_vocab_size==32000 and len(tokenizer)==51008) or \
       (model_vocab_size==32000 and len(tokenizer)==32000) or \
       (model_vocab_size==51008 and len(tokenizer)==51008) or \
       (model_vocab_size==49954 and len(tokenizer)==49954)
       
    ):
        raise ValueError(
            f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
            "Valid configurations (base model / tokenizer):\n"
            "- Continue pre-training original LLaMA: 32000 / 32000 \n"
            "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
            "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
            "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n")

    model.resize_token_embeddings(len(tokenizer))
    if training_args.peft_path is not None:
        logger.info("Peft from pre-trained model")
        model = PeftModel.from_pretrained(model, training_args.peft_path)
    else:
        logger.info("Init new peft model")
        target_modules = training_args.trainable.split(',')
        modules_to_save = training_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')
        lora_rank = training_args.lora_rank
        lora_dropout = training_args.lora_dropout
        lora_alpha = training_args.lora_alpha
        logger.info(f"target_modules: {target_modules}")
        logger.info(f"lora_rank: {lora_rank}")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=lora_rank, lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            modules_to_save=modules_to_save)
        model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

                                

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
        modules_to_save = ["embed_tokens","lm_head"]
    )

    """
        config = {
        'lora_config': lora_config,
        'learning_rate': 2e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 2,
        'per_device_train_batch_size': 1,
        'per_device_eval_batch_size': 1,
        'gradient_checkpointing': True,
    }
    """


    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=fault_tolerance_data_collator,
        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
        if training_args.do_eval and not is_torch_tpu_available()
        else None,
    )

    """
    output_dir=training_args.output_dir,
    overwrite_output_dir=True,
    bf16=True,          logging_dir=f"{training_args.output_dir}/logs",
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="steps",
    save_steps=5000,
    optim="adamw_torch_fused",
    max_steps=-1,     **{k:v for k,v in config.items() if k != 'lora_config'}
    """

    trainer.add_callback(SavePeftModelCallback)
        if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)

        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
    model.save_pretrained(training_args.output_dir)
        if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()

        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
        try:
            perplexity = math.exp(metrics["eval_loss"])
        except OverflowError:
            perplexity = float("inf")
        metrics["perplexity"] = perplexity

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)


if __name__ == "__main__":
    main()
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=/model/Llama-2-7b-hf
amharic_tokenizer_path=/model/Llama-2-7b-hf
dataset_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/data/cleaned
data_cache=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/cache
per_device_train_batch_size=32
per_device_eval_batch_size=1
gradient_accumulation_steps=1
output_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output

python pretrain.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${amharic_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 1 \
    --save_steps 7528 \
    --evaluation_strategy steps \
    --eval_steps 3000 \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --bf16 \
    --overwrite_output_dir \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --gradient_checkpointing \
import pandas as pd

df = pd.read_json("/data/fine_tun_data2.json")

df.tail()
df[df['output']=='not advertisement'].iloc[1]['input']
'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: ለኢሬቻ በኣል ወደቢሾፍቱ የተጓዙት የሲዳማ ወጣቶች(ኤጄቶዎች) ከሰኣታት በፊት ቢሾፍቱ ገብተዋል። @tsegabwolde @tikvahethiopia''
import pandas as pd

df = pd.read_csv("/data/wasu_mohammed_labeled.csv")

df.head()
df.shape
df['label'] = df['label'].fillna("Not Advertisement")

df.tail(5)
df
from datasets import Dataset




data_dict = {"text": df['text'].tolist()}




dataset = Dataset.from_dict(data_dict)

dataset.save_to_disk("../data/train")




from peft import PeftModel

from transformers import LlamaForCausalLM, LlamaConfig




def load_model(model_name, quantization):

    model = LlamaForCausalLM.from_pretrained(

        model_name,

        return_dict=True,

        load_in_8bit=quantization,

        device_map="auto",

        low_cpu_mem_usage=True,

    )

    return model






def load_peft_model(model, peft_model):

    peft_model = PeftModel.from_pretrained(model, peft_model)

    return peft_model




def load_llama_from_config(config_path):

    model_config = LlamaConfig.from_pretrained(config_path) 

    model = LlamaForCausalLM(config=model_config)

    return model
from transformers import LlamaTokenizer





MAIN_PATH = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)



example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685





tokens = tokenizer.tokenize(example)

print(tokens)
print(len(tokenizer))
example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685




tokens = tokenizer.tokenize(example)

print(tokens)
df['text'][0]
df = pd.read_csv("/data/wasu_mohammed_labeled.csv")




total_word_count = 0

total_tokens = 0


for index, row in df.iterrows():

    
    text = row['text']

    if not isinstance(text, str): 

        continue



    

    
    word_count = len(text.split())



    
    total_word_count += word_count

    tokens = tokenizer.tokenize(text)

    total_tokens+=tokens

    print(tokens)

    






print("Total Word Count:", total_word_count)

print("Total tokens count: ",total_tokens)
total_tokens
df.shape
from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    HfArgumentParser,

    TrainingArguments,

    pipeline,

    LlamaForCausalLM, 

    LlamaTokenizer,

    logging,

)

from peft import (

    LoraConfig,

    TaskType,

    prepare_model_for_int8_training,

    PeftModel

)

import torch

LLAMA_DIR = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)



model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)

embedding_size = model.get_input_embeddings().weight.shape[0]



if len(tokenizer) != embedding_size:

    print("resize the embedding size by the size of the tokenizer")

    model.resize_token_embeddings(len(tokenizer))





new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'

model = PeftModel.from_pretrained(model, new_model)




prompt = "Who is Leonardo Da Vinci?"

pipe = pipeline(task="text-generation", model=model, tokenizer=model, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
ቨርቹዋል ረዳቶች እንደ Amazon&
run-20240203_155644-4hm9i4tp
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from transformers import AutoTokenizer, AutoModel
import os
import torch
import shutil
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

model_name = 'Davlan/bert-base-multilingual-cased-finetuned-amharic'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

data_path = '../data/'
CHROMA_PATH = '../RAG/chromadb'

def embed_doc(document):
    
        encoded_input = tokenizer(document, padding=True, truncation=True, return_tensors='pt')
    
        with torch.no_grad():
        outputs1 = model(**encoded_input)

    

        embeddings1 = outputs1.last_hidden_state.squeeze(dim=0)
    

        
    return embeddings1

def load_documents(data_path):    
    try:
        loader = DirectoryLoader(data_path)
        documents = loader.load()       
        print("data loaded sucessfully")
        return documents[0].page_content
    except:
        print("document not found!")
        return None
    

def split_text(documents:list[Document]):
    try:
        text_spliter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=500,
            length_function=len,
            add_start_index = True
        )
        chunk = text_spliter.split_documents(documents)
        print("data splited successfuly!")
        return chunk
    except:
        print("document not found")

def save_chunks_to_chroma(chunks):
        if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
    try:
        db = Chroma.from_documents(chunks,embed_doc(),\
                                persist_directory=CHROMA_PATH)
        db.persist()
        print("Vectorstore created successfully!")
    except:
        print("Couldn't create the vectore database")

def generate_data_store():
    documents = load_documents(data_path)
    chunks = split_text(documents)
    embeding1 = embed_doc(chunks)
    print(embeding1)
    save_chunks_to_chroma(embeding1) 

def main():
    generate_data_store()      


if __name__ == "__main__":
    main()
! pip install transformers bitsandbytes peft trl accelerate
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
base_model = "NousResearch/Llama-2-7b-chat-hf"

guanaco_dataset = "mlabonne/guanaco-llama2-1k"

new_model = "LLama-2-7b-chat-prac"
print(peft.__version__)
dataset = load_dataset(guanaco_dataset, split="train")
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=compute_dtype,

    bnb_4bit_use_double_quant=False,

)
model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=quant_config,

    device_map={"": 0}

)



model.config.use_cache = False

model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_size= "right"
peft_params = LoraConfig(

    lora_alpha = 16,

    lora_dropout = 0.1,

    r=64,

    bias="none",

    task_type= "CAUSAL_LM"

)
training_params = TrainingArguments(

    output_dir = "./results",

    num_train_epochs=1,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim="paged_adamw_32bit",

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-4,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=-1,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type="constant",

    report_to="tensorboard"

)
!pip install --upgrade peft
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
import pandas as pd

import numpy as np

import sys, os

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util

from concurrent.futures import ThreadPoolExecutor
util = Util()

cleaned_dir = "../cleaned"

json_file_path = '../raw/dilela_page.json'




df = pd.read_json(json_file_path)

df.head()
df.shape

columns = ["id", "channel_name", "type", "message_id", "message_type","text", "label", "created_at", "updated_at", ]

new_df = pd.DataFrame(columns=columns)

new_df

telegeram_channel_id  = df["id"][0]

telegram_channel_name = df["name"][0]

telegeram_channel_type = df["type"][0]

message_df = df["messages"]

data = [{

       'telegeram_channel_id': telegeram_channel_id,

       'telegram_channel_name': telegram_channel_name,

       'telegeram_channel_type': telegeram_channel_type,

       'message_id': message.get('id', np.nan),

        'message_type': message.get('type', np.nan),

        'text': message['text_entities'][0]['text'] if message.get('text_entities') and message['text_entities'] else np.nan,

        'created_at': message.get('date', np.nan),

        'update_at': message.get('edited', np.nan),

        }for message in message_df]

message_df = pd.DataFrame(data)

message_df = message_df.sort_values(by='message_id')

message_df.head(20)
message_df.shape

nan_rows_count = message_df.isna().any(axis=1).sum()

nan_rows_count

message_df = message_df.dropna()

message_df.head()
message_df.shape
message_df = message_df.replace('\n', ' ', regex=True)

message_df.head()

message_df["hashtags"] = message_df['text'].apply(lambda text: util.extract_hashtags(text))

message_df.head()

message_df["text"] = message_df["text"].str.replace(r'\
message_df.head()
message_df["emojis"] = message_df["text"].apply(util.extract_emojis)

message_df.head()

message_df['text'] = message_df['text'].apply(util.remove_emojis_using_emoji_pattern)

message_df.tail()
def remove_emojis_parallel(text):

    return util.remove_emojis(text)





with ThreadPoolExecutor() as executor:

    message_df['text'] = list(executor.map(remove_emojis_parallel, message_df['text']))
message_df.head()

message_df.replace('', pd.NA, inplace=True)

nan_rows_count = message_df.isna().any(axis=1).sum()


message_df = message_df.dropna()

message_df.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    message_df['text'] = message_df['text'].str.replace(letter[0][i], letter[1][i])
message_df['symbols'] = message_df['text'].apply(util.extract_symbols)

message_df.head()
message_df['text'] = message_df['text'].apply(util.remove_symbols)

message_df.tail()
message_df['links'] = message_df['text'].apply(util.extract_urls)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

message_df.head()
message_df['mentions'] = message_df['text'].apply(util.extract_mentions)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

message_df.tail()

message_df['text'] = message_df['text'].str.replace('\s+', ' ', regex=True).str.strip()
message_df['text'] = message_df['text'].replace(r'!+', '!', regex=True)

message_df['text'] = message_df['text'].replace(r'\.+', '', regex=True)
message_df.head()

nan_rows_count = message_df['text'].isna().sum()

nan_rows_count


message_df = message_df.dropna(subset='text')

message_df.tail()


message_df = message_df[message_df['text'].str.len() >= 20]
message_df.to_csv(f"{cleaned_dir}/dilela_page.csv")
message_df['text'].to_csv(f"{cleaned_dir}/dilela_page.txt", index=False, header=False)
df = pd.read_csv(f"{cleaned_dir}/dilela_page.csv")

df.head()
df['word_count'] = df['text'].str.split().str.len()

df.columns


df_labeled = df.drop(['Unnamed: 0','telegram_channel_name','telegeram_channel_type','message_id','message_type','hashtags', 'emojis', 'created_at','symbols', 'links','mentions'],axis=1)

df_labeled.rename(columns={'update_at':'date','telegeram_channel_id':'channel_id'},inplace=True)

df_labeled.to_csv(f"{cleaned_dir}/dilela_page_labeled.csv")

len = df_labeled['word_count'].sum()

len
from fastapi import FastAPI, HTTPException, Depends
from typing import Annotated, List
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from utils import simple_rag
from utils import hugging_face_hub

app = FastAPI()

origins = ["http://localhost:5173"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
)


class RagResponseBase(BaseModel):
    question: str
    answer: str


class HugResponseBase(BaseModel):
    question: str
    answer: str


class AmharicModelWithRAGBase(BaseModel):
    question: str
    answer: str


@app.get("/getanswer", response_model=RagResponseBase)
async def return_answer(question: str):
    result = simple_rag.test_RAG(question)
    return result


@app.get("/getHuggingFaceAnswer", response_model=HugResponseBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.invoke_current_hugging_face_model(model, prompt)
    return result


@app.get("/getAmharicModelWithRAGAnswer", response_model=AmharicModelWithRAGBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.use_amharic_model(model, prompt)
    return result
from langchain import OpenAI
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from models import simple_rag_response
import os
from dotenv import load_dotenv

load_dotenv()


def load_data():
    loader = TextLoader("/week_6_challenge_doc.txt")
    documents = loader.load()
    return documents


def return_chunks(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)
    texts = text_splitter.split_documents(documents)
    return texts


def return_chain(texts):
    embeddings = OpenAIEmbeddings()
    store = Chroma.from_documents(
        texts, embeddings, collection_name="challenge_document"
    )
    llm = OpenAI(temperature=0)
    return RetrievalQA.from_chain_type(llm, retriever=store.as_retriever())


def test_RAG(question):
    documents = load_data()
    chunks = return_chunks(documents)
    chain = return_chain(chunks)
    response = chain.run(question)
    return simple_rag_response.RagResponse(question, response)
import React, { useState , useRef} from 'react'
import 'bootstrap/dist/css/bootstrap.css'
import FileInput from  './components/FileInput'
import TextInputWithLable from  './components/TextInputWithLable'
import Dropdown from './components/Dropdown'
import NavBarComp from './components/Navbar'
import SpinnerWithText from './components/SpinnerWithText'
// import './App.css'
import api from './api/api'
function App() {
  const [answer,setAnswer] = useState([]);
  const [isShow,setShow] = useState(false);

  const fetchResponse = async () =>{
    console.log(ref.current.value);
    const question = ref.current.value;
    setShow(true)
    const response = await api.get('/getanswer?question='+question);
    console.log(response.data);
    setAnswer(response.data)
    setShow(false)
  }
  const ref = useRef(null);

  return (
    <React.Fragment>
      
      <NavBarComp />
      <main className='container'>
        <form className="row g-3" >
          
            <div>
              <label htmlFor="inputLable" className="form-label">Input Ad description to be generated</label>
              <textarea className="form-control" id="inputTextarea" rows="7" ref={ref}/>
            </div>

            {isShow && <SpinnerWithText />}

            <button type="button" className="btn btn-primary mb-4" onClick={fetchResponse}>Get Ad</button> 

            <div>
              <TextInputWithLable value= {answer}/>
            </div>

        </form>
      </main>

    </React.Fragment>
  )
}

export default App
import pandas as pd

import json
df = pd.read_json('sheger.json')

df.head()
df.info()
df.messages.iloc[0]
df.columns
message_df = pd.json_normalize(df.messages)
message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)

    return ''.join(extracted_text)




message_df['extracted_text'] = message_df['text'].apply(extract_text_from_data)
message_df = message_df[['id','type','date','extracted_text']]

message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)



    
    full_text = ''.join(extracted_text)



    
    substrings_to_remove = ['\n\n', '@sheger_press\n@sheger_press', '❗️❗️❗️']

    for substring in substrings_to_remove:

        full_text = full_text.replace(substring, '')



    
    full_text = ''.join(char for char in full_text if char.isalnum() or char.isspace())



    return full_text.strip()




message_df['cleaned_text'] = message_df['extracted_text'].apply(extract_text_from_data)

message_df.head()
df = df[['name','type','id']]

df.rename(columns={'name':'channel_name',

                  'type':'channel_type',

                  'id':'channel_id'}, inplace=True)
df.head()
message_df=message_df[['id','type','date','cleaned_text']]

message_df.rename(columns = {'id':'message_id',

                  'type':'message_type',

                  'date':'message_date',

                  'cleaned_text':'text'}, inplace = True)

message_df.head()
sheger_df= pd.concat([df,message_df], axis =1)
sheger_df.head()
press_df = press_df[press_df.text != '']

press_df.head()
sheger_df.to_csv('sheger_press.csv', index = None)
import pandas as pd

import json

import os

from pprint import pprint

import bitsandbytes as bnb

import torch

import torch.nn as nn

import transformers

from datasets import load_dataset, Dataset

from huggingface_hub import notebook_login



from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipelines, logging
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa' 
dataset = load_dataset(dataset_name, split="train")
MODEL_NAME = "mistralai/Mistral-7B-v0.1"

new_model = "amharic-mistral-7b"

config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, load_in_4bit=True)






bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_use_double_quant=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.bfloat16

)





model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    device_map="auto",

    trust_remote_code=True,

    quantization_config=bnb_config,

)



tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tokenizer.pad_token = tokenizer.eos_token
model = prepare_model_for_kbit_training(model)
use_4bit = True




bnb_4bit_compute_dtype = "float16"




bnb_4bit_quant_type = "nf4"




use_nested_quant = False

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

if compute_dtype == torch.float16 and use_4bit:

    major, _ = torch.cuda.get_device_capability()

    if major >= 8:

        print("=" * 80)

        print("Your GPU supports bfloat16: accelerate training with bf16=True")

        print("=" * 80)
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=2,

    lora_alpha=32,

    target_modules=get_last_layer_linears(model),

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM"

)



model = get_peft_model(model, config)





output_dir = "./results"




num_train_epochs = 1




fp16 = False

bf16 = False




per_device_train_batch_size = 4




per_device_eval_batch_size = 4




gradient_accumulation_steps = 1




max_grad_norm = 0.3




learning_rate = 2e-4




weight_decay = 0.001




optim = "paged_adamw_32bit"




lr_scheduler_type = "constant"




warmup_ratio = 0.03





group_by_length = True




save_steps = 25




logging_steps = 25


base_model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(base_model, new_model)

model = model.merge_and_unload()

training_arguments = TrainingArguments(

    output_dir=output_dir,

    num_train_epochs=num_train_epochs,

    per_device_train_batch_size=per_device_train_batch_size,

    gradient_accumulation_steps=gradient_accumulation_steps,

    optim=optim,

    save_steps=save_steps,

    logging_steps=logging_steps,

    learning_rate=learning_rate,

    weight_decay=weight_decay,

    fp16=fp16,

    bf16=bf16,

    max_grad_norm=max_grad_norm,

    max_steps=25,

    warmup_ratio=warmup_ratio,

    group_by_length=group_by_length,

    lr_scheduler_type=lr_scheduler_type,

    report_to="tensorboard"

)
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
class RagResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class HugResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class AmharicModelResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass
from dotenv import load_dotenv
from models import simple_rag_response

load_dotenv()

from langchain import HuggingFaceHub


def invoke_current_hugging_face_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.HugResponse(prompt, response)


def use_amharic_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.AmharicModelResponse(prompt, response)
import time

import sentencepiece as spm
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=m --vocab_size=100000')

sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))




print(sp.encode_as_pieces('ሃይ ሰላም ናችሁ?'))



spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=am-word --model_type=word  --vocab_size=100000')



sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.encode_as_ids('ፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.decode_ids([47914, 1024, 33, 7716, 29922, 26700]))
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U00002049 \U0000FE0F"                                         u"\U0000203C"                                         u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'
        print(self.emoji_pattern.pattern)

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub(' ', text)
        
    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import pandas as pd

import csv, os, sys

from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import
"""
Convert .txt to csv

"""

import csv
from sklearn.model_selection import train_test_split

def convert_txt_to_csv(input_txt, output_csv):
    with open(input_txt, 'r', encoding='utf-8') as infile, open(output_csv, 'w', encoding='utf-8', newline='') as outfile:
       
        reader = infile.readlines()
        data = [line.strip().split() for line in reader]
        csv_writer = csv.writer(outfile)
        csv_writer.writerows(data)

def split_data(input_csv, output_train_csv, output_test_csv, output_val_csv, test_size=0.2, val_size=0.1, random_seed=42):
    with open(input_csv, 'r', encoding='utf-8') as file:
        csv_reader = csv.reader(file)
        data = list(csv_reader)
        
    train_data, test_val_data = train_test_split(data, test_size=(test_size + val_size), random_state=random_seed)
    test_data, val_data = train_test_split(test_val_data, test_size=(val_size / (test_size + val_size)), random_state=random_seed)

    with open(output_train_csv, 'w', encoding='utf-8', newline='') as train_file:
        csv_writer = csv.writer(train_file)
        csv_writer.writerows(train_data)

    with open(output_test_csv, 'w', encoding='utf-8', newline='') as test_file:
        csv_writer = csv.writer(test_file)
        csv_writer.writerows(test_data)

    with open(output_val_csv, 'w', encoding='utf-8', newline='') as val_file:
        csv_writer = csv.writer(val_file)
        csv_writer.writerows(val_data)

if __name__ == "__main__":
    input_txt_file = '/home/biniyam_ajaw/finetuning/data/dataset.txt'
    output_csv_file = '/home/biniyam_ajaw/finetuning/data/output_data.csv'
    output_train_csv = '/home/biniyam_ajaw/finetuning/data/train_data.csv'
    output_test_csv = '/home/biniyam_ajaw/finetuning/data/test_data.csv'
    output_val_csv = '/home/biniyam_ajaw/finetuning/data/val_data.csv'

    convert_txt_to_csv(input_txt_file, output_csv_file)
    split_data(output_csv_file, output_train_csv, output_test_csv, output_val_csv)
    print("Conversion to CSV and data split completed.")
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=100000)

import tokenizers

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

folder = 'data'
files = [f"/home/biniyam_ajaw/finetuning/{folder}/{split}.csv" for split in ["test_data", "train_data", "valid_data"]]
tokenizer.train(files, trainer)

from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")

from transformers import PreTrainedTokenizerFast

custom_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
custom_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
custom_tokenizer.save_pretrained("amharic_tokenizer")

custom_tokenizer.push_to_hub("amharic_tokenizer")
max-width: 100%;
  width: 100%;
  height: 100%;
  margin: 5 auto;
  padding: 2rem;
  text-align: start;
  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function FileInput (){
    return(
        <div>
            <div className="input-group mb-3">
                <input type="file" className="form-control" id="inputGroupFile02"/>
                <label clclassNameass="input-group-text" for="inputGroupFile02">Upload</label>
            </div>
        </div>
      
    );
}


export default FileInput;
/* :root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;
  width: 100%;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}
a {
  font-weight: 500;
  color:   text-decoration: inherit;
}
a:hover {
  color: }

body {
  margin: 0;
  display: flex;
  place-items: center;
  min-width: 320px;
  min-height: 100vh;
}

h1 {
  font-size: 3.2em;
  line-height: 1.1;
}

button {
  border-radius: 8px;
  border: 1px solid transparent;
  padding: 0.6em 1.2em;
  font-size: 1em;
  font-weight: 500;
  font-family: inherit;
  background-color:   cursor: pointer;
  transition: border-color 0.25s;
}
button:hover {
  border-color: }
button:focus,
button:focus-visible {
  outline: 4px auto -webkit-focus-ring-color;
}

@media (prefers-color-scheme: light) {
  :root {
    color:     background-color:   }
  a:hover {
    color:   }
  button {
    background-color:   }
} */
import Container from 'react-bootstrap/Container';
import Nav from 'react-bootstrap/Nav';
import Navbar from 'react-bootstrap/Navbar';
import NavDropdown from 'react-bootstrap/NavDropdown';

function NavBarComp() {
  return (
    <Navbar expand="lg" className="bg-body-tertiary container-fluid">
      <Container >
        <Navbar.Brand href="        <Navbar.Toggle aria-controls="basic-navbar-nav" />
        <Navbar.Collapse id="basic-navbar-nav">
          <Nav className="me-auto">
            <NavDropdown title="Select Model" id="basic-nav-dropdown">
                <NavDropdown.Item href="                <NavDropdown.Item href="                <NavDropdown.Item href="            </NavDropdown>
          </Nav>
        </Navbar.Collapse>
      </Container>
    </Navbar>
  );
}

export default NavBarComp;
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function TextInputWithLabel (props) {
    return(
        <div>
            <div className="mb-3">
                <label htmlFor="exampleFormControlTextarea1" className="form-label">Generated Ad</label>
                <textarea className="form-control" id="exampleFormControlTextarea1" rows="7"  value={props.value.answer}/>
            </div>
        </div>
      
    );
}


export default TextInputWithLabel;
import torch

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging

from datasets import load_dataset

import os, sys

from huggingface_hub import notebook_login

import torch.nn as nn

import getpass

from trl import SFTTrainer

from peft import PeftConfig, LoraConfig
os.environ["HUGGING_FACE_HUB_TOKEN"] = getpass.getpass("Token:")

assert os.environ["HUGGING_FACE_HUB_TOKEN"]
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")

double_quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model_id = "microsoft/phi-2"

new_model = 'amharic-phi'

model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cuda:0', quantization_config=nf4_config)
tokenizer = AutoTokenizer.from_pretrained("dagim/amharic_tokenizer")



tokenizer.tokenize("ከአሜሪካ ወደ አዲስ አበባለመጓዝምንያህልጊዜይወስዳል??")
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa'

dataset = load_dataset(dataset_name, split="train")
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=4,

    lora_alpha=32,

    
    lora_dropout=0.03,

    bias='none',

    task_type="CAUSAL_LM"

)

training_arguments = TrainingArguments(

    output_dir="./results",

    num_train_epochs=2,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim='paged_adamw_32bit',

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-8,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=25,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type='constant',

    report_to="tensorboard",

    gradient_checkpointing=True

)
trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=config,

    dataset_text_field='inputs',

    max_seq_length=None,

    tokenizer=tokenizer,

    args=training_arguments,

    packing=False

)
trainer.train()
trainer.model.save_pretrained(new_model)
logging.set_verbosity(logging.CRITICAL)



prompt = "የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?"

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="/home/biniyam_ajaw/llama-2-amharic-3784m/tokenizer.json")
print(len(tokenizer.encode('የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?')))
import logging
import numpy as np
import math
import os, sys
import torch
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Tuple, Dict, Any, Mapping
from pathlib import Path
import datasets
from datasets import Dataset, DatasetDict, load_dataset, load_metric, concatenate_datasets

from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,  
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
    is_torch_gpu_available,
)

from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.testing_utils import CaptureLogger
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version_core

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora-Model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")
            
        
        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)
        
    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

def accuracy(predictions, references, normalize=True, sample_weight=None):
    return {
        "accuracy": float(
            accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
        )
    }

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)

def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
        logits = logits[0]
    return logits.argmax(dim=-1)

def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}
    
    if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["label"] = torch.tensor([f["label"] for f in features], dtype=dtype)
        
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
            
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)
            
    
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else: batch[k] = torch.tensor([f[k] for f in features])
                
    except ValueError:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                                        batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else: 
                    batch[k] = torch.tensor([features[0][k]] * len(features))
                    
    return batch

MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={"help": ("The tokenizer for weights initialization.")},
    )
    
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    
    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides cannot be used with --config_name or --model_name_or_path. To override some of "
            )
            
@dataclass
class DataTrainingArguments:
    '''
    Arguments pertaining to what data we are going to input our model for training and eval.
    '''
    
    dataset_dir: Optional[str] = field(
        default=None, metadata={"the name of the dataset to use"}
    )

    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name opf the dataset to use"}
    )
    
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training file"}
    )
    
    validation_file: Optional[str] = field(
        default=None, metadata={"help": "This is optional but recommended if you want to use early stopping"}
    )
    
    max_training_sample: Optional[int] = field(
        default=None,
        metadata={
            "help": "Debugging purposes"
        },
    )
    
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging"
        },
    )
    
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    
        block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional"
                "Training dataset will be truncated into a block of this size for training"
                "Default to the model max input sequence"
            )
        }
    )
    
            
    cache_dir: bool = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    
            validation_strategy: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "Percentage of the validation set used at the end of each epoch"
        }
        
    )
        preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes to use for preprocessing"}
    )
    
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep the linebreaks when using txt files or not"}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed store"})
    
    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets >= 2.0.0`")
            
            
@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj, v_proj")
    lora_rank : Optional[str] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.03)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[str] = field(default=False)
    peft_path : Optional[str] = field(default=None)
    
logger = logging.getLogger(__name__)

def main():
    
    parser = HfArgumentParser(ModelArguments, DataTrainingArguments, MyTrainingArguments)
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                model_args, data_args, training_args = parser.parse_parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_to_dataclasses()
        
    send-example_telemetry("run_clm", model_args, data_args)
    
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", datefmt="%m/%d/%Y %H:%M:%S",
                        level=logging.INFO,                         handlers=[logging.StreamHandler(sys.stdout)],)
    
    if training_args.should_log:
        transformers.utils.logging.set_verbosity_info()
        
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
    logger.warning(
        f"Process rank: {training_args.output_dir}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"Distributed training: {bool(training_args.local_rank != -1)}, 16-bits-training: {training_args.fp16}"
    )
    
        
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError (
                f"Outpur dir {training_args.output_dir} already exists and is not mt"
                "Use --overwrite_output_dir to overcome"
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is not None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this, change "
                "the --output-dir or --overwrite_output_dir to train from scratch"
            )
            
        set_seed(training_args.seed)
    
    config_kwargs = {
        "cache_dir": model.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model-args.use_auth_token else None
    }
    
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else: 
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("This is a new config from scratch")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")
            
            
    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None
    }
    
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "Instantiating a tokenizer from scratch"
        )
        
            
    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            return tokenizer(examples[text])
        
        if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^ PLease ignore the warning above ^^^^^^^"
            )
            
        return output

    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024  
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                "Override with `--block_size xxx`"
            
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)
        
        
    def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        
        if total_length >= block_size:
                        total_length = {total_length // block_size} *  block_size
                        result = {
                k: [t[i: i + block_size] for i in range(0, total_length, block_size)]
                for k, t in concatenated_examples.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result
        with training_args.main_process_first(desc="dataset map tokenizer"):
            lm_datasets = []
            path = Path(data_args.dataset_dir)
            filename = [file.name for file in path.glob("*.txt")]
            
            if training_args.debug_mode:
                files = [files[0]]
            for idx, file in enumerate(files):
                data_file = os.path.join(path, file)
                filename = ''.join(file.split('.')[:-1])
                cache_path = os.path.join(data_args.data_cache_dir, filename)
                os.makedirs(cache_path, exist_ok=True)
                try:
                    processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=True)
                    logger.info(f'Training datasets-{filename} has been loaded from disk')
                except Exception:
                    cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                    os.makedirs(cache_dir, exist_ok=True)
                    raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                    logger.info(f"{file} has been loaded")
                    tokenized_dataset = raw_dataset.map(
                        tokenize_function,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        remove_columns="text",
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "tokenized.arrow") for k in raw_dataset},
                        desc="Running tokenizer on the dataset",
                    )
                    
                    grouped_datasets = tokenized_dataset.map(
                        group_texts,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "grouped.arrow") for k in tokenized_dataset},
                        desc=f'Grouping texts in chunks of {block_size}',
            
                    )
                    
                    processed_dataset = grouped_datasets
                    processed_dataset.save_to_disk(cache_path)
                    
                if idx == 0:
                    lm_datasets = processed_dataset['train']
                else:
                    assert lm_datasets.features.type == processed_dataset['train'].features.type
                    lm_dataset = concatenate_datasets([lm_datasets, processed_dataset['train']])
                    
            lm_datasets = lm_datasets.train_test_split(test_size= data_args.validation_split_percentage())
            
        if training_args.do_train:
            train_dataset = lm_datasets["train"]
            
            
            if data_args.max_train_samples is not None:
                max_train_samples = min(len(train_dataset), data_args.max_train_samples)
                train_dataset = train_dataset.select(range(max_train_samples))
                logger.info(f"Num train samples {len(train_dataset)}")
                logger.info("Training example: ")
                logger.info(tokenizer.decode(train_dataset[0]["input_ids"]))
                
                
        if model_args.model_name_or_path:
            torch_dtype = (
                model_args.torch_dtype
                if model_args.torch_dtype in ["auto", None]
                else getattr(torch, model_args.torch_dtype)
            )
            
            model = LlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                from_tf=bool(".cpkt" in model_args.model_name_or_path),
                config=config,
                cache_dir=model_args.cache_dir,
                revision=model_args.model_revision,
                use_auth_token=True if model_args.use_auth_token else None,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
            )
            
        else:
            model = AutoModelForCausalLM.from_config(config)
            n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
            logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M parameters")
        
        model_vocab_size = model.get_output_embeddings().weight.size(0)
        
        if not (
            (model_vocab_size==32000 and len(tokenizer)==51008) or \
            (model_vocab_size==32000 and len(tokenizer)==32000) or \
            (model_vocab_size==51008 and len(tokenizer)==51008) or \
            (model_vocab_size==49954 and len(tokenizer)==49954)
        ):
            raise ValueError(
                f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
                "Valid configurations (base model / tokenizer):\n"
                "- Continue pre-training original LLaMA: 32000 / 32000 \n"
                "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
                "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
                "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n"
            )
            
                model.resize_token_embeddings(len(tokenizer))
        
        if training_args.peft_path is not None:
            logger.info("PEFT from pretrained model")
            model = PeftModel.from_pretrained(model, training_args.peft_path)
        else:
            logger.info("Init new peft model")
            target_modules = training_args.trainable.split(",")
            modules_to_save = training_args.modules_to_save
            if modules_to_save is not None:
                modules_to_save = modules_to_save.split(",")
            lora_rank = training_args.lora_rank
            lora_dropout = training_args.lora_dropout
            lora_alpha = training_args.lora_alpha
            logger.info(f"Target modules: {target_modules}")
            logger.info(f"LoRA Rank: {lora_rank}")
            peft_config = LoraConfig(
                task_type = TaskType.CAUSAL_LM,
                targert_modules = target_modules,
                inference_mode=False,
                r = lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                modules_to_save=modules_to_save,
            )    
            
            model= get_peft_model(model, peft_config)
        model.print_trainable_parameters()
!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb
import os

from dotenv import load_dotenv




load_dotenv()




hf_token = os.getenv("hf_token")



import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
import pandas as pd

file_path = '../../merged.csv'




df = pd.read_csv(file_path)

df
dataset=df[['Text']]

dataset
dataset_2=dataset.copy()










dataset_2
!pip install scikit-learn



from sklearn.model_selection import train_test_split





train_val_data, test_data = train_test_split(dataset_2, test_size=0.20, random_state=42)

train_data, evaluation_data = train_test_split(train_val_data, test_size=0.10, random_state=42)



print('Training dataset shape:', len(train_data))

print('evaluation dataset shape:', len(evaluation_data))

print('Testing dataset shape:', len(test_data))
evaluation_data
import numpy as np


msk = np.random.rand(len(dataset_2)) < 0.8

train_dataset = dataset_2[msk]

test_dataset = dataset_2[~msk]

from datasets import Dataset



test_dataset=Dataset.from_pandas(test_dataset)



train_dataset=Dataset.from_pandas(train_dataset)



evaluation_dataset=Dataset.from_pandas(evaluation_data)
test_dataset



test_dataset=test_dataset.remove_columns("__index_level_0__")

train_dataset=train_dataset.remove_columns("__index_level_0__")

evaluation_dataset=evaluation_dataset.remove_columns("__index_level_0__")


import datasets


main_dataset= datasets.DatasetDict({

    'train': train_dataset,

    'test': test_dataset,

    'evaluate': evaluation_dataset

})
main_dataset
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    AutoTokenizer,

    TrainingArguments,

    pipeline,

)

from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training

from trl import SFTTrainer

base_model = "NousResearch/Llama-2-7b-hf"

new_model = "llama-2-7b-Amh"






tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)

tokenizer.pad_token = tokenizer.unk_token

tokenizer.padding_side = "right"

bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.float16,

    bnb_4bit_use_double_quant=True,

)




peft_config = LoraConfig(

    r=16,

    lora_alpha=32,

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM",

    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']

)

def load_model(model_name, bnb_config):

    n_gpus = torch.cuda.device_count()

    max_memory = f'{23000}MB'



load_model(base_model,bnb_config)
import torch

torch.cuda.empty_cache()


model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=bnb_config,

    device_map={"": 0}

)




model = prepare_model_for_kbit_training(model)
training_dataset=main_dataset
model
import torch




device = torch.device("cuda:0")




torch.cuda.empty_cache()




print(torch.cuda.memory_summary(device))

import torch

torch.cuda.empty_cache()

import torch




n_gpus = torch.cuda.device_count()

print(f"Number of available GPUs: {n_gpus}")




for i in range(n_gpus):

    gpu_memory = torch.cuda.get_device_properties(i).total_memory

    print(f"GPU {i}: Total memory: {gpu_memory / (1024**3)} GB")

from transformers import Trainer, TrainingArguments, BitsAndBytesConfig


training_arguments = TrainingArguments(

        output_dir="../results",

        num_train_epochs=1,

        per_device_train_batch_size=10,

        per_device_eval_batch_size=1,

        gradient_accumulation_steps=1,

        gradient_checkpointing=True,

        fp16=True,

        evaluation_strategy="steps",

        eval_steps=1000,

        logging_steps=1,

        optim="paged_adamw_8bit",

        learning_rate=2e-4,

        lr_scheduler_type="linear",

        warmup_steps=10,

        
        max_steps=10, 
)




trainer = SFTTrainer(

    model=model,

    train_dataset=main_dataset["train"],

    eval_dataset=main_dataset["evaluate"],

    peft_config=peft_config,

    dataset_text_field="Text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_arguments,

)


model.config.use_cache = False  



trainer.train()




trainer.model.save_pretrained(new_model)


prompt = "የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "how can i treat flu, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "tell me about ethiopian politics, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "who the prime minister of ethiopia, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "3 Ethiopian premier league club, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

del model

del pipe

del trainer

import gc

gc.collect()

gc.collect()
import torch

torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(

    base_model,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(model, new_model)

model = model.merge_and_unload()




tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_side = "right"
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{parsed_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape
df = df.dropna()

df.head()
df.shape
df = df.replace('\n', ' ', regex=True)

df.head()

df['hashtags'] = df['text'].apply(lambda x: preprocess.extract_hashtags(x))

df.head()

df['text'] = df['text'].str.replace(r'\
df.head()

df['emojis'] = df['text'].apply(preprocess.extract_emojis)

df.tail()

df['text'] = df['text'].apply(preprocess.remove_emojis)






letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

    

df['symbols'] = df['text'].apply(preprocess.extract_symbols)

df.tail()
df['text'] = df['text'].apply(preprocess.remove_symbols)

df.tail()
df['links'] = df['text'].apply(preprocess.extract_urls)

df.tail()
df['text'] = df['text'].str.replace(preprocess.url_pattern, '', regex=True).str.strip()

df.tail()
df['mentions'] = df['text'].apply(preprocess.extract_mentions)

df.tail()
df['text'] = df['text'].str.replace(preprocess.mention_pattern, '', regex=True).str.strip()

df.tail()
df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()
df['text'] = df['text'].replace(r'!+', '!', regex=True)

df['text'] = df['text'].replace(r'\.+', '', regex=True)
df.tail()
df.to_csv(f"{cleaned_dir}/{file_name}.csv")
df['text'].to_csv(f"{cleaned_dir}/{file_name}.txt", index=False, header=False)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
data = preprocess.read_file(f'{raw_dir}/{file_name}.json')
parsed_message = preprocess.parse_messages(data['messages'])


df = pd.DataFrame(parsed_message)

df.set_index('id', inplace=True)

df.head()
df.shape
df.to_csv(f'{parsed_dir}/{file_name}.csv')
import sys, os

import pandas as pd

from gensim.models import Word2Vec

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
cleaned_dir = "../data/cleaned"

final_dir = "../data/final"

file_name = "TIKVAH"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{cleaned_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape

tokenized_corpus = [str(sentence).lower().split() for sentence in df['text']]

embedding_size = 100 
window_size = 5 
min_count = 5 
model = Word2Vec(sentences=tokenized_corpus, vector_size=embedding_size, window=window_size, min_count=min_count, workers=4)

vector = model.wv['ኢትዮጵያ'] 
vector

similar_words = model.wv.most_similar('ኢትዮጵያ', topn=5)

similar_words

model.save(f'{final_dir}/{file_name}_word2vec_model.bin')
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util
cleaned_dir = "../data/cleaned"

final_dir = "../data/final"

file_name = "TIKVAH-ETHIOPIA"

util = Util()
df = pd.read_csv(f"{cleaned_dir}/{file_name}.csv", index_col='id')

df.head()

from functools import reduce

import operator





def term_freq(x):

    
    word_lists = [str(text).split() for text in x.tolist()]

    

    
    words = reduce(operator.add, word_lists)

    

    return words

from collections import Counter



def counting(x, y):

  counter = Counter(x)

  most_occurrences = counter.most_common()

  count_df = pd.DataFrame(most_occurrences, columns = ['Word', 'Count'])

  return count_df

df_frequency = counting(term_freq(df['text']), 2)

df_frequency.head()
df_frequency.to_csv(f"{final_dir}/{file_name}_frequency.csv")
import re





def list_and_tokenize(data):

    return str(data).split()

import collections



def count_n_grams(lines, min_length=2, max_length=4):

    lengths = range(min_length, max_length + 1)

    n_grams = {length: collections.Counter() for length in lengths}

    queue = collections.deque(maxlen=max_length)



    
    def add_queue():

        current = tuple(queue)

        for length in lengths:

            if len(current) >= length:

                n_grams[length][current[:length]] += 1



    
    for line in lines:

        for word in list_and_tokenize(line):

            
            queue.append(word)

            if len(queue) >= max_length:

                add_queue()



    
    while len(queue) > min_length:

        queue.popleft()

        add_queue()



    return n_grams
bigram_to_df = pd.DataFrame({'2-grams': [], '2-grams freq': []})

trigram_to_df = pd.DataFrame({'3-grams': [], '3-grams freq': []})

quadgram_to_df = pd.DataFrame({'4-grams': [], '4-grams freq': []})



bigram = {'2-grams': [], '2-grams freq': []}

trigram = {'3-grams': [], '3-grams freq': []}

quadgram = {'4-grams': [], '4-grams freq': []}

    

def print_most_freq_ng(n_grams, num=30):

    global bigram_to_df, trigram_to_df, quadgram_to_df

    for n in sorted(n_grams):

        for gram, count in n_grams[n].most_common(num):

            if n == 2:

                bigram['2-grams'].append(gram)

                bigram['2-grams freq'].append(count)

            elif n == 3:

                trigram['3-grams'].append(gram)

                trigram['3-grams freq'].append(count)

            else:

                quadgram['4-grams'].append(gram)

                quadgram['4-grams freq'].append(count)

                

    bigram_to_df = pd.DataFrame({'2-grams': bigram['2-grams'], '2-grams freq': bigram['2-grams freq']})

    trigram_to_df = pd.DataFrame({'3-grams': trigram['3-grams'], '3-grams freq': trigram['3-grams freq']})

    quadgram_to_df = pd.DataFrame({'4-grams': quadgram['4-grams'], '4-grams freq': quadgram['4-grams freq']})
print_most_freq_ng(count_n_grams(df['text']))

n_gram_df = pd.concat([bigram_to_df, trigram_to_df, quadgram_to_df], axis=1)



n_gram_df
n_gram_df.to_csv(f"{final_dir}/{file_name}_n_gram.csv")
import os 

from huggingface_hub import hf_hub_download
HUGGING_FACE_API_KEY = os.environ.get("HUGGING_FACE_API_KEY")
model_id = "iocuydi/llama-2-amharic-3784m"

filenames = [

    ".gitattributes", "adapter_config.json", "adapter_model.bin", "config.json", "generation_config.json", 

    "inference_demo.py", "special_tokens_map.json", "tokenizer.json", "tokenizer.model", "tokenizer_config.json"

]
for filename in filenames:

    downloaded_model_path = hf_hub_download(

        repo_id = model_id,

        filename = filename,

        token = HUGGING_FACE_API_KEY

    )

    print(downloaded_model_path)
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM



tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)

model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

pipeline = pipeline ("Text-Generation", model=model, device=-1, tokenizer=tokenizer, max_length=1000 )
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils import utils
letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

util = utils.Util()
def clean_parsed_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return

    

    
    if not os.path.exists(cleaned_dir):

            os.makedirs(cleaned_dir)



    
    for file_name in os.listdir(folder_path):

        base_name, extension = os.path.splitext(file_name)

        print(base_name,extension)

        if extension =='.csv':

            df = pd.read_csv(f"{folder_path}/{file_name}", index_col='id')

            df = df.dropna()

            df = df.replace('\n', ' ', regex=True)

            
            df['hashtags'] = df['text'].apply(lambda x: util.extract_hashtags(x))

            df['text'] = df['text'].str.replace(r'\
            

            
            df['emojis'] = df['text'].apply(util.extract_emojis)

            


            for letter in letters:

                for i in range(len(letter[0])):

                    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

            
            df['symbols'] = df['text'].apply(util.extract_symbols)

            df['text'] = df['text'].apply(util.remove_symbols)

            
            df['links'] = df['text'].apply(util.extract_urls)

            df['text'] = df['text'].apply(util.remove_links)



            df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

            df['text'] = df['text'].replace(r'!+', '!', regex=True)

            df['text'] = df['text'].replace(r'\.+', '', regex=True)

            base_name, extension = os.path.splitext(file_name)

            df.to_csv(f"{cleaned_dir}/{base_name}.csv")

            df['text'].to_csv(f"{cleaned_dir}/{base_name}.txt", index=False, header=False)





        
clean_parsed_data(parsed_dir)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils.utils import Util
util = Util()
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"
def process_raw_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return



    
    for filename in os.listdir(folder_path):

        print(filename)

        data = util.read_file(f'{folder_path}/{filename}')

        parsed_message = util.parse_messages(data['messages'])



        
        df = pd.DataFrame(parsed_message)

        df.set_index('id', inplace=True)

        base_name, extension = os.path.splitext(filename)

        
        if not os.path.exists(parsed_dir):

            os.makedirs(parsed_dir)

        df.to_csv(f'{parsed_dir}/{base_name}.csv')

        
process_raw_data(raw_dir)
!pip install transformers


from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM



model_name = "Samuael/llama-2-7b-tebot-amharic"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(model_name)


input_text = "እኔ አማርኛ መናገር እረዳለሁ"




input_ids = tokenizer.encode(input_text, return_tensors="pt")




output_ids = model.generate(input_ids)




output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Amharic text:", output_text)
import json
import re


class Util:
    def __init__(self) -> None:
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"              "\U0001F300-\U0001F5FF"              "\U0001F680-\U0001F6FF"              "\U0001F700-\U0001F77F"              "\U0001F780-\U0001F7FF"              "\U0001F800-\U0001F8FF"              "\U0001F900-\U0001F9FF"              "\U0001FA00-\U0001FA6F"              "\U0001FA70-\U0001FAFF"              "\u2600-\u26FF"              "\u2700-\u27BF"              "\u2B50"              "\U0001F1E6-\U0001F1FF"              "]+",
            flags=re.UNICODE,
        )
        self.symbols = re.compile(
            "[" '"' "\“" '"' "'" "\-" "\*" "\•" "\ℹ" "\﻿" "\_" "]+"
        )
        self.url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        self.mention_pattern = r"@(\w+)"

    def read_file(self, file_path: str) -> dict:
                with open(file_path, "r") as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, "w") as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item["text"])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {"id": [], "text": [], "date": []}
        for message in messages:
            if message["type"] != "message" or len(message["text"]) == 0:
                continue
            parsed_messages["id"].append(message["id"])
            message_content = self.parse_text(message["text"])
            parsed_messages["text"].append(message_content)
            parsed_messages["date"].append(message["date"])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith("
    def extract_emojis(self, text):
        return "".join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub("", text)

    def extract_symbols(self, text):
        return "".join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(" ", text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def remove_links(self, text):
        return re.sub(self.url_pattern, " ", text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import argparse
from dataclasses import dataclass
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
import os
from langchain_openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

CHROMA_PATH = './chromadb/'

client = OpenAI(
    api_key=OPENAI_API_KEY
)

core_embeddings_model = None
def get_context():
        vectorstore = Chroma(persist_directory="./cachce",embedding_function=core_embeddings_model)
    
    retriever = vectorstore.as_retriever()
        return retriever

def generate_add(user_input, context):
    template = f'''
    Generate an advertisement given the following context.    
    You must use the following context:
    {context}
    '''   
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": template},{"role": "user", "content": user_input}],
        n=3,
    )

    return response
from peft import PeftModel
from transformers import LlamaForCausalLM, LlamaConfig

def load_model(model_name, quantization):
    model = LlamaForCausalLM.from_pretrained(
        model_name,
        return_dict=True,
        load_in_8bit=quantization,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    return model


def load_peft_model(model, peft_model):
    peft_model = PeftModel.from_pretrained(model, peft_model)
    return peft_model

def load_llama_from_config(config_path):
    model_config = LlamaConfig.from_pretrained(config_path) 
    model = LlamaForCausalLM(config=model_config)
    return model
import fire
import torch
import os
import sys
import time
import json 
from typing import List

from transformers import LlamaTokenizer, LlamaForCausalLM
from model_utils import load_model, load_peft_model

BASE_PROMPT = """Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers.
Human: {}
Assistant [Amharic] : """

def main(
    model_name: str="",
    peft_model: str=None,
    quantization: bool=False,
    max_new_tokens =400,     prompt_file: str=None,
    seed: int=42,     do_sample: bool=True,     min_length: int=None,     use_cache: bool=True,      top_p: float=1.0,     temperature: float=1.0,     top_k: int=1,     repetition_penalty: float=1.0,     length_penalty: int=1,     enable_azure_content_safety: bool=False,     enable_sensitive_topics: bool=False,     enable_saleforce_content_safety: bool=False,     **kwargs
):    
    
    print("***Note: model is not set up for chat use case, history is reset after each response.")
    print("***Ensure that you have replaced the default LLAMA2 tokenizer with the Amharic tokenizer")
    
        torch.cuda.manual_seed(seed)
    torch.manual_seed(seed)
    
    MAIN_PATH = '/model/Llama-2-7b-hf'
        peft_model = '/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'
    model_name = MAIN_PATH
    quantization = True
    model = load_model(model_name, quantization)

    tokenizer = LlamaTokenizer.from_pretrained(model_name)
    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))

    if peft_model:
        model = load_peft_model(model, peft_model)

    model.eval()

    while True:

        
        user_query = input('Type question in Amharic or English: ')
        user_prompt = BASE_PROMPT.format(user_query)
        batch = tokenizer(user_prompt, return_tensors="pt")
        batch = {k: v.to("cuda") for k, v in batch.items()}
        start = time.perf_counter()
        with torch.no_grad():
            outputs = model.generate(
                **batch,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                top_p=top_p,
                temperature=temperature,
                min_length=min_length,
                use_cache=use_cache,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                length_penalty=length_penalty,
                **kwargs 
            )
        e2e_inference_time = (time.perf_counter()-start)*1000
        print(f"the inference time is {e2e_inference_time} ms")
        
        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("MODEL_OUTPUT: {}".format(output_text))
        
if __name__ == "__main__":
    fire.Fire(main)
import torch
from contextlib import nullcontext
from transformers import (
    LlamaForCausalLM, 
    LlamaTokenizer, 
    TrainerCallback, 
    default_data_collator, 
    Trainer, 
    TrainingArguments
)
from peft import (
    LoraConfig,
    TaskType,
    prepare_model_for_int8_training,
    PeftModel
)

from pathlib import Path
from utils.dataset_utils import get_preprocessed_dataset
from configs.datasets import amharic_dataset

def print_trainable_parameters(model):
    print("Trainable Parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f" - {name}")


def finetune():
    LLAMA_DIR = '/model/Llama-2-7b-hf'
    PT_DIR = '/model/llama-2-amharic-3784m'
    OUTPUT_DIR = "/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output"

    tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)

    model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)


    train_dataset = get_preprocessed_dataset(tokenizer, amharic_dataset, 'train')


    model.train()



    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))


    print('loading the pretrained model from config')

    model = prepare_model_for_int8_training(model)
    model = PeftModel.from_pretrained(model, PT_DIR)
    model.print_trainable_parameters()
    lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            modules_to_save = ["embed_tokens","lm_head"]
        )

    enable_profiler = False


    config = {
        'lora_config': lora_config,
        'learning_rate': 1e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 1,
        'per_device_train_batch_size': 2,
        'gradient_checkpointing': False,
    }

        if enable_profiler:
        wait, warmup, active, repeat = 1, 1, 2, 1
        total_steps = (wait + warmup + active) * (1 + repeat)
        schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
        profiler = torch.profiler.profile(
            schedule=schedule,
            on_trace_ready=torch.profiler.tensorboard_trace_handler(f"{OUTPUT_DIR}/logs/tensorboard"),
            record_shapes=True,
            profile_memory=True,
            with_stack=True)

        class ProfilerCallback(TrainerCallback):
            def __init__(self, profiler):
                self.profiler = profiler

            def on_step_end(self, *args, **kwargs):
                self.profiler.step()

        profiler_callback = ProfilerCallback(profiler)
    else:
        profiler = nullcontext()


        training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        bf16=True,                  logging_dir=f"{OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=10,
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=1,
        warmup_ratio=0.03,
        optim="adamw_torch_fused",
        max_steps=total_steps if enable_profiler else -1,
        **{k:v for k,v in config.items() if k != 'lora_config'}
    )

    with profiler:
                trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=default_data_collator,
            callbacks=[profiler_callback] if enable_profiler else [],
        )

        print_trainable_parameters(model)

                trainer.train()

    model.save_pretrained(OUTPUT_DIR)


finetune()
from dataclasses import dataclass

@dataclass
class amharic_dataset:
    dataset: str = "amharic_dataset"
    train_split: str = "train"
    test_split: str = "val"
    data_path: str = "/data/fine_tun_data6.json"
import copy
import json
import torch


from torch.utils.data import Dataset



class InstructionDataset(Dataset):
    def __init__(self, dataset_config, tokenizer, partition="train", max_words=50):
        self.ann = json.load(open(dataset_config.data_path))

        if partition == "train":
            self.ann = self.ann
        else:
            self.ann = self.ann[:200]

        self.max_words = max_words
    
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.ann)

    def __getitem__(self, index):

        ann = self.ann[index]
        data = self.create_prompt_formats(ann)
        prompt = data['prompt']
        example = data['text']  
        prompt = torch.tensor(
            self.tokenizer.encode(prompt), dtype=torch.int64
        )

        example = self.tokenizer.encode(example)

        example.append(self.tokenizer.eos_token_id)
        example = torch.tensor(
            example, dtype=torch.int64
        )
        padding = self.max_words - example.shape[0]

        if padding > 0:
            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - 1))
        elif padding < 0:
            example = example[: self.max_words]

        labels = copy.deepcopy(example)

        labels[: len(prompt)] = -1

        example_mask = example.ge(0)
        label_mask = labels.ge(0)
        example[~example_mask] = 0
        labels[~label_mask] = 0
        example_mask = example_mask.float()
        label_mask = label_mask.float()

        return {
            "input_ids": example,
            "labels": labels,
            "attention_mask":example_mask,
        }
    
    def create_prompt_formats(self,sample):
        """
        Format various fields of the sample ('text', 'label',)
        Then concatenate them using two newline characters
        :param sample: Sample dictionnary
        """

        INTRO_BLURB = "Generate an advertisement given a category"
        INSTRUCTION_KEY = "        RESPONSE_KEY = "Response:"
        END_KEY = "
        blurb = f"{INTRO_BLURB}"
        text = f"{INSTRUCTION_KEY}\n{sample['input']}"
        response = f"{RESPONSE_KEY}\n{sample['output']}"
        end = f"{END_KEY}"

        parts = [part for part in [blurb, text, response, end] if part]

        formatted_prompt = "\n\n".join(parts)

        sample["text"] = formatted_prompt
        parts = [part for part in [blurb, text,] if part]
        formatted_prompt = "\n\n".join(parts)

        sample["prompt"]= formatted_prompt

        return sample
"""
Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.

Here is the full list of checkpoints on the hub that can be fine-tuned by this script:
https://huggingface.co/models?filter=text-generation
"""

import logging
import numpy as np
import math
import os
import sys
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Dict, Any, Mapping
from pathlib import Path
import datasets
import torch
from datasets import load_dataset, concatenate_datasets

import transformers
from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    is_torch_tpu_available,
    set_seed,
)
from transformers.testing_utils import CaptureLogger
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version

from sklearn.metrics import accuracy_score
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora_model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")

        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)


def accuracy(predictions, references, normalize=True, sample_weight=None):
        return {
            "accuracy": float(
                accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
            )
        }


def compute_metrics(eval_preds):
    preds, labels = eval_preds
            labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)


def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
                        logits = logits[0]
    return logits.argmax(dim=-1)


def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}

                if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["labels"] = torch.tensor([f["label"] for f in features], dtype=dtype)
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)

        
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else:
                    batch[k] = torch.tensor([f[k] for f in features])
    except ValueError:         for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else:
                    batch[k] = torch.tensor([features[0][k]] * len(features))

    return batch


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The tokenizer for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides can't be used in combination with --config_name or --model_name_or_path"
            )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    dataset_dir: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional input sequence length after tokenization. "
                "The training dataset will be truncated in block of this size for training. "
                "Default to the model max input length for single sentence inputs (take into account special tokens)."
            )
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep line breaks when using TXT files or not."}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed stored"})

    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")


@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj,v_proj")
    lora_rank : Optional[int] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.1)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[bool] = field(default=False)
    peft_path : Optional[str] = field(default=None)


logger = logging.getLogger(__name__)


def main():

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, MyTrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

            send_example_telemetry("run_clm", model_args, data_args)

        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,          handlers=[logging.StreamHandler(sys.stdout)],)


    if training_args.should_log:
                transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
        logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )

        last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

        set_seed(training_args.seed)

    config_kwargs = {
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else:
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("You are instantiating a new config instance from scratch.")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")

    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script."
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )

                tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            output = tokenizer(examples["text"])
                if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return output
    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)

        def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
                        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
                result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    with training_args.main_process_first(desc="dataset map tokenization and grouping"):
        lm_datasets = []
        path = Path(data_args.dataset_dir)
        files = [file.name for file in path.glob("*.txt")]
        if training_args.debug_mode is True:
            files = [files[0]]
        print("printing files")
        print(files)    
        for idx, file in enumerate(files):
            data_file = os.path.join(path, file)
            filename = ''.join(file.split(".")[:-1])
            cache_path = os.path.join(data_args.data_cache_dir, filename)
            os.makedirs(cache_path, exist_ok=True)
            try:
                processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=False)
                logger.info(f'training datasets-{filename} has been loaded from disk')
            except Exception:
                cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                os.makedirs(cache_dir, exist_ok=True)
                raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                logger.info(f"{file} has been loaded")
                tokenized_dataset = raw_dataset.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns="text",
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'tokenized.arrow') for k in raw_dataset},
                    desc="Running tokenizer on dataset",
                )
                grouped_datasets = tokenized_dataset.map(
                    group_texts,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'grouped.arrow') for k in tokenized_dataset},
                    desc=f"Grouping texts in chunks of {block_size}",
                )
                processed_dataset = grouped_datasets
                processed_dataset.save_to_disk(cache_path)
            if idx == 0:
                lm_datasets = processed_dataset['train']
            else:
                assert lm_datasets.features.type == processed_dataset["train"].features.type
                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset["train"]])

        lm_datasets = lm_datasets.train_test_split(test_size = data_args.validation_split_percentage)

    if training_args.do_train:
        train_dataset = lm_datasets['train']
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))
        logger.info(f"Num train_samples  {len(train_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(train_dataset[0]['input_ids']))
    if training_args.do_eval:
        eval_dataset = lm_datasets["test"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        logger.info(f"Num eval_samples  {len(eval_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(eval_dataset[0]['input_ids']))



    if model_args.model_name_or_path:
        torch_dtype = (
            model_args.torch_dtype
            if model_args.torch_dtype in ["auto", None]
            else getattr(torch, model_args.torch_dtype)
        )
        model = LlamaForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool(".ckpt" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            use_auth_token=True if model_args.use_auth_token else None,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
        )
    else:
        model = AutoModelForCausalLM.from_config(config)
        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")

    model_vocab_size = model.get_output_embeddings().weight.size(0)
    if not (
       (model_vocab_size==32000 and len(tokenizer)==51008) or \
       (model_vocab_size==32000 and len(tokenizer)==32000) or \
       (model_vocab_size==51008 and len(tokenizer)==51008) or \
       (model_vocab_size==49954 and len(tokenizer)==49954)
       
    ):
        raise ValueError(
            f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
            "Valid configurations (base model / tokenizer):\n"
            "- Continue pre-training original LLaMA: 32000 / 32000 \n"
            "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
            "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
            "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n")

    model.resize_token_embeddings(len(tokenizer))
    if training_args.peft_path is not None:
        logger.info("Peft from pre-trained model")
        model = PeftModel.from_pretrained(model, training_args.peft_path)
    else:
        logger.info("Init new peft model")
        target_modules = training_args.trainable.split(',')
        modules_to_save = training_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')
        lora_rank = training_args.lora_rank
        lora_dropout = training_args.lora_dropout
        lora_alpha = training_args.lora_alpha
        logger.info(f"target_modules: {target_modules}")
        logger.info(f"lora_rank: {lora_rank}")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=lora_rank, lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            modules_to_save=modules_to_save)
        model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

                                

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
        modules_to_save = ["embed_tokens","lm_head"]
    )

    """
        config = {
        'lora_config': lora_config,
        'learning_rate': 2e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 2,
        'per_device_train_batch_size': 1,
        'per_device_eval_batch_size': 1,
        'gradient_checkpointing': True,
    }
    """


    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=fault_tolerance_data_collator,
        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
        if training_args.do_eval and not is_torch_tpu_available()
        else None,
    )

    """
    output_dir=training_args.output_dir,
    overwrite_output_dir=True,
    bf16=True,          logging_dir=f"{training_args.output_dir}/logs",
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="steps",
    save_steps=5000,
    optim="adamw_torch_fused",
    max_steps=-1,     **{k:v for k,v in config.items() if k != 'lora_config'}
    """

    trainer.add_callback(SavePeftModelCallback)
        if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)

        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
    model.save_pretrained(training_args.output_dir)
        if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()

        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
        try:
            perplexity = math.exp(metrics["eval_loss"])
        except OverflowError:
            perplexity = float("inf")
        metrics["perplexity"] = perplexity

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)


if __name__ == "__main__":
    main()
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=/model/Llama-2-7b-hf
amharic_tokenizer_path=/model/Llama-2-7b-hf
dataset_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/data/cleaned
data_cache=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/cache
per_device_train_batch_size=32
per_device_eval_batch_size=1
gradient_accumulation_steps=1
output_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output

python pretrain.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${amharic_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 1 \
    --save_steps 7528 \
    --evaluation_strategy steps \
    --eval_steps 3000 \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --bf16 \
    --overwrite_output_dir \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --gradient_checkpointing \
import pandas as pd

df = pd.read_json("/data/fine_tun_data2.json")

df.tail()
df[df['output']=='not advertisement'].iloc[1]['input']
'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: ለኢሬቻ በኣል ወደቢሾፍቱ የተጓዙት የሲዳማ ወጣቶች(ኤጄቶዎች) ከሰኣታት በፊት ቢሾፍቱ ገብተዋል። @tsegabwolde @tikvahethiopia''
import pandas as pd

df = pd.read_csv("/data/wasu_mohammed_labeled.csv")

df.head()
df.shape
df['label'] = df['label'].fillna("Not Advertisement")

df.tail(5)
df
from datasets import Dataset




data_dict = {"text": df['text'].tolist()}




dataset = Dataset.from_dict(data_dict)

dataset.save_to_disk("../data/train")




from peft import PeftModel

from transformers import LlamaForCausalLM, LlamaConfig




def load_model(model_name, quantization):

    model = LlamaForCausalLM.from_pretrained(

        model_name,

        return_dict=True,

        load_in_8bit=quantization,

        device_map="auto",

        low_cpu_mem_usage=True,

    )

    return model






def load_peft_model(model, peft_model):

    peft_model = PeftModel.from_pretrained(model, peft_model)

    return peft_model




def load_llama_from_config(config_path):

    model_config = LlamaConfig.from_pretrained(config_path) 

    model = LlamaForCausalLM(config=model_config)

    return model
from transformers import LlamaTokenizer





MAIN_PATH = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)



example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685





tokens = tokenizer.tokenize(example)

print(tokens)
print(len(tokenizer))
example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685




tokens = tokenizer.tokenize(example)

print(tokens)
df['text'][0]
df = pd.read_csv("/data/wasu_mohammed_labeled.csv")




total_word_count = 0

total_tokens = 0


for index, row in df.iterrows():

    
    text = row['text']

    if not isinstance(text, str): 

        continue



    

    
    word_count = len(text.split())



    
    total_word_count += word_count

    tokens = tokenizer.tokenize(text)

    total_tokens+=tokens

    print(tokens)

    






print("Total Word Count:", total_word_count)

print("Total tokens count: ",total_tokens)
total_tokens
df.shape
from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    HfArgumentParser,

    TrainingArguments,

    pipeline,

    LlamaForCausalLM, 

    LlamaTokenizer,

    logging,

)

from peft import (

    LoraConfig,

    TaskType,

    prepare_model_for_int8_training,

    PeftModel

)

import torch

LLAMA_DIR = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)



model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)

embedding_size = model.get_input_embeddings().weight.shape[0]



if len(tokenizer) != embedding_size:

    print("resize the embedding size by the size of the tokenizer")

    model.resize_token_embeddings(len(tokenizer))





new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'

model = PeftModel.from_pretrained(model, new_model)




prompt = "Who is Leonardo Da Vinci?"

pipe = pipeline(task="text-generation", model=model, tokenizer=model, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
ቨርቹዋል ረዳቶች እንደ Amazon&
run-20240203_155644-4hm9i4tp
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from transformers import AutoTokenizer, AutoModel
import os
import torch
import shutil
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

model_name = 'Davlan/bert-base-multilingual-cased-finetuned-amharic'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

data_path = '../data/'
CHROMA_PATH = '../RAG/chromadb'

def embed_doc(document):
    
        encoded_input = tokenizer(document, padding=True, truncation=True, return_tensors='pt')
    
        with torch.no_grad():
        outputs1 = model(**encoded_input)

    

        embeddings1 = outputs1.last_hidden_state.squeeze(dim=0)
    

        
    return embeddings1

def load_documents(data_path):    
    try:
        loader = DirectoryLoader(data_path)
        documents = loader.load()       
        print("data loaded sucessfully")
        return documents[0].page_content
    except:
        print("document not found!")
        return None
    

def split_text(documents:list[Document]):
    try:
        text_spliter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=500,
            length_function=len,
            add_start_index = True
        )
        chunk = text_spliter.split_documents(documents)
        print("data splited successfuly!")
        return chunk
    except:
        print("document not found")

def save_chunks_to_chroma(chunks):
        if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
    try:
        db = Chroma.from_documents(chunks,embed_doc(),\
                                persist_directory=CHROMA_PATH)
        db.persist()
        print("Vectorstore created successfully!")
    except:
        print("Couldn't create the vectore database")

def generate_data_store():
    documents = load_documents(data_path)
    chunks = split_text(documents)
    embeding1 = embed_doc(chunks)
    print(embeding1)
    save_chunks_to_chroma(embeding1) 

def main():
    generate_data_store()      


if __name__ == "__main__":
    main()
! pip install transformers bitsandbytes peft trl accelerate
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
base_model = "NousResearch/Llama-2-7b-chat-hf"

guanaco_dataset = "mlabonne/guanaco-llama2-1k"

new_model = "LLama-2-7b-chat-prac"
print(peft.__version__)
dataset = load_dataset(guanaco_dataset, split="train")
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=compute_dtype,

    bnb_4bit_use_double_quant=False,

)
model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=quant_config,

    device_map={"": 0}

)



model.config.use_cache = False

model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_size= "right"
peft_params = LoraConfig(

    lora_alpha = 16,

    lora_dropout = 0.1,

    r=64,

    bias="none",

    task_type= "CAUSAL_LM"

)
training_params = TrainingArguments(

    output_dir = "./results",

    num_train_epochs=1,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim="paged_adamw_32bit",

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-4,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=-1,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type="constant",

    report_to="tensorboard"

)
!pip install --upgrade peft
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
import pandas as pd

import numpy as np

import sys, os

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util

from concurrent.futures import ThreadPoolExecutor
util = Util()

cleaned_dir = "../cleaned"

json_file_path = '../raw/dilela_page.json'




df = pd.read_json(json_file_path)

df.head()
df.shape

columns = ["id", "channel_name", "type", "message_id", "message_type","text", "label", "created_at", "updated_at", ]

new_df = pd.DataFrame(columns=columns)

new_df

telegeram_channel_id  = df["id"][0]

telegram_channel_name = df["name"][0]

telegeram_channel_type = df["type"][0]

message_df = df["messages"]

data = [{

       'telegeram_channel_id': telegeram_channel_id,

       'telegram_channel_name': telegram_channel_name,

       'telegeram_channel_type': telegeram_channel_type,

       'message_id': message.get('id', np.nan),

        'message_type': message.get('type', np.nan),

        'text': message['text_entities'][0]['text'] if message.get('text_entities') and message['text_entities'] else np.nan,

        'created_at': message.get('date', np.nan),

        'update_at': message.get('edited', np.nan),

        }for message in message_df]

message_df = pd.DataFrame(data)

message_df = message_df.sort_values(by='message_id')

message_df.head(20)
message_df.shape

nan_rows_count = message_df.isna().any(axis=1).sum()

nan_rows_count

message_df = message_df.dropna()

message_df.head()
message_df.shape
message_df = message_df.replace('\n', ' ', regex=True)

message_df.head()

message_df["hashtags"] = message_df['text'].apply(lambda text: util.extract_hashtags(text))

message_df.head()

message_df["text"] = message_df["text"].str.replace(r'\
message_df.head()
message_df["emojis"] = message_df["text"].apply(util.extract_emojis)

message_df.head()

message_df['text'] = message_df['text'].apply(util.remove_emojis_using_emoji_pattern)

message_df.tail()
def remove_emojis_parallel(text):

    return util.remove_emojis(text)





with ThreadPoolExecutor() as executor:

    message_df['text'] = list(executor.map(remove_emojis_parallel, message_df['text']))
message_df.head()

message_df.replace('', pd.NA, inplace=True)

nan_rows_count = message_df.isna().any(axis=1).sum()


message_df = message_df.dropna()

message_df.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    message_df['text'] = message_df['text'].str.replace(letter[0][i], letter[1][i])
message_df['symbols'] = message_df['text'].apply(util.extract_symbols)

message_df.head()
message_df['text'] = message_df['text'].apply(util.remove_symbols)

message_df.tail()
message_df['links'] = message_df['text'].apply(util.extract_urls)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

message_df.head()
message_df['mentions'] = message_df['text'].apply(util.extract_mentions)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

message_df.tail()

message_df['text'] = message_df['text'].str.replace('\s+', ' ', regex=True).str.strip()
message_df['text'] = message_df['text'].replace(r'!+', '!', regex=True)

message_df['text'] = message_df['text'].replace(r'\.+', '', regex=True)
message_df.head()

nan_rows_count = message_df['text'].isna().sum()

nan_rows_count


message_df = message_df.dropna(subset='text')

message_df.tail()


message_df = message_df[message_df['text'].str.len() >= 20]
message_df.to_csv(f"{cleaned_dir}/dilela_page.csv")
message_df['text'].to_csv(f"{cleaned_dir}/dilela_page.txt", index=False, header=False)
df = pd.read_csv(f"{cleaned_dir}/dilela_page.csv")

df.head()
df['word_count'] = df['text'].str.split().str.len()

df.columns


df_labeled = df.drop(['Unnamed: 0','telegram_channel_name','telegeram_channel_type','message_id','message_type','hashtags', 'emojis', 'created_at','symbols', 'links','mentions'],axis=1)

df_labeled.rename(columns={'update_at':'date','telegeram_channel_id':'channel_id'},inplace=True)

df_labeled.to_csv(f"{cleaned_dir}/dilela_page_labeled.csv")

len = df_labeled['word_count'].sum()

len
from fastapi import FastAPI, HTTPException, Depends
from typing import Annotated, List
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from utils import simple_rag
from utils import hugging_face_hub

app = FastAPI()

origins = ["http://localhost:5173"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
)


class RagResponseBase(BaseModel):
    question: str
    answer: str


class HugResponseBase(BaseModel):
    question: str
    answer: str


@app.get("/getanswer", response_model=RagResponseBase)
async def return_answer(question: str):
    result = simple_rag.test_RAG(question)
    return result


@app.get("/getHuggingFaceAnswer", response_model=HugResponseBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.invoke_current_hugging_face_model(model, prompt)
    return result
from langchain import OpenAI
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from models import simple_rag_response
import os
from dotenv import load_dotenv

load_dotenv()


def load_data():
    loader = TextLoader("/week_6_challenge_doc.txt")
    documents = loader.load()
    return documents


def return_chunks(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)
    texts = text_splitter.split_documents(documents)
    return texts


def return_chain(texts):
    embeddings = OpenAIEmbeddings()
    store = Chroma.from_documents(
        texts, embeddings, collection_name="challenge_document"
    )
    llm = OpenAI(temperature=0)
    return RetrievalQA.from_chain_type(llm, retriever=store.as_retriever())


def test_RAG(question):
    documents = load_data()
    chunks = return_chunks(documents)
    chain = return_chain(chunks)
    response = chain.run(question)
    return simple_rag_response.RagResponse(question, response)
import React, { useState , useRef} from 'react'
import 'bootstrap/dist/css/bootstrap.css'
import FileInput from  './components/FileInput'
import TextInputWithLable from  './components/TextInputWithLable'
import Dropdown from './components/Dropdown'
import NavBarComp from './components/Navbar'
import SpinnerWithText from './components/SpinnerWithText'
import api from './api/api'
function App() {
  const [answer,setAnswer] = useState([]);
  const [isShow,setShow] = useState(false);

  const fetchResponse = async () =>{
    console.log(ref.current.value);
    const question = ref.current.value;
    setShow(true)
    const response = await api.get('/getanswer?question='+question);
    console.log(response.data);
    setAnswer(response.data)
    setShow(false)
  }
  const ref = useRef(null);

  return (
    <React.Fragment>
      
      <NavBarComp />
      <main className='container'>
        <form className="row g-3" >
          
          <div>
            <label htmlFor="inputLable" className="form-label">Input Ad description to be generated</label>
            <textarea className="form-control" id="inputTextarea" rows="7" ref={ref}/>
          </div>

          {isShow && <SpinnerWithText />}

          <button type="button" className="btn btn-primary mb-4" onClick={fetchResponse}>Get Ad</button> 

          <div>
            <TextInputWithLable value= {answer}/>
          </div>

        </form>
      </main>

    </React.Fragment>
  )
}

export default App
import pandas as pd

import json
df = pd.read_json('sheger.json')

df.head()
df.info()
df.messages.iloc[0]
df.columns
message_df = pd.json_normalize(df.messages)
message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)

    return ''.join(extracted_text)




message_df['extracted_text'] = message_df['text'].apply(extract_text_from_data)
message_df = message_df[['id','type','date','extracted_text']]

message_df.head()

def extract_text_from_data(data):

    extracted_text = []

    for item in data:

        if isinstance(item, dict) and 'text' in item:

            extracted_text.append(item['text'])

        elif isinstance(item, str):

            extracted_text.append(item)



    
    full_text = ''.join(extracted_text)



    
    substrings_to_remove = ['\n\n', '@sheger_press\n@sheger_press', '❗️❗️❗️']

    for substring in substrings_to_remove:

        full_text = full_text.replace(substring, '')



    
    full_text = ''.join(char for char in full_text if char.isalnum() or char.isspace())



    return full_text.strip()




message_df['cleaned_text'] = message_df['extracted_text'].apply(extract_text_from_data)

message_df.head()
df = df[['name','type','id']]

df.rename(columns={'name':'channel_name',

                  'type':'channel_type',

                  'id':'channel_id'}, inplace=True)
df.head()
message_df=message_df[['id','type','date','cleaned_text']]

message_df.rename(columns = {'id':'message_id',

                  'type':'message_type',

                  'date':'message_date',

                  'cleaned_text':'text'}, inplace = True)

message_df.head()
sheger_df= pd.concat([df,message_df], axis =1)
sheger_df.head()
press_df = press_df[press_df.text != '']

press_df.head()
sheger_df.to_csv('sheger_press.csv', index = None)
import pandas as pd

import json

import os

from pprint import pprint

import bitsandbytes as bnb

import torch

import torch.nn as nn

import transformers

from datasets import load_dataset, Dataset

from huggingface_hub import notebook_login



from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipelines, logging
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa' 
dataset = load_dataset(dataset_name, split="train")
MODEL_NAME = "mistralai/Mistral-7B-v0.1"

new_model = "amharic-mistral-7b"

config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, load_in_4bit=True)






bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_use_double_quant=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.bfloat16

)





model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    device_map="auto",

    trust_remote_code=True,

    quantization_config=bnb_config,

)



tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tokenizer.pad_token = tokenizer.eos_token
model = prepare_model_for_kbit_training(model)
use_4bit = True




bnb_4bit_compute_dtype = "float16"




bnb_4bit_quant_type = "nf4"




use_nested_quant = False

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

if compute_dtype == torch.float16 and use_4bit:

    major, _ = torch.cuda.get_device_capability()

    if major >= 8:

        print("=" * 80)

        print("Your GPU supports bfloat16: accelerate training with bf16=True")

        print("=" * 80)
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=2,

    lora_alpha=32,

    target_modules=get_last_layer_linears(model),

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM"

)



model = get_peft_model(model, config)





output_dir = "./results"




num_train_epochs = 1




fp16 = False

bf16 = False




per_device_train_batch_size = 4




per_device_eval_batch_size = 4




gradient_accumulation_steps = 1




max_grad_norm = 0.3




learning_rate = 2e-4




weight_decay = 0.001




optim = "paged_adamw_32bit"




lr_scheduler_type = "constant"




warmup_ratio = 0.03





group_by_length = True




save_steps = 25




logging_steps = 25


base_model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(base_model, new_model)

model = model.merge_and_unload()

training_arguments = TrainingArguments(

    output_dir=output_dir,

    num_train_epochs=num_train_epochs,

    per_device_train_batch_size=per_device_train_batch_size,

    gradient_accumulation_steps=gradient_accumulation_steps,

    optim=optim,

    save_steps=save_steps,

    logging_steps=logging_steps,

    learning_rate=learning_rate,

    weight_decay=weight_decay,

    fp16=fp16,

    bf16=bf16,

    max_grad_norm=max_grad_norm,

    max_steps=25,

    warmup_ratio=warmup_ratio,

    group_by_length=group_by_length,

    lr_scheduler_type=lr_scheduler_type,

    report_to="tensorboard"

)
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
class RagResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class HugResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass
from dotenv import load_dotenv
from models import simple_rag_response

load_dotenv()

from langchain import HuggingFaceHub


def invoke_current_hugging_face_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.HugResponse(prompt, response)
import time

import sentencepiece as spm
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=m --vocab_size=100000')

sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))




print(sp.encode_as_pieces('ሃይ ሰላም ናችሁ?'))



spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=am-word --model_type=word  --vocab_size=100000')



sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.encode_as_ids('ፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.decode_ids([47914, 1024, 33, 7716, 29922, 26700]))
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U00002049 \U0000FE0F"                                         u"\U0000203C"                                         u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'
        print(self.emoji_pattern.pattern)

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub(' ', text)
        
    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import pandas as pd

import csv, os, sys

from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import
"""
Convert .txt to csv

"""

import csv
from sklearn.model_selection import train_test_split

def convert_txt_to_csv(input_txt, output_csv):
    with open(input_txt, 'r', encoding='utf-8') as infile, open(output_csv, 'w', encoding='utf-8', newline='') as outfile:
       
        reader = infile.readlines()
        data = [line.strip().split() for line in reader]
        csv_writer = csv.writer(outfile)
        csv_writer.writerows(data)

def split_data(input_csv, output_train_csv, output_test_csv, output_val_csv, test_size=0.2, val_size=0.1, random_seed=42):
    with open(input_csv, 'r', encoding='utf-8') as file:
        csv_reader = csv.reader(file)
        data = list(csv_reader)
        
    train_data, test_val_data = train_test_split(data, test_size=(test_size + val_size), random_state=random_seed)
    test_data, val_data = train_test_split(test_val_data, test_size=(val_size / (test_size + val_size)), random_state=random_seed)

    with open(output_train_csv, 'w', encoding='utf-8', newline='') as train_file:
        csv_writer = csv.writer(train_file)
        csv_writer.writerows(train_data)

    with open(output_test_csv, 'w', encoding='utf-8', newline='') as test_file:
        csv_writer = csv.writer(test_file)
        csv_writer.writerows(test_data)

    with open(output_val_csv, 'w', encoding='utf-8', newline='') as val_file:
        csv_writer = csv.writer(val_file)
        csv_writer.writerows(val_data)

if __name__ == "__main__":
    input_txt_file = '/home/biniyam_ajaw/finetuning/data/dataset.txt'
    output_csv_file = '/home/biniyam_ajaw/finetuning/data/output_data.csv'
    output_train_csv = '/home/biniyam_ajaw/finetuning/data/train_data.csv'
    output_test_csv = '/home/biniyam_ajaw/finetuning/data/test_data.csv'
    output_val_csv = '/home/biniyam_ajaw/finetuning/data/val_data.csv'

    convert_txt_to_csv(input_txt_file, output_csv_file)
    split_data(output_csv_file, output_train_csv, output_test_csv, output_val_csv)
    print("Conversion to CSV and data split completed.")
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=100000)

import tokenizers

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

folder = 'data'
files = [f"/home/biniyam_ajaw/finetuning/{folder}/{split}.csv" for split in ["test_data", "train_data", "valid_data"]]
tokenizer.train(files, trainer)

from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")

from transformers import PreTrainedTokenizerFast

custom_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
custom_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
custom_tokenizer.save_pretrained("amharic_tokenizer")

custom_tokenizer.push_to_hub("amharic_tokenizer")
max-width: fit-content;
  width: 100%;
  margin: 5 auto;
  padding: 2rem;
  text-align: start;
}
.container {
  max-width: '100%' 
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function FileInput (){
    return(
        <div>
            <div className="input-group mb-3">
                <input type="file" className="form-control" id="inputGroupFile02"/>
                <label clclassNameass="input-group-text" for="inputGroupFile02">Upload</label>
            </div>
        </div>
      
    );
}


export default FileInput;
/* :root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;
  width: 100%;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}
a {
  font-weight: 500;
  color:   text-decoration: inherit;
}
a:hover {
  color: }

body {
  margin: 0;
  display: flex;
  place-items: center;
  min-width: 320px;
  min-height: 100vh;
}

h1 {
  font-size: 3.2em;
  line-height: 1.1;
}

button {
  border-radius: 8px;
  border: 1px solid transparent;
  padding: 0.6em 1.2em;
  font-size: 1em;
  font-weight: 500;
  font-family: inherit;
  background-color:   cursor: pointer;
  transition: border-color 0.25s;
}
button:hover {
  border-color: }
button:focus,
button:focus-visible {
  outline: 4px auto -webkit-focus-ring-color;
}

@media (prefers-color-scheme: light) {
  :root {
    color:     background-color:   }
  a:hover {
    color:   }
  button {
    background-color:   }
} */
import Container from 'react-bootstrap/Container';
import Nav from 'react-bootstrap/Nav';
import Navbar from 'react-bootstrap/Navbar';
import NavDropdown from 'react-bootstrap/NavDropdown';

function NavBarComp() {
  return (
    <Navbar expand="lg" className="bg-body-tertiary container-fluid">
      <Container >
        <Navbar.Brand href="        <Navbar.Toggle aria-controls="basic-navbar-nav" />
        <Navbar.Collapse id="basic-navbar-nav">
          <Nav className="me-auto">
            <NavDropdown title="Select Model" id="basic-nav-dropdown">
                <NavDropdown.Item href="                <NavDropdown.Item href="                <NavDropdown.Item href="            </NavDropdown>
          </Nav>
        </Navbar.Collapse>
      </Container>
    </Navbar>
  );
}

export default NavBarComp;
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function TextInputWithLabel (props) {
    return(
        <div>
            <div className="mb-3">
                <label htmlFor="exampleFormControlTextarea1" className="form-label">Generated Ad</label>
                <textarea className="form-control" id="exampleFormControlTextarea1" rows="7"  value={props.value.answer}/>
            </div>
        </div>
      
    );
}


export default TextInputWithLabel;
import torch

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging

from datasets import load_dataset

import os, sys

from huggingface_hub import notebook_login

import torch.nn as nn

import getpass

from trl import SFTTrainer

from peft import PeftConfig, LoraConfig
os.environ["HUGGING_FACE_HUB_TOKEN"] = getpass.getpass("Token:")

assert os.environ["HUGGING_FACE_HUB_TOKEN"]
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")

double_quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model_id = "microsoft/phi-2"

new_model = 'amharic-phi'

model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cuda:0', quantization_config=nf4_config)
tokenizer = AutoTokenizer.from_pretrained("dagim/amharic_tokenizer")



tokenizer.tokenize("ከአሜሪካ ወደ አዲስ አበባለመጓዝምንያህልጊዜይወስዳል??")
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa'

dataset = load_dataset(dataset_name, split="train")
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=4,

    lora_alpha=32,

    
    lora_dropout=0.03,

    bias='none',

    task_type="CAUSAL_LM"

)

training_arguments = TrainingArguments(

    output_dir="./results",

    num_train_epochs=2,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim='paged_adamw_32bit',

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-8,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=25,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type='constant',

    report_to="tensorboard",

    gradient_checkpointing=True

)
trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=config,

    dataset_text_field='inputs',

    max_seq_length=None,

    tokenizer=tokenizer,

    args=training_arguments,

    packing=False

)
trainer.train()
trainer.model.save_pretrained(new_model)
logging.set_verbosity(logging.CRITICAL)



prompt = "የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?"

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="/home/biniyam_ajaw/llama-2-amharic-3784m/tokenizer.json")
print(len(tokenizer.encode('የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?')))
import logging
import numpy as np
import math
import os, sys
import torch
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Tuple, Dict, Any, Mapping
from pathlib import Path
import datasets
from datasets import Dataset, DatasetDict, load_dataset, load_metric, concatenate_datasets

from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,  
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
    is_torch_gpu_available,
)

from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.testing_utils import CaptureLogger
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version_core

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora-Model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")
            
        
        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)
        
    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

def accuracy(predictions, references, normalize=True, sample_weight=None):
    return {
        "accuracy": float(
            accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
        )
    }

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)

def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
        logits = logits[0]
    return logits.argmax(dim=-1)

def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}
    
    if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["label"] = torch.tensor([f["label"] for f in features], dtype=dtype)
        
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
            
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)
            
    
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else: batch[k] = torch.tensor([f[k] for f in features])
                
    except ValueError:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                                        batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else: 
                    batch[k] = torch.tensor([features[0][k]] * len(features))
                    
    return batch

MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={"help": ("The tokenizer for weights initialization.")},
    )
    
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    
    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides cannot be used with --config_name or --model_name_or_path. To override some of "
            )
            
@dataclass
class DataTrainingArguments:
    '''
    Arguments pertaining to what data we are going to input our model for training and eval.
    '''
    
    dataset_dir: Optional[str] = field(
        default=None, metadata={"the name of the dataset to use"}
    )

    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name opf the dataset to use"}
    )
    
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training file"}
    )
    
    validation_file: Optional[str] = field(
        default=None, metadata={"help": "This is optional but recommended if you want to use early stopping"}
    )
    
    max_training_sample: Optional[int] = field(
        default=None,
        metadata={
            "help": "Debugging purposes"
        },
    )
    
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging"
        },
    )
    
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    
        block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional"
                "Training dataset will be truncated into a block of this size for training"
                "Default to the model max input sequence"
            )
        }
    )
    
            
    cache_dir: bool = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    
            validation_strategy: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "Percentage of the validation set used at the end of each epoch"
        }
        
    )
        preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes to use for preprocessing"}
    )
    
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep the linebreaks when using txt files or not"}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed store"})
    
    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets >= 2.0.0`")
            
            
@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj, v_proj")
    lora_rank : Optional[str] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.03)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[str] = field(default=False)
    peft_path : Optional[str] = field(default=None)
    
logger = logging.getLogger(__name__)

def main():
    
    parser = HfArgumentParser(ModelArguments, DataTrainingArguments, MyTrainingArguments)
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                model_args, data_args, training_args = parser.parse_parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_to_dataclasses()
        
    send-example_telemetry("run_clm", model_args, data_args)
    
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", datefmt="%m/%d/%Y %H:%M:%S",
                        level=logging.INFO,                         handlers=[logging.StreamHandler(sys.stdout)],)
    
    if training_args.should_log:
        transformers.utils.logging.set_verbosity_info()
        
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
    logger.warning(
        f"Process rank: {training_args.output_dir}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"Distributed training: {bool(training_args.local_rank != -1)}, 16-bits-training: {training_args.fp16}"
    )
    
        
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError (
                f"Outpur dir {training_args.output_dir} already exists and is not mt"
                "Use --overwrite_output_dir to overcome"
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is not None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this, change "
                "the --output-dir or --overwrite_output_dir to train from scratch"
            )
            
        set_seed(training_args.seed)
    
    config_kwargs = {
        "cache_dir": model.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model-args.use_auth_token else None
    }
    
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else: 
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("This is a new config from scratch")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")
            
            
    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None
    }
    
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "Instantiating a tokenizer from scratch"
        )
        
            
    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            return tokenizer(examples[text])
        
        if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^ PLease ignore the warning above ^^^^^^^"
            )
            
        return output

    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024  
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                "Override with `--block_size xxx`"
            
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)
        
        
    def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        
        if total_length >= block_size:
                        total_length = {total_length // block_size} *  block_size
                        result = {
                k: [t[i: i + block_size] for i in range(0, total_length, block_size)]
                for k, t in concatenated_examples.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result
        with training_args.main_process_first(desc="dataset map tokenizer"):
            lm_datasets = []
            path = Path(data_args.dataset_dir)
            filename = [file.name for file in path.glob("*.txt")]
            
            if training_args.debug_mode:
                files = [files[0]]
            for idx, file in enumerate(files):
                data_file = os.path.join(path, file)
                filename = ''.join(file.split('.')[:-1])
                cache_path = os.path.join(data_args.data_cache_dir, filename)
                os.makedirs(cache_path, exist_ok=True)
                try:
                    processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=True)
                    logger.info(f'Training datasets-{filename} has been loaded from disk')
                except Exception:
                    cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                    os.makedirs(cache_dir, exist_ok=True)
                    raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                    logger.info(f"{file} has been loaded")
                    tokenized_dataset = raw_dataset.map(
                        tokenize_function,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        remove_columns="text",
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "tokenized.arrow") for k in raw_dataset},
                        desc="Running tokenizer on the dataset",
                    )
                    
                    grouped_datasets = tokenized_dataset.map(
                        group_texts,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "grouped.arrow") for k in tokenized_dataset},
                        desc=f'Grouping texts in chunks of {block_size}',
            
                    )
                    
                    processed_dataset = grouped_datasets
                    processed_dataset.save_to_disk(cache_path)
                    
                if idx == 0:
                    lm_datasets = processed_dataset['train']
                else:
                    assert lm_datasets.features.type == processed_dataset['train'].features.type
                    lm_dataset = concatenate_datasets([lm_datasets, processed_dataset['train']])
                    
            lm_datasets = lm_datasets.train_test_split(test_size= data_args.validation_split_percentage())
            
        if training_args.do_train:
            train_dataset = lm_datasets["train"]
            
            
            if data_args.max_train_samples is not None:
                max_train_samples = min(len(train_dataset), data_args.max_train_samples)
                train_dataset = train_dataset.select(range(max_train_samples))
                logger.info(f"Num train samples {len(train_dataset)}")
                logger.info("Training example: ")
                logger.info(tokenizer.decode(train_dataset[0]["input_ids"]))
                
                
        if model_args.model_name_or_path:
            torch_dtype = (
                model_args.torch_dtype
                if model_args.torch_dtype in ["auto", None]
                else getattr(torch, model_args.torch_dtype)
            )
            
            model = LlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                from_tf=bool(".cpkt" in model_args.model_name_or_path),
                config=config,
                cache_dir=model_args.cache_dir,
                revision=model_args.model_revision,
                use_auth_token=True if model_args.use_auth_token else None,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
            )
            
        else:
            model = AutoModelForCausalLM.from_config(config)
            n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
            logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M parameters")
        
        model_vocab_size = model.get_output_embeddings().weight.size(0)
        
        if not (
            (model_vocab_size==32000 and len(tokenizer)==51008) or \
            (model_vocab_size==32000 and len(tokenizer)==32000) or \
            (model_vocab_size==51008 and len(tokenizer)==51008) or \
            (model_vocab_size==49954 and len(tokenizer)==49954)
        ):
            raise ValueError(
                f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
                "Valid configurations (base model / tokenizer):\n"
                "- Continue pre-training original LLaMA: 32000 / 32000 \n"
                "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
                "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
                "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n"
            )
            
                model.resize_token_embeddings(len(tokenizer))
        
        if training_args.peft_path is not None:
            logger.info("PEFT from pretrained model")
            model = PeftModel.from_pretrained(model, training_args.peft_path)
        else:
            logger.info("Init new peft model")
            target_modules = training_args.trainable.split(",")
            modules_to_save = training_args.modules_to_save
            if modules_to_save is not None:
                modules_to_save = modules_to_save.split(",")
            lora_rank = training_args.lora_rank
            lora_dropout = training_args.lora_dropout
            lora_alpha = training_args.lora_alpha
            logger.info(f"Target modules: {target_modules}")
            logger.info(f"LoRA Rank: {lora_rank}")
            peft_config = LoraConfig(
                task_type = TaskType.CAUSAL_LM,
                targert_modules = target_modules,
                inference_mode=False,
                r = lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                modules_to_save=modules_to_save,
            )    
            
            model= get_peft_model(model, peft_config)
        model.print_trainable_parameters()
!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb
import os

from dotenv import load_dotenv




load_dotenv()




hf_token = os.getenv("hf_token")



import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
import pandas as pd

file_path = '../../merged.csv'




df = pd.read_csv(file_path)

df
dataset=df[['Text']]

dataset
dataset_2=dataset.copy()










dataset_2
!pip install scikit-learn



from sklearn.model_selection import train_test_split





train_val_data, test_data = train_test_split(dataset_2, test_size=0.20, random_state=42)

train_data, evaluation_data = train_test_split(train_val_data, test_size=0.10, random_state=42)



print('Training dataset shape:', len(train_data))

print('evaluation dataset shape:', len(evaluation_data))

print('Testing dataset shape:', len(test_data))
evaluation_data
import numpy as np


msk = np.random.rand(len(dataset_2)) < 0.8

train_dataset = dataset_2[msk]

test_dataset = dataset_2[~msk]

from datasets import Dataset



test_dataset=Dataset.from_pandas(test_dataset)



train_dataset=Dataset.from_pandas(train_dataset)



evaluation_dataset=Dataset.from_pandas(evaluation_data)
test_dataset



test_dataset=test_dataset.remove_columns("__index_level_0__")

train_dataset=train_dataset.remove_columns("__index_level_0__")

evaluation_dataset=evaluation_dataset.remove_columns("__index_level_0__")


import datasets


main_dataset= datasets.DatasetDict({

    'train': train_dataset,

    'test': test_dataset,

    'evaluate': evaluation_dataset

})
main_dataset
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    AutoTokenizer,

    TrainingArguments,

    pipeline,

)

from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training

from trl import SFTTrainer

base_model = "NousResearch/Llama-2-7b-hf"

new_model = "llama-2-7b-Amh"






tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)

tokenizer.pad_token = tokenizer.unk_token

tokenizer.padding_side = "right"

bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.float16,

    bnb_4bit_use_double_quant=True,

)




peft_config = LoraConfig(

    r=16,

    lora_alpha=32,

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM",

    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']

)

def load_model(model_name, bnb_config):

    n_gpus = torch.cuda.device_count()

    max_memory = f'{23000}MB'



load_model(base_model,bnb_config)
import torch

torch.cuda.empty_cache()


model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=bnb_config,

    device_map={"": 0}

)




model = prepare_model_for_kbit_training(model)
training_dataset=main_dataset
model
import torch




device = torch.device("cuda:0")




torch.cuda.empty_cache()




print(torch.cuda.memory_summary(device))

import torch

torch.cuda.empty_cache()

import torch




n_gpus = torch.cuda.device_count()

print(f"Number of available GPUs: {n_gpus}")




for i in range(n_gpus):

    gpu_memory = torch.cuda.get_device_properties(i).total_memory

    print(f"GPU {i}: Total memory: {gpu_memory / (1024**3)} GB")

from transformers import Trainer, TrainingArguments, BitsAndBytesConfig


training_arguments = TrainingArguments(

        output_dir="../results",

        num_train_epochs=1,

        per_device_train_batch_size=10,

        per_device_eval_batch_size=1,

        gradient_accumulation_steps=1,

        gradient_checkpointing=True,

        fp16=True,

        evaluation_strategy="steps",

        eval_steps=1000,

        logging_steps=1,

        optim="paged_adamw_8bit",

        learning_rate=2e-4,

        lr_scheduler_type="linear",

        warmup_steps=10,

        
        max_steps=10, 
)




trainer = SFTTrainer(

    model=model,

    train_dataset=main_dataset["train"],

    eval_dataset=main_dataset["evaluate"],

    peft_config=peft_config,

    dataset_text_field="Text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_arguments,

)


model.config.use_cache = False  



trainer.train()




trainer.model.save_pretrained(new_model)


prompt = "የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "how can i treat flu, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "tell me about ethiopian politics, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "who the prime minister of ethiopia, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "3 Ethiopian premier league club, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

del model

del pipe

del trainer

import gc

gc.collect()

gc.collect()
import torch

torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(

    base_model,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(model, new_model)

model = model.merge_and_unload()




tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_side = "right"
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils import utils
letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

util = utils.Util()
def clean_parsed_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return

    

    
    if not os.path.exists(cleaned_dir):

            os.makedirs(cleaned_dir)



    
    for file_name in os.listdir(folder_path):

        base_name, extension = os.path.splitext(file_name)

        print(base_name,extension)

        if extension =='.csv':

            df = pd.read_csv(f"{folder_path}/{file_name}", index_col='id')

            df = df.dropna()

            df = df.replace('\n', ' ', regex=True)

            
            df['hashtags'] = df['text'].apply(lambda x: util.extract_hashtags(x))

            df['text'] = df['text'].str.replace(r'\
            

            
            df['emojis'] = df['text'].apply(util.extract_emojis)

            


            for letter in letters:

                for i in range(len(letter[0])):

                    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

            
            df['symbols'] = df['text'].apply(util.extract_symbols)

            df['text'] = df['text'].apply(util.remove_symbols)

            
            df['links'] = df['text'].apply(util.extract_urls)

            df['text'] = df['text'].apply(util.remove_links)



            df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

            df['text'] = df['text'].replace(r'!+', '!', regex=True)

            df['text'] = df['text'].replace(r'\.+', '', regex=True)

            base_name, extension = os.path.splitext(file_name)

            df.to_csv(f"{cleaned_dir}/{base_name}.csv")

            df['text'].to_csv(f"{cleaned_dir}/{base_name}.txt", index=False, header=False)





        
clean_parsed_data(parsed_dir)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils.utils import Util
util = Util()
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"
def process_raw_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return



    
    for filename in os.listdir(folder_path):

        print(filename)

        data = util.read_file(f'{folder_path}/{filename}')

        parsed_message = util.parse_messages(data['messages'])



        
        df = pd.DataFrame(parsed_message)

        df.set_index('id', inplace=True)

        base_name, extension = os.path.splitext(filename)

        
        if not os.path.exists(parsed_dir):

            os.makedirs(parsed_dir)

        df.to_csv(f'{parsed_dir}/{base_name}.csv')

        
process_raw_data(raw_dir)
!pip install transformers


from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM



model_name = "Samuael/llama-2-7b-tebot-amharic"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(model_name)


input_text = "እኔ አማርኛ መናገር እረዳለሁ"




input_ids = tokenizer.encode(input_text, return_tensors="pt")




output_ids = model.generate(input_ids)




output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Amharic text:", output_text)
import json
import re


class Util:
    def __init__(self) -> None:
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"              "\U0001F300-\U0001F5FF"              "\U0001F680-\U0001F6FF"              "\U0001F700-\U0001F77F"              "\U0001F780-\U0001F7FF"              "\U0001F800-\U0001F8FF"              "\U0001F900-\U0001F9FF"              "\U0001FA00-\U0001FA6F"              "\U0001FA70-\U0001FAFF"              "\u2600-\u26FF"              "\u2700-\u27BF"              "\u2B50"              "\U0001F1E6-\U0001F1FF"              "]+",
            flags=re.UNICODE,
        )
        self.symbols = re.compile(
            "[" '"' "\“" '"' "'" "\-" "\*" "\•" "\ℹ" "\﻿" "\_" "]+"
        )
        self.url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        self.mention_pattern = r"@(\w+)"

    def read_file(self, file_path: str) -> dict:
                with open(file_path, "r") as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, "w") as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item["text"])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {"id": [], "text": [], "date": []}
        for message in messages:
            if message["type"] != "message" or len(message["text"]) == 0:
                continue
            parsed_messages["id"].append(message["id"])
            message_content = self.parse_text(message["text"])
            parsed_messages["text"].append(message_content)
            parsed_messages["date"].append(message["date"])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith("
    def extract_emojis(self, text):
        return "".join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub("", text)

    def extract_symbols(self, text):
        return "".join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(" ", text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def remove_links(self, text):
        return re.sub(self.url_pattern, " ", text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import argparse
from dataclasses import dataclass
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
import os
from langchain_openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

CHROMA_PATH = './chromadb/'

client = OpenAI(
    api_key=OPENAI_API_KEY
)

core_embeddings_model = None
def get_context():
        vectorstore = Chroma(persist_directory="./cachce",embedding_function=core_embeddings_model)
    
    retriever = vectorstore.as_retriever()
        return retriever

def generate_add(user_input, context):
    template = f'''
    Generate an advertisement given the following context.    
    You must use the following context:
    {context}
    '''   
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": template},{"role": "user", "content": user_input}],
        n=3,
    )

    return response
from peft import PeftModel
from transformers import LlamaForCausalLM, LlamaConfig

def load_model(model_name, quantization):
    model = LlamaForCausalLM.from_pretrained(
        model_name,
        return_dict=True,
        load_in_8bit=quantization,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    return model


def load_peft_model(model, peft_model):
    peft_model = PeftModel.from_pretrained(model, peft_model)
    return peft_model

def load_llama_from_config(config_path):
    model_config = LlamaConfig.from_pretrained(config_path) 
    model = LlamaForCausalLM(config=model_config)
    return model
import fire
import torch
import os
import sys
import time
import json 
from typing import List

from transformers import LlamaTokenizer, LlamaForCausalLM
from model_utils import load_model, load_peft_model

BASE_PROMPT = """Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers.
Human: {}
Assistant [Amharic] : """

def main(
    model_name: str="",
    peft_model: str=None,
    quantization: bool=False,
    max_new_tokens =400,     prompt_file: str=None,
    seed: int=42,     do_sample: bool=True,     min_length: int=None,     use_cache: bool=True,      top_p: float=1.0,     temperature: float=1.0,     top_k: int=1,     repetition_penalty: float=1.0,     length_penalty: int=1,     enable_azure_content_safety: bool=False,     enable_sensitive_topics: bool=False,     enable_saleforce_content_safety: bool=False,     **kwargs
):    
    
    print("***Note: model is not set up for chat use case, history is reset after each response.")
    print("***Ensure that you have replaced the default LLAMA2 tokenizer with the Amharic tokenizer")
    
        torch.cuda.manual_seed(seed)
    torch.manual_seed(seed)
    
    MAIN_PATH = '/model/Llama-2-7b-hf'
        peft_model = '/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'
    model_name = MAIN_PATH
    quantization = True
    model = load_model(model_name, quantization)

    tokenizer = LlamaTokenizer.from_pretrained(model_name)
    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))

    if peft_model:
        model = load_peft_model(model, peft_model)

    model.eval()

    while True:

        
        user_query = input('Type question in Amharic or English: ')
        user_prompt = BASE_PROMPT.format(user_query)
        batch = tokenizer(user_prompt, return_tensors="pt")
        batch = {k: v.to("cuda") for k, v in batch.items()}
        start = time.perf_counter()
        with torch.no_grad():
            outputs = model.generate(
                **batch,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                top_p=top_p,
                temperature=temperature,
                min_length=min_length,
                use_cache=use_cache,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                length_penalty=length_penalty,
                **kwargs 
            )
        e2e_inference_time = (time.perf_counter()-start)*1000
        print(f"the inference time is {e2e_inference_time} ms")
        
        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("MODEL_OUTPUT: {}".format(output_text))
        
if __name__ == "__main__":
    fire.Fire(main)
import torch
from contextlib import nullcontext
from transformers import (
    LlamaForCausalLM, 
    LlamaTokenizer, 
    TrainerCallback, 
    default_data_collator, 
    Trainer, 
    TrainingArguments
)
from peft import (
    LoraConfig,
    TaskType,
    prepare_model_for_int8_training,
    PeftModel
)

from pathlib import Path
from utils.dataset_utils import get_preprocessed_dataset
from configs.datasets import amharic_dataset

def print_trainable_parameters(model):
    print("Trainable Parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f" - {name}")


def finetune():
    LLAMA_DIR = '/model/Llama-2-7b-hf'
    PT_DIR = '/model/llama-2-amharic-3784m'
    OUTPUT_DIR = "/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output"

    tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)

    model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)


    train_dataset = get_preprocessed_dataset(tokenizer, amharic_dataset, 'train')


    model.train()



    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))


    print('loading the pretrained model from config')

    model = prepare_model_for_int8_training(model)
    model = PeftModel.from_pretrained(model, PT_DIR)
    model.print_trainable_parameters()
    lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            modules_to_save = ["embed_tokens","lm_head"]
        )

    enable_profiler = False


    config = {
        'lora_config': lora_config,
        'learning_rate': 1e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 1,
        'per_device_train_batch_size': 2,
        'gradient_checkpointing': False,
    }

        if enable_profiler:
        wait, warmup, active, repeat = 1, 1, 2, 1
        total_steps = (wait + warmup + active) * (1 + repeat)
        schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
        profiler = torch.profiler.profile(
            schedule=schedule,
            on_trace_ready=torch.profiler.tensorboard_trace_handler(f"{OUTPUT_DIR}/logs/tensorboard"),
            record_shapes=True,
            profile_memory=True,
            with_stack=True)

        class ProfilerCallback(TrainerCallback):
            def __init__(self, profiler):
                self.profiler = profiler

            def on_step_end(self, *args, **kwargs):
                self.profiler.step()

        profiler_callback = ProfilerCallback(profiler)
    else:
        profiler = nullcontext()


        training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        bf16=True,                  logging_dir=f"{OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=10,
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=1,
        warmup_ratio=0.03,
        optim="adamw_torch_fused",
        max_steps=total_steps if enable_profiler else -1,
        **{k:v for k,v in config.items() if k != 'lora_config'}
    )

    with profiler:
                trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=default_data_collator,
            callbacks=[profiler_callback] if enable_profiler else [],
        )

        print_trainable_parameters(model)

                trainer.train()

    model.save_pretrained(OUTPUT_DIR)


finetune()
from dataclasses import dataclass

@dataclass
class amharic_dataset:
    dataset: str = "amharic_dataset"
    train_split: str = "train"
    test_split: str = "val"
    data_path: str = "/data/fine_tun_data6.json"
import copy
import json
import torch


from torch.utils.data import Dataset



class InstructionDataset(Dataset):
    def __init__(self, dataset_config, tokenizer, partition="train", max_words=50):
        self.ann = json.load(open(dataset_config.data_path))

        if partition == "train":
            self.ann = self.ann
        else:
            self.ann = self.ann[:200]

        self.max_words = max_words
    
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.ann)

    def __getitem__(self, index):

        ann = self.ann[index]
        data = self.create_prompt_formats(ann)
        prompt = data['prompt']
        example = data['text']  
        prompt = torch.tensor(
            self.tokenizer.encode(prompt), dtype=torch.int64
        )

        example = self.tokenizer.encode(example)

        example.append(self.tokenizer.eos_token_id)
        example = torch.tensor(
            example, dtype=torch.int64
        )
        padding = self.max_words - example.shape[0]

        if padding > 0:
            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - 1))
        elif padding < 0:
            example = example[: self.max_words]

        labels = copy.deepcopy(example)

        labels[: len(prompt)] = -1

        example_mask = example.ge(0)
        label_mask = labels.ge(0)
        example[~example_mask] = 0
        labels[~label_mask] = 0
        example_mask = example_mask.float()
        label_mask = label_mask.float()

        return {
            "input_ids": example,
            "labels": labels,
            "attention_mask":example_mask,
        }
    
    def create_prompt_formats(self,sample):
        """
        Format various fields of the sample ('text', 'label',)
        Then concatenate them using two newline characters
        :param sample: Sample dictionnary
        """

        INTRO_BLURB = "Generate an advertisement given a category"
        INSTRUCTION_KEY = "        RESPONSE_KEY = "Response:"
        END_KEY = "
        blurb = f"{INTRO_BLURB}"
        text = f"{INSTRUCTION_KEY}\n{sample['input']}"
        response = f"{RESPONSE_KEY}\n{sample['output']}"
        end = f"{END_KEY}"

        parts = [part for part in [blurb, text, response, end] if part]

        formatted_prompt = "\n\n".join(parts)

        sample["text"] = formatted_prompt
        parts = [part for part in [blurb, text,] if part]
        formatted_prompt = "\n\n".join(parts)

        sample["prompt"]= formatted_prompt

        return sample
"""
Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.

Here is the full list of checkpoints on the hub that can be fine-tuned by this script:
https://huggingface.co/models?filter=text-generation
"""

import logging
import numpy as np
import math
import os
import sys
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Dict, Any, Mapping
from pathlib import Path
import datasets
import torch
from datasets import load_dataset, concatenate_datasets

import transformers
from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    is_torch_tpu_available,
    set_seed,
)
from transformers.testing_utils import CaptureLogger
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version

from sklearn.metrics import accuracy_score
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora_model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")

        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)


def accuracy(predictions, references, normalize=True, sample_weight=None):
        return {
            "accuracy": float(
                accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
            )
        }


def compute_metrics(eval_preds):
    preds, labels = eval_preds
            labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)


def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
                        logits = logits[0]
    return logits.argmax(dim=-1)


def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}

                if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["labels"] = torch.tensor([f["label"] for f in features], dtype=dtype)
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)

        
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else:
                    batch[k] = torch.tensor([f[k] for f in features])
    except ValueError:         for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else:
                    batch[k] = torch.tensor([features[0][k]] * len(features))

    return batch


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The tokenizer for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides can't be used in combination with --config_name or --model_name_or_path"
            )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    dataset_dir: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional input sequence length after tokenization. "
                "The training dataset will be truncated in block of this size for training. "
                "Default to the model max input length for single sentence inputs (take into account special tokens)."
            )
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep line breaks when using TXT files or not."}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed stored"})

    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")


@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj,v_proj")
    lora_rank : Optional[int] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.1)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[bool] = field(default=False)
    peft_path : Optional[str] = field(default=None)


logger = logging.getLogger(__name__)


def main():

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, MyTrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

            send_example_telemetry("run_clm", model_args, data_args)

        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,          handlers=[logging.StreamHandler(sys.stdout)],)


    if training_args.should_log:
                transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
        logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )

        last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

        set_seed(training_args.seed)

    config_kwargs = {
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else:
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("You are instantiating a new config instance from scratch.")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")

    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script."
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )

                tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            output = tokenizer(examples["text"])
                if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return output
    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)

        def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
                        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
                result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    with training_args.main_process_first(desc="dataset map tokenization and grouping"):
        lm_datasets = []
        path = Path(data_args.dataset_dir)
        files = [file.name for file in path.glob("*.txt")]
        if training_args.debug_mode is True:
            files = [files[0]]
        print("printing files")
        print(files)    
        for idx, file in enumerate(files):
            data_file = os.path.join(path, file)
            filename = ''.join(file.split(".")[:-1])
            cache_path = os.path.join(data_args.data_cache_dir, filename)
            os.makedirs(cache_path, exist_ok=True)
            try:
                processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=False)
                logger.info(f'training datasets-{filename} has been loaded from disk')
            except Exception:
                cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                os.makedirs(cache_dir, exist_ok=True)
                raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                logger.info(f"{file} has been loaded")
                tokenized_dataset = raw_dataset.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns="text",
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'tokenized.arrow') for k in raw_dataset},
                    desc="Running tokenizer on dataset",
                )
                grouped_datasets = tokenized_dataset.map(
                    group_texts,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'grouped.arrow') for k in tokenized_dataset},
                    desc=f"Grouping texts in chunks of {block_size}",
                )
                processed_dataset = grouped_datasets
                processed_dataset.save_to_disk(cache_path)
            if idx == 0:
                lm_datasets = processed_dataset['train']
            else:
                assert lm_datasets.features.type == processed_dataset["train"].features.type
                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset["train"]])

        lm_datasets = lm_datasets.train_test_split(test_size = data_args.validation_split_percentage)

    if training_args.do_train:
        train_dataset = lm_datasets['train']
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))
        logger.info(f"Num train_samples  {len(train_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(train_dataset[0]['input_ids']))
    if training_args.do_eval:
        eval_dataset = lm_datasets["test"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        logger.info(f"Num eval_samples  {len(eval_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(eval_dataset[0]['input_ids']))



    if model_args.model_name_or_path:
        torch_dtype = (
            model_args.torch_dtype
            if model_args.torch_dtype in ["auto", None]
            else getattr(torch, model_args.torch_dtype)
        )
        model = LlamaForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool(".ckpt" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            use_auth_token=True if model_args.use_auth_token else None,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
        )
    else:
        model = AutoModelForCausalLM.from_config(config)
        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")

    model_vocab_size = model.get_output_embeddings().weight.size(0)
    if not (
       (model_vocab_size==32000 and len(tokenizer)==51008) or \
       (model_vocab_size==32000 and len(tokenizer)==32000) or \
       (model_vocab_size==51008 and len(tokenizer)==51008) or \
       (model_vocab_size==49954 and len(tokenizer)==49954)
       
    ):
        raise ValueError(
            f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
            "Valid configurations (base model / tokenizer):\n"
            "- Continue pre-training original LLaMA: 32000 / 32000 \n"
            "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
            "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
            "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n")

    model.resize_token_embeddings(len(tokenizer))
    if training_args.peft_path is not None:
        logger.info("Peft from pre-trained model")
        model = PeftModel.from_pretrained(model, training_args.peft_path)
    else:
        logger.info("Init new peft model")
        target_modules = training_args.trainable.split(',')
        modules_to_save = training_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')
        lora_rank = training_args.lora_rank
        lora_dropout = training_args.lora_dropout
        lora_alpha = training_args.lora_alpha
        logger.info(f"target_modules: {target_modules}")
        logger.info(f"lora_rank: {lora_rank}")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=lora_rank, lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            modules_to_save=modules_to_save)
        model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

                                

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
        modules_to_save = ["embed_tokens","lm_head"]
    )

    """
        config = {
        'lora_config': lora_config,
        'learning_rate': 2e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 2,
        'per_device_train_batch_size': 1,
        'per_device_eval_batch_size': 1,
        'gradient_checkpointing': True,
    }
    """


    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=fault_tolerance_data_collator,
        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
        if training_args.do_eval and not is_torch_tpu_available()
        else None,
    )

    """
    output_dir=training_args.output_dir,
    overwrite_output_dir=True,
    bf16=True,          logging_dir=f"{training_args.output_dir}/logs",
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="steps",
    save_steps=5000,
    optim="adamw_torch_fused",
    max_steps=-1,     **{k:v for k,v in config.items() if k != 'lora_config'}
    """

    trainer.add_callback(SavePeftModelCallback)
        if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)

        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
    model.save_pretrained(training_args.output_dir)
        if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()

        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
        try:
            perplexity = math.exp(metrics["eval_loss"])
        except OverflowError:
            perplexity = float("inf")
        metrics["perplexity"] = perplexity

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)


if __name__ == "__main__":
    main()
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=/model/Llama-2-7b-hf
amharic_tokenizer_path=/model/Llama-2-7b-hf
dataset_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/data/cleaned
data_cache=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/cache
per_device_train_batch_size=32
per_device_eval_batch_size=1
gradient_accumulation_steps=1
output_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output

python pretrain.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${amharic_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 1 \
    --save_steps 7528 \
    --evaluation_strategy steps \
    --eval_steps 3000 \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --bf16 \
    --overwrite_output_dir \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --gradient_checkpointing \
import pandas as pd

df = pd.read_json("/data/fine_tun_data2.json")

df.tail()
df[df['output']=='not advertisement'].iloc[1]['input']
'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: ለኢሬቻ በኣል ወደቢሾፍቱ የተጓዙት የሲዳማ ወጣቶች(ኤጄቶዎች) ከሰኣታት በፊት ቢሾፍቱ ገብተዋል። @tsegabwolde @tikvahethiopia''
import pandas as pd

df = pd.read_csv("/data/wasu_mohammed_labeled.csv")

df.head()
df.shape
df['label'] = df['label'].fillna("Not Advertisement")

df.tail(5)
df
from datasets import Dataset




data_dict = {"text": df['text'].tolist()}




dataset = Dataset.from_dict(data_dict)

dataset.save_to_disk("../data/train")




from peft import PeftModel

from transformers import LlamaForCausalLM, LlamaConfig




def load_model(model_name, quantization):

    model = LlamaForCausalLM.from_pretrained(

        model_name,

        return_dict=True,

        load_in_8bit=quantization,

        device_map="auto",

        low_cpu_mem_usage=True,

    )

    return model






def load_peft_model(model, peft_model):

    peft_model = PeftModel.from_pretrained(model, peft_model)

    return peft_model




def load_llama_from_config(config_path):

    model_config = LlamaConfig.from_pretrained(config_path) 

    model = LlamaForCausalLM(config=model_config)

    return model
from transformers import LlamaTokenizer





MAIN_PATH = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)



example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685





tokens = tokenizer.tokenize(example)

print(tokens)
print(len(tokenizer))
example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685




tokens = tokenizer.tokenize(example)

print(tokens)
df['text'][0]
df = pd.read_csv("/data/wasu_mohammed_labeled.csv")




total_word_count = 0

total_tokens = 0


for index, row in df.iterrows():

    
    text = row['text']

    if not isinstance(text, str): 

        continue



    

    
    word_count = len(text.split())



    
    total_word_count += word_count

    tokens = tokenizer.tokenize(text)

    total_tokens+=tokens

    print(tokens)

    






print("Total Word Count:", total_word_count)

print("Total tokens count: ",total_tokens)
total_tokens
df.shape
from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    HfArgumentParser,

    TrainingArguments,

    pipeline,

    LlamaForCausalLM, 

    LlamaTokenizer,

    logging,

)

from peft import (

    LoraConfig,

    TaskType,

    prepare_model_for_int8_training,

    PeftModel

)

import torch

LLAMA_DIR = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)



model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)

embedding_size = model.get_input_embeddings().weight.shape[0]



if len(tokenizer) != embedding_size:

    print("resize the embedding size by the size of the tokenizer")

    model.resize_token_embeddings(len(tokenizer))





new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'

model = PeftModel.from_pretrained(model, new_model)




prompt = "Who is Leonardo Da Vinci?"

pipe = pipeline(task="text-generation", model=model, tokenizer=model, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
ቨርቹዋል ረዳቶች እንደ Amazon&
run-20240203_155644-4hm9i4tp
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from transformers import AutoTokenizer, AutoModel
import os
import torch
import shutil
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

model_name = 'Davlan/bert-base-multilingual-cased-finetuned-amharic'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

data_path = '../data/'
CHROMA_PATH = '../RAG/chromadb'

def embed_doc(document):
    
        encoded_input = tokenizer(document, padding=True, truncation=True, return_tensors='pt')
    
        with torch.no_grad():
        outputs1 = model(**encoded_input)

    

        embeddings1 = outputs1.last_hidden_state.squeeze(dim=0)
    

        
    return embeddings1

def load_documents(data_path):    
    try:
        loader = DirectoryLoader(data_path)
        documents = loader.load()       
        print("data loaded sucessfully")
        return documents[0].page_content
    except:
        print("document not found!")
        return None
    

def split_text(documents:list[Document]):
    try:
        text_spliter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=500,
            length_function=len,
            add_start_index = True
        )
        chunk = text_spliter.split_documents(documents)
        print("data splited successfuly!")
        return chunk
    except:
        print("document not found")

def save_chunks_to_chroma(chunks):
        if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
    try:
        db = Chroma.from_documents(chunks,embed_doc(),\
                                persist_directory=CHROMA_PATH)
        db.persist()
        print("Vectorstore created successfully!")
    except:
        print("Couldn't create the vectore database")

def generate_data_store():
    documents = load_documents(data_path)
    chunks = split_text(documents)
    embeding1 = embed_doc(chunks)
    print(embeding1)
    save_chunks_to_chroma(embeding1) 

def main():
    generate_data_store()      


if __name__ == "__main__":
    main()
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{parsed_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape
df = df.dropna()

df.head()
df.shape
df = df.replace('\n', ' ', regex=True)

df.head()

df['hashtags'] = df['text'].apply(lambda x: preprocess.extract_hashtags(x))

df.head()

df['text'] = df['text'].str.replace(r'\
df.head()

df['emojis'] = df['text'].apply(preprocess.extract_emojis)

df.tail()

df['text'] = df['text'].apply(preprocess.remove_emojis)






letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

    

df['symbols'] = df['text'].apply(preprocess.extract_symbols)

df.tail()
df['text'] = df['text'].apply(preprocess.remove_symbols)

df.tail()
df['links'] = df['text'].apply(preprocess.extract_urls)

df.tail()
df['text'] = df['text'].str.replace(preprocess.url_pattern, '', regex=True).str.strip()

df.tail()
df['mentions'] = df['text'].apply(preprocess.extract_mentions)

df.tail()
df['text'] = df['text'].str.replace(preprocess.mention_pattern, '', regex=True).str.strip()

df.tail()
df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()
df['text'] = df['text'].replace(r'!+', '!', regex=True)

df['text'] = df['text'].replace(r'\.+', '', regex=True)
df.tail()
df.to_csv(f"{cleaned_dir}/{file_name}.csv")
df['text'].to_csv(f"{cleaned_dir}/{file_name}.txt", index=False, header=False)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
data = preprocess.read_file(f'{raw_dir}/{file_name}.json')
parsed_message = preprocess.parse_messages(data['messages'])


df = pd.DataFrame(parsed_message)

df.set_index('id', inplace=True)

df.head()
df.shape
df.to_csv(f'{parsed_dir}/{file_name}.csv')
import sys, os

import pandas as pd

from gensim.models import Word2Vec

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
cleaned_dir = "../data/cleaned"

final_dir = "../data/final"

file_name = "TIKVAH"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{cleaned_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape

tokenized_corpus = [str(sentence).lower().split() for sentence in df['text']]

embedding_size = 100 
window_size = 5 
min_count = 5 
model = Word2Vec(sentences=tokenized_corpus, vector_size=embedding_size, window=window_size, min_count=min_count, workers=4)

vector = model.wv['ኢትዮጵያ'] 
vector

similar_words = model.wv.most_similar('ኢትዮጵያ', topn=5)

similar_words

model.save(f'{final_dir}/{file_name}_word2vec_model.bin')
import os 

from huggingface_hub import hf_hub_download
HUGGING_FACE_API_KEY = os.environ.get("HUGGING_FACE_API_KEY")
model_id = "iocuydi/llama-2-amharic-3784m"

filenames = [

    ".gitattributes", "adapter_config.json", "adapter_model.bin", "config.json", "generation_config.json", 

    "inference_demo.py", "special_tokens_map.json", "tokenizer.json", "tokenizer.model", "tokenizer_config.json"

]
for filename in filenames:

    downloaded_model_path = hf_hub_download(

        repo_id = model_id,

        filename = filename,

        token = HUGGING_FACE_API_KEY

    )

    print(downloaded_model_path)
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM



tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)

model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

pipeline = pipeline ("Text-Generation", model=model, device=-1, tokenizer=tokenizer, max_length=1000 )
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from PyPDF2 import PdfReader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,
)



persist_directory = "db"


class MySpecialFunctions:
    def __init__(self):
        pass
    
    def get_file_text(self, files):
        text = ""
        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    text += content
            except Exception as e:
                print(f"Error reading file {file}: {e}")

        return text
    
    
    def get_pdf_text(self, pdf):
        text = ""
        for doc in pdf:
            reader = PdfReader(doc)
            for page in reader.pages:
                text += page.extract_text()

        return text


    
    
    def get_text_chunks(self, text):
        text_siplitter = RecursiveCharacterTextSplitter(
            chunk_size = 1000, 
            chunk_overlap = 200,
            separators=['\n', '\n\n'],
            length_function = len)
        chunk = text_siplitter.split_text(text)
        return chunk
    

    def get_vectorstore(self, chunks):
                        hf_embedding = HuggingFaceEmbeddings()

                                
                embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        
                vector_db = Chroma.from_documents(
            documents = chunks,
            embedding = hf_embedding,
                    )

        return vector_db
import streamlit as st 
from MyFunctions import MySpecialFunctions
from dotenv import load_dotenv


special_functions_instance = MySpecialFunctions()

def get_text(external_data ):
    return special_functions_instance.get_file_text(external_data )

def get_pdf_text(external_data ):
    return special_functions_instance.get_pdf_text(external_data )

def get_chunks(text):
    return special_functions_instance.get_text_chunks(text)

def get_vectorstore(text_chunks):
    return special_functions_instance.get_vectorstore(text_chunks) 





def main():
    load_dotenv()
    st.set_page_config(page_title="Generation of Telegram Ads in Amharic", page_icon= ":smile")
    

    with st.sidebar:
                pass

    st.markdown("    external_data = st.file_uploader(" Upload the generative text from fune-tuning", accept_multiple_files= True)
    if st.button("Retrieval"):
        with st.spinner("Processing"):
                                    text = get_pdf_text(external_data )
            
                        text_chunks = get_chunks(text)
            st.write(text_chunks)


                        vectorstore_db = get_vectorstore(text_chunks)
                                    




if __name__ == "__main__":
    main()
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')




sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))




print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))
print(sp.decode_ids([460, 133, 774, 1276]))


print(sp.get_piece_size())




print(sp.id_to_piece(460))

print(sp.piece_to_id('▁በአዲስ'))




print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))





for id in range(3):

  print(sp.id_to_piece(id), sp.is_control(id))


import tensorflow as tf




serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()



sp = spm.SentencePieceProcessor()

sp.load_from_serialized_proto(serialized_model_proto)



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))










spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')



sp_ctrl = spm.SentencePieceProcessor()

sp_ctrl.load('m_ctrl.model')




print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))

print(sp_ctrl.piece_to_id('<sep>'))  
print(sp_ctrl.piece_to_id('<cls>'))  
print('3=', sp_ctrl.decode_ids([3]))  
print('4=', sp_ctrl.decode_ids([4]))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   


sp = spm.SentencePieceProcessor()

sp.load('m_bos_as_user.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   

spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')



print('bos=', sp.bos_id())

print('eos=', sp.eos_id())

print('unk=', sp.unk_id())

print('pad=', sp.pad_id())  




print(sp.encode_as_ids('በአዲስ አበባ'))




print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])


spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')





for n in range(10):

  print(sp.sample_encode_as_pieces('በአዲስ አበባ', -1, 0.1))



for n in range(10):

  print(sp.sample_encode_as_ids('በአዲስ አበባ', -1, 0.1))

print(sp.nbest_encode_as_pieces('በአዲስ አበባ', 10))

print(sp.nbest_encode_as_ids('በአዲስ አበባ', 10))


spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')

sp_bpe = spm.SentencePieceProcessor()

sp_bpe.load('m_bpe.model')



print('*** BPE ***')

print(sp_bpe.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_bpe.nbest_encode_as_pieces('በአዲስ አበባ', 5))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')

sp_unigram = spm.SentencePieceProcessor()

sp_unigram.load('m_unigram.model')



print('*** Unigram ***')

print(sp_unigram.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_unigram.nbest_encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ', 5))
! pip install transformers bitsandbytes peft trl accelerate
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
base_model = "NousResearch/Llama-2-7b-chat-hf"

guanaco_dataset = "mlabonne/guanaco-llama2-1k"

new_model = "LLama-2-7b-chat-prac"
print(peft.__version__)
dataset = load_dataset(guanaco_dataset, split="train")
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=compute_dtype,

    bnb_4bit_use_double_quant=False,

)
model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=quant_config,

    device_map={"": 0}

)



model.config.use_cache = False

model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_size= "right"
peft_params = LoraConfig(

    lora_alpha = 16,

    lora_dropout = 0.1,

    r=64,

    bias="none",

    task_type= "CAUSAL_LM"

)
training_params = TrainingArguments(

    output_dir = "./results",

    num_train_epochs=1,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim="paged_adamw_32bit",

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-4,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=-1,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type="constant",

    report_to="tensorboard"

)
!pip install --upgrade peft
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
import pandas as pd

import numpy as np

import sys, os

sys.path.append(os.path.abspath(os.path.join('../scripts')))

from util import Util

from concurrent.futures import ThreadPoolExecutor
util = Util()

cleaned_dir = "../cleaned"

json_file_path = '../raw/dilela_page.json'




df = pd.read_json(json_file_path)

df.head()
df.shape

columns = ["id", "channel_name", "type", "message_id", "message_type","text", "label", "created_at", "updated_at", ]

new_df = pd.DataFrame(columns=columns)

new_df

telegeram_channel_id  = df["id"][0]

telegram_channel_name = df["name"][0]

telegeram_channel_type = df["type"][0]

message_df = df["messages"]

data = [{

       'telegeram_channel_id': telegeram_channel_id,

       'telegram_channel_name': telegram_channel_name,

       'telegeram_channel_type': telegeram_channel_type,

       'message_id': message.get('id', np.nan),

        'message_type': message.get('type', np.nan),

        'text': message['text_entities'][0]['text'] if message.get('text_entities') and message['text_entities'] else np.nan,

        'created_at': message.get('date', np.nan),

        'update_at': message.get('edited', np.nan),

        }for message in message_df]

message_df = pd.DataFrame(data)

message_df = message_df.sort_values(by='message_id')

message_df.head(20)
message_df.shape

nan_rows_count = message_df.isna().any(axis=1).sum()

nan_rows_count

message_df = message_df.dropna()

message_df.head()
message_df.shape
message_df = message_df.replace('\n', ' ', regex=True)

message_df.head()

message_df["hashtags"] = message_df['text'].apply(lambda text: util.extract_hashtags(text))

message_df.head()

message_df["text"] = message_df["text"].str.replace(r'\
message_df.head()
message_df["emojis"] = message_df["text"].apply(util.extract_emojis)

message_df.head()

message_df['text'] = message_df['text'].apply(util.remove_emojis_using_emoji_pattern)

message_df.tail()
def remove_emojis_parallel(text):

    return util.remove_emojis(text)





with ThreadPoolExecutor() as executor:

    message_df['text'] = list(executor.map(remove_emojis_parallel, message_df['text']))
message_df.head()

message_df.replace('', pd.NA, inplace=True)

nan_rows_count = message_df.isna().any(axis=1).sum()


message_df = message_df.dropna()

message_df.head()





letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    message_df['text'] = message_df['text'].str.replace(letter[0][i], letter[1][i])
message_df['symbols'] = message_df['text'].apply(util.extract_symbols)

message_df.head()
message_df['text'] = message_df['text'].apply(util.remove_symbols)

message_df.tail()
message_df['links'] = message_df['text'].apply(util.extract_urls)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

message_df.head()
message_df['mentions'] = message_df['text'].apply(util.extract_mentions)

message_df.head()
message_df['text'] = message_df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

message_df.tail()

message_df['text'] = message_df['text'].str.replace('\s+', ' ', regex=True).str.strip()
message_df['text'] = message_df['text'].replace(r'!+', '!', regex=True)

message_df['text'] = message_df['text'].replace(r'\.+', '', regex=True)
message_df.head()

nan_rows_count = message_df['text'].isna().sum()

nan_rows_count


message_df = message_df.dropna(subset='text')

message_df.tail()


message_df = message_df[message_df['text'].str.len() >= 20]
message_df.to_csv(f"{cleaned_dir}/dilela_page.csv")
message_df['text'].to_csv(f"{cleaned_dir}/dilela_page.txt", index=False, header=False)
df = pd.read_csv(f"{cleaned_dir}/dilela_page.csv")

df.head()
df['word_count'] = df['text'].str.split().str.len()

df.columns


df_labeled = df.drop(['Unnamed: 0','telegram_channel_name','telegeram_channel_type','message_id','message_type','hashtags', 'emojis', 'created_at','symbols', 'links','mentions'],axis=1)

df_labeled.rename(columns={'update_at':'date','telegeram_channel_id':'channel_id'},inplace=True)

df_labeled.to_csv(f"{cleaned_dir}/dilela_page_labeled.csv")

len = df_labeled['word_count'].sum()

len
from fastapi import FastAPI, HTTPException, Depends
from typing import Annotated, List
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from utils import simple_rag
from utils import hugging_face_hub

app = FastAPI()

origins = ["http://localhost:5173"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
)


class RagResponseBase(BaseModel):
    question: str
    answer: str


class HugResponseBase(BaseModel):
    question: str
    answer: str


@app.get("/getanswer", response_model=RagResponseBase)
async def return_answer(question: str):
    result = simple_rag.test_RAG(question)
    return result


@app.get("/getHuggingFaceAnswer", response_model=HugResponseBase)
async def return_answer(model: str, prompt: str):
    result = hugging_face_hub.invoke_current_hugging_face_model(model, prompt)
    return result
from langchain import OpenAI
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from models import simple_rag_response
import os
from dotenv import load_dotenv

load_dotenv()


def load_data():
    loader = TextLoader("/week_6_challenge_doc.txt")
    documents = loader.load()
    return documents


def return_chunks(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)
    texts = text_splitter.split_documents(documents)
    return texts


def return_chain(texts):
    embeddings = OpenAIEmbeddings()
    store = Chroma.from_documents(
        texts, embeddings, collection_name="challenge_document"
    )
    llm = OpenAI(temperature=0)
    return RetrievalQA.from_chain_type(llm, retriever=store.as_retriever())


def test_RAG(question):
    documents = load_data()
    chunks = return_chunks(documents)
    chain = return_chain(chunks)
    response = chain.run(question)
    return simple_rag_response.RagResponse(question, response)
import React, { useState , useRef} from 'react'
import 'bootstrap/dist/css/bootstrap.css'
import FileInput from  './components/FileInput'
import TextInputWithLable from  './components/TextInputWithLable'
import Dropdown from './components/Dropdown'
import NavBarComp from './components/Navbar'
import SpinnerWithText from './components/SpinnerWithText'
import api from './api/api'
function App() {
  const [answer,setAnswer] = useState([]);
  const [isShow,setShow] = useState(false);

  const fetchResponse = async () =>{
    console.log(ref.current.value);
    const question = ref.current.value;
    setShow(true)
    const response = await api.get('/getanswer?question='+question);
    console.log(response.data);
    setAnswer(response.data)
    setShow(false)
  }
  const ref = useRef(null);

  return (
    <React.Fragment>
      
      <NavBarComp />
      <main className='container'>
        <form className="row g-3" >
          
          <div>
            <label htmlFor="inputLable" className="form-label">Input Ad description to be generated</label>
            <textarea className="form-control" id="inputTextarea" rows="7" ref={ref}/>
          </div>

          {isShow && <SpinnerWithText />}

          <button type="button" className="btn btn-primary mb-4" onClick={fetchResponse}>Get Ad</button> 

          <div>
            <TextInputWithLable value= {answer}/>
          </div>

        </form>
      </main>

    </React.Fragment>
  )
}

export default App
import pandas as pd

import json

import os

from pprint import pprint

import bitsandbytes as bnb

import torch

import torch.nn as nn

import transformers

from datasets import load_dataset, Dataset

from huggingface_hub import notebook_login



from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipelines, logging
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa' 
dataset = load_dataset(dataset_name, split="train")
MODEL_NAME = "mistralai/Mistral-7B-v0.1"

new_model = "amharic-mistral-7b"

config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, load_in_4bit=True)






bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_use_double_quant=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.bfloat16

)





model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    device_map="auto",

    trust_remote_code=True,

    quantization_config=bnb_config,

)



tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tokenizer.pad_token = tokenizer.eos_token
model = prepare_model_for_kbit_training(model)
use_4bit = True




bnb_4bit_compute_dtype = "float16"




bnb_4bit_quant_type = "nf4"




use_nested_quant = False

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

if compute_dtype == torch.float16 and use_4bit:

    major, _ = torch.cuda.get_device_capability()

    if major >= 8:

        print("=" * 80)

        print("Your GPU supports bfloat16: accelerate training with bf16=True")

        print("=" * 80)
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=2,

    lora_alpha=32,

    target_modules=get_last_layer_linears(model),

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM"

)



model = get_peft_model(model, config)





output_dir = "./results"




num_train_epochs = 1




fp16 = False

bf16 = False




per_device_train_batch_size = 4




per_device_eval_batch_size = 4




gradient_accumulation_steps = 1




max_grad_norm = 0.3




learning_rate = 2e-4




weight_decay = 0.001




optim = "paged_adamw_32bit"




lr_scheduler_type = "constant"




warmup_ratio = 0.03





group_by_length = True




save_steps = 25




logging_steps = 25


base_model = AutoModelForCausalLM.from_pretrained(

    MODEL_NAME,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(base_model, new_model)

model = model.merge_and_unload()

training_arguments = TrainingArguments(

    output_dir=output_dir,

    num_train_epochs=num_train_epochs,

    per_device_train_batch_size=per_device_train_batch_size,

    gradient_accumulation_steps=gradient_accumulation_steps,

    optim=optim,

    save_steps=save_steps,

    logging_steps=logging_steps,

    learning_rate=learning_rate,

    weight_decay=weight_decay,

    fp16=fp16,

    bf16=bf16,

    max_grad_norm=max_grad_norm,

    max_steps=25,

    warmup_ratio=warmup_ratio,

    group_by_length=group_by_length,

    lr_scheduler_type=lr_scheduler_type,

    report_to="tensorboard"

)
from trl import SFTTrainer

trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=peft_params,

    dataset_text_field="text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_params,

    packing=False,

)
trainer.train()
class RagResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass


class HugResponse:
    def __init__(self, question, answer) -> None:
        self.question = question
        self.answer = answer
        pass
from dotenv import load_dotenv
from models import simple_rag_response

load_dotenv()

from langchain import HuggingFaceHub


def invoke_current_hugging_face_model(model, prompt):
    llm = HuggingFaceHub(
        repo_id=model, model_kwargs={"temperature": 0, "max_length": 64}
    )
        response = llm(prompt)
    return simple_rag_response.HugResponse(prompt, response)
import time

import sentencepiece as spm
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=m --vocab_size=100000')

sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))




print(sp.encode_as_pieces('ሃይ ሰላም ናችሁ?'))



spm.SentencePieceTrainer.train('--input=merged_text.txt --model_prefix=am-word --model_type=word  --vocab_size=100000')



sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር?'))

print(sp.encode_as_ids('የኢትዮጽያ ጂዲፒ ምን ያህል ነበር?'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

sp = spm.SentencePieceProcessor()

sp.load('am-word.model')



print(sp.encode_as_pieces('የፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.encode_as_ids('ፒዛ ምግብ ቤት ለመክፈት የሚጠቅሙ ምክሮች'))

print(sp.decode_ids([47914, 1024, 33, 7716, 29922, 26700]))
import json
import re


class Util():
    def __init__(self) -> None:
        self.emoji_pattern = re.compile("["
                                        u"\U0001F600-\U0001F64F"                                          u"\U0001F300-\U0001F5FF"                                          u"\U0001F680-\U0001F6FF"                                          u"\U0001F700-\U0001F77F"                                          u"\U0001F780-\U0001F7FF"                                          u"\U0001F800-\U0001F8FF"                                          u"\U0001F900-\U0001F9FF"                                          u"\U0001FA00-\U0001FA6F"                                          u"\U0001FA70-\U0001FAFF"                                          u"\u2600-\u26FF"                                          u"\u2700-\u27BF"                                          u"\u2B50"                                          u"\U00002049 \U0000FE0F"                                         u"\U0000203C"                                         u"\U0001F1E6-\U0001F1FF"                                          "]+", flags=re.UNICODE)
        self.symbols = re.compile("["
                                  "\""
                                  "\“"
                                  "\""
                                  "\'"
                                  "\-"
                                  "\*"
                                  "\•"
                                  "\ℹ"
                                  "\﻿"
                                  "\_"
                                  "]+")
        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        self.mention_pattern = r'@(\w+)'
        print(self.emoji_pattern.pattern)

    def read_file(self, file_path: str) -> dict:
                with open(file_path, 'r') as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, 'w') as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item['text'])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {
            'id': [],
            'text': [],
            'date': []
        }
        for message in messages:
            if message['type'] != 'message' or len(message['text']) == 0:
                continue
            parsed_messages['id'].append(message['id'])
            message_content = self.parse_text(message['text'])
            parsed_messages['text'].append(message_content)
            parsed_messages['date'].append(message['date'])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith('
    def extract_emojis(self, text):
        return ''.join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub(' ', text)
        
    def extract_symbols(self, text):
        return ''.join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(' ', text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import pandas as pd

import csv, os, sys

from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import
"""
Convert .txt to csv

"""

import csv
from sklearn.model_selection import train_test_split

def convert_txt_to_csv(input_txt, output_csv):
    with open(input_txt, 'r', encoding='utf-8') as infile, open(output_csv, 'w', encoding='utf-8', newline='') as outfile:
       
        reader = infile.readlines()
        data = [line.strip().split() for line in reader]
        csv_writer = csv.writer(outfile)
        csv_writer.writerows(data)

def split_data(input_csv, output_train_csv, output_test_csv, output_val_csv, test_size=0.2, val_size=0.1, random_seed=42):
    with open(input_csv, 'r', encoding='utf-8') as file:
        csv_reader = csv.reader(file)
        data = list(csv_reader)
        
    train_data, test_val_data = train_test_split(data, test_size=(test_size + val_size), random_state=random_seed)
    test_data, val_data = train_test_split(test_val_data, test_size=(val_size / (test_size + val_size)), random_state=random_seed)

    with open(output_train_csv, 'w', encoding='utf-8', newline='') as train_file:
        csv_writer = csv.writer(train_file)
        csv_writer.writerows(train_data)

    with open(output_test_csv, 'w', encoding='utf-8', newline='') as test_file:
        csv_writer = csv.writer(test_file)
        csv_writer.writerows(test_data)

    with open(output_val_csv, 'w', encoding='utf-8', newline='') as val_file:
        csv_writer = csv.writer(val_file)
        csv_writer.writerows(val_data)

if __name__ == "__main__":
    input_txt_file = '/home/biniyam_ajaw/finetuning/data/dataset.txt'
    output_csv_file = '/home/biniyam_ajaw/finetuning/data/output_data.csv'
    output_train_csv = '/home/biniyam_ajaw/finetuning/data/train_data.csv'
    output_test_csv = '/home/biniyam_ajaw/finetuning/data/test_data.csv'
    output_val_csv = '/home/biniyam_ajaw/finetuning/data/val_data.csv'

    convert_txt_to_csv(input_txt_file, output_csv_file)
    split_data(output_csv_file, output_train_csv, output_test_csv, output_val_csv)
    print("Conversion to CSV and data split completed.")
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=100000)

import tokenizers

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

folder = 'data'
files = [f"/home/biniyam_ajaw/finetuning/{folder}/{split}.csv" for split in ["test_data", "train_data", "valid_data"]]
tokenizer.train(files, trainer)

from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")

from transformers import PreTrainedTokenizerFast

custom_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
custom_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
custom_tokenizer.save_pretrained("amharic_tokenizer")

custom_tokenizer.push_to_hub("amharic_tokenizer")
max-width: fit-content;
  width: 100%;
  margin: 5 auto;
  padding: 2rem;
  text-align: start;
}
.container {
  max-width: '100%' 
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function FileInput (){
    return(
        <div>
            <div className="input-group mb-3">
                <input type="file" className="form-control" id="inputGroupFile02"/>
                <label clclassNameass="input-group-text" for="inputGroupFile02">Upload</label>
            </div>
        </div>
      
    );
}


export default FileInput;
/* :root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;
  width: 100%;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: 
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.container {
  width: 100%;
  padding-right: 15px;
  padding-left: 15px;
  margin-right: auto;
  margin-left: auto;
}
a {
  font-weight: 500;
  color:   text-decoration: inherit;
}
a:hover {
  color: }

body {
  margin: 0;
  display: flex;
  place-items: center;
  min-width: 320px;
  min-height: 100vh;
}

h1 {
  font-size: 3.2em;
  line-height: 1.1;
}

button {
  border-radius: 8px;
  border: 1px solid transparent;
  padding: 0.6em 1.2em;
  font-size: 1em;
  font-weight: 500;
  font-family: inherit;
  background-color:   cursor: pointer;
  transition: border-color 0.25s;
}
button:hover {
  border-color: }
button:focus,
button:focus-visible {
  outline: 4px auto -webkit-focus-ring-color;
}

@media (prefers-color-scheme: light) {
  :root {
    color:     background-color:   }
  a:hover {
    color:   }
  button {
    background-color:   }
} */
import Container from 'react-bootstrap/Container';
import Nav from 'react-bootstrap/Nav';
import Navbar from 'react-bootstrap/Navbar';
import NavDropdown from 'react-bootstrap/NavDropdown';

function NavBarComp() {
  return (
    <Navbar expand="lg" className="bg-body-tertiary container-fluid">
      <Container >
        <Navbar.Brand href="        <Navbar.Toggle aria-controls="basic-navbar-nav" />
        <Navbar.Collapse id="basic-navbar-nav">
          <Nav className="me-auto">
            <NavDropdown title="Select Model" id="basic-nav-dropdown">
                <NavDropdown.Item href="                <NavDropdown.Item href="                <NavDropdown.Item href="            </NavDropdown>
          </Nav>
        </Navbar.Collapse>
      </Container>
    </Navbar>
  );
}

export default NavBarComp;
import React from 'react';
import 'bootstrap/dist/css/bootstrap.css'
function TextInputWithLabel (props) {
    return(
        <div>
            <div className="mb-3">
                <label htmlFor="exampleFormControlTextarea1" className="form-label">Generated Ad</label>
                <textarea className="form-control" id="exampleFormControlTextarea1" rows="7"  value={props.value.answer}/>
            </div>
        </div>
      
    );
}


export default TextInputWithLabel;
import torch

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging

from datasets import load_dataset

import os, sys

from huggingface_hub import notebook_login

import torch.nn as nn

import getpass

from trl import SFTTrainer

from peft import PeftConfig, LoraConfig
os.environ["HUGGING_FACE_HUB_TOKEN"] = getpass.getpass("Token:")

assert os.environ["HUGGING_FACE_HUB_TOKEN"]
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")

double_quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model_id = "microsoft/phi-2"

new_model = 'amharic-phi'

model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cuda:0', quantization_config=nf4_config)
tokenizer = AutoTokenizer.from_pretrained("dagim/amharic_tokenizer")



tokenizer.tokenize("ከአሜሪካ ወደ አዲስ አበባለመጓዝምንያህልጊዜይወስዳል??")
from datasets import load_dataset

dataset_name = 'Henok/amharic-qa'

dataset = load_dataset(dataset_name, split="train")
import re

def get_num_layers(model):

    numbers = set()

    for name, _ in model.named_parameters():

        for number in re.findall(r'\d+', name):

            numbers.add(int(number))

    return max(numbers)



def get_last_layer_linears(model):

    names = []

    

    num_layers = get_num_layers(model)

    for name, module in model.named_modules():

        if str(num_layers) in name and not "encoder" in name:

            if isinstance(module, torch.nn.Linear):

                names.append(name)

    return names
config = LoraConfig(

    r=4,

    lora_alpha=32,

    
    lora_dropout=0.03,

    bias='none',

    task_type="CAUSAL_LM"

)

training_arguments = TrainingArguments(

    output_dir="./results",

    num_train_epochs=2,

    per_device_train_batch_size=4,

    gradient_accumulation_steps=1,

    optim='paged_adamw_32bit',

    save_steps=25,

    logging_steps=25,

    learning_rate=2e-8,

    weight_decay=0.001,

    fp16=False,

    bf16=False,

    max_grad_norm=0.3,

    max_steps=25,

    warmup_ratio=0.03,

    group_by_length=True,

    lr_scheduler_type='constant',

    report_to="tensorboard",

    gradient_checkpointing=True

)
trainer = SFTTrainer(

    model=model,

    train_dataset=dataset,

    peft_config=config,

    dataset_text_field='inputs',

    max_seq_length=None,

    tokenizer=tokenizer,

    args=training_arguments,

    packing=False

)
trainer.train()
trainer.model.save_pretrained(new_model)
logging.set_verbosity(logging.CRITICAL)



prompt = "የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?"

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="/home/biniyam_ajaw/llama-2-amharic-3784m/tokenizer.json")
print(len(tokenizer.encode('የኢትዮጵያ ጂዲፒ ምን ያህል ነበር?')))
import logging
import numpy as np
import math
import os, sys
import torch
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Tuple, Dict, Any, Mapping
from pathlib import Path
import datasets
from datasets import Dataset, DatasetDict, load_dataset, load_metric, concatenate_datasets

from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,  
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
    is_torch_gpu_available,
)

from transformers.trainer_utils import get_last_checkpoint, is_main_process
from transformers.testing_utils import CaptureLogger
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version_core

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import classification_report
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora-Model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")
            
        
        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)
        
    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

def accuracy(predictions, references, normalize=True, sample_weight=None):
    return {
        "accuracy": float(
            accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
        )
    }

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)

def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
        logits = logits[0]
    return logits.argmax(dim=-1)

def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}
    
    if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["label"] = torch.tensor([f["label"] for f in features], dtype=dtype)
        
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
            
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)
            
    
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else: batch[k] = torch.tensor([f[k] for f in features])
                
    except ValueError:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                                        batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else: 
                    batch[k] = torch.tensor([features[0][k]] * len(features))
                    
    return batch

MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={"help": ("The tokenizer for weights initialization.")},
    )
    
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    
    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides cannot be used with --config_name or --model_name_or_path. To override some of "
            )
            
@dataclass
class DataTrainingArguments:
    '''
    Arguments pertaining to what data we are going to input our model for training and eval.
    '''
    
    dataset_dir: Optional[str] = field(
        default=None, metadata={"the name of the dataset to use"}
    )

    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name opf the dataset to use"}
    )
    
    train_file: Optional[str] = field(
        default=None, metadata={"help": "The input training file"}
    )
    
    validation_file: Optional[str] = field(
        default=None, metadata={"help": "This is optional but recommended if you want to use early stopping"}
    )
    
    max_training_sample: Optional[int] = field(
        default=None,
        metadata={
            "help": "Debugging purposes"
        },
    )
    
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging"
        },
    )
    
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    
        block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional"
                "Training dataset will be truncated into a block of this size for training"
                "Default to the model max input sequence"
            )
        }
    )
    
            
    cache_dir: bool = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    
            validation_strategy: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "Percentage of the validation set used at the end of each epoch"
        }
        
    )
        preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes to use for preprocessing"}
    )
    
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep the linebreaks when using txt files or not"}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed store"})
    
    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets >= 2.0.0`")
            
            
@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj, v_proj")
    lora_rank : Optional[str] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.03)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[str] = field(default=False)
    peft_path : Optional[str] = field(default=None)
    
logger = logging.getLogger(__name__)

def main():
    
    parser = HfArgumentParser(ModelArguments, DataTrainingArguments, MyTrainingArguments)
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                model_args, data_args, training_args = parser.parse_parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_to_dataclasses()
        
    send-example_telemetry("run_clm", model_args, data_args)
    
    logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", datefmt="%m/%d/%Y %H:%M:%S",
                        level=logging.INFO,                         handlers=[logging.StreamHandler(sys.stdout)],)
    
    if training_args.should_log:
        transformers.utils.logging.set_verbosity_info()
        
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
    logger.warning(
        f"Process rank: {training_args.output_dir}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"Distributed training: {bool(training_args.local_rank != -1)}, 16-bits-training: {training_args.fp16}"
    )
    
        
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError (
                f"Outpur dir {training_args.output_dir} already exists and is not mt"
                "Use --overwrite_output_dir to overcome"
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is not None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this, change "
                "the --output-dir or --overwrite_output_dir to train from scratch"
            )
            
        set_seed(training_args.seed)
    
    config_kwargs = {
        "cache_dir": model.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model-args.use_auth_token else None
    }
    
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else: 
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("This is a new config from scratch")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")
            
            
    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None
    }
    
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "Instantiating a tokenizer from scratch"
        )
        
            
    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            return tokenizer(examples[text])
        
        if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^ PLease ignore the warning above ^^^^^^^"
            )
            
        return output

    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024  
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                "Override with `--block_size xxx`"
            
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)
        
        
    def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        
        if total_length >= block_size:
                        total_length = {total_length // block_size} *  block_size
                        result = {
                k: [t[i: i + block_size] for i in range(0, total_length, block_size)]
                for k, t in concatenated_examples.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result
        with training_args.main_process_first(desc="dataset map tokenizer"):
            lm_datasets = []
            path = Path(data_args.dataset_dir)
            filename = [file.name for file in path.glob("*.txt")]
            
            if training_args.debug_mode:
                files = [files[0]]
            for idx, file in enumerate(files):
                data_file = os.path.join(path, file)
                filename = ''.join(file.split('.')[:-1])
                cache_path = os.path.join(data_args.data_cache_dir, filename)
                os.makedirs(cache_path, exist_ok=True)
                try:
                    processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=True)
                    logger.info(f'Training datasets-{filename} has been loaded from disk')
                except Exception:
                    cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                    os.makedirs(cache_dir, exist_ok=True)
                    raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                    logger.info(f"{file} has been loaded")
                    tokenized_dataset = raw_dataset.map(
                        tokenize_function,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        remove_columns="text",
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "tokenized.arrow") for k in raw_dataset},
                        desc="Running tokenizer on the dataset",
                    )
                    
                    grouped_datasets = tokenized_dataset.map(
                        group_texts,
                        batched=True,
                        num_proc=data_args.preprocessing_num_workers,
                        load_from_cache_file=True,
                        keep_in_memory=False,
                        cache_file_names = {k: os.path.join(cache_dir, "grouped.arrow") for k in tokenized_dataset},
                        desc=f'Grouping texts in chunks of {block_size}',
            
                    )
                    
                    processed_dataset = grouped_datasets
                    processed_dataset.save_to_disk(cache_path)
                    
                if idx == 0:
                    lm_datasets = processed_dataset['train']
                else:
                    assert lm_datasets.features.type == processed_dataset['train'].features.type
                    lm_dataset = concatenate_datasets([lm_datasets, processed_dataset['train']])
                    
            lm_datasets = lm_datasets.train_test_split(test_size= data_args.validation_split_percentage())
            
        if training_args.do_train:
            train_dataset = lm_datasets["train"]
            
            
            if data_args.max_train_samples is not None:
                max_train_samples = min(len(train_dataset), data_args.max_train_samples)
                train_dataset = train_dataset.select(range(max_train_samples))
                logger.info(f"Num train samples {len(train_dataset)}")
                logger.info("Training example: ")
                logger.info(tokenizer.decode(train_dataset[0]["input_ids"]))
                
                
        if model_args.model_name_or_path:
            torch_dtype = (
                model_args.torch_dtype
                if model_args.torch_dtype in ["auto", None]
                else getattr(torch, model_args.torch_dtype)
            )
            
            model = LlamaForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                from_tf=bool(".cpkt" in model_args.model_name_or_path),
                config=config,
                cache_dir=model_args.cache_dir,
                revision=model_args.model_revision,
                use_auth_token=True if model_args.use_auth_token else None,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
            )
            
        else:
            model = AutoModelForCausalLM.from_config(config)
            n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
            logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M parameters")
        
        model_vocab_size = model.get_output_embeddings().weight.size(0)
        
        if not (
            (model_vocab_size==32000 and len(tokenizer)==51008) or \
            (model_vocab_size==32000 and len(tokenizer)==32000) or \
            (model_vocab_size==51008 and len(tokenizer)==51008) or \
            (model_vocab_size==49954 and len(tokenizer)==49954)
        ):
            raise ValueError(
                f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
                "Valid configurations (base model / tokenizer):\n"
                "- Continue pre-training original LLaMA: 32000 / 32000 \n"
                "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
                "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
                "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n"
            )
            
                model.resize_token_embeddings(len(tokenizer))
        
        if training_args.peft_path is not None:
            logger.info("PEFT from pretrained model")
            model = PeftModel.from_pretrained(model, training_args.peft_path)
        else:
            logger.info("Init new peft model")
            target_modules = training_args.trainable.split(",")
            modules_to_save = training_args.modules_to_save
            if modules_to_save is not None:
                modules_to_save = modules_to_save.split(",")
            lora_rank = training_args.lora_rank
            lora_dropout = training_args.lora_dropout
            lora_alpha = training_args.lora_alpha
            logger.info(f"Target modules: {target_modules}")
            logger.info(f"LoRA Rank: {lora_rank}")
            peft_config = LoraConfig(
                task_type = TaskType.CAUSAL_LM,
                targert_modules = target_modules,
                inference_mode=False,
                r = lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                modules_to_save=modules_to_save,
            )    
            
            model= get_peft_model(model, peft_config)
        model.print_trainable_parameters()
!pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb
import os

from dotenv import load_dotenv




load_dotenv()




hf_token = os.getenv("hf_token")



import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    TrainingArguments,

    pipeline,

    logging,

)



import peft



from peft import LoraConfig

from trl import SFTTrainer
import pandas as pd

file_path = '../../merged.csv'




df = pd.read_csv(file_path)

df
dataset=df[['Text']]

dataset
dataset_2=dataset.copy()










dataset_2
!pip install scikit-learn



from sklearn.model_selection import train_test_split





train_val_data, test_data = train_test_split(dataset_2, test_size=0.20, random_state=42)

train_data, evaluation_data = train_test_split(train_val_data, test_size=0.10, random_state=42)



print('Training dataset shape:', len(train_data))

print('evaluation dataset shape:', len(evaluation_data))

print('Testing dataset shape:', len(test_data))
evaluation_data
import numpy as np


msk = np.random.rand(len(dataset_2)) < 0.8

train_dataset = dataset_2[msk]

test_dataset = dataset_2[~msk]

from datasets import Dataset



test_dataset=Dataset.from_pandas(test_dataset)



train_dataset=Dataset.from_pandas(train_dataset)



evaluation_dataset=Dataset.from_pandas(evaluation_data)
test_dataset



test_dataset=test_dataset.remove_columns("__index_level_0__")

train_dataset=train_dataset.remove_columns("__index_level_0__")

evaluation_dataset=evaluation_dataset.remove_columns("__index_level_0__")


import datasets


main_dataset= datasets.DatasetDict({

    'train': train_dataset,

    'test': test_dataset,

    'evaluate': evaluation_dataset

})
main_dataset
import os

import torch

from datasets import load_dataset

from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    AutoTokenizer,

    TrainingArguments,

    pipeline,

)

from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training

from trl import SFTTrainer

base_model = "NousResearch/Llama-2-7b-hf"

new_model = "llama-2-7b-Amh"






tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)

tokenizer.pad_token = tokenizer.unk_token

tokenizer.padding_side = "right"

bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,

    bnb_4bit_quant_type="nf4",

    bnb_4bit_compute_dtype=torch.float16,

    bnb_4bit_use_double_quant=True,

)




peft_config = LoraConfig(

    r=16,

    lora_alpha=32,

    lora_dropout=0.05,

    bias="none",

    task_type="CAUSAL_LM",

    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']

)

def load_model(model_name, bnb_config):

    n_gpus = torch.cuda.device_count()

    max_memory = f'{23000}MB'



load_model(base_model,bnb_config)
import torch

torch.cuda.empty_cache()


model = AutoModelForCausalLM.from_pretrained(

    base_model,

    quantization_config=bnb_config,

    device_map={"": 0}

)




model = prepare_model_for_kbit_training(model)
training_dataset=main_dataset
model
import torch




device = torch.device("cuda:0")




torch.cuda.empty_cache()




print(torch.cuda.memory_summary(device))

import torch

torch.cuda.empty_cache()

import torch




n_gpus = torch.cuda.device_count()

print(f"Number of available GPUs: {n_gpus}")




for i in range(n_gpus):

    gpu_memory = torch.cuda.get_device_properties(i).total_memory

    print(f"GPU {i}: Total memory: {gpu_memory / (1024**3)} GB")

from transformers import Trainer, TrainingArguments, BitsAndBytesConfig


training_arguments = TrainingArguments(

        output_dir="../results",

        num_train_epochs=1,

        per_device_train_batch_size=10,

        per_device_eval_batch_size=1,

        gradient_accumulation_steps=1,

        gradient_checkpointing=True,

        fp16=True,

        evaluation_strategy="steps",

        eval_steps=1000,

        logging_steps=1,

        optim="paged_adamw_8bit",

        learning_rate=2e-4,

        lr_scheduler_type="linear",

        warmup_steps=10,

        
        max_steps=10, 
)




trainer = SFTTrainer(

    model=model,

    train_dataset=main_dataset["train"],

    eval_dataset=main_dataset["evaluate"],

    peft_config=peft_config,

    dataset_text_field="Text",

    max_seq_length=512,

    tokenizer=tokenizer,

    args=training_arguments,

)


model.config.use_cache = False  



trainer.train()




trainer.model.save_pretrained(new_model)


prompt = "የኢትዮጽያ  ጂዲፒ ምን ያህል ነበር"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "how can i treat flu, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "tell me about ethiopian politics, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "who the prime minister of ethiopia, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

prompt = "3 Ethiopian premier league club, give the response in amharic language"

instruction = f"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=128)

result = pipe(instruction)

print(result[0]['generated_text'][len(instruction):])

del model

del pipe

del trainer

import gc

gc.collect()

gc.collect()
import torch

torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained(

    base_model,

    low_cpu_mem_usage=True,

    return_dict=True,

    torch_dtype=torch.float16,

    device_map={"": 0},

)

model = PeftModel.from_pretrained(model, new_model)

model = model.merge_and_unload()




tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

tokenizer.padding_side = "right"
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils import utils
letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

util = utils.Util()
def clean_parsed_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return

    

    
    if not os.path.exists(cleaned_dir):

            os.makedirs(cleaned_dir)



    
    for file_name in os.listdir(folder_path):

        base_name, extension = os.path.splitext(file_name)

        print(base_name,extension)

        if extension =='.csv':

            df = pd.read_csv(f"{folder_path}/{file_name}", index_col='id')

            df = df.dropna()

            df = df.replace('\n', ' ', regex=True)

            
            df['hashtags'] = df['text'].apply(lambda x: util.extract_hashtags(x))

            df['text'] = df['text'].str.replace(r'\
            

            
            df['emojis'] = df['text'].apply(util.extract_emojis)

            


            for letter in letters:

                for i in range(len(letter[0])):

                    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

            
            df['symbols'] = df['text'].apply(util.extract_symbols)

            df['text'] = df['text'].apply(util.remove_symbols)

            
            df['links'] = df['text'].apply(util.extract_urls)

            df['text'] = df['text'].apply(util.remove_links)



            df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

            df['text'] = df['text'].replace(r'!+', '!', regex=True)

            df['text'] = df['text'].replace(r'\.+', '', regex=True)

            base_name, extension = os.path.splitext(file_name)

            df.to_csv(f"{cleaned_dir}/{base_name}.csv")

            df['text'].to_csv(f"{cleaned_dir}/{base_name}.txt", index=False, header=False)





        
clean_parsed_data(parsed_dir)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('../src/')))
from utils.utils import Util
util = Util()
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"
def process_raw_data(folder_path):

    
    if not os.path.isdir(folder_path):

        print(f"{folder_path} is not a valid directory.")

        return



    
    for filename in os.listdir(folder_path):

        print(filename)

        data = util.read_file(f'{folder_path}/{filename}')

        parsed_message = util.parse_messages(data['messages'])



        
        df = pd.DataFrame(parsed_message)

        df.set_index('id', inplace=True)

        base_name, extension = os.path.splitext(filename)

        
        if not os.path.exists(parsed_dir):

            os.makedirs(parsed_dir)

        df.to_csv(f'{parsed_dir}/{base_name}.csv')

        
process_raw_data(raw_dir)
!pip install transformers


from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM



model_name = "Samuael/llama-2-7b-tebot-amharic"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(model_name)


input_text = "እኔ አማርኛ መናገር እረዳለሁ"




input_ids = tokenizer.encode(input_text, return_tensors="pt")




output_ids = model.generate(input_ids)




output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Amharic text:", output_text)
import json
import re


class Util:
    def __init__(self) -> None:
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"              "\U0001F300-\U0001F5FF"              "\U0001F680-\U0001F6FF"              "\U0001F700-\U0001F77F"              "\U0001F780-\U0001F7FF"              "\U0001F800-\U0001F8FF"              "\U0001F900-\U0001F9FF"              "\U0001FA00-\U0001FA6F"              "\U0001FA70-\U0001FAFF"              "\u2600-\u26FF"              "\u2700-\u27BF"              "\u2B50"              "\U0001F1E6-\U0001F1FF"              "]+",
            flags=re.UNICODE,
        )
        self.symbols = re.compile(
            "[" '"' "\“" '"' "'" "\-" "\*" "\•" "\ℹ" "\﻿" "\_" "]+"
        )
        self.url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        self.mention_pattern = r"@(\w+)"

    def read_file(self, file_path: str) -> dict:
                with open(file_path, "r") as file:
                        data = json.load(file)
            return data

    def write_file(self, file_path: str, data: dict) -> None:
                with open(file_path, "w") as file:
                        json.dump(data, file, indent=2)

    def parse_text(self, text: any) -> str:
        if isinstance(text, str):
            return text
        elif isinstance(text, list):
            contents = []
            for item in text:
                if isinstance(item, str):
                    contents.append(item)
                elif isinstance(item, dict):
                    contents.append(item["text"])
            return "".join(contents)
        else:
            return ""

    def parse_messages(self, messages: list) -> dict:
        parsed_messages = {"id": [], "text": [], "date": []}
        for message in messages:
            if message["type"] != "message" or len(message["text"]) == 0:
                continue
            parsed_messages["id"].append(message["id"])
            message_content = self.parse_text(message["text"])
            parsed_messages["text"].append(message_content)
            parsed_messages["date"].append(message["date"])
        return parsed_messages

    def extract_hashtags(self, text: str) -> list:
        return [word for word in text.split() if word.startswith("
    def extract_emojis(self, text):
        return "".join(self.emoji_pattern.findall(text))

    def remove_emojis(self, text):
        return self.emoji_pattern.sub("", text)

    def extract_symbols(self, text):
        return "".join(self.symbols.findall(text))

    def remove_symbols(self, text):
        return self.symbols.sub(" ", text)

    def extract_urls(self, text):
        return re.findall(self.url_pattern, text)

    def remove_links(self, text):
        return re.sub(self.url_pattern, " ", text)

    def extract_mentions(self, text):
        return re.findall(self.mention_pattern, text)
import argparse
from dataclasses import dataclass
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
import os
from langchain_openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

CHROMA_PATH = './chromadb/'

client = OpenAI(
    api_key=OPENAI_API_KEY
)

core_embeddings_model = None
def get_context():
        vectorstore = Chroma(persist_directory="./cachce",embedding_function=core_embeddings_model)
    
    retriever = vectorstore.as_retriever()
        return retriever

def generate_add(user_input, context):
    template = f'''
    Generate an advertisement given the following context.    
    You must use the following context:
    {context}
    '''   
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": template},{"role": "user", "content": user_input}],
        n=3,
    )

    return response
from peft import PeftModel
from transformers import LlamaForCausalLM, LlamaConfig

def load_model(model_name, quantization):
    model = LlamaForCausalLM.from_pretrained(
        model_name,
        return_dict=True,
        load_in_8bit=quantization,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    return model


def load_peft_model(model, peft_model):
    peft_model = PeftModel.from_pretrained(model, peft_model)
    return peft_model

def load_llama_from_config(config_path):
    model_config = LlamaConfig.from_pretrained(config_path) 
    model = LlamaForCausalLM(config=model_config)
    return model
import fire
import torch
import os
import sys
import time
import json 
from typing import List

from transformers import LlamaTokenizer, LlamaForCausalLM
from model_utils import load_model, load_peft_model

BASE_PROMPT = """Below is an interaction between a human and an AI fluent in English and Amharic, providing reliable and informative answers.
Human: {}
Assistant [Amharic] : """

def main(
    model_name: str="",
    peft_model: str=None,
    quantization: bool=False,
    max_new_tokens =400,     prompt_file: str=None,
    seed: int=42,     do_sample: bool=True,     min_length: int=None,     use_cache: bool=True,      top_p: float=1.0,     temperature: float=1.0,     top_k: int=1,     repetition_penalty: float=1.0,     length_penalty: int=1,     enable_azure_content_safety: bool=False,     enable_sensitive_topics: bool=False,     enable_saleforce_content_safety: bool=False,     **kwargs
):    
    
    print("***Note: model is not set up for chat use case, history is reset after each response.")
    print("***Ensure that you have replaced the default LLAMA2 tokenizer with the Amharic tokenizer")
    
        torch.cuda.manual_seed(seed)
    torch.manual_seed(seed)
    
    MAIN_PATH = '/model/Llama-2-7b-hf'
        peft_model = '/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'
    model_name = MAIN_PATH
    quantization = True
    model = load_model(model_name, quantization)

    tokenizer = LlamaTokenizer.from_pretrained(model_name)
    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))

    if peft_model:
        model = load_peft_model(model, peft_model)

    model.eval()

    while True:

        
        user_query = input('Type question in Amharic or English: ')
        user_prompt = BASE_PROMPT.format(user_query)
        batch = tokenizer(user_prompt, return_tensors="pt")
        batch = {k: v.to("cuda") for k, v in batch.items()}
        start = time.perf_counter()
        with torch.no_grad():
            outputs = model.generate(
                **batch,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                top_p=top_p,
                temperature=temperature,
                min_length=min_length,
                use_cache=use_cache,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                length_penalty=length_penalty,
                **kwargs 
            )
        e2e_inference_time = (time.perf_counter()-start)*1000
        print(f"the inference time is {e2e_inference_time} ms")
        
        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("MODEL_OUTPUT: {}".format(output_text))
        
if __name__ == "__main__":
    fire.Fire(main)
import torch
from contextlib import nullcontext
from transformers import (
    LlamaForCausalLM, 
    LlamaTokenizer, 
    TrainerCallback, 
    default_data_collator, 
    Trainer, 
    TrainingArguments
)
from peft import (
    LoraConfig,
    TaskType,
    prepare_model_for_int8_training,
    PeftModel
)

from pathlib import Path
from utils.dataset_utils import get_preprocessed_dataset
from configs.datasets import amharic_dataset

def print_trainable_parameters(model):
    print("Trainable Parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f" - {name}")


def finetune():
    LLAMA_DIR = '/model/Llama-2-7b-hf'
    PT_DIR = '/model/llama-2-amharic-3784m'
    OUTPUT_DIR = "/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output"

    tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)

    model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)


    train_dataset = get_preprocessed_dataset(tokenizer, amharic_dataset, 'train')


    model.train()



    embedding_size = model.get_input_embeddings().weight.shape[0]

    if len(tokenizer) != embedding_size:
        print("resize the embedding size by the size of the tokenizer")
        model.resize_token_embeddings(len(tokenizer))


    print('loading the pretrained model from config')

    model = prepare_model_for_int8_training(model)
    model = PeftModel.from_pretrained(model, PT_DIR)
    model.print_trainable_parameters()
    lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            modules_to_save = ["embed_tokens","lm_head"]
        )

    enable_profiler = False


    config = {
        'lora_config': lora_config,
        'learning_rate': 1e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 1,
        'per_device_train_batch_size': 2,
        'gradient_checkpointing': False,
    }

        if enable_profiler:
        wait, warmup, active, repeat = 1, 1, 2, 1
        total_steps = (wait + warmup + active) * (1 + repeat)
        schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
        profiler = torch.profiler.profile(
            schedule=schedule,
            on_trace_ready=torch.profiler.tensorboard_trace_handler(f"{OUTPUT_DIR}/logs/tensorboard"),
            record_shapes=True,
            profile_memory=True,
            with_stack=True)

        class ProfilerCallback(TrainerCallback):
            def __init__(self, profiler):
                self.profiler = profiler

            def on_step_end(self, *args, **kwargs):
                self.profiler.step()

        profiler_callback = ProfilerCallback(profiler)
    else:
        profiler = nullcontext()


        training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        bf16=True,                  logging_dir=f"{OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=10,
        save_strategy="steps",
        save_steps=1000,
        save_total_limit=1,
        warmup_ratio=0.03,
        optim="adamw_torch_fused",
        max_steps=total_steps if enable_profiler else -1,
        **{k:v for k,v in config.items() if k != 'lora_config'}
    )

    with profiler:
                trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=default_data_collator,
            callbacks=[profiler_callback] if enable_profiler else [],
        )

        print_trainable_parameters(model)

                trainer.train()

    model.save_pretrained(OUTPUT_DIR)


finetune()
from dataclasses import dataclass

@dataclass
class amharic_dataset:
    dataset: str = "amharic_dataset"
    train_split: str = "train"
    test_split: str = "val"
    data_path: str = "/data/fine_tun_data6.json"
import copy
import json
import torch


from torch.utils.data import Dataset



class InstructionDataset(Dataset):
    def __init__(self, dataset_config, tokenizer, partition="train", max_words=50):
        self.ann = json.load(open(dataset_config.data_path))

        if partition == "train":
            self.ann = self.ann
        else:
            self.ann = self.ann[:200]

        self.max_words = max_words
    
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.ann)

    def __getitem__(self, index):

        ann = self.ann[index]
        data = self.create_prompt_formats(ann)
        prompt = data['prompt']
        example = data['text']  
        prompt = torch.tensor(
            self.tokenizer.encode(prompt), dtype=torch.int64
        )

        example = self.tokenizer.encode(example)

        example.append(self.tokenizer.eos_token_id)
        example = torch.tensor(
            example, dtype=torch.int64
        )
        padding = self.max_words - example.shape[0]

        if padding > 0:
            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - 1))
        elif padding < 0:
            example = example[: self.max_words]

        labels = copy.deepcopy(example)

        labels[: len(prompt)] = -1

        example_mask = example.ge(0)
        label_mask = labels.ge(0)
        example[~example_mask] = 0
        labels[~label_mask] = 0
        example_mask = example_mask.float()
        label_mask = label_mask.float()

        return {
            "input_ids": example,
            "labels": labels,
            "attention_mask":example_mask,
        }
    
    def create_prompt_formats(self,sample):
        """
        Format various fields of the sample ('text', 'label',)
        Then concatenate them using two newline characters
        :param sample: Sample dictionnary
        """

        INTRO_BLURB = "Generate an advertisement given a category"
        INSTRUCTION_KEY = "        RESPONSE_KEY = "Response:"
        END_KEY = "
        blurb = f"{INTRO_BLURB}"
        text = f"{INSTRUCTION_KEY}\n{sample['input']}"
        response = f"{RESPONSE_KEY}\n{sample['output']}"
        end = f"{END_KEY}"

        parts = [part for part in [blurb, text, response, end] if part]

        formatted_prompt = "\n\n".join(parts)

        sample["text"] = formatted_prompt
        parts = [part for part in [blurb, text,] if part]
        formatted_prompt = "\n\n".join(parts)

        sample["prompt"]= formatted_prompt

        return sample
"""
Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.

Here is the full list of checkpoints on the hub that can be fine-tuned by this script:
https://huggingface.co/models?filter=text-generation
"""

import logging
import numpy as np
import math
import os
import sys
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, List, Dict, Any, Mapping
from pathlib import Path
import datasets
import torch
from datasets import load_dataset, concatenate_datasets

import transformers
from transformers import (
    CONFIG_MAPPING,
    MODEL_FOR_CAUSAL_LM_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    LlamaForCausalLM,
    LlamaTokenizer,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    is_torch_tpu_available,
    set_seed,
)
from transformers.testing_utils import CaptureLogger
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import send_example_telemetry
from transformers.utils.versions import require_version

from sklearn.metrics import accuracy_score
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, get_peft_model_state_dict
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(transformers.TrainerCallback):
    def save_model(self, args, state, kwargs):
        if state.best_model_checkpoint is not None:
            checkpoint_folder = os.path.join(state.best_model_checkpoint, "pt_lora_model")
        else:
            checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")

        peft_model_path = os.path.join(checkpoint_folder, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)

    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        peft_model_path = os.path.join(args.output_dir, "pt_lora_model")
        kwargs["model"].save_pretrained(peft_model_path)
        kwargs["tokenizer"].save_pretrained(peft_model_path)


def accuracy(predictions, references, normalize=True, sample_weight=None):
        return {
            "accuracy": float(
                accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight)
            )
        }


def compute_metrics(eval_preds):
    preds, labels = eval_preds
            labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    return accuracy(predictions=preds, references=labels)


def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
                        logits = logits[0]
    return logits.argmax(dim=-1)


def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:
    if not isinstance(features[0], Mapping):
        features = [vars(f) for f in features]
    first = features[0]
    batch = {}

                if "label" in first and first["label"] is not None:
        label = first["label"].item() if isinstance(first["label"], torch.Tensor) else first["label"]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch["labels"] = torch.tensor([f["label"] for f in features], dtype=dtype)
    elif "label_ids" in first and first["label_ids"] is not None:
        if isinstance(first["label_ids"], torch.Tensor):
            batch["labels"] = torch.stack([f["label_ids"] for f in features])
        else:
            dtype = torch.long if isinstance(first["label_ids"][0], int) else torch.float
            batch["labels"] = torch.tensor([f["label_ids"] for f in features], dtype=dtype)

        
    try:
        for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([f[k] for f in features])
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))
                else:
                    batch[k] = torch.tensor([f[k] for f in features])
    except ValueError:         for k, v in first.items():
            if k not in ("label", "label_ids") and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    batch[k] = torch.stack([features[0][k]] * len(features))
                elif isinstance(v, np.ndarray):
                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))
                else:
                    batch[k] = torch.tensor([features[0][k]] * len(features))

    return batch


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The tokenizer for weights initialization.Don't set if you want to train a model from scratch."
            )
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override some existing default config settings when a model is trained from scratch. Example: "
                "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError(
                "--config_overrides can't be used in combination with --config_name or --model_name_or_path"
            )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    dataset_dir: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    streaming: bool = field(default=False, metadata={"help": "Enable streaming mode"})
    block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional input sequence length after tokenization. "
                "The training dataset will be truncated in block of this size for training. "
                "Default to the model max input length for single sentence inputs (take into account special tokens)."
            )
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[float] = field(
        default=0.01,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep line breaks when using TXT files or not."}
    )
    data_cache_dir: Optional[str] = field(default="./", metadata={"help": "The datasets processed stored"})

    def __post_init__(self):
        if self.streaming:
            require_version("datasets>=2.0.0", "The streaming feature requires `datasets>=2.0.0`")


@dataclass
class MyTrainingArguments(TrainingArguments):
    trainable : Optional[str] = field(default="q_proj,v_proj")
    lora_rank : Optional[int] = field(default=8)
    lora_dropout : Optional[float] = field(default=0.1)
    lora_alpha : Optional[float] = field(default=32.)
    modules_to_save : Optional[str] = field(default=None)
    debug_mode : Optional[bool] = field(default=False)
    peft_path : Optional[str] = field(default=None)


logger = logging.getLogger(__name__)


def main():

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, MyTrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
                        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

            send_example_telemetry("run_clm", model_args, data_args)

        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,          handlers=[logging.StreamHandler(sys.stdout)],)


    if training_args.should_log:
                transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    
        logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )

        last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

        set_seed(training_args.seed)

    config_kwargs = {
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.config_name:
        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)
    elif model_args.model_name_or_path:
        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
    else:
        config = CONFIG_MAPPING[model_args.model_type]()
        logger.warning("You are instantiating a new config instance from scratch.")
        if model_args.config_overrides is not None:
            logger.info(f"Overriding config: {model_args.config_overrides}")
            config.update_from_string(model_args.config_overrides)
            logger.info(f"New config: {config}")

    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "revision": model_args.model_revision,
        "use_auth_token": True if model_args.use_auth_token else None,
    }
    if model_args.tokenizer_name:
        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
    elif model_args.tokenizer_name_or_path:
        tokenizer = LlamaTokenizer.from_pretrained(model_args.tokenizer_name_or_path, **tokenizer_kwargs)
    else:
        raise ValueError(
            "You are instantiating a new tokenizer from scratch. This is not supported by this script."
            "You can do it from another script, save it, and load it from here, using --tokenizer_name."
        )

                tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            output = tokenizer(examples["text"])
                if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return output
    if data_args.block_size is None:
        block_size = tokenizer.model_max_length
        if block_size > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            block_size = 1024
    else:
        if data_args.block_size > tokenizer.model_max_length:
            logger.warning(
                f"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model"
                f"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}."
            )
        block_size = min(data_args.block_size, tokenizer.model_max_length)

        def group_texts(examples):
                concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
                        if total_length >= block_size:
            total_length = (total_length // block_size) * block_size
                result = {
            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    with training_args.main_process_first(desc="dataset map tokenization and grouping"):
        lm_datasets = []
        path = Path(data_args.dataset_dir)
        files = [file.name for file in path.glob("*.txt")]
        if training_args.debug_mode is True:
            files = [files[0]]
        print("printing files")
        print(files)    
        for idx, file in enumerate(files):
            data_file = os.path.join(path, file)
            filename = ''.join(file.split(".")[:-1])
            cache_path = os.path.join(data_args.data_cache_dir, filename)
            os.makedirs(cache_path, exist_ok=True)
            try:
                processed_dataset = datasets.load_from_disk(cache_path, keep_in_memory=False)
                logger.info(f'training datasets-{filename} has been loaded from disk')
            except Exception:
                cache_dir = os.path.join(data_args.data_cache_dir, filename+"_text")
                os.makedirs(cache_dir, exist_ok=True)
                raw_dataset = load_dataset("text", data_files=data_file, cache_dir=cache_dir, keep_in_memory=False)
                logger.info(f"{file} has been loaded")
                tokenized_dataset = raw_dataset.map(
                    tokenize_function,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    remove_columns="text",
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'tokenized.arrow') for k in raw_dataset},
                    desc="Running tokenizer on dataset",
                )
                grouped_datasets = tokenized_dataset.map(
                    group_texts,
                    batched=True,
                    num_proc=data_args.preprocessing_num_workers,
                    load_from_cache_file=True,
                    keep_in_memory=False,
                    cache_file_names = {k: os.path.join(cache_dir, 'grouped.arrow') for k in tokenized_dataset},
                    desc=f"Grouping texts in chunks of {block_size}",
                )
                processed_dataset = grouped_datasets
                processed_dataset.save_to_disk(cache_path)
            if idx == 0:
                lm_datasets = processed_dataset['train']
            else:
                assert lm_datasets.features.type == processed_dataset["train"].features.type
                lm_datasets = concatenate_datasets([lm_datasets, processed_dataset["train"]])

        lm_datasets = lm_datasets.train_test_split(test_size = data_args.validation_split_percentage)

    if training_args.do_train:
        train_dataset = lm_datasets['train']
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))
        logger.info(f"Num train_samples  {len(train_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(train_dataset[0]['input_ids']))
    if training_args.do_eval:
        eval_dataset = lm_datasets["test"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        logger.info(f"Num eval_samples  {len(eval_dataset)}")
        logger.info("training example:")
        logger.info(tokenizer.decode(eval_dataset[0]['input_ids']))



    if model_args.model_name_or_path:
        torch_dtype = (
            model_args.torch_dtype
            if model_args.torch_dtype in ["auto", None]
            else getattr(torch, model_args.torch_dtype)
        )
        model = LlamaForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            from_tf=bool(".ckpt" in model_args.model_name_or_path),
            config=config,
            cache_dir=model_args.cache_dir,
            revision=model_args.model_revision,
            use_auth_token=True if model_args.use_auth_token else None,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
        )
    else:
        model = AutoModelForCausalLM.from_config(config)
        n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())
        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")

    model_vocab_size = model.get_output_embeddings().weight.size(0)
    if not (
       (model_vocab_size==32000 and len(tokenizer)==51008) or \
       (model_vocab_size==32000 and len(tokenizer)==32000) or \
       (model_vocab_size==51008 and len(tokenizer)==51008) or \
       (model_vocab_size==49954 and len(tokenizer)==49954)
       
    ):
        raise ValueError(
            f"The combination of base model (size: {model_vocab_size}) and tokenizer (size: {len(tokenizer)}) is not a valid configuration. Please check our project wiki for further information. \n"
            "Valid configurations (base model / tokenizer):\n"
            "- Continue pre-training original LLaMA: 32000 / 32000 \n"
            "- Pre-training (Chinese) Amharic LLaMA based on original LLaMA: 32000 / 51008 \n"
            "- Continue pre-training (Chinese) Amharic LLaMA: 51008 / 51008 \n"
            "- Continue pre-training Chinese Alpaca: 49954 / 49954 \n")

    model.resize_token_embeddings(len(tokenizer))
    if training_args.peft_path is not None:
        logger.info("Peft from pre-trained model")
        model = PeftModel.from_pretrained(model, training_args.peft_path)
    else:
        logger.info("Init new peft model")
        target_modules = training_args.trainable.split(',')
        modules_to_save = training_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')
        lora_rank = training_args.lora_rank
        lora_dropout = training_args.lora_dropout
        lora_alpha = training_args.lora_alpha
        logger.info(f"target_modules: {target_modules}")
        logger.info(f"lora_rank: {lora_rank}")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=lora_rank, lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            modules_to_save=modules_to_save)
        model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

                                

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
        modules_to_save = ["embed_tokens","lm_head"]
    )

    """
        config = {
        'lora_config': lora_config,
        'learning_rate': 2e-4,
        'num_train_epochs': 1,
        'gradient_accumulation_steps': 2,
        'per_device_train_batch_size': 1,
        'per_device_eval_batch_size': 1,
        'gradient_checkpointing': True,
    }
    """


    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=fault_tolerance_data_collator,
        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
        if training_args.do_eval and not is_torch_tpu_available()
        else None,
    )

    """
    output_dir=training_args.output_dir,
    overwrite_output_dir=True,
    bf16=True,          logging_dir=f"{training_args.output_dir}/logs",
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="steps",
    save_steps=5000,
    optim="adamw_torch_fused",
    max_steps=-1,     **{k:v for k,v in config.items() if k != 'lora_config'}
    """

    trainer.add_callback(SavePeftModelCallback)
        if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)

        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
    model.save_pretrained(training_args.output_dir)
        if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()

        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
        try:
            perplexity = math.exp(metrics["eval_loss"])
        except OverflowError:
            perplexity = float("inf")
        metrics["perplexity"] = perplexity

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)


if __name__ == "__main__":
    main()
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=/model/Llama-2-7b-hf
amharic_tokenizer_path=/model/Llama-2-7b-hf
dataset_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/data/cleaned
data_cache=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/cache
per_device_train_batch_size=32
per_device_eval_batch_size=1
gradient_accumulation_steps=1
output_dir=/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output

python pretrain.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${amharic_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 1 \
    --save_steps 7528 \
    --evaluation_strategy steps \
    --eval_steps 3000 \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --bf16 \
    --overwrite_output_dir \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --gradient_checkpointing \
import pandas as pd

df = pd.read_json("/data/fine_tun_data2.json")

df.tail()
df[df['output']=='not advertisement'].iloc[1]['input']
'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: ለኢሬቻ በኣል ወደቢሾፍቱ የተጓዙት የሲዳማ ወጣቶች(ኤጄቶዎች) ከሰኣታት በፊት ቢሾፍቱ ገብተዋል። @tsegabwolde @tikvahethiopia''
import pandas as pd

df = pd.read_csv("/data/wasu_mohammed_labeled.csv")

df.head()
df.shape
df['label'] = df['label'].fillna("Not Advertisement")

df.tail(5)
df
from datasets import Dataset




data_dict = {"text": df['text'].tolist()}




dataset = Dataset.from_dict(data_dict)

dataset.save_to_disk("../data/train")




from peft import PeftModel

from transformers import LlamaForCausalLM, LlamaConfig




def load_model(model_name, quantization):

    model = LlamaForCausalLM.from_pretrained(

        model_name,

        return_dict=True,

        load_in_8bit=quantization,

        device_map="auto",

        low_cpu_mem_usage=True,

    )

    return model






def load_peft_model(model, peft_model):

    peft_model = PeftModel.from_pretrained(model, peft_model)

    return peft_model




def load_llama_from_config(config_path):

    model_config = LlamaConfig.from_pretrained(config_path) 

    model = LlamaForCausalLM(config=model_config)

    return model
from transformers import LlamaTokenizer





MAIN_PATH = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)



example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685





tokens = tokenizer.tokenize(example)

print(tokens)
print(len(tokenizer))
example = 'አፖሎ ካለ " ኢንተርኔት ተቋርጦ ገንዘብ መላክም መቀበልም አልቻልኩ" ማለት የለም። *685




tokens = tokenizer.tokenize(example)

print(tokens)
df['text'][0]
df = pd.read_csv("/data/wasu_mohammed_labeled.csv")




total_word_count = 0

total_tokens = 0


for index, row in df.iterrows():

    
    text = row['text']

    if not isinstance(text, str): 

        continue



    

    
    word_count = len(text.split())



    
    total_word_count += word_count

    tokens = tokenizer.tokenize(text)

    total_tokens+=tokens

    print(tokens)

    






print("Total Word Count:", total_word_count)

print("Total tokens count: ",total_tokens)
total_tokens
df.shape
from transformers import (

    AutoModelForCausalLM,

    AutoTokenizer,

    BitsAndBytesConfig,

    HfArgumentParser,

    TrainingArguments,

    pipeline,

    LlamaForCausalLM, 

    LlamaTokenizer,

    logging,

)

from peft import (

    LoraConfig,

    TaskType,

    prepare_model_for_int8_training,

    PeftModel

)

import torch

LLAMA_DIR = '/model/Llama-2-7b-hf'

tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)



model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)

embedding_size = model.get_input_embeddings().weight.shape[0]



if len(tokenizer) != embedding_size:

    print("resize the embedding size by the size of the tokenizer")

    model.resize_token_embeddings(len(tokenizer))





new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'

model = PeftModel.from_pretrained(model, new_model)




prompt = "Who is Leonardo Da Vinci?"

pipe = pipeline(task="text-generation", model=model, tokenizer=model, max_length=200)

result = pipe(f"<s>[INST] {prompt} [/INST]")

print(result[0]['generated_text'])
ቨርቹዋል ረዳቶች እንደ Amazon&
run-20240203_155644-4hm9i4tp
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from transformers import AutoTokenizer, AutoModel
import os
import torch
import shutil
from dotenv import load_dotenv
load_dotenv()
import sys



OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

model_name = 'Davlan/bert-base-multilingual-cased-finetuned-amharic'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

data_path = '../data/'
CHROMA_PATH = '../RAG/chromadb'

def embed_doc(document):
    
        encoded_input = tokenizer(document, padding=True, truncation=True, return_tensors='pt')
    
        with torch.no_grad():
        outputs1 = model(**encoded_input)

    

        embeddings1 = outputs1.last_hidden_state.squeeze(dim=0)
    

        
    return embeddings1

def load_documents(data_path):    
    try:
        loader = DirectoryLoader(data_path)
        documents = loader.load()       
        print("data loaded sucessfully")
        return documents[0].page_content
    except:
        print("document not found!")
        return None
    

def split_text(documents:list[Document]):
    try:
        text_spliter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=500,
            length_function=len,
            add_start_index = True
        )
        chunk = text_spliter.split_documents(documents)
        print("data splited successfuly!")
        return chunk
    except:
        print("document not found")

def save_chunks_to_chroma(chunks):
        if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
    try:
        db = Chroma.from_documents(chunks,embed_doc(),\
                                persist_directory=CHROMA_PATH)
        db.persist()
        print("Vectorstore created successfully!")
    except:
        print("Couldn't create the vectore database")

def generate_data_store():
    documents = load_documents(data_path)
    chunks = split_text(documents)
    embeding1 = embed_doc(chunks)
    print(embeding1)
    save_chunks_to_chroma(embeding1) 

def main():
    generate_data_store()      


if __name__ == "__main__":
    main()
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
parsed_dir = "../data/parsed"

cleaned_dir = "../data/cleaned"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{parsed_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape
df = df.dropna()

df.head()
df.shape
df = df.replace('\n', ' ', regex=True)

df.head()

df['hashtags'] = df['text'].apply(lambda x: preprocess.extract_hashtags(x))

df.head()

df['text'] = df['text'].str.replace(r'\
df.head()

df['emojis'] = df['text'].apply(preprocess.extract_emojis)

df.tail()

df['text'] = df['text'].apply(preprocess.remove_emojis)






letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]

for letter in letters:

  for i in range(len(letter[0])):

    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

    

df['symbols'] = df['text'].apply(preprocess.extract_symbols)

df.tail()
df['text'] = df['text'].apply(preprocess.remove_symbols)

df.tail()
df['links'] = df['text'].apply(preprocess.extract_urls)

df.tail()
df['text'] = df['text'].str.replace(preprocess.url_pattern, '', regex=True).str.strip()

df.tail()
df['mentions'] = df['text'].apply(preprocess.extract_mentions)

df.tail()
df['text'] = df['text'].str.replace(preprocess.mention_pattern, '', regex=True).str.strip()

df.tail()
df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()
df['text'] = df['text'].replace(r'!+', '!', regex=True)

df['text'] = df['text'].replace(r'\.+', '', regex=True)
df.tail()
df.to_csv(f"{cleaned_dir}/{file_name}.csv")
df['text'].to_csv(f"{cleaned_dir}/{file_name}.txt", index=False, header=False)
import sys, os

import pandas as pd

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
raw_dir = "../data/raw"

parsed_dir = "../data/parsed"

file_name = "አዲስ ነገር መረጃ"

preprocess = MyPreprocessing()
data = preprocess.read_file(f'{raw_dir}/{file_name}.json')
parsed_message = preprocess.parse_messages(data['messages'])


df = pd.DataFrame(parsed_message)

df.set_index('id', inplace=True)

df.head()
df.shape
df.to_csv(f'{parsed_dir}/{file_name}.csv')
import sys, os

import pandas as pd

from gensim.models import Word2Vec

sys.path.append(os.path.abspath(os.path.join('..')))

from utils.preprocessing import MyPreprocessing
cleaned_dir = "../data/cleaned"

final_dir = "../data/final"

file_name = "TIKVAH"

preprocess = MyPreprocessing()
df = pd.read_csv(f"{cleaned_dir}/{file_name}.csv", index_col='id')

df.head()
df.shape

tokenized_corpus = [str(sentence).lower().split() for sentence in df['text']]

embedding_size = 100 
window_size = 5 
min_count = 5 
model = Word2Vec(sentences=tokenized_corpus, vector_size=embedding_size, window=window_size, min_count=min_count, workers=4)

vector = model.wv['ኢትዮጵያ'] 
vector

similar_words = model.wv.most_similar('ኢትዮጵያ', topn=5)

similar_words

model.save(f'{final_dir}/{file_name}_word2vec_model.bin')
import os 

from huggingface_hub import hf_hub_download
HUGGING_FACE_API_KEY = os.environ.get("HUGGING_FACE_API_KEY")
model_id = "iocuydi/llama-2-amharic-3784m"

filenames = [

    ".gitattributes", "adapter_config.json", "adapter_model.bin", "config.json", "generation_config.json", 

    "inference_demo.py", "special_tokens_map.json", "tokenizer.json", "tokenizer.model", "tokenizer_config.json"

]
for filename in filenames:

    downloaded_model_path = hf_hub_download(

        repo_id = model_id,

        filename = filename,

        token = HUGGING_FACE_API_KEY

    )

    print(downloaded_model_path)
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM



tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)

model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

pipeline = pipeline ("Text-Generation", model=model, device=-1, tokenizer=tokenizer, max_length=1000 )
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from PyPDF2 import PdfReader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,
)



persist_directory = "db"


class MySpecialFunctions:
    def __init__(self):
        pass
    
    def get_file_text(self, files):
        text = ""
        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    text += content
            except Exception as e:
                print(f"Error reading file {file}: {e}")

        return text
    
    
    def get_pdf_text(self, pdf):
        text = ""
        for doc in pdf:
            reader = PdfReader(doc)
            for page in reader.pages:
                text += page.extract_text()

        return text


    
    
    def get_text_chunks(self, text):
        text_siplitter = RecursiveCharacterTextSplitter(
            chunk_size = 1000, 
            chunk_overlap = 200,
            separators=['\n', '\n\n'],
            length_function = len)
        chunk = text_siplitter.split_text(text)
        return chunk
    

    def get_vectorstore(self, chunks):
                        hf_embedding = HuggingFaceEmbeddings()

                                
                embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        
                vector_db = Chroma.from_documents(
            documents = chunks,
            embedding = hf_embedding,
                    )

        return vector_db
import streamlit as st 
from MyFunctions import MySpecialFunctions
from dotenv import load_dotenv


special_functions_instance = MySpecialFunctions()

def get_text(external_data ):
    return special_functions_instance.get_file_text(external_data )

def get_pdf_text(external_data ):
    return special_functions_instance.get_pdf_text(external_data )

def get_chunks(text):
    return special_functions_instance.get_text_chunks(text)

def get_vectorstore(text_chunks):
    return special_functions_instance.get_vectorstore(text_chunks) 





def main():
    load_dotenv()
    st.set_page_config(page_title="Generation of Telegram Ads in Amharic", page_icon= ":smile")
    

    with st.sidebar:
                pass

    st.markdown("    external_data = st.file_uploader(" Upload the generative text from fune-tuning", accept_multiple_files= True)
    if st.button("Retrieval"):
        with st.spinner("Processing"):
                                    text = get_pdf_text(external_data )
            
                        text_chunks = get_chunks(text)
            st.write(text_chunks)


                        vectorstore_db = get_vectorstore(text_chunks)
                                    




if __name__ == "__main__":
    main()
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')




sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))




print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))
print(sp.decode_ids([460, 133, 774, 1276]))


print(sp.get_piece_size())




print(sp.id_to_piece(460))

print(sp.piece_to_id('▁በአዲስ'))




print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))





for id in range(3):

  print(sp.id_to_piece(id), sp.is_control(id))


import tensorflow as tf




serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()



sp = spm.SentencePieceProcessor()

sp.load_from_serialized_proto(serialized_model_proto)



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))










spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')



sp_ctrl = spm.SentencePieceProcessor()

sp_ctrl.load('m_ctrl.model')




print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))

print(sp_ctrl.piece_to_id('<sep>'))  
print(sp_ctrl.piece_to_id('<cls>'))  
print('3=', sp_ctrl.decode_ids([3]))  
print('4=', sp_ctrl.decode_ids([4]))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   


sp = spm.SentencePieceProcessor()

sp.load('m_bos_as_user.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   

spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')



print('bos=', sp.bos_id())

print('eos=', sp.eos_id())

print('unk=', sp.unk_id())

print('pad=', sp.pad_id())  




print(sp.encode_as_ids('በአዲስ አበባ'))




print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])


spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')





for n in range(10):

  print(sp.sample_encode_as_pieces('በአዲስ አበባ', -1, 0.1))



for n in range(10):

  print(sp.sample_encode_as_ids('በአዲስ አበባ', -1, 0.1))

print(sp.nbest_encode_as_pieces('በአዲስ አበባ', 10))

print(sp.nbest_encode_as_ids('በአዲስ አበባ', 10))


spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')

sp_bpe = spm.SentencePieceProcessor()

sp_bpe.load('m_bpe.model')



print('*** BPE ***')

print(sp_bpe.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_bpe.nbest_encode_as_pieces('በአዲስ አበባ', 5))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')

sp_unigram = spm.SentencePieceProcessor()

sp_unigram.load('m_unigram.model')



print('*** Unigram ***')

print(sp_unigram.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_unigram.nbest_encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ', 5))
%pip install emoji
import sys, os, json, re, zipfile, csv

import pandas as pd

import emoji

class Util():

    def __init__(self) -> None:

        self.emoji_pattern = re.compile(r"[\U0001F000-\U0001F9FF\U0001FA00-\U0001FFFF\U00020000-\U0002FFFF\U00030000-\U0003FFFF]+", flags=re.UNICODE)



        

        self.symbols = re.compile("["

                                  "\""

                                  "\“"

                                  "\""

                                  "\'"

                                  "\-"

                                  "\*"

                                  "\•"

                                  "\ℹ"

                                  "\﻿"

                                  "\_"

                                  "]+")

        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

        self.mention_pattern = r'@(\w+)'



    def read_file(self, file_path: str) -> dict:

        
        with open(file_path, 'r') as file:

            
            data = json.load(file)

            return data



    def write_file(self, file_path: str, data: dict) -> None:

        
        with open(file_path, 'w') as file:

            
            json.dump(data, file, indent=2)



    def parse_text(self, text: any) -> str:

        if isinstance(text, str):

            return text

        elif isinstance(text, list):

            contents = []

            for item in text:

                if isinstance(item, str):

                    contents.append(item)

                elif isinstance(item, dict):

                    contents.append(item['text'])

            return "".join(contents)

        else:

            return ""



    def parse_messages(self, messages: list) -> dict:

        parsed_messages = {

            'id': [],

            'text': [],

            'date': []

        }

        for message in messages:

            if message['type'] != 'message' or len(message['text']) == 0:

                continue

            parsed_messages['id'].append(message['id'])

            message_content = self.parse_text(message['text'])

            parsed_messages['text'].append(message_content)

            parsed_messages['date'].append(message['date'])

        return parsed_messages



    def extract_hashtags(self, text: str) -> list:

        return [word for word in text.split() if word.startswith('


    def extract_emojis(self, text):

        return ''.join(self.emoji_pattern.findall(text))



    def remove_emojis(self, text):

        return self.emoji_pattern.sub('', text)



    def extract_symbols(self, text):

        return ''.join(self.symbols.findall(text))



    def remove_symbols(self, text):

        return self.symbols.sub(' ', text)



    def extract_urls(self, text):

        return re.findall(self.url_pattern, text)



    def extract_mentions(self, text):

        return re.findall(self.mention_pattern, text)

    

    def extract_fields(self, message):

        """

        Extracts relevant fields from the message.

        Returns a tuple containing (channel_id, text, date, labels).

        """

        text = ' '.join(item['text'] for item in message['text_entities'] if item['type'] in 'plain')

        date = message['date']

        labels = "LABEL"  
        return text, date, labels



    def process_json_file(self, json_file, csv_writer):

        """

        Processes a JSON file, extracts relevant fields, and writes to CSV.

        """

        data = json.load(json_file)



        channel_id = data['id']

        for message in data['messages']:

            text, date, labels = self.extract_fields(message)

            csv_writer.writerow([channel_id, text, date, labels])



    def process_zip(self, zip_file_path, output_csv_path):

        """

        Processes a zip file, extracts data from JSON files, and writes to a CSV file.

        """

        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:

            with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:

                csv_writer = csv.writer(csv_file)

                csv_writer.writerow(['id', 'text', 'date', 'label'])



                for file_info in zip_file.infolist():

                    with zip_file.open(file_info.filename) as json_file:

                        print(json_file)

                        self.process_json_file(json_file, csv_writer)



    def process_zip_files(self, zip_file_path, output_directory):

        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:

            
            for file_info in zip_file.infolist():

                with zip_file.open(file_info.filename) as json_file:

                    
                    data = json.load(json_file)

                    parsed_data = self.parse_json_data(data)



                    
                    df = pd.DataFrame(parsed_data)



                    
                    output_file_name = os.path.splitext(os.path.basename(file_info.filename))[0]

                    output_csv_path = os.path.join(output_directory, f"{output_file_name}_parsed.csv")

                    df.to_csv(output_csv_path, index=False)



    def parse_json_data(self, data):

        
        
        parsed_data = {

            'id': [],

            'text': [],

            'date': [],

            'label': []

        }



        for message in data['messages']:

            
            text, date, labels = self.extract_fields(message)

            parsed_data['id'].append(data['id'])

            parsed_data['text'].append(text)

            parsed_data['date'].append(date)

            parsed_data['label'].append(labels)

            

        return parsed_data

                        

    def file_reader(self, path: str, ) -> str:

        fname = os.path.join(path)

        with open(fname, 'r') as f:

            system_message = f.read()

        return system_message

util = Util()










zip_file_path = "../data/raw/raw.zip"

output_csv_path = "../data/parsed/parsed.csv"



util.process_zip(zip_file_path, output_csv_path)

print("Parsing completed. Output saved to", output_csv_path)























parsed_csv_path = "../data/parsed/parsed.csv"

output_cleaned_csv_path = "../data/parsed/cleaned_parsed.csv"




df = pd.read_csv(parsed_csv_path)

df = df.dropna()

df.head()

df['text'] = df['text'].replace('\n', ' ', regex=True)

df.head()

df['text'] = df['text'].str.replace(r'\
df.head()


df['text'] = df['text'].apply(util.remove_emojis)

df.head()

df['text'] = df['text'].apply(util.remove_symbols)

df.head()

df['text'] = df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

df.head()
df['text'] = df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

df.head()

df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

df['text'] = df['text'].replace(r'!+', '!', regex=True)

df['text'] = df['text'].replace(r'\.+', '', regex=True)

df.head()







letters = [

  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

]



for letter in letters:

  for i in range(len(letter[0])):

    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])



df.head()  


df['text'] = df['text'].str.replace(r'[A-Za-z]+', '', regex=True)

df.head()

cleaned_output_path = "../data/parsed/cleaned_parsed_data.csv"

df.to_csv(cleaned_output_path, index=False)

output_text_path = "../data/cleaned/cleaned.txt"

df['text'] = df['text'].apply(util.remove_emojis)

df['text'].to_csv(output_text_path, index=False, header=False, sep='\t')
zip_file_path = "../data/raw/raw.zip"

output_directory = "../data/parsed/"

util.process_zip_files(zip_file_path, output_directory)
import os

import pandas as pd




parsed_files_directory = "../data/parsed/"

cleaned_files_directory = "../data/cleaned/"




for filename in os.listdir(parsed_files_directory):

    if filename.endswith("_parsed.csv"):

        
        filepath = os.path.join(parsed_files_directory, filename)

        df = pd.read_csv(filepath)



        
        df = df.dropna()



        
        df['text'] = df['text'].replace('\n', ' ', regex=True)



        
        df['text'] = df['text'].str.replace(r'\


        
        df['text'] = df['text'].apply(util.remove_emojis)



        
        df['text'] = df['text'].apply(util.remove_symbols)



        
        df['text'] = df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()

        df['text'] = df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()



        
        df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()

        df['text'] = df['text'].replace(r'!+', '!', regex=True)

        df['text'] = df['text'].replace(r'\.+', '', regex=True)



        
        letters = [

            [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

            [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],

            [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],

            [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],

            [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]

        ]



        for letter in letters:

            for i in range(len(letter[0])):

                df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])



        
        df['text'] = df['text'].str.replace(r'[A-Za-z]+', '', regex=True)



        
        df.to_csv(filepath, index=False)



        
        cleaned_text_path = os.path.join(cleaned_files_directory, f"{os.path.splitext(filename)[0]}.txt")

        df['text'].to_csv(cleaned_text_path, index=False, header=False)
import os
import pandas as pd
from utils import Util

def clean(filepath):
    df = pd.read_csv(filepath)

        df = df.dropna()

        df['text'] = df['text'].replace('\n', ' ', regex=True)

        df['text'] = df['text'].str.replace(r'\
        df['text'] = df['text'].apply(util.remove_emojis)

        df['text'] = df['text'].apply(util.remove_symbols)

        df['text'] = df['text'].str.replace(util.url_pattern, '', regex=True).str.strip()
    df['text'] = df['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()

        df['text'] = df['text'].str.replace('\s+', ' ', regex=True).str.strip()
    df['text'] = df['text'].replace(r'!+', '!', regex=True)
    df['text'] = df['text'].replace(r'\.+', '', regex=True)

        letters = [
        [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
        [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
        [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],
        [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],
        [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]
    ]

    for letter in letters:
        for i in range(len(letter[0])):
            df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])

        df['text'] = df['text'].str.replace(r'[A-Za-z]+', '', regex=True)


        cleaned_text_path = os.path.join(cleaned_files_directory, f"{os.path.splitext(filename)[0]}.txt")
    df['text'].to_csv(cleaned_text_path, index=False, header=False)

def clean_all_in_one(input_path, output_path):
        parsed_csv_path = "../data/parsed/parsed.csv"
    output_cleaned_csv_path = "../data/parsed/cleaned_parsed.csv"

    




if __name__ == "__main__":
    util = Util()
import os
import pandas as pd
from utils import Util

def parse_all_in_one():
    zip_file_path = "../data/raw/raw.zip"
    output_csv_path = "../data/parsed/parsed.csv"

    util.process_zip(zip_file_path, output_csv_path)
    print("Parsing completed. Output saved to", output_csv_path)

def parse_cleaned_individual_files():
    zip_file_path = "../data/raw/raw.zip"
    output_directory = "../data/parsed/"
    util.process_zip_files(zip_file_path, output_directory)


if __name__ == "__main__":
    util = Util()
    parse_all_in_one()
    parse_cleaned_individual_files()
Backend code for Amharic Ad generator.


This backend handles prompt generation based on user input.



- Python (version 3.11.7)
- Flask (install via `pip install fastapi`)
- Uvicorn (install via `pip install uvicorn`)
- ...


```bash
git clone https://github.com/group-3-collab-team/Amharic-RAG-Ad-Builder.git
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```
import sentencepiece as spm





spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')




sp = spm.SentencePieceProcessor()

sp.load('m.model')




print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))

print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))




print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))
print(sp.decode_ids([460, 133, 774, 1276]))


print(sp.get_piece_size())




print(sp.id_to_piece(460))

print(sp.piece_to_id('▁በአዲስ'))




print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))





for id in range(3):

  print(sp.id_to_piece(id), sp.is_control(id))


import tensorflow as tf




serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()



sp = spm.SentencePieceProcessor()

sp.load_from_serialized_proto(serialized_model_proto)



print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))










spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')



sp_user = spm.SentencePieceProcessor()

sp_user.load('m_user.model')






print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))

print(sp_user.piece_to_id('<sep>'))  
print(sp_user.piece_to_id('<cls>'))  
print('3=', sp_user.decode_ids([3]))  
print('4=', sp_user.decode_ids([4]))  
spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')



sp_ctrl = spm.SentencePieceProcessor()

sp_ctrl.load('m_ctrl.model')




print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))

print(sp_ctrl.piece_to_id('<sep>'))  
print(sp_ctrl.piece_to_id('<cls>'))  
print('3=', sp_ctrl.decode_ids([3]))  
print('4=', sp_ctrl.decode_ids([4]))  spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   


sp = spm.SentencePieceProcessor()

sp.load('m_bos_as_user.model')

print(sp.encode_as_pieces('<s> በአዲስ</s>'))   

spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')



sp = spm.SentencePieceProcessor()

sp.load('m.model')



print('bos=', sp.bos_id())

print('eos=', sp.eos_id())

print('unk=', sp.unk_id())

print('pad=', sp.pad_id())  




print(sp.encode_as_ids('በአዲስ አበባ'))




print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])


spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')





for n in range(10):

  print(sp.sample_encode_as_pieces('በአዲስ አበባ', -1, 0.1))



for n in range(10):

  print(sp.sample_encode_as_ids('በአዲስ አበባ', -1, 0.1))

print(sp.nbest_encode_as_pieces('በአዲስ አበባ', 10))

print(sp.nbest_encode_as_ids('በአዲስ አበባ', 10))


spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')

sp_bpe = spm.SentencePieceProcessor()

sp_bpe.load('m_bpe.model')



print('*** BPE ***')

print(sp_bpe.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_bpe.nbest_encode_as_pieces('በአዲስ አበባ', 5))  spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')

sp_unigram = spm.SentencePieceProcessor()

sp_unigram.load('m_unigram.model')



print('*** Unigram ***')

print(sp_unigram.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))

print(sp_unigram.nbest_encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ', 5))
import json

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter  
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Weaviate
import logging
from typing import List, Optional, Union
from langchain.prompts import PromptTemplate
from transformers import pipeline


import transformers
from sentence_transformers import SentenceTransformer



from datasets import Dataset

import weaviate
from dotenv import load_dotenv,find_dotenv
from weaviate.embedded import EmbeddedOptions

 
load_dotenv(find_dotenv())

logger = logging.getLogger(__name__)

def data_loader(file_path: str, chunk_size: int = 500, chunk_overlap: int = 50) -> Union[List[str], None]:
    """
    Load data from a file, split it into chunks, and return the chunks.

    Parameters:
    - file_path (str): The path to the file containing the data.
    - chunk_size (int): The size of each data chunk. Default is 500.
    - database (int): The overlap between consecutive chunks. Default is 50.

    Returns:
    - list: A list of data chunks.
    """
    try:
        loader = TextLoader(file_path)
        documents = loader.load()

                text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = text_splitter.split_documents(documents)
        
        logger.info("Data loaded to vector database successfully")
        return chunks
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        return None 


        





    
    
def create_retriever(chunks, model):
   try:
              load_dotenv(find_dotenv())

    

           client = weaviate.Client(embedded_options=EmbeddedOptions())

              vectorstore = Weaviate.from_documents(
           client=client,
           documents=chunks,
           embedding=model.encode,             by_text=False
       )

              retriever = vectorstore.as_retriever()
       print("Retriever created successfully.")

       return retriever

   except Exception as e:
       print(f"An unexpected error occurred: {e}")
       return None




        



    

def create_langchain_pipeline(retriever, template, temperature=0, model_name="meta-llama/Llama-2-7b-chat-hf"):
    try:
                model_name = "meta-llama/Llama-2-7b-chat-hf"
        token = "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"
        
        
                        llm = pipeline("text-generation", model=model_name, temperature=temperature)

                prompt = PromptTemplate.from_template(template)

                rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        print("Langchain with RAG pipeline created successfully.")
        return rag_chain

    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None
    

def generate_testcase_and_context(questions, ground_truths, retriever, rag_chain):
    try:
        answers = []
        contexts = []

                for query in questions:

            answers.append(rag_chain.invoke(query))
            contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])

            
        data = {
            "question": questions,             "answer": answers,             "contexts": contexts,             "ground_truths": ground_truths         }


                dataset = Dataset.from_dict(data) 
        print("automatic evaluation data generated succesfully.")

        return  dataset
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 


    


def load_file(file_path):
    try:

                with open(file_path, 'r') as file:
                        file_contents = file.read()   
        
        return file_contents
        
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None 


def get_generated_prompt_with_evaulation(question):
    try:
        chunks = data_loader()
        retriever = create_retriever(chunks)

        prompt_template = load_file('../prompts/prompt-generation-prompt.txt')
        evaluation_tempate = load_file('../prompts/evaluation-data-generation.txt')


        prompt_rag_chain = create_langchain_pipeline(retriever, prompt_template)
        evaulation_rag_chain = create_langchain_pipeline(retriever, evaluation_tempate, temperature=0.2)


        generated_prompts = prompt_rag_chain.invoke(question)
        prompt_list  = json.loads(generated_prompts)

        questions = [item['prompt'] for item in prompt_list]
        ground_truths = [[item['ground_truth']] for item in prompt_list]

        response = generate_testcase_and_context(questions, ground_truths, retriever, evaulation_rag_chain)
        return response
    
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None
Welcome to the front-end repository of the Enterprise-Grade RAG System, built with React.js.


To run the development server, use one of the following commands:

```bash
npm run dev
yarn dev
npm dev
max-width: 1440px;
  margin: 0 auto;
  /* padding: 2rem; */
  text-align: center;
}

nav{
  background-color: }
.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
}
.logo:hover {
  filter: drop-shadow(0 0 2em }
.logo.react:hover {
  filter: drop-shadow(0 0 2em }

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: }
@tailwind base;
@tailwind components;
@tailwind utilities;

* {
  font-family: "Inter", sans-serif;
}

@media screen and (min-width: 480px) {
  .card:nth-child(7n + 1) {
    grid-column: auto/span 2;
    grid-row: auto/span 2;
  }
}

.prompt::-webkit-scrollbar {
  width: 5px;
}

.prompt::-webkit-scrollbar-thumb {
  background-color:   border-radius: 5px;
}

input {
  color: }

body {
  background-color: theme("colors.hero") ;
  color: theme("colors.grey");
  scroll-behavior: smooth;
  font-size: 14px;
}

.paginate,
.next,
.prev {
  border-radius: 50%;
  background-color: theme("colors.grey");
  padding: 0.2rem 0.6rem;
  font-size: 0.8rem;
  display: flex;
  justify-content: center;
  align-items: center;
  color: theme("colors.dark");
  font-weight: bold;
  margin: 0 0.1rem;
  cursor: pointer;
  transition: all 0.5s ease;
}

.next:hover,
.prev:hover,
.paginate:hover {
  color: theme("colors.brand");
}

header {
  z-index: 100;
}
.paginate.active {
  background-color: theme("colors.accent");
}
.next,
.prev {
  border-radius: 10px;
  background-color: theme("colors.grey");
}

.next:disabled,
.prev:disabled {
  background-color: theme("colors.darkgrey");
  color: theme("colors.grey");
  opacity: 0.3;
  cursor: not-allowed;
}

h1 {
  font-size: 2rem;
  font-weight: 600;
}

.hero {
  display: grid;
  grid-template-columns: 100%;
  align-items: center;
  grid-gap: 10%;
  justify-content: center;
}

.hero__img {
  width: 90%;
  height: 80%;
  position: relative;
  overflow: hidden;
  border-radius: 10px 0 0 300px;
  display: none;
}

.hero__img img {
  transition: all 0.6s ease;
  cursor: pointer;
}

.hero__img img:hover {
  transform: scale(1.2);
}

::placeholder {
  color:   font-size: 0.75rem;
}
.form {
  display: grid;
  grid-template-columns: 1fr;
  grid-gap: 2rem;
}

.form_photo {
  width: 100%;
  height: 100%;
  margin-top: auto;
}
@media screen and (min-width: 768px) {
  .form {
    grid-template-columns: 1fr 1fr;
  }
  .form_photo {
    width: 80%;
    height: 80%;
  }
  .hero {
    grid-template-columns: 40% 50%;
  }
  .hero__img {
    display: block;
  }
  h1 {
    font-size: 2.5rem;
  }
  body{
    font-size: 16px;
  }
}
import React from 'react'
import { BrowserRouter, Route, Routes, Link } from 'react-router-dom'
import { aiqem_logo } from "./assets"
import { Home, CreatePost } from "./pages"
import ChatPage from './pages/ChatPage'


const App = () => {
  return (
    <BrowserRouter>
      <header className="w-full fixed flex justify-between items-center bg-blue-800 sm:px-8 px-4 py-4 border-b border-blue-800">
        <Link to="/">
          <img src={aiqem_logo} alt="Logo" /> 
        </Link>
        <Link to="/" className="font-inter font-medium text-white px-2 ml-auto">Home</Link>
        <Link to="/create" className="font-inter font-bold bg-blue-800 text-white px-2 py-1 rounded-md">Chat</Link>
      </header>
      <main className="py-8 w-full bg-white  min-h-[calc(100vh)]">
        <Routes>
          <Route path="/create" element={<CreatePost />} />
          <Route path="/" element={<Home />} />
          {/* <Route path="/chat" element={<ChatPage />} /> */}

        </Routes>
      </main>
    </BrowserRouter>
  )
}

export default App

//106e75
import download from "./download.png";
import aiqem_logo from "./aiqem_logo.svg";
import logo from "./aiqem_logo.svg";
import preview from "./preview.png";
import hero from "./hero.png";
import africa1 from "./africa1.jpg";
import images from "./images.jpeg"
import telegram from "./telegram.jpeg";
export { download, aiqem_logo, logo,  preview, hero, africa1 , images, telegram};
import React from 'react'
import { useState, useRef} from 'react';
import axios from "axios";

const FileUpload = () => {
    const fileInputRef = useRef(null);

    const [text, setText] = useState("");

    const handleButtonClick = () => {
        // Trigger the file input when the button is clicked
        fileInputRef.current.click();
      };

    const handleFileChange = async (event) => {
    const selectedFile = event.target.files[0];

    const formData = new FormData();
    formData.append('file', selectedFile);


    try {
        const response = await axios.post('http://127.0.0.1:5000/extract-text', formData, {
        headers: { 'Content-Type': 'multipart/form-data' },
        });
        setText(response.data.data);
    } catch (error) {
        console.error(error);
    }

    };

      
  return (
    <div>
    <button onClick={handleButtonClick}>
      <span role="img" aria-label="attachment">📎</span>
    </button>
    <input
      ref={fileInputRef}
      type="file"
      accept=".pdf"
      style={{ display: 'none' }}
      onChange={handleFileChange}
    />  
</div>
  )
}

export default FileUpload
import React, { useState } from "react";
import { useNavigate } from "react-router-dom";

import { preview } from "../assets";
import { getRandomPrompt } from "../utils";
import { FormFields, Loader } from "../components";
import FileUpload from "../components/FileUpload";
import { africa1 } from "../assets/index";
import ChatPage from "./ChatPage";

const CreatePost = () => {
    const navigate = useNavigate();
    const models_list = [
        {"model_name": "Llama2 Model",
        "model_type": ""
    },
    {"model_name": "Finetuned Llama2 Model",
    "model_type": ""
    },
    {"model_name": "GPT 3.5 TURBO Model",
    "model_type": "gpt-3.5-turbo"
    },
    {"model_name": "GPT 4.0 Model",
    "model_type": "gpt-4-1106-preview"
    }
    ]
    const [selectedModel, setSelectedModel] = useState(models_list[0].model_name); // Set the default selected model
    console.log(selectedModel)
    const [message, setMessage] = useState('');
    const [chatResponse, setchatResponse] = useState('');

    console.log("selectedModel create page: ", selectedModel)


    const submitQuery= async () => {
        if (form.scenario) {
            try {
                setGeneratingprompt(true);
                const response = await fetch(
                    "https://192.168.137.236/api/generate",
                    {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                        },
                        body: JSON.stringify({
                            prompt: form.scenario,
                        }),
                    }
                );
                const data = await response.json();
                setForm({ ...form, preview: `data:image/jpeg;base64,${data.photo}` });
                setResult(data.result); // Set the result in the state
            } catch (err) {
                console.log(err);
            } finally {
                setGeneratingprompt(false);
            }
        } else {
            alert("Please provide a proper prompt");
        }
    };

    const handleMessageChange = (event) => {
        setMessage(event.target.value);
    };

    console.log('Submitted input:', message);
    console.log('Submitted input:', chatResponse);

    const submitInput = async () => {
        // Handle submitting the input, you can use the 'message' state here
        // Add your logic for submitting the input to the backend
        try {
        const response = await fetch(
            'https://9bba-196-189-127-38.ngrok-free.app/api/v1/chat', {

            method: 'POST',
            headers: {
            'Content-Type': 'application/json',
            },
            body: JSON.stringify({ "message":message , "model_type":models_list[selectedModel].model_type}),
        });

        if (response.ok) {
            
            setMessage('')
            setchatResponse(response);
        } else {
            console.error('Failed to submit input to the backend');
        }
        } catch (error) {
        console.error('Error during API call:', error);
        }
    };


    const handleSubmit = async (e) => {
        e.preventDefault();
    
        if (form.scenario && form.preview) {
            setLoading(true);
            try {
                const response = await fetch(
                    "https://192.168.137.236/api/generate",
                    {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                        },
                        body: JSON.stringify({ ...form}),
                    }
                );
    
                if (response.ok) {
                    const responseData = await response.json();
                    // Assuming the response has a property named "result"
                    const result = responseData.result;
    
                    // Do something with the result
                    console.log(result);
                    // You can also update your UI or state with the received result
                } else {
                    console.log("Failed to get a successful response from the server");
                }
            } catch (err) {
                console.error(err);
            } finally {
                setLoading(false);
            }
        } else {
            alert("Please generate a prompt with proper details");
        }
    };
    
    return (
        <section className="flex flex-row bg-white min-h-[calc(100vh)]">
                <div className="sm:hidden  md:flex md:flex-col md:w-1/3 md:flex-shrink-0 md:lg:w-[240px] md:h-[calc(100vh-120px)] md:whitespace-nowrap md:fixed bg-white md:overflow-x-hidden md:transition-all md:ease-in-out pt-2">
                    <div className="flex flex-col mt-24 items-start space-y-4  md:h-full ml-[16px]">
                        <label className="text-lg ml-4 font-bold text-black" id="demo-radio-buttons-group-label">
                            Select Model
                        </label>

                        <div className="flex flex-col space-y-2 pl-2 items-start">
      {models_list.map((model, index) => (
        <div key={index}>
          <input
            type="radio"
            id={`model-${index}`}
            name="radio-buttons-group"
            value={model.model_name}
            className="mr-2"
            checked={index === selectedModel}
            onChange={() => setSelectedModel(index)}
          />
          <label htmlFor={`model-${index}`} className="text-base text-black">
            {model.model_name}
          </label>
        </div>
      ))}
      
    </div>
                    </div>
                </div>
  {/* Main Content */}
                <div className="flex flex-col h-full md:w-3/4 px-4 py-6 sm:w-full">
                    {/* <div className="sm:flex sm:flex-col md:ml-[530px]  sm:ml-[100px] sm:mt-16 sm:font-extrabold sm:text-text sm:text-[42px]">
                        <h1 className="md:ml-[100px] text-black sm:text-[40px] sm:ml-[160px]">አድባር</h1>

                        <div className="flex justify-center space-x-6 mt-8 ml-[-4px]">

                            <div className="md:text-3xl text-xl text-black bg-gray-100 rounded-lg p-6 shadow-md sm:max-w-[400px] md:max-w-[1600px]">
                                <h2 className="font-bold ml-4">Retail</h2>
                                <p className="text-gray-600">Generate Telegram Ad</p>
                            </div>

                            <div className="md:text-3xl text-xl text-black bg-gray-100 rounded-lg p-6 shadow-md sm:max-w-[400px] md:max-w-[1600px]">
                                <h2 className="font-bold ml-4">Automotive</h2>
                                <p className="text-gray-600">Generate Telegram Ad</p>
                            </div>

                            <div className="md:text-3xl text-xl text-black bg-gray-100 rounded-lg p-6 shadow-md sm:max-w-[400px] md:max-w-[1600px]">
                                <h2 className="font-bold ml-4">Real Estate</h2>
                                <p className="text-gray-600">Generate Telegram Ad</p>
                            </div>
                            </div>
                </div> */}

                    <ChatPage selectedModel={selectedModel} />
                    
{/* 
                    <div className="sm:flex sm:flex-col md:ml-32 sm:ml-[-40px] sm:w-3/4">

                        <footer className="flex-row-2 mt-2 mb-2 border-blue-800 p-4 absolute bottom-0 ml-36 w-3/4" onSubmit={handleSubmit}>
                        <label for="chat" class="sr-only">Your message</label>
                            <div class="flex items-center py-2 px-3 bg-blue-800 rounded-lg dark:bg-blue-800">
                            <FileUpload/>
                            <div>

    
                                <textarea
                                    id="chat" 
                                    rows="1" 
                                    
                                    class="block mx-4 p-2.5 w-full text-sm text-gray-900 bg-white rounded-lg border focus:ring-blue-500 focus:border-blue-500 dark:bg-white-800 dark:border-blue-800 dark:placeholder-blue-800 dark:text-black dark:focus:ring-blue-500 dark:focus:border-blue-500"
                                    placeholder="Your message..."
                                    value={message}
                                    onChange={handleMessageChange}
                                />
      

                                <button
                                    type="submit"
                                    onClick={submitInput}
                                    class="inline-flex justify-center p-2 text-blue-600 rounded-full cursor-pointer hover:bg-blue-100 dark:text-blue-500 dark:hover:bg-gray-600">
                                    <svg
                                    className="w-6 h-6 rotate-90"
                                    fill="white"
                                    viewBox="0 0 20 20"
                                    xmlns="http://www.w3.org/2000/svg"
                                    >
                                    <path d="M10.894 2.553a1 1 0 00-1.788 0l-7 14a1 1 0 001.169 1.409l5-1.429A1 1 0 009 15.571V11a1 1 0 112 0v4.571a1 1 0 00.725.962l5 1.428a1 1 0 001.17-1.408l-7-14z"></path>
                                    </svg>
                                </button>
                            </div>

    {chatResponse && (
        <div style={{ border: '1px solid           <p>Response:</p>
          <pre>{JSON.stringify(chatResponse, null, 2)}</pre>
        </div>
      )}
                            </div>
                    </footer>
                                
                    </div> */}
                </div>

                <ChatPage/>
</section>
    );
};

export default CreatePost;
import React, { useState, useEffect } from "react";
import { Loader, FormFields, Card } from "../components";
import { Link } from "react-router-dom";
import { africa1, telegram } from "../assets/index";

const RenderCards = ({ data, title }) => {
    if (data?.length > 0) {
        return data.map((post) => <Card key={post._id} {...post} />);
    } else {
        return <h2 className="text-brand font-bold text-xl">{title}</h2>;
    }
};

const Home = () => {
    const [loading, setLoading] = useState(false);
    const [allPosts, setAllPosts] = useState([]);
    const [searchText, setSearchText] = useState("");
    const [filteredPosts, setFilteredPosts] = useState([]);
    const [searchTimeout, setSearchTimeout] = useState(null);

    useEffect(() => {
        const fetchPosts = async () => {
            setLoading(true);
            try {
                const response = await fetch(
                    "https://dalle-hn3a.onrender.com/api/v1/post",
                    {
                        method: "GET",
                        headers: {
                            "Content-Type": "application/json",
                        },
                    }
                );

                if (response.ok) {
                    const result = await response.json();
                    setAllPosts(result.data.reverse());
                }
            } catch (err) {
                console.log(err);
            } finally {
                setLoading(false);
            }
        };
        fetchPosts();
    }, []);

    const handleSearchChange = async (e) => {
        clearTimeout(searchTimeout);
        setSearchText(e.target.value);

        setSearchTimeout(
            setTimeout(() => {
                const filteredPosts = allPosts.filter((post) =>
                    post.prompt.toLowerCase().includes(searchText.toLowerCase())
                );
                setFilteredPosts(filteredPosts);
                setLoading(false);
            }, 500)
        );
    };

    // set dynamic imgPerPage value according to screen size
    if (window.innerWidth <= 768) {
        var dynamicPerPage = 3;
    } else {
        dynamicPerPage = 6;
    }

    // implement pagination
    const [currentPage, setCurrentPage] = useState(1);
    const [postsPerPage] = useState(dynamicPerPage);
    const indexOfLastPost = currentPage * postsPerPage;
    const indexOfFirstRepo = indexOfLastPost - postsPerPage;
    const currentPosts = allPosts.slice(indexOfFirstRepo, indexOfLastPost);

    const paginate = (pageNumber) => {
        setCurrentPage(pageNumber);
        window.scrollTo({ top: 0, behavior: "smooth" });
    };

    // calculate page numbers
    const pageNumbers = [];
    for (let i = 1; i <= Math.ceil(allPosts.length / postsPerPage); i++) {
        pageNumbers.push(i);
    }

    return (
        <section className="mx-auto">
            <div className="md:grid md:grid-cols-2 md:grid-flow-row md:gap-4 max-w-7xl mt-16 sm:p-8 px-4 py-8 m-auto bg-white">
                <div className="hero__text grid-col-1 flex flex-col"> <br />
                    <h1 className="text-text text-blue-800">አድባር</h1>
                    <p className="mt-2 text-text max-w-[520px] text-hero text-[15px]">
                    Welcome to AIQEM, where innovation meets impact in the heart of African technology! 🌍
                    Unleashing the power of AI and Blockchain, AIQEM proudly presents አድባር – our groundbreaking Telegram Ad solution tailored for Ethiopian businesses.
                    Elevate your advertising strategy with አድባር, our end-to-end AI-based platform designed to optimize ad placements across diverse Telegram channels.
                    Explore the future of marketing with AIQEM's Amharic RAG pipeline, revolutionizing the creation of engaging Amharic text Ad content for unparalleled campaign success.
                    Join us on the forefront of technological innovation as we reshape the landscape of AI and Blockchain solutions for Ethiopian and African businesses. 🚀    
                    </p>
                    <br />
                    <Link
                        to="/create"
                        className="font-inter font-bold bg-blue-800 text-white px-2 py-1 rounded-md w-[60px]"
                    >
                        Chat
                    </Link>
                </div>
            <div className="mt-16]">
                <img src={telegram} style={{ width: 500, height: 400 }} alt="img" className=""/>
            </div>
            </div>
        </section>
    );
};

export default Home;
import json

import os

import cv2

import matplotlib.pyplot as plt


with open('/home/habte/Downloads/concepts.json', 'r') as file:

    sample_concepts = json.load(file)
sample_concepts

for concept in sample_concepts:

    concept_name = concept.get('concept', 'N/A')

    print("Concept:", concept_name)

    

    explanation = concept.get('explanation', 'N/A')

    print("Explanation:", explanation)

    

    implementation = concept.get('implementation', {})

    print("Implementation:")

for frame_name, frame_details in implementation.items():

    print(f"Frame: {frame_name}")

    description = frame_details.get('description', 'N/A')

    print("Description:", description)

        

    interaction_type = frame_details.get('interaction_type', 'N/A')

    print("Interaction Type:", interaction_type)

        

    next_frame = frame_details.get('next_frame', 'N/A')

    print("Next Frame:", next_frame)

        

    duration = frame_details.get('duration', 'N/A')

    print("Duration:", duration)

        

    print()  
storyboard_dir = '/home/habte/Downloads/data-20240212T140836Z-002/data/adludio storyboard examples'

storyboard_files = os.listdir(storyboard_dir)

for file in storyboard_files:

    if file.endswith('.pdf'):

        
        pass




storyboard_files
import cv2

import os




image_dir = '/home/habte/Downloads/data-20240212T140836Z-002/data/adludio storyboard examples'




image_files = [

    'Adludio-CocaCola-[BR]-[LIVE]-ifood-christmas2033-TapAndHold-FS-V3-sb.png',

    'Adludio-Detran_RS-[BR]-[LIVE]-Carnival_24-User_Choice_Quiz-FS-Version_2-Taxi.png',

    'Adludio-Estee_Lauder-[UK]-[LIVE]-luxury_fragrance-cube_flip-MPU-Version_1.png',

    'Adludio-google-[UK]-[RFP]-brand_saftey-swipe_left_or_right-FS-v1.png',

    'Adludio-LEGO-[UK]-[RFP]-Ninjago24-Gamified-FlyingArcade-FS-V2-sb.png',

    'Adludio-Microsoft-[FR]-[LIVE]-Windows_11_version_2-Swipe-MPU-v2.png',

    'Adludio-Volvo-[UK]-[RFP]-Volvo_Vehicle_Electrification_XC40-Tap-FS-Version_2_AJ.png',

    'Amazon-ThirteenLives_FS_600x900-storyboard.png',

    'bestuy-fs-600x900-tapandhold-storyboard.png',

    'boa_FS_600x900_storyboard.png',

    'chevy-mpu-600x500-hotspot-storyboard.png',

    'dior-fs-320x480-slider-storyboard.png',

    'Disney-DrStrange-FS-600x900-UserSlider-Storyboard.png',

    'ITC-FS-320x480-SensorySwipe-Storyboard-Rev.png',

    'kfc-fs-320x480-sensoryvideo-storyboard.png',

    'Ladbroked_FS_320x480_spinwheel_-storyboard-uk.png',

    'Meta_FS_320x480_swipe-storyboard.png',

    'nyle.png',

    'RL-PinkPony-FS-320x480-Sensory-Scrub-Storyboard.png',

    'western-union-philippines_FS_600x900_storyboard.png'

]


for file in image_files:

    
    image_path = os.path.join(image_dir, file)

    

    
    image = cv2.imread(image_path)

    

    
    cv2.imshow('Image', image)

    cv2.waitKey(0)

    cv2.destroyAllWindows()





import json

import os




with open('/home/habte/Downloads/concepts.json', 'r') as file:

    sample_concepts = json.load(file)




storyboard_dir = '/home/habte/Downloads/data-20240212T140836Z-002/data/adludio storyboard examples'




assets_dir = '/home/habte/Downloads/data-20240212T140836Z-002/data/archive/Assets'


print("Number of sample concepts:", len(sample_concepts))

for concept in sample_concepts:

    
    concept_name = concept.get('concept', 'N/A')

    explanation = concept.get('explanation', 'N/A')

    implementation = concept.get('implementation', {})

    

    
    print("Concept:", concept_name)

    print("Explanation:", explanation)

    print("Number of frames in Implementation:", len(implementation))

    print()

storyboard_files = os.listdir(storyboard_dir)

print("Number of storyboard examples:", len(storyboard_files))







import json




with open('/home/habte/Downloads/concepts.json', 'r') as json_file:

    data = json.load(json_file)




descriptions = {}

for concept in data:

    concept_name = concept['concept']

    descriptions[concept_name] = {}

    for frame, frame_data in concept['implementation'].items():

        description = frame_data['description']

        descriptions[concept_name][frame] = description




objects = set()

scenes = set()

themes = set()



for concept, frames in descriptions.items():

    for frame, description in frames.items():

        
        if 'controller' in description.lower():

            objects.add('Controller')

        if 'virtual world' in description.lower():

            scenes.add('Virtual World')

        if 'journey' in description.lower():

            themes.add('Journey')




key_elements = {

    'Objects': list(objects),

    'Scenes': list(scenes),

    'Themes': list(themes)

}








print("Key Elements:")

for category, elements in key_elements.items():

    print(f"{category}: {elements}")

import torch

import torchvision

import torchvision.transforms as transforms

import torch.nn as nn

import torch.optim as optim

import matplotlib.pyplot as plt




image_size = 64

num_channels = 3




latent_dim = 100




num_epochs = 10




batch_size = 64




transform = transforms.Compose([

    transforms.Resize(image_size),

    transforms.CenterCrop(image_size),

    transforms.ToTensor(),

    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),

])




train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)




class Generator(nn.Module):

    def __init__(self, latent_dim):

        super(Generator, self).__init__()

        self.latent_dim = latent_dim

        self.main = nn.Sequential(

            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),

            nn.BatchNorm2d(512),

            nn.ReLU(True),

            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),

            nn.BatchNorm2d(256),

            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),

            nn.BatchNorm2d(128),

            nn.ReLU(True),

            nn.ConvTranspose2d(128, num_channels, 4, 2, 1, bias=False),

            nn.Tanh()

        )



    def forward(self, input):

        return self.main(input)




generator = Generator(latent_dim)




criterion = nn.BCELoss()

optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))




for epoch in range(num_epochs):

    for i, data in enumerate(train_loader, 0):

        
        discriminator.zero_grad()

        real_cpu = data[0].to(device)

        b_size = real_cpu.size(0)

        label = torch.full((b_size,), real_label, device=device)



        output = discriminator(real_cpu).view(-1)

        errD_real = criterion(output, label)

        errD_real.backward()

        D_x = output.mean().item()



        noise = torch.randn(b_size, latent_dim, 1, 1, device=device)

        fake = generator(noise)

        label.fill_(fake_label)

        output = discriminator(fake.detach()).view(-1)

        errD_fake = criterion(output, label)

        errD_fake.backward()

        D_G_z1 = output.mean().item()

        errD = errD_real + errD_fake

        optimizerD.step()



        
        generator.zero_grad()

        label.fill_(real_label)

        output = discriminator(fake).view(-1)

        errG = criterion(output, label)

        errG.backward()

        D_G_z2 = output.mean().item()

        optimizerG.step()



        
        if i % 50 == 0:

            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'

                  % (epoch, num_epochs, i, len(train_loader),

                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))



    
    with torch.no_grad():

        fake = generator(fixed_noise).detach().cpu()

        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))




plt.figure(figsize=(10,10))

plt.axis("off")

plt.title("Generated Images")

plt.imshow(np.transpose(img_list[-1],(1,2,0)))

plt.show()

import torch

import torchvision.transforms as transforms

from PIL import Image

from models import AttnGAN

from datasets import TextDataset

from nltk.tokenize import RegexpTokenizer

from nltk.corpus import stopwords

import nltk




nltk.download('punkt')

nltk.download('stopwords')




model = AttnGAN()




checkpoint = torch.load('attnGAN.pth', map_location=torch.device('cpu'))

model.load_state_dict(checkpoint['state_dict'])




model.eval()




transform = transforms.Compose([

    transforms.Resize((128, 128)),

    transforms.ToTensor(),

    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),

])




tokenizer = RegexpTokenizer(r'\w+')

stop_words = set(stopwords.words('english'))




text_description = "A scenic view of a mountain landscape with a flowing river."




tokens = tokenizer.tokenize(text_description.lower())

filtered_tokens = [token for token in tokens if token not in stop_words]

text = ' '.join(filtered_tokens)




with torch.no_grad():

    input_text = torch.LongTensor(TextDataset.preprocess(text)).unsqueeze(0)

    input_text = input_text.to(torch.device('cpu'))

    noise = torch.randn(1, 100)

    noise = noise.to(torch.device('cpu'))

    image_features, _, _, _ = model.module.extract_features(input_text, noise)

    generated_image = model.module.generate(image_features, noise)




generated_image = generated_image.squeeze().permute(1, 2, 0).cpu().detach().numpy()

generated_image = (generated_image + 1) / 2.0 * 255

generated_image = generated_image.astype('uint8')

pil_image = Image.fromarray(generated_image)




pil_image.show()

from PIL import Image




generated_image = Image.open('generated_image.jpg')




desired_width = 800

desired_height = 600

resized_image = generated_image.resize((desired_width, desired_height), Image.ANTIALIAS)





aspect_ratio = generated_image.width / generated_image.height




new_height = int(desired_width / aspect_ratio)




resized_image = generated_image.resize((desired_width, new_height), Image.ANTIALIAS)




if new_height > desired_height:

    top_crop = (new_height - desired_height) // 2

    bottom_crop = new_height - top_crop

    cropped_image = resized_image.crop((0, top_crop, desired_width, bottom_crop))

else:

    
    padding = (0, (desired_height - new_height) // 2, 0, (desired_height - new_height) // 2)

    padded_image = Image.new("RGB", (desired_width, desired_height), (255, 255, 255))

    padded_image.paste(resized_image, padding)




cropped_image.save('formatted_image.jpg')

cropped_image.show()
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score
import numpy as np




def generate_storyboard(concept):

    
    pass




with open('data/sample_concepts.json', 'r') as file:

    sample_concepts = json.load(file)




for concept in sample_concepts:

    generate_storyboard(concept)
from google.colab import drive

drive.mount('/content/drive')
import os

import shutil




assets_folder = '/home/habte/Challenge_Data/Assets'




for project_folder in os.listdir(assets_folder):

    project_path = os.path.join(assets_folder, project_folder)

    if os.path.isdir(project_path):

        
        project_output_folder = f'/home/habte/weekten/{project_folder}'

        os.makedirs(project_output_folder, exist_ok=True)



        
        for image_file in os.listdir(project_path):

            image_path = os.path.join(project_path, image_file)

            shutil.copy(image_path, project_output_folder)

import nltk




nltk.download('punkt')




def tokenize_text(text):

    
    tokens = nltk.word_tokenize(text)

    return tokens




input_text = "Tokenization involves breaking down textual descriptions into individual words or tokens."

tokens = tokenize_text(input_text)

print("Tokenized Text:", tokens)
from gensim.models import Word2Vec

import nltk




tokenized_text = [['tokenization', 'involves', 'breaking', 'down', 'textual', 'descriptions', 'into', 'individual', 'words', 'or', 'tokens']]




word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)




def encode_text(tokenized_text, word2vec_model):

    encoded_text = []

    for word in tokenized_text:

        if word in word2vec_model.wv:

            encoded_text.append(word2vec_model.wv[word])

        else:

            
            encoded_text.append([0] * word2vec_model.vector_size)

    return encoded_text




encoded_text = encode_text(tokenized_text[0], word2vec_model)

print("Encoded Text:", encoded_text)

import os




assets_directory = '/home/habte/Challenge_Data/Assets'




for storyboard_folder in os.listdir(assets_directory):

    storyboard_path = os.path.join(assets_directory, storyboard_folder)

    if os.path.isdir(storyboard_path):

        print(f"Storyboard: {storyboard_folder}")



        
        for asset_name in os.listdir(storyboard_path):

            asset_path = os.path.join(storyboard_path, asset_name)

            if os.path.isfile(asset_path):

                print(f"- Asset: {asset_name}")

                
import os




def extract_image_paths(directory):

    image_paths = []

    for root, _, files in os.walk(directory):

        for file in files:

            if file.lower().endswith(('.jpg', '.jpeg', '.png')):

                image_path = os.path.join(root, file)

                image_paths.append(image_path)

    return image_paths




main_directory = '/home/habte/Challenge_Data/Assets'




image_paths = extract_image_paths(main_directory)




for path in image_paths:

    print(path)

import os

import shutil

import zipfile

zip_file_path = '/content/drive/MyDrive/Challenge_Data.zip'


extracted_folder = '/content/mydata'

output_folder = '/content/output'

os.makedirs(output_folder, exist_ok=True)

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:

    zip_ref.extractall(extracted_folder)
import os

import shutil



extracted_folder = "/content/mydata/Challenge_Data/Assets"

output_folder = "/content/mycopydata"




for project_folder in os.listdir(extracted_folder):

    project_path = os.path.join(extracted_folder, project_folder)

    if os.path.isdir(project_path):  
        landing_image_path = None

        endframe_image_path = None



        
        for file_name in os.listdir(project_path):

            if file_name.lower() == 'landing.jpg':

                landing_image_path = os.path.join(project_path, file_name)

                
            elif file_name.lower() == 'endframe.jpg':

                endframe_image_path = os.path.join(project_path, file_name)

                


        
        if landing_image_path and endframe_image_path:



            shutil.copy(landing_image_path, output_folder)

            shutil.copy(endframe_image_path, output_folder)

import os

import shutil



extracted_folder = "/content/mydata/Challenge_Data/Assets"

output_folder = "/content/lastofdata"




os.makedirs(output_folder, exist_ok=True)




copied_images = set()




for project_folder in os.listdir(extracted_folder):

    project_path = os.path.join(extracted_folder, project_folder)

    if os.path.isdir(project_path):  
        landing_image_path = None

        endframe_image_path = None



        
        for file_name in os.listdir(project_path):

            if file_name.lower() == 'landing.jpg':

                landing_image_path = os.path.join(project_path, file_name)

            elif file_name.lower() == 'endframe.jpg':

                endframe_image_path = os.path.join(project_path, file_name)



        
        if landing_image_path and endframe_image_path:

            
            if landing_image_path not in copied_images:

                shutil.copy(landing_image_path, output_folder)

                copied_images.add(landing_image_path)



            
            if endframe_image_path not in copied_images:

                shutil.copy(endframe_image_path, output_folder)

                copied_images.add(endframe_image_path)





import os

import shutil

from PIL import Image

from IPython.display import display



extracted_folder = "/content/mydata/Challenge_Data/Assets"

output_folder = "/content/latmydata"




os.makedirs(output_folder, exist_ok=True)




copied_images = set()




for project_folder in os.listdir(extracted_folder):

    project_path = os.path.join(extracted_folder, project_folder)

    if os.path.isdir(project_path):  
        landing_image_path = None

        endframe_image_path = None



        
        for file_name in os.listdir(project_path):

            if file_name.lower() == 'landing.jpg':

                landing_image_path = os.path.join(project_path, file_name)

            elif file_name.lower() == 'endframe.jpg':

                endframe_image_path = os.path.join(project_path, file_name)



        
        if landing_image_path and endframe_image_path:

            
            if landing_image_path not in copied_images:

                shutil.copy(landing_image_path, output_folder)

                copied_images.add(landing_image_path)



            
            if endframe_image_path not in copied_images:

                shutil.copy(endframe_image_path, output_folder)

                copied_images.add(endframe_image_path)



            
            landing_image = Image.open(landing_image_path)



            
            endframe_image = Image.open(endframe_image_path)



            
            
            landing_image_resized = landing_image.resize((128, 128))

            endframe_image_resized = endframe_image.resize((128, 128))



            
            display(landing_image_resized)

            display(endframe_image_resized)





import os

import shutil

import numpy as np

from PIL import Image



extracted_folder = "/content/mydata/Challenge_Data/Assets"

output_folder = "/content/latmydata"




os.makedirs(output_folder, exist_ok=True)




copied_images = set()




for project_folder in os.listdir(extracted_folder):

    project_path = os.path.join(extracted_folder, project_folder)

    if os.path.isdir(project_path):  
        landing_image_path = None

        endframe_image_path = None



        
        for file_name in os.listdir(project_path):

            if file_name.lower() == 'landing.jpg':

                landing_image_path = os.path.join(project_path, file_name)

            elif file_name.lower() == 'endframe.jpg':

                endframe_image_path = os.path.join(project_path, file_name)



        
        if landing_image_path and endframe_image_path:

            
            landing_image = Image.open(landing_image_path)



            
            endframe_image = Image.open(endframe_image_path)



            
            landing_array = np.array(landing_image)

            endframe_array = np.array(endframe_image)



            
            landing_dimensions = landing_image.size

            endframe_dimensions = endframe_image.size



            
            landing_histogram = landing_image.histogram()

            endframe_histogram = endframe_image.histogram()



            
            landing_mean_pixel = np.mean(landing_array, axis=(0, 1))

            endframe_mean_pixel = np.mean(endframe_array, axis=(0, 1))



            
            print("Image Dimensions:")

            print("Landing Image:", landing_dimensions)

            print("Endframe Image:", endframe_dimensions)

            print()

            print("Color Histograms:")

            print("Landing Image:", landing_histogram[:10])  
            print("Endframe Image:", endframe_histogram[:10])  
            print()

            print("Mean Pixel Values:")

            print("Landing Image:", landing_mean_pixel)

            print("Endframe Image:", endframe_mean_pixel)





import os

import shutil

import numpy as np

from PIL import Image

import matplotlib.pyplot as plt



extracted_folder = "/content/mydata/Challenge_Data/Assets"

output_folder = "/content/latmydata"




os.makedirs(output_folder, exist_ok=True)




copied_images = set()




for project_folder in os.listdir(extracted_folder):

    project_path = os.path.join(extracted_folder, project_folder)

    if os.path.isdir(project_path):  
        landing_image_path = None

        endframe_image_path = None



        
        for file_name in os.listdir(project_path):

            if file_name.lower() == 'landing.jpg':

                landing_image_path = os.path.join(project_path, file_name)

            elif file_name.lower() == 'endframe.jpg':

                endframe_image_path = os.path.join(project_path, file_name)



        
        if landing_image_path and endframe_image_path:

            
            landing_image = Image.open(landing_image_path)



            
            endframe_image = Image.open(endframe_image_path)



            
            landing_array = np.array(landing_image)

            endframe_array = np.array(endframe_image)



            
            landing_histogram = landing_image.histogram()

            endframe_histogram = endframe_image.histogram()



            
            
            num_bins = len(landing_histogram)



            
            bin_edges = range(num_bins)



            
            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 6))



            
            axes[0].bar(bin_edges, landing_histogram, color='blue')

            axes[0].set_title('Landing Image Color Histogram')

            axes[0].set_xlabel('Bin')

            axes[0].set_ylabel('Frequency')



            
            axes[1].bar(bin_edges, endframe_histogram, color='red')

            axes[1].set_title('Endframe Image Color Histogram')

            axes[1].set_xlabel('Bin')

            axes[1].set_ylabel('Frequency')



            
            plt.tight_layout()



            
            plt.show()





from PIL import Image

import numpy as np

from tensorflow.keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences




def preprocess_images(image_paths, target_size=(128, 128)):

    images = []

    for path in image_paths:

        image = Image.open(path)  
        image = image.resize(target_size)  
        image = np.array(image) / 255.0  
        images.append(image)

    return np.array(images)




def preprocess_textual_descriptions(textual_descriptions):

    tokenizer = Tokenizer()  
    tokenizer.fit_on_texts(textual_descriptions)  
    sequences = tokenizer.texts_to_sequences(textual_descriptions)  
    max_length = max(len(seq) for seq in sequences)  
    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')  
    vocabulary_size = len(tokenizer.word_index) + 1  
    return padded_sequences, vocabulary_size





image_paths = ['landing_image.jpg', 'endframe_image.jpg']

textual_descriptions = ['description 1', 'description 2']




images = preprocess_images(image_paths)




sequences, vocabulary_size = preprocess_textual_descriptions(textual_descriptions)




print("Preprocessed Images:", images.shape)

print("Preprocessed Textual Descriptions:")

print("Sequences:", sequences)

print("Vocabulary Size:", vocabulary_size)

import os

import shutil



extracted_folder = "/content/mydata/Challenge_Data/Assets"

output_folder = "/content/mycopydata"




image_text_mapping = {}




for project_folder in os.listdir(extracted_folder):

    project_path = os.path.join(extracted_folder, project_folder)

    if os.path.isdir(project_path):  
        landing_image_path = None

        endframe_image_path = None



        
        for file_name in os.listdir(project_path):

            if file_name.lower() == 'landing.jpg':

                landing_image_path = os.path.join(project_path, file_name)

                print("Landing Image Path:", landing_image_path)

            elif file_name.lower() == 'endframe.jpg':

                endframe_image_path = os.path.join(project_path, file_name)

                print("Endframe Image Path:", endframe_image_path)



        
        if landing_image_path and endframe_image_path:

            
            image_text_mapping[landing_image_path] = "Text for landing image"  
            image_text_mapping[endframe_image_path] = "Text for endframe image"  


            
            
            
            shutil.copy(landing_image_path, output_folder)

            shutil.copy(endframe_image_path, output_folder)




for image_path, text in image_text_mapping.items():

    print("Image Path:", image_path)

    print("Textual Description:", text)

    print()





import os

import shutil




image_folder = "/content/mydata/Challenge_Data/Assets/002dbbd85ef3fe6a2e7d0754fb9f9a1a"




for file_name in os.listdir(image_folder):

    image_path = os.path.join(image_folder, file_name)

    if os.path.isfile(image_path):  
        
        print("Image Path:", image_path)



        
        


        
        image_text = "Text for the image"



        
        print("Textual Description:", image_text)



        
        
import os




assets_directory = '/content/mydata/Challenge_Data/Assets'




image_text_mapping = {}




for storyboard_folder in os.listdir(assets_directory):

    storyboard_path = os.path.join(assets_directory, storyboard_folder)

    if os.path.isdir(storyboard_path):

        print(f"Storyboard: {storyboard_folder}")



        
        for asset_name in os.listdir(storyboard_path):

            asset_path = os.path.join(storyboard_path, asset_name)

            if os.path.isfile(asset_path):

                print(f"- Asset: {asset_name}")

                
                image_name = os.path.splitext(os.path.basename(asset_path))[0]

                
                image_text_mapping[asset_path] = image_name



                



for image_path, text in image_text_mapping.items():

    print("Image Path:", image_path)

    print("Textual Description:", text)

    print()

import os




assets_directory = '/content/mydata/Challenge_Data/Assets'




storyboard_mapping = {}




for storyboard_folder in os.listdir(assets_directory):

    storyboard_path = os.path.join(assets_directory, storyboard_folder)

    if os.path.isdir(storyboard_path):

        print(f"Storyboard: {storyboard_folder}")



        
        for asset_name in os.listdir(storyboard_path):

            asset_path = os.path.join(storyboard_path, asset_name)

            if os.path.isfile(asset_path):

                print(f"- Asset: {asset_name}")

                
                image_name = os.path.splitext(os.path.basename(asset_path))[0]



                
                
                
                
                


                
                
                storyboard_elements = image_name.split('_')



                
                storyboard_mapping[asset_path] = {

                    'image_name': image_name,

                    'storyboard_elements': storyboard_elements

                }



                



for image_path, mapping in storyboard_mapping.items():

    print("Image Path:", image_path)

    print("Storyboard Elements:", mapping['storyboard_elements'])

    print()


concepts_data = [

    {

        "Concept": "Concept 1",

        "Explanation": "Explanation for Concept 1",

        "Implementation": [

            {

                "Frame": 1,

                "VisualRepresentation": "Visual representation for Frame 1",

                "Explanation": "Explanation for Frame 1",

                "AssetSuggestions": [

                    {"Category": "Background Image", "Description": "Description of background image"},

                    {"Category": "Logo", "Description": "Description of logo"},

                    {"Category": "Text Elements", "Description": "Description of text elements"}

                ]

            },

            {

                "Frame": 2,

                "VisualRepresentation": "Visual representation for Frame 2",

                "Explanation": "Explanation for Frame 2",

                "AssetSuggestions": [

                    {"Category": "Product Image", "Description": "Description of product image"},

                    {"Category": "Call-To-Action (CTA) Button", "Description": "Description of CTA button"}

                ]

            }

        ]

    },

    {

        "Concept": "Concept 2",

        "Explanation": "Explanation for Concept 2",

        "Implementation": [

            {

                "Frame": 1,

                "VisualRepresentation": "Visual representation for Frame 1 of Concept 2",

                "Explanation": "Explanation for Frame 1 of Concept 2",

                "AssetSuggestions": [

                    {"Category": "Infographic", "Description": "Description of infographic"},

                    {"Category": "Icon", "Description": "Description of icon"}

                ]

            }

        ]

    }

]




for concept in concepts_data:

    concept_name = concept['Concept']

    print(f"Concept: {concept_name}")



    
    print("Explanation:", concept['Explanation'])



    
    for frame in concept['Implementation']:

        frame_number = frame['Frame']

        print(f"Frame {frame_number}:")

        print("Visual Representation:", frame['VisualRepresentation'])

        print("Explanation:", frame['Explanation'])



        
        print("Asset Suggestions:")

        for asset_suggestion in frame['AssetSuggestions']:

            category = asset_suggestion['Category']

            description = asset_suggestion['Description']

            print(f"- Category: {category}")

            print(f"  Description: {description}")



        print()



    print()

import os

import json




assets_directory = '/content/mydata/Challenge_Data/Assets'




image_text_mappings = []




for storyboard_folder in os.listdir(assets_directory):

    storyboard_path = os.path.join(assets_directory, storyboard_folder)

    if os.path.isdir(storyboard_path):

        print(f"Storyboard: {storyboard_folder}")



        
        for asset_name in os.listdir(storyboard_path):

            asset_path = os.path.join(storyboard_path, asset_name)

            if os.path.isfile(asset_path):

                print(f"- Asset: {asset_name}")

                
                image_name = os.path.splitext(os.path.basename(asset_path))[0]

                
                image_text_mappings.append({"image_path": asset_path, "textual_description": image_name})



                



for mapping in image_text_mappings:

    print("Image Path:", mapping["image_path"])

    print("Textual Description:", mapping["textual_description"])

    print()




output_json_path = 'image_text_mappings.json'

with open(output_json_path, 'w') as json_file:

    json.dump(image_text_mappings, json_file, indent=4)



print(f"Image-text mappings saved to {output_json_path}")

import os

import json




assets_directory = '/content/mydata/Challenge_Data/Assets'




concepts = []




for storyboard_folder in os.listdir(assets_directory):

    storyboard_path = os.path.join(assets_directory, storyboard_folder)

    if os.path.isdir(storyboard_path):

        print(f"Storyboard: {storyboard_folder}")



        
        concept_data = {

            "Concept": storyboard_folder,

            "Explanation": f"Explanation for {storyboard_folder}",

            "Implementation": []

        }



        
        for asset_name in os.listdir(storyboard_path):

            asset_path = os.path.join(storyboard_path, asset_name)

            if os.path.isfile(asset_path):

                print(f"- Asset: {asset_name}")



                
                image_name = os.path.splitext(os.path.basename(asset_path))[0]

                image_description = image_name.split('/')[-1]



                
                parent_folder_name = ""

                if len(image_name.split('/')) > 1:

                    parent_folder_name = image_name.split('/')[-2]



                
                frame_data = {

                    "Frame": 1,  
                    "VisualRepresentation": f"Visual representation for {image_description}",

                    "Explanation": f"Explanation for {image_description}",

                    "AssetSuggestions": [

                        {"Category": "Background Image", "Description": "Description of background image"},

                        {"Category": image_description, "Description": f"Description of {image_description}"},

                        {"Category": "Text Elements", "Description": parent_folder_name}  
                    ]

                }



                
                concept_data["Implementation"].append(frame_data)



        
        concepts.append(concept_data)




with open('concepts.json', 'w') as json_file:

    json.dump(concepts, json_file, indent=4)

import os

import json




assets_directory = '/content/mydata/Challenge_Data/Assets'




concepts = []




for storyboard_folder in os.listdir(assets_directory):

    storyboard_path = os.path.join(assets_directory, storyboard_folder)

    if os.path.isdir(storyboard_path):

        
        concept_data = {

            "Concept": storyboard_folder,

            "Implementation": []

        }



        
        for asset_name in os.listdir(storyboard_path):

            asset_path = os.path.join(storyboard_path, asset_name)

            if os.path.isfile(asset_path):

                
                textual_description = asset_name.split('/')[-1].split('.')[0]

                
                concept_data["Implementation"].append({

                    "Image Path": asset_path[len(assets_directory) + 1:],  
                    "Textual Description": textual_description

                })



        
        concepts.append(concept_data)




with open('concepts.json', 'w') as json_file:

    json.dump(concepts, json_file, indent=4)

import json




with open('concepts.json', 'r') as json_file:

    while True:

        chunk = json_file.read(1024)  
        if not chunk:

            break

        print(chunk)

import os




assets_directory = '/content/mydata/Challenge_Data/Assets'




def preprocess_text(text):

    
    
    pass



def preprocess_image(image_path):

    
    
    pass




preprocessed_data = {}




for folder_name in os.listdir(assets_directory):

    folder_path = os.path.join(assets_directory, folder_name)

    if os.path.isdir(folder_path):

        


        
        files = os.listdir(folder_path)



        
        image_files = [file for file in files if file.endswith(('.jpg', '.png', '.jpeg'))]



        
        text_files = [file for file in files if file.endswith('.txt')]



        
        for text_file in text_files:

            text_file_path = os.path.join(folder_path, text_file)

            with open(text_file_path, 'r') as file:

                text = file.read()

                preprocessed_text = preprocess_text(text)

                preprocessed_data[text_file] = preprocessed_text



        
        for image_file in image_files:

            image_file_path = os.path.join(folder_path, image_file)

            preprocessed_image = preprocess_image(image_file_path)

            preprocessed_data[image_file] = preprocessed_image




import os

import numpy as np

from PIL import Image

from tensorflow.keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

assets_directory = '/content/mydata/Challenge_Data/Assets'
textual_descriptions = []

image_paths = []



for storyboard_folder in os.listdir(assets_directory):

    storyboard_path = os.path.join(assets_directory, storyboard_folder)

    if os.path.isdir(storyboard_path):

        for asset_name in os.listdir(storyboard_path):

            asset_path = os.path.join(storyboard_path, asset_name)

            if os.path.isfile(asset_path):

                image_paths.append(asset_path)

                image_name = os.path.splitext(os.path.basename(asset_path))[0]

                textual_descriptions.append(image_name)

tokenizer = Tokenizer()

tokenizer.fit_on_texts(textual_descriptions)

sequences = tokenizer.texts_to_sequences(textual_descriptions)

max_length = max(len(seq) for seq in sequences)

padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')

vocabulary_size = len(tokenizer.word_index) + 1

def preprocess_images(image_paths, target_size=(128, 128)):

    images = []

    skipped_paths = []

    for path in image_paths:

        
        if path.lower().endswith(('.png', '.jpg', '.jpeg')):

            try:

                image = Image.open(path)  
                image = image.resize(target_size)  
                image = np.array(image) / 255.0  
                images.append(image)

            except Exception as e:

                print(f"Error processing image at {path}: {e}")

                skipped_paths.append(path)

        else:

            print(f"Skipping non-image file: {path}")

            skipped_paths.append(path)

    return np.array(images), skipped_paths

import tensorflow as tf

from tensorflow.keras import layers




text_input = layers.Input(shape=(max_length,), name='text_input')




image_input = layers.Input(shape=(128, 128, 3), name='image_input')




text_embedding = layers.Embedding(vocabulary_size, 100)(text_input)

text_embedding = layers.GlobalAveragePooling1D()(text_embedding)




image_embedding = layers.Conv2D(32, (3, 3), activation='relu')(image_input)

image_embedding = layers.MaxPooling2D((2, 2))(image_embedding)

image_embedding = layers.Conv2D(64, (3, 3), activation='relu')(image_embedding)

image_embedding = layers.MaxPooling2D((2, 2))(image_embedding)

image_embedding = layers.Conv2D(64, (3, 3), activation='relu')(image_embedding)

image_embedding = layers.Flatten()(image_embedding)




combined_embedding = layers.concatenate([text_embedding, image_embedding])




output = layers.Dense(128, activation='relu')(combined_embedding)

output = layers.Dense(3, activation='softmax')(output)  



model = tf.keras.Model(inputs=[text_input, image_input], outputs=output)




model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])




model.summary()
import os
from PIL import Image
import numpy as np

def preprocess_assets(assets_folder, output_folder, target_size=(224, 224)):
    """
    Preprocesses assets by resizing images and saving them to the output folder.

    Args:
    - assets_folder (str): Path to the folder containing the assets.
    - output_folder (str): Path to the folder where preprocessed assets will be saved.
    - target_size (tuple): Target size for resizing images. Default is (224, 224).
    """
        if not os.path.exists(output_folder):
        os.makedirs(output_folder)

        for project_folder in os.listdir(assets_folder):
        project_path = os.path.join(assets_folder, project_folder)
        
                landing_image_path = os.path.join(project_path, 'landing.jpg')
        endframe_image_path = os.path.join(project_path, 'endframe.jpg')
        
                landing_image = preprocess_image(landing_image_path, target_size)
        endframe_image = preprocess_image(endframe_image_path, target_size)

                output_project_folder = os.path.join(output_folder, project_folder)
        if not os.path.exists(output_project_folder):
            os.makedirs(output_project_folder)
        
        landing_output_path = os.path.join(output_project_folder, 'landing.jpg')
        endframe_output_path = os.path.join(output_project_folder, 'endframe.jpg')
        
        save_image(landing_image, landing_output_path)
        save_image(endframe_image, endframe_output_path)

def preprocess_image(image_path, target_size):
    """
    Preprocesses an individual image by resizing it and normalizing pixel values.

    Args:
    - image_path (str): Path to the image file.
    - target_size (tuple): Target size for resizing the image.

    Returns:
    - numpy.ndarray: Preprocessed image as a NumPy array.
    """
    img = Image.open(image_path)
    img = img.resize(target_size)      img = np.array(img) / 255.0      return img

def save_image(image, output_path):
    """
    Saves an image to the specified output path.

    Args:
    - image (numpy.ndarray): Image as a NumPy array.
    - output_path (str): Path where the image will be saved.
    """
    Image.fromarray((image * 255).astype(np.uint8)).save(output_path)
def calculate_similarity_score(image1, image2):
    """
    Calculates the similarity score between two images.

    Args:
    - image1 (numpy.ndarray): First image as a NumPy array.
    - image2 (numpy.ndarray): Second image as a NumPy array.

    Returns:
    - float: Similarity score between the images.
    """
            similarity_score = 0.0      return similarity_score
import os



assets_folder = 'Assets'




for project_folder in os.listdir(assets_folder):

    project_path = os.path.join(assets_folder, project_folder)

    

    
    if os.path.isdir(project_path):

        print("Project:", project_folder)

        

        
        for asset_file in os.listdir(project_path):

            asset_path = os.path.join(project_path, asset_file)

            

            
            if os.path.isfile(asset_path):

                print("Asset:", asset_file)

                

        print("\n")

import os



assets_folder = 'Assets'




for project_folder in os.listdir(assets_folder):

    project_path = os.path.join(assets_folder, project_folder)

    

    
    if os.path.isdir(project_path):

        print("Project:", project_folder)

        

        
        landing_image_path = None

        endframe_image_path = None

        

        
        for asset_file in os.listdir(project_path):

            asset_path = os.path.join(project_path, asset_file)

            

            
            if os.path.isfile(asset_path):

                
                if asset_file.startswith('landing'):

                    landing_image_path = asset_path

                
                elif asset_file.startswith('endframe'):

                    endframe_image_path = asset_path

        

        
        if landing_image_path and endframe_image_path:

            print("Landing Image:", landing_image_path)

            print("Endframe Image:", endframe_image_path)

            
        else:

            print("Error: 'landing' or 'endframe' image not found for project", project_folder)

                

        print("\n")

from PIL import Image

import os



assets_folder = 'Assets'

output_folder = 'Preprocessed_Assets'




if not os.path.exists(output_folder):

    os.makedirs(output_folder)




for project_folder in os.listdir(assets_folder):

    project_path = os.path.join(assets_folder, project_folder)

    

    
    if os.path.isdir(project_path):

        
        landing_image_path = None

        endframe_image_path = None

        

        
        for asset_file in os.listdir(project_path):

            asset_path = os.path.join(project_path, asset_file)

            

            
            if os.path.isfile(asset_path):

                
                if asset_file.startswith('landing'):

                    landing_image_path = asset_path

                
                elif asset_file.startswith('endframe'):

                    endframe_image_path = asset_path

        

        
        if landing_image_path and endframe_image_path:

            
            landing_image = Image.open(landing_image_path)

            landing_image = landing_image.resize((224, 224))  
            landing_image = landing_image.convert('RGB')  
            landing_image = landing_image / 255.0  
            

            
            endframe_image = Image.open(endframe_image_path)

            endframe_image = endframe_image.resize((224, 224))  
            endframe_image = endframe_image.convert('RGB')  
            endframe_image = endframe_image / 255.0  
            

            
            output_project_folder = os.path.join(output_folder, project_folder)

            if not os.path.exists(output_project_folder):

                os.makedirs(output_project_folder)

            

            landing_output_path = os.path.join(output_project_folder, 'landing.jpg')

            endframe_output_path = os.path.join(output_project_folder, 'endframe.jpg')

            

            landing_image.save(landing_output_path)

            endframe_image.save(endframe_output_path)

            

            print("Preprocessed images saved for project:", project_folder)

        else:

            print("Error: 'landing' or 'endframe' image not found for project", project_folder)

from PIL import Image

import numpy as np

import os



assets_folder = 'Assets'

output_folder = 'Preprocessed_Assets'




if not os.path.exists(output_folder):

    os.makedirs(output_folder)




for project_folder in os.listdir(assets_folder):

    project_path = os.path.join(assets_folder, project_folder)

    

    
    if os.path.isdir(project_path):

        
        landing_image_path = None

        endframe_image_path = None

        other_assets = []

        

        
        for asset_file in os.listdir(project_path):

            asset_path = os.path.join(project_path, asset_file)

            

            
            if os.path.isfile(asset_path):

                
                if asset_file.startswith('landing'):

                    landing_image_path = asset_path

                
                elif asset_file.startswith('endframe'):

                    endframe_image_path = asset_path

                
                else:

                    other_assets.append(asset_path)

        

        
        if landing_image_path and endframe_image_path:

            
            landing_image = Image.open(landing_image_path)

            landing_image = landing_image.resize((224, 224))  
            landing_image = landing_image.convert('RGB')  
            landing_image = np.array(landing_image) / 255.0  
            

            
            endframe_image = Image.open(endframe_image_path)

            endframe_image = endframe_image.resize((224, 224))  
            endframe_image = endframe_image.convert('RGB')  
            endframe_image = np.array(endframe_image) / 255.0  
            

            
            for asset_path in other_assets:

                
                asset_image = Image.open(asset_path)

                asset_image = asset_image.resize((224, 224))  
                asset_image = asset_image.convert('RGB')  
                asset_image = np.array(asset_image) / 255.0  
                

                
                

                
                

                
                

            print("Preprocessed images saved for project:", project_folder)

        else:

            print("Error: 'landing' or 'endframe' image not found for project", project_folder)

import os

import numpy as np



assets_folder = 'Assets'

output_folder = 'Preprocessed_Assets'




preprocessed_data = []




for project_folder in os.listdir(assets_folder):

    project_path = os.path.join(assets_folder, project_folder)

    

    
    if os.path.isdir(project_path):

        
        landing_image_path = None

        endframe_image_path = None

        other_assets = []

        

        
        for asset_file in os.listdir(project_path):

            asset_path = os.path.join(project_path, asset_file)

            

            
            if os.path.isfile(asset_path):

                
                if asset_file.startswith('landing'):

                    landing_image_path = asset_path

                
                elif asset_file.startswith('endframe'):

                    endframe_image_path = asset_path

                
                else:

                    other_assets.append(asset_path)

        

        
        if landing_image_path and endframe_image_path:

            
            landing_image = np.load(landing_image_path)

            

            
            endframe_image = np.load(endframe_image_path)

            

            
            processed_other_assets = []

            for asset_path in other_assets:

                asset_image = np.load(asset_path)

                
                

                
                

                processed_other_assets.append(asset_image)

            

            
            preprocessed_project_data = {

                'project_name': project_folder,

                'landing_image': landing_image,

                'endframe_image': endframe_image,

                'other_assets': processed_other_assets

            }

            

            preprocessed_data.append(preprocessed_project_data)

            

print("Preprocessed data organized successfully.")

import numpy as np

from PIL import Image



def preprocess_image(image_path, target_size=(224, 224)):

    img = Image.open(image_path)

    img = img.resize(target_size)

    img = np.array(img)

    img = img / 255.0  
    return img



def preprocess_text(text):

    
    
    return encoded_text




background_image_path = 'background.jpg'

logo_image_path = 'logo.png'

cta_button_image_path = 'cta_button.jpg'

icon_image_path = 'icon.png'

product_image_path = 'product.jpg'



background_image = preprocess_image(background_image_path)

logo_image = preprocess_image(logo_image_path)

cta_button_image = preprocess_image(cta_button_image_path)

icon_image = preprocess_image(icon_image_path)

product_image = preprocess_image(product_image_path)



text_element = "Special offer - 50% off!"

testimonial_quote = "Amazing product! Highly recommended."

social_proof = "Rated 5 stars by over 1000 customers."

graph_chart_text = "Sales graph showing exponential growth."

banner_text = "New arrival - Limited time offer!"

infographic_text = "10 ways our product can improve your life."

coupon_code_text = "Use code SAVE20 for 20% off."

legal_disclaimer_text = "Terms and conditions apply. Offer valid until XX/XX/XXXX."

contact_info_text = "Contact us: email@example.com | 123-456-7890"

encoded_text_element = preprocess_text(text_element)

encoded_testimonial_quote = preprocess_text(testimonial_quote)

encoded_social_proof = preprocess_text(social_proof)

encoded_graph_chart_text = preprocess_text(graph_chart_text)

encoded_banner_text = preprocess_text(banner_text)

encoded_infographic_text = preprocess_text(infographic_text)

encoded_coupon_code_text = preprocess_text(coupon_code_text)

encoded_legal_disclaimer_text = preprocess_text(legal_disclaimer_text)

encoded_contact_info_text = preprocess_text(contact_info_text)

import os

import numpy as np

from PIL import Image



assets_folder = 'Assets'




def preprocess_image(image_path, target_size=(224, 224)):

    img = Image.open(image_path)

    img = img.resize(target_size)

    img = np.array(img)

    img = img / 255.0  
    return img



def preprocess_text(text):

    
    
    return encoded_text




for project_folder in os.listdir(assets_folder):

    project_path = os.path.join(assets_folder, project_folder)

    

    
    if os.path.isdir(project_path):

        print("Project:", project_folder)

        

        
        preprocessed_assets = {

            'Background Image': [],

            'Logo': [],

            'Call-To-Action (CTA) Button': [],

            'Icon': [],

            'Product Image': [],

            'Text Elements': [],

            'Infographic': [],

            'Banner': [],

            'Illustration': [],

            'Photograph': [],

            'Mascot': [],

            'Testimonial Quotes': [],

            'Social Proof': [],

            'Seal or Badge': [],

            'Graphs and Charts': [],

            'Decorative Elements': [],

            'Interactive Elements': [],

            'Animation Frames': [],

            'Coupon or Offer Code': [],

            'Legal Disclaimers or Terms': [],

            'Contact Information': [],

            'Map or Location Image': [],

            'QR Code': []

        }

        

        
        for asset_file in os.listdir(project_path):

            asset_path = os.path.join(project_path, asset_file)

            

            
            if os.path.isfile(asset_path):

                
                if 'background' in asset_file.lower():

                    background_image = preprocess_image(asset_path)

                    preprocessed_assets['Background Image'].append(background_image)

                elif 'logo' in asset_file.lower():

                    logo_image = preprocess_image(asset_path)

                    preprocessed_assets['Logo'].append(logo_image)

                elif 'cta' in asset_file.lower():

                    cta_button_image = preprocess_image(asset_path)

                    preprocessed_assets['Call-To-Action (CTA) Button'].append(cta_button_image)

                elif 'icon' in asset_file.lower():

                    icon_image = preprocess_image(asset_path)

                    preprocessed_assets['Icon'].append(icon_image)

                elif 'product' in asset_file.lower():

                    product_image = preprocess_image(asset_path)

                    preprocessed_assets['Product Image'].append(product_image)

                elif 'text' in asset_file.lower():

                    text_element = preprocess_text(asset_file)

                    preprocessed_assets['Text Elements'].append(text_element)

                
                

        print("Preprocessed assets stored for project:", project_folder)

        print("\n")
import os
from PIL import Image
import torch
from torch.utils.data import Dataset

class AdvertisementDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        for project_folder in os.listdir(self.root_dir):
            project_path = os.path.join(self.root_dir, project_folder)
            if os.path.isdir(project_path):
                samples.append(project_path)
        return samples

    def _load_assets(self, project_path):
        landing_image = None
        endframe_image = None
        other_assets = []

        for asset_file in os.listdir(project_path):
            asset_path = os.path.join(project_path, asset_file)
            if os.path.isfile(asset_path):
                if 'landing' in asset_file.lower():
                    landing_image = Image.open(asset_path)
                elif 'endframe' in asset_file.lower():
                    endframe_image = Image.open(asset_path)
                else:
                    other_assets.append(Image.open(asset_path))

        return landing_image, endframe_image, other_assets

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        project_path = self.samples[idx]
        landing_image, endframe_image, other_assets = self._load_assets(project_path)
                return landing_image, endframe_image, other_assets
import torch
import torch.nn as nn

class TextUnderstandingModule(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(TextUnderstandingModule, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.relu = nn.ReLU()

    def forward(self, text):
        embedded = self.embedding(text)
        lstm_out, _ = self.lstm(embedded)
        lstm_out = lstm_out[:, -1, :]
        out = self.fc(lstm_out)
        out = self.relu(out)
        return out
import torch
import torch.nn as nn

class ImageGenerationModule(nn.Module):
    def __init__(self, latent_dim, img_channels=3, img_size=64):
        super(ImageGenerationModule, self).__init__()
        self.latent_dim = latent_dim
        self.img_channels = img_channels
        self.img_size = img_size

                self.generator = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, img_size * img_size * img_channels),
            nn.Tanh()
        )

    def forward(self, z):
        img = self.generator(z)
        img = img.view(-1, self.img_channels, self.img_size, self.img_size)
        return img
import torch

def calculate_accuracy(outputs, labels):
    """
    Calculates the accuracy of model predictions.

    Args:
    - outputs (torch.Tensor): Model predictions.
    - labels (torch.Tensor): Ground truth labels.

    Returns:
    - float: Accuracy score.
    """
    _, predicted = torch.max(outputs, 1)
    correct = (predicted == labels).sum().item()
    total = labels.size(0)
    accuracy = correct / total
    return accuracy

def evaluate_model(model, dataloader, criterion):
    """
    Evaluates a model on a given dataset.

    Args:
    - model (torch.nn.Module): The model to evaluate.
    - dataloader (torch.utils.data.DataLoader): DataLoader for the dataset.
    - criterion (torch.nn.Module): Loss function.

    Returns:
    - float: Average loss.
    - float: Accuracy.
    """
    model.eval()      total_loss = 0.0
    total_accuracy = 0.0

    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            total_accuracy += calculate_accuracy(outputs, labels)

    avg_loss = total_loss / len(dataloader)
    avg_accuracy = total_accuracy / len(dataloader)

    return avg_loss, avg_accuracy
import numpy as np
from PIL import Image

def preprocess_image(image_path, target_size=(224, 224)):
    """
    Preprocesses an image by resizing and normalizing pixel values.
    
    Args:
    - image_path (str): Path to the image file.
    - target_size (tuple): Desired size for the image after resizing.
    
    Returns:
    - np.ndarray: Preprocessed image as a numpy array.
    """
    img = Image.open(image_path)
    img = img.resize(target_size)
    img = np.array(img)
    img = img / 255.0      return img

def preprocess_text(text):
    """
    Preprocesses text data.
    
    Args:
    - text (str): Input text.
    
    Returns:
    - np.ndarray: Preprocessed text data.
    """
                return processed_text
input_size = 300
hidden_size = 256
num_layers = 2
num_classes = 10  
latent_dim = 100
img_channels = 3  img_size = 64  
num_epochs = 10
batch_size = 16
learning_rate = 0.001
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from data.dataset import AdvertisementDataset
from models.text_module import TextUnderstandingModule
from models.image_module import ImageGenerationModule
from utils.preprocess import preprocess_image, preprocess_text
from utils.evaluation import evaluate_model

from config import *

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

dataset = AdvertisementDataset(root_dir='data/Preprocessed_Assets')
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

text_module = TextUnderstandingModule(input_size, hidden_size, num_layers, num_classes).to(device)

image_module = ImageGenerationModule(latent_dim, img_channels, img_size).to(device)

criterion = nn.MSELoss()

optimizer = optim.Adam(list(text_module.parameters()) + list(image_module.parameters()), lr=learning_rate)

for epoch in range(num_epochs):
    text_module.train()
    image_module.train()
    total_loss = 0.0

    for landing_images, endframe_images, other_assets in dataloader:
                landing_images = landing_images.to(device)
        endframe_images = endframe_images.to(device)
        other_assets = [asset.to(device) for asset in other_assets]

                        text_inputs = torch.randn(batch_size, input_size).to(device)

                text_features = text_module(text_inputs)
        generated_images = image_module(text_features)

                loss = criterion(generated_images, endframe_images)
        total_loss += loss.item()

                optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        avg_loss = total_loss / len(dataloader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')

        
torch.save(text_module.state_dict(), 'saved_models/text_module.pth')
torch.save(image_module.state_dict(), 'saved_models/image_module.pth')
import torch
from models.text_module import TextUnderstandingModule
from models.image_module import ImageGenerationModule
from utils.preprocess import preprocess_text

text_module = TextUnderstandingModule(input_size, hidden_size, num_layers, num_classes)
text_module.load_state_dict(torch.load('saved_models/text_module.pth'))
text_module.eval()

image_module = ImageGenerationModule(latent_dim, img_channels, img_size)
image_module.load_state_dict(torch.load('saved_models/image_module.pth'))
image_module.eval()

input_text = "Your input text here..."

text_inputs = preprocess_text(input_text)

text_inputs = torch.tensor(text_inputs).unsqueeze(0)  
with torch.no_grad():
    text_features = text_module(text_inputs)
    generated_images = image_module(text_features)
