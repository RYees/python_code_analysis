import json
import re
 
class Util():
  def __init__(self) -> None:
  self.emoji_pattern = re.compile("["
  u"\U0001F600-\U0001F64F"   u"\U0001F300-\U0001F5FF"   u"\U0001F680-\U0001F6FF"   u"\U0001F700-\U0001F77F"   u"\U0001F780-\U0001F7FF"   u"\U0001F800-\U0001F8FF"   u"\U0001F900-\U0001F9FF"   u"\U0001FA00-\U0001FA6F"   u"\U0001FA70-\U0001FAFF"   u"\u2600-\u26FF"   u"\u2700-\u27BF"   u"\u2B50"   u"\U0001F1E6-\U0001F1FF"   "]+", flags=re.UNICODE)
  self.symbols = re.compile("["
  "\""
  "\“"
  "\""
  "\'"
  "\-"
  "\*"
  "\•"
  "\ℹ"
  "\﻿"
  "\_"
  "]+")
  self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
  self.mention_pattern = r'@(\w+)'
  def read_file(self, file_path: str) -> dict:
  with open(file_path, 'r') as file:
  data = json.load(file)
  return data
  def write_file(self, file_path: str, data: dict) -> None:
  with open(file_path, 'w') as file:
  json.dump(data, file, indent=2)
  def parse_text(self, text: any) -> str:
  if isinstance(text, str):
  return text
  elif isinstance(text, list):
  contents = []
  for item in text:
  if isinstance(item, str):
  contents.append(item)
  elif isinstance(item, dict):
  contents.append(item['text'])
  return "".join(contents)
  else:
  return ""
  def parse_messages(self, messages: list) -> dict:
  parsed_messages = {
  'id': [],
  'text': [],
  'date': []
  }
  for message in messages:
  if message['type'] != 'message' or len(message['text']) == 0:
  continue
  parsed_messages['id'].append(message['id'])
  message_content = self.parse_text(message['text'])
  parsed_messages['text'].append(message_content)
  parsed_messages['date'].append(message['date'])
  return parsed_messages
  def extract_hashtags(self, text: str) -> list:
  return [word for word in text.split() if word.startswith('
  def extract_emojis(self, text):
  return ''.join(self.emoji_pattern.findall(text))
  def remove_emojis(self, text):
  return self.emoji_pattern.sub('', text)
  def extract_symbols(self, text):
  return ''.join(self.symbols.findall(text))
  def remove_symbols(self, text):
  return self.symbols.sub(' ', text)
  def extract_urls(self, text):
  return re.findall(self.url_pattern, text)
  def extract_mentions(self, text):
  return re.findall(self.mention_pattern, text)
import unittest
import sys, os
sys.path.append(os.path.abspath(os.path.join('..')))
from scripts.util import find_average, count_occurence
 
class TestCases(unittest.TestCase):
  def test_find_average(self):
  """
  Test that it retunrs the average of a given list
  """
  data = [1, 2, 3]
  result = find_average(data)
  self.assertEqual(result, 2.0)
  def test_input_value(self):
  """
  Provide an assertion level for arg input
  """
  self.assertRaises(TypeError, find_average, True)
class TestCountOccurence(unittest.TestCase):
  def test_count_occurence(self):
  """
  Test that it returns the count of each unique values in the given list
  """
  data = [0,0,9,0,8,9,0,7]
  result = count_occurence(data)
  output = {0: 4, 9: 2, 8: 1, 7: 1}
  self.assertAlmostEqual(result, output)
  def test_input_value(self):
  """
  Provide an assertion level for arg input
  """
  self.assertRaises(TypeError, count_occurence, True)
if __name__ == '__main__':
  unittest.main()
import json
import re
 
class Util():
  def __init__(self) -> None:
  self.emoji_pattern = re.compile("["
  u"\U0001F600-\U0001F64F"   u"\U0001F300-\U0001F5FF"   u"\U0001F680-\U0001F6FF"   u"\U0001F700-\U0001F77F"   u"\U0001F780-\U0001F7FF"   u"\U0001F800-\U0001F8FF"   u"\U0001F900-\U0001F9FF"   u"\U0001FA00-\U0001FA6F"   u"\U0001FA70-\U0001FAFF"   u"\u2600-\u26FF"   u"\u2700-\u27BF"   u"\u2B50"   u"\U0001F1E6-\U0001F1FF"   "]+", flags=re.UNICODE)
  self.symbols = re.compile("["
  "\""
  "\“"
  "\""
  "\'"
  "\-"
  "\*"
  "\•"
  "\ℹ"
  "\﻿"
  "\_"
  "]+")
  self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
  self.mention_pattern = r'@(\w+)'
  def read_file(self, file_path: str) -> dict:
  with open(file_path, 'r') as file:
  data = json.load(file)
  return data
  def write_file(self, file_path: str, data: dict) -> None:
  with open(file_path, 'w') as file:
  json.dump(data, file, indent=2)
  def parse_text(self, text: any) -> str:
  if isinstance(text, str):
  return text
  elif isinstance(text, list):
  contents = []
  for item in text:
  if isinstance(item, str):
  contents.append(item)
  elif isinstance(item, dict):
  contents.append(item['text'])
  return "".join(contents)
  else:
  return ""
  def parse_messages(self, messages: list) -> dict:
  parsed_messages = {
  'id': [],
  'text': [],
  'date': []
  }
  for message in messages:
  if message['type'] != 'message' or len(message['text']) == 0:
  continue
  parsed_messages['id'].append(message['id'])
  message_content = self.parse_text(message['text'])
  parsed_messages['text'].append(message_content)
  parsed_messages['date'].append(message['date'])
  return parsed_messages
  def extract_hashtags(self, text: str) -> list:
  return [word for word in text.split() if word.startswith('
  def extract_emojis(self, text):
  return ''.join(self.emoji_pattern.findall(text))
  def remove_emojis(self, text):
  return self.emoji_pattern.sub('', text)
  def extract_symbols(self, text):
  return ''.join(self.symbols.findall(text))
  def remove_symbols(self, text):
  return self.symbols.sub(' ', text)
  def extract_urls(self, text):
  return re.findall(self.url_pattern, text)
  def extract_mentions(self, text):
  return re.findall(self.mention_pattern, text)
import sys, os
import pandas as pd
sys.path.append(os.path.abspath(os.path.join('../scripts')))
from util import Util
parsed_dir = "../data/raw/"
cleaned_dir = "../data/parsed/"
file_name = "TIKVAH-data"
util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")
df.head()
df = pd.DataFrame(df['messages'])
df_messages = pd.DataFrame(df['messages'].tolist())
df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))
df_messages = df_messages[['id', 'text', 'date']]
df_messages
df_filtered = df_messages[(df_messages['id'] >= 60000) & (df_messages['id'] <= 75000)]
df_filtered.head()
data_null_removed = df_filtered.dropna()
data_null_removed.head()
tigvah_data = data_null_removed.replace('\n', ' ', regex=True)
tigvah_data.head()
tigvah_data['hashtags'] = tigvah_data['text'].apply(lambda x: util.extract_hashtags(x))
tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace(r'\
tigvah_data.head()
tigvah_data['emojis'] = tigvah_data['text'].apply(util.extract_emojis)
tigvah_data.tail()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_emojis)
tigvah_data.head()
 
letters = [
  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],
  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],
  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]
]
for letter in letters:
  for i in range(len(letter[0])):
  tigvah_data['text'] = tigvah_data['text'].str.replace(letter[0][i], letter[1][i])
tigvah_data.head()
tigvah_data['symbols'] = tigvah_data['text'].apply(util.extract_symbols)
tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].apply(util.remove_symbols)
tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)
tigvah_data.head()
tigvah_data['links'] = tigvah_data['text'].apply(util.extract_urls)
tigvah_data.head()
tigvah_data['mentions'] = tigvah_data['text'].apply(util.extract_mentions)
tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()
tigvah_data.head()
tigvah_data['text'] = tigvah_data['text'].str.replace('\s+', ' ', regex=True).str.strip()
tigvah_data['text'] = tigvah_data['text'].replace(r'!+', '!', regex=True)
tigvah_data['text'] = tigvah_data['text'].replace(r'\.+', '', regex=True)
tigvah_data.head()
tigvah_data = tigvah_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
tigvah_data
tigvah_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
import sys, os
import pandas as pd
sys.path.append(os.path.abspath(os.path.join('../scripts')))
from util import Util
parsed_dir = "../data/raw/"
cleaned_dir = "../data/parsed/"
file_name = "YeneTube"
util = Util()
df = pd.read_json(f"{parsed_dir}/{file_name}.json")
df.head()
df = pd.DataFrame(df['messages'])
df_messages = pd.DataFrame(df['messages'].tolist())
df_messages['text'] = df_messages['text'].apply(lambda x: ' '.join([i if isinstance(i, str) else i['text'] for i in x if isinstance(i, str) or ('text' in i and isinstance(i['text'], str))]))
df_messages = df_messages[['id', 'text', 'date']]
df_messages
data_null_removed = df_messages.dropna()
data_null_removed.head()
yenetube_data = data_null_removed.replace('\n', ' ', regex=True)
yenetube_data.head()
yenetube_data['hashtags'] = yenetube_data['text'].apply(lambda x: util.extract_hashtags(x))
yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace(r'\
yenetube_data.head()
yenetube_data['emojis'] = yenetube_data['text'].apply(util.extract_emojis)
yenetube_data.tail()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_emojis)
yenetube_data.head()
 
letters = [
  [['ሐ', 'ሑ', 'ሒ', 'ሓ', 'ሔ', 'ሖ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
  [['ኀ', 'ኁ', 'ኂ', 'ኃ', 'ኄ', 'ኅ', 'ኆ'], ['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ']],
  [['ሠ', 'ሡ', 'ሢ', 'ሣ', 'ሤ', 'ሦ', 'ሦ', 'ሧ'], ['ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ']],
  [['ዐ', 'ዑ', 'ዒ', 'ዓ', 'ዔ', 'ዕ', 'ዖ'], ['አ', 'ኡ', 'ኢ', 'ኣ', 'ኤ', 'እ', 'ኦ']],
  [['ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ'], ['ፀ', 'ፁ', 'ፂ', 'ፃ', 'ፄ', 'ፅ', 'ፆ']]
]
for letter in letters:
  for i in range(len(letter[0])):
  yenetube_data['text'] = yenetube_data['text'].str.replace(letter[0][i], letter[1][i])
yenetube_data.head()
yenetube_data['symbols'] = yenetube_data['text'].apply(util.extract_symbols)
yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].apply(util.remove_symbols)
yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)
yenetube_data.head()
yenetube_data['links'] = yenetube_data['text'].apply(util.extract_urls)
yenetube_data.head()
yenetube_data['mentions'] = yenetube_data['text'].apply(util.extract_mentions)
yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace(util.mention_pattern, '', regex=True).str.strip()
yenetube_data.head()
yenetube_data['text'] = yenetube_data['text'].str.replace('\s+', ' ', regex=True).str.strip()
yenetube_data['text'] = yenetube_data['text'].replace(r'!+', '!', regex=True)
yenetube_data['text'] = yenetube_data['text'].replace(r'\.+', '', regex=True)
yenetube_data.head()
yenetube_data = yenetube_data.drop(['hashtags', 'emojis', 'symbols', 'links', 'mentions'], axis=1)
yenetube_data
yenetube_data.to_csv(f"{cleaned_dir}/{file_name}.csv")
def combine_sentences(sentences, buffer_size=1):
  for i in range(len(sentences)):
  combined_sentence = ''
  for j in range(i - buffer_size, i):
  if j >= 0:
  combined_sentence += sentences[j]['sentence'] + ' '
  combined_sentence += sentences[i]['sentence']
  for j in range(i + 1, i + 1 + buffer_size):
  if j < len(sentences):
  combined_sentence += ' ' + sentences[j]['sentence']
  sentences[i]['combined_sentence'] = combined_sentence
  return sentences
import json
 
text_values = []
 
with open('manchester.json', 'r') as file:
  data = json.load(file)
  messages = data.get('messages', [])
  for message in messages:
  text_content = message.get('text', None)
  if text_content and isinstance(text_content, list):
  first_part_of_text = text_content[0] if text_content and isinstance(text_content[0], str) else None
  if first_part_of_text:
  text_values.append(first_part_of_text)
 
concatenated_text = '\n'.join(text_values)
 
print(len(concatenated_text))
def combine_sentences(sentences, buffer_size=1):
  for i in range(len(sentences)):
  combined_sentence = ''
  for j in range(i - buffer_size, i):
  if j >= 0:
  combined_sentence += sentences[j]['sentence'] + ' '
  combined_sentence += sentences[i]['sentence']
  for j in range(i + 1, i + 1 + buffer_size):
  if j < len(sentences):
  combined_sentence += ' ' + sentences[j]['sentence']
  sentences[i]['combined_sentence'] = combined_sentence
  return sentences
 
def pretty_print_docs(docs):
  print(
  f"\n{'-' * 100}\n".join(
  [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
  )
  )
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
 
documents = TextLoader(concatenated_text).load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()
 
docs = retriever.get_relevant_documents(
  "What did the president say about Ketanji Brown Jackson"
)
pretty_print_docs(docs)
from sklearn.metrics.pairwise import cosine_similarity
def calculate_cosine_distances(sentences):
  distances = []
  for i in range(len(sentences) - 1):
  embedding_current = sentences[i]['combined_sentence_embedding']
  embedding_next = sentences[i + 1]['combined_sentence_embedding']
  similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
  distance = 1 - similarity
  distances.append(distance)
  sentences[i]['distance_to_next'] = distance
  return distances, sentences
import re
import numpy as np
from combine_sentences import combine_sentences
import calculate_cosine_distance
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from calculate_cosine_distance import calculate_cosine_distances
from dotenv import load_dotenv
import os
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
def semantic_retriever(essay):
  try:
  single_sentences_list = re.split(r'(?<=[.?!])\s+', essay)
  sentences = [{'sentence': x, 'index': i} for i, x in enumerate(single_sentences_list)]
  try:
  sentences = combine_sentences(sentences)
  except Exception as e:
  print(f"Error in combine_sentences: {e}")
  return []
  try:
  oaiembeds = OpenAIEmbeddings()
  embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])
  except Exception as e:
  print(f"Error in OpenAI embeddings: {e}")
  return []
  for i, sentence in enumerate(sentences):
  sentence['combined_sentence_embedding'] = embeddings[i]
  try:
  distances, sentences = calculate_cosine_distances(sentences)
  except Exception as e:
  print(f"Error in calculate_cosine_distance: {e}")
  return []
  breakpoint_percentile_threshold = 95
  breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)
  indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]
  start_index = 0
  chunks = []
  for index in indices_above_thresh:
  end_index = index
  group = sentences[start_index:end_index + 1]
  combined_text = ' '.join([d['sentence'] for d in group])
  chunks.append(combined_text)
  start_index = index + 1
  if start_index < len(sentences):
  combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])
  chunks.append(combined_text)
  return chunks
  except Exception as e:
  print(f"An unexpected error occurred: {e}")
  return []
from extract_json import extract_json
a= extract_json('ብስራት ስፖርት.json')
from semantic_chuncking import semantic_retriever
chunks = semantic_retriever(a)
from operator import itemgetter
 
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
vectorstore = FAISS.from_texts(
  chunks, embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever(search_kwargs={"k" : 1}) relevant_docs = retriever.get_relevant_documents("ቤቲንግ")
relevant_docs
for doc in relevant_docs:
  print(doc.page_content)
  print('\n')
template = """<human>: Create a compelling Amharic or Amharic mixed with English advertisement for {question}.
Highlight the key features,unique selling points, and the exceptional services offered by the brand.  
Craft a persuasive narrative that resonates with the target audience, emphasizing the brand's values  
and commitment to customer satisfaction. Use vibrant and engaging language to evoke a positive emotional
response and encourage potential customers to explore and choose {question}.  
Ensure the ad reflects the brand's identity and leaves a lasting impression on the audience.
 
Make it a highly personalized promotional text that resonates with {context}, drawing insights from their  
contextual data. Leverage historical interactions, discussions, or online activities to tailor the ad  
closely to {context} unique preferences and interests. Highlight features and services that align with  
their needs based on the available contextual information. Tailor the language to address specific topics,
preferences, or issues derived from {context} historical interactions. Ensure the personalized content feels
natural and aligns with [User's] communication style, fostering a deep connection and encouraging meaningful
engagement. The goal is to create an ad that demonstrates an understanding of {context} context, providing
value and relevance in a way that feels organic.
 
{context}
 
Question: {question}
 
\n
 
<bot>:
"""
 
prompt = ChatPromptTemplate.from_template(template)
 
model = Mistral_7B
chain = (
  {"context": retriever, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()
)
import json
import re
def extract_json(json_file):
  last_25_percent_text = ""
  with open(json_file, 'r') as file:
  data = json.load(file)
  messages = data.get('messages', [])
  last_25_percent_length = int(len(messages) * 0.99)
  for message in messages[-last_25_percent_length:]:
  text_content = message.get('text', [])
  if isinstance(text_content, list):
  for item in text_content:
  if isinstance(item, str):
  last_25_percent_text += item
  cleaned_text = re.sub(r'[!@
  return last_25_percent_text
