{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import json \n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "from collections import Counter\n",
    "\n",
    "# from __future__ import print_function\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, json\n",
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "import utils.llm_model as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data_clean.csv\")\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI-EMEBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "env_vars = dotenv_values('.env')\n",
    "openai.api_key = env_vars.get('OPENAI_API_KEY')\n",
    "client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"function add(a, b) { return a + b; }\",\n",
    "model=\"text-embedding-ada-002\"\n",
    "\n",
    "def get_embedding(text, model):\n",
    "   #text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "get_embedding(input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "embeddings_2d = embed_model.embed_documents(\"function add(a, b) { return a + b; }\")\n",
    "len(embeddings_2d), len(embeddings_2d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "# data['code_embedding'] = data['code_content'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
    "\n",
    "# def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
    "#    embedding = get_embedding(code_query, model='text-embedding-3-small')\n",
    "#    df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "#    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "#    return res\n",
    "# res = search_functions(data, 'Completions API tests', n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_snippets = [\n",
    "#   \"\"\"\n",
    "#     def factorial(n):\n",
    "#     result = 1\n",
    "#     for i in range(1, n + 1):\n",
    "#         result *= i\n",
    "#     return result\n",
    "#   \"\"\",\n",
    "#   \"\"\"\n",
    "#   def fibonacci(n, memo={}):\n",
    "#     if n in memo:\n",
    "#         return memo[n]\n",
    "#     if n <= 2:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo)\n",
    "#         memo[n] = result\n",
    "#         return result\n",
    "#   \"\"\",\n",
    "#   \"\"\"\n",
    "#     unsigned long long emi(unsigned int n) {\n",
    "#       if (n == 0 || n == 1) {\n",
    "#           return 1;\n",
    "#       } else {\n",
    "#           return n * emi(n - 1);\n",
    "#       }\n",
    "#     }\n",
    "#   \"\"\",\n",
    "#   \"\"\"\n",
    "#     def looping(n):    \n",
    "#           print(n) \n",
    "#   \"\"\",\n",
    "# ]\n",
    "\n",
    "# code_snippets = [\n",
    "#     \"def sum_of_digits(n):\\n    digit_sum = 0\\n    while n > 0:\\n        digit_sum += n % 10\\n        n //= 10\\n    return digit_sum\",\n",
    "#     \"def is_prime(n):\\n    if n < 2:\\n        return False\\n    for i in range(2, int(n ** 0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\",\n",
    "#     \"def reverse_string(s):\\n    return s[::-1]\"\n",
    "# ]\n",
    "\n",
    "\n",
    "code_snippets = [\n",
    "    \"\"\"\n",
    "        def square(x):\n",
    "            return x ** 2\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "        class Rectangle:\n",
    "            def __init__(self, width, height):\n",
    "                self.width = width\n",
    "                self.height = height\n",
    "\n",
    "            def area(self):\n",
    "                return self.width * self.height\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "        def greet(name):\n",
    "            print(f\"Hello, {name}!\")\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "# code_snippets = [\n",
    "#     \"\"\"\n",
    "#        def greet(name):\n",
    "#           return f\"Hello, {name}!\"\n",
    "#     \"\"\",\n",
    "#     \"\"\"\n",
    "#     numbers = [1, 2, 3, 4, 5]\n",
    "#     squared_numbers = [num ** 2 for num in numbers]\n",
    " \n",
    "#     \"\"\",\n",
    "#     \"\"\"\n",
    "#     person = {\n",
    "#         'name': 'John',\n",
    "#         'age': 30,\n",
    "#         'city': 'New York'\n",
    "#     }\n",
    "#     for key, value in person.items():\n",
    "#         print(f'{key}: {value}')\n",
    "#     \"\"\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../txt/python.txt', 'r') as file:\n",
    "    code_snip = file.readlines()\n",
    "\n",
    "# code_snip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.         0.69629985 0.67785584 ... 0.66006583 0.69641109 0.69629985]\n",
      " [0.69629985 1.         0.66694725 ... 0.67794261 0.99999686 1.        ]\n",
      " [0.67785584 0.66694725 1.         ... 0.66439169 0.66711958 0.66694725]\n",
      " ...\n",
      " [0.66006583 0.67794261 0.66439169 ... 1.         0.67804164 0.67794261]\n",
      " [0.69641109 0.99999686 0.66711958 ... 0.67804164 1.         0.99999686]\n",
      " [0.69629985 1.         0.66694725 ... 0.67794261 0.99999686 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a collection of code snippets\n",
    "embeddings=[]\n",
    "for code in code_snip:\n",
    "    response = client.embeddings.create(input=code, model=\"text-embedding-ada-002\")\n",
    "    embedding = response.data[0].embedding  # Access the 'embedding' attribute within the 'data' attribute\n",
    "    embeddings.append(embedding)\n",
    "# text-embedding-ada-002\n",
    "# text-embedding-3-small\n",
    "# Generate embeddings for each code snippet\n",
    "# embeddings = [client.embeddings.create(input=code, model=\"text-embedding-ada-002\")['embedding'] for code in code_snippets]\n",
    "\n",
    "# Convert the embeddings to a format suitable for cosine similarity\n",
    "embeddings_matrix = [embedding for embedding in embeddings]\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Print the similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Cosine Similarity Matrix:\n",
    "# [[1.         0.42035923 0.29017105]\n",
    "#  [0.42035923 1.         0.26411547]\n",
    "#  [0.29017105 0.26411547 1.        ]]\n",
    "\n",
    "\n",
    "# Cosine Similarity Matrix:\n",
    "# [[1.         0.42035124 0.5452424 ]\n",
    "#  [0.42035124 1.         0.35463255]\n",
    "#  [0.5452424  0.35463255 1.        ]]\n",
    "\n",
    "### text-ada\n",
    "# Cosine Similarity Matrix:\n",
    "# [[1.         0.83043881 0.85228109]\n",
    "#  [0.83043881 1.         0.79774699]\n",
    "#  [0.85228109 0.79774699 1.        ]]\n",
    "\n",
    "#one square\n",
    "# Cosine Similarity Matrix:\n",
    "# [[1.         0.83043962 0.8523307 ]\n",
    "#  [0.83043962 1.         0.79774699]\n",
    "#  [0.8523307  0.79774699 1.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999976, 0.8908432 , 0.85120744, ..., 0.88848   , 0.8908432 ,\n",
       "        0.8908432 ],\n",
       "       [0.8908432 , 1.0000001 , 0.73301077, ..., 0.90846777, 1.0000001 ,\n",
       "        1.0000001 ],\n",
       "       [0.85120744, 0.73301077, 0.99999964, ..., 0.83173513, 0.73301077,\n",
       "        0.73301077],\n",
       "       ...,\n",
       "       [0.88848   , 0.90846777, 0.83173513, ..., 0.9999999 , 0.90846777,\n",
       "        0.90846777],\n",
       "       [0.8908432 , 1.0000001 , 0.73301077, ..., 0.90846777, 1.0000001 ,\n",
       "        1.0000001 ],\n",
       "       [0.8908432 , 1.0000001 , 0.73301077, ..., 0.90846777, 1.0000001 ,\n",
       "        1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# Tokenize and encode the code snippets\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "encoded_inputs = tokenizer(code_snip, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate code embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    embeddings_bert = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "\n",
    "embeddings_matrix_bert = [embedding for embedding in embeddings_bert]\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity_matrix_bert = cosine_similarity(embeddings_matrix_bert)\n",
    "similarity_matrix_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\"This is the first document.\",\n",
    "\t\t\t\"This document is the second document.\",\n",
    "\t\t\t\"And this is the third one.\",\n",
    "\t\t\t\"Is this the first document?\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary (Feature Names):\", feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AST code embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between code snippet 1 and 2: 1\n",
      "Similarity between code snippet 1 and 3: 1\n",
      "Similarity between code snippet 2 and 3: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def ast_embedding(code):\n",
    "    # Parse code into an Abstract Syntax Tree (AST)\n",
    "    tree = ast.parse(code)\n",
    "\n",
    "    # Example: Transform AST into a fixed-size vector representation\n",
    "    embedding = np.zeros(100)  # Placeholder for the vector representation\n",
    "\n",
    "    # Perform operations to traverse or transform the AST and update the embedding vector\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# Array of code snippets to evaluate\n",
    "code_snippets = [\n",
    "    \"def add(a, b):\\n    return a + b\",\n",
    "    \"def multiply(a, b):\\n    return a * b\",\n",
    "    \"var = 10\"\n",
    "]\n",
    "\n",
    "# Calculate AST embeddings for each code snippet\n",
    "embeddings = [ast_embedding(code) for code in code_snippets]\n",
    "\n",
    "# Evaluate syntactic similarity between code snippets\n",
    "for i in range(len(code_snippets)):\n",
    "    for j in range(i+1, len(code_snippets)):\n",
    "        similarity = 1 - cosine(embeddings[i], embeddings[j])\n",
    "        print(f\"Similarity between code snippet {i+1} and {j+1}: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TreeLSTM - TOKEN LEVEL EMEBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TreeLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TreeLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define the Tree-LSTM layers and operations\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Implement the forward pass of the Tree-LSTM model\n",
    "        # using the input code snippet\n",
    "\n",
    "        return embedding\n",
    "\n",
    "\n",
    "def tree_lstm_embedding(code):\n",
    "    # Example: Using Tree-LSTM for token-level code embedding\n",
    "    input_size = 100  # Example input size\n",
    "    hidden_size = 200  # Example hidden size\n",
    "\n",
    "    model = TreeLSTM(input_size, hidden_size)\n",
    "    input_tensor = torch.tensor(code)  # Example input tensor\n",
    "\n",
    "    # Obtain the embedding using the Tree-LSTM model\n",
    "    embedding = model(input_tensor)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install code2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that Code2Vec typically requires a training phase with a large code corpus\n",
    "from code2vec import Code2VecModel\n",
    "\n",
    "# Load pre-trained Code2Vec model\n",
    "model = Code2VecModel.load('pretrained_model')\n",
    "\n",
    "def code2vec_embedding(code):\n",
    "    # Example: Embedding a code snippet using Code2Vec\n",
    "    embedding = model.code_vectorize(code)\n",
    "\n",
    "    return embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
