{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "env_vars = dotenv_values('.env')\n",
    "openai.api_key = env_vars.get('OPENAI_API_KEY')\n",
    "openai_client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullopenai_model_answer(query, retrieved_documents, model=\"gpt-4-turbo-preview\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"As an assistant, your role is to be helpful and provide answers in a concise and summary-style manner. Your objective is to draw from the available document and your knowledge to deliver accurate and concise information. If you encounter a question for which you lack the answer or cannot find the relevant information, it's best to respond with a brief summary or let the user know that the answer is not available. Avoid including specific code examples and instead focus on offering assistance whenever possible. Your primary aim is to deliver clear and precise responses, ensuring that the information you provide is helpful to the user.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_model_answer(query, retrieved_documents, model=\"gpt-4-turbo-preview\"):\n",
    "    information_chunks = split_into_chunks(retrieved_documents, max_chunk_length=4096)  \n",
    "    conversation = create_initial_conversation(query, information_chunks[0])\n",
    "    for chunk in information_chunks[:2]:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=conversation['messages']\n",
    "        )\n",
    "        process_response(response)\n",
    "\n",
    "        conversation['messages'].append(create_user_message(chunk))\n",
    "\n",
    "    final_response = get_final_response(conversation)\n",
    "    return final_response\n",
    "\n",
    "def split_into_chunks(text, max_chunk_length):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for line in text.splitlines():\n",
    "        if len(current_chunk) + len(line) > max_chunk_length:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \"\"\n",
    "        current_chunk += line + \"\\n\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def create_initial_conversation(query, initial_chunk):\n",
    "    return {\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': \"As an assistant, your role is to be helpful and provide answers in a concise and summary-style manner. Your objective is to draw from the available document and your knowledge to deliver accurate and concise information. If you encounter a question for which you lack the answer or cannot find the relevant information, it's best to respond with a brief summary or let the user know that the answer is not available. Avoid including specific code examples and instead focus on offering assistance whenever possible. Your primary aim is to deliver clear and precise responses, ensuring that the information you provide is helpful to the user.\"},\n",
    "            {'role': 'user', 'content': f'Question: {query}'},\n",
    "            {'role': 'assistant', 'content': initial_chunk}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_user_message(content):\n",
    "    return {'role': 'user', 'content': content}\n",
    "\n",
    "def process_response(response):\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "def get_final_response(conversation):\n",
    "    return conversation['messages'][-1]['content']\n",
    "\n",
    "with open('../txt/w7/cluster_4_texts.txt', 'r') as file:\n",
    "    retrieved_documents = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document showcases a series of Python code blocks aimed at processing textual data for various tasks such as cleaning, normalizing, and vectorizing text, primarily focused on Amharic, a language spoken in Ethiopia. The text processing includes removing null values, hashtags, emojis, symbols, links, mentions, and standardizing characters for consistency. Additionally, it demonstrates tokenizing texts, extracting features like symbols and emojis, and then applying a Word2Vec model to tokenize sentences for embedding. Towards the end, it shifts towards downloading and utilizing models from Hugging Face, a popular repository for machine learning models, specifically for natural language processing tasks.\n",
      "\n",
      "Here's a brief overview of the main processes covered:\n",
      "\n",
      "1. **Preprocessing and Cleaning Data:** This includes reading CSV files, handling missing values, normalizing text (like replacing variants of letters with a consistent version), and extracting specific features such as hashtags, emojis, symbols, and URLs. Text is also cleaned to remove unwanted characters and symbols.\n",
      "\n",
      "2. **Text Normalization for Amharic:** It attempts to standardize various forms of letters in the Amharic script to a uniform representation, which is critical for consistency in language processing tasks.\n",
      "\n",
      "3. **Feature Extraction:** Identifying and extracting emojis, symbols, and hashtags from the texts. Plus, removing or replacing specific patterns like URLs and mentions with appropriate substitutions.\n",
      "\n",
      "4. **Tokenization and Vectorization:** Using the Word2Vec model from the Gensim library to create word embeddings. This process involves tokenizing the corpus into words or sentences and training the Word2Vec model to generate word vectors.\n",
      "\n",
      "5. **Utilizing Hugging Face Models:** Shows steps to download various components of a pretrained model from Hugging Face with the aim of using this model for tasks such as text generation or translation.\n",
      "\n",
      "6. **Saving Processed Data:** The final processed texts are saved in CSV and TXT formats for potential downstream tasks.\n",
      "\n",
      "The document indicates a comprehensive approach to processing and analyzing Amharic text data, deploying both classic NLP techniques like Word2Vec for embedding and modern transformer-based models through Hugging Face. However, it assumes familiarity with specific libraries and programming concepts in Python, such as data manipulation with pandas, text processing, working with vectors and models in natural language processing, and accessing external resources like the Hugging Face hub.\n",
      "The document showcases a sequence of programming instructions and processes involved in handling, cleaning, and analyzing textual data, primarily in what appears to be the Amharic language. The steps are parted into several segments, highlighting the usage of Python, libraries like pandas for data manipulation, Gensim for word embedding, and transformers for sequence-to-sequence learning models.\n",
      "\n",
      "**Key Components:**\n",
      "\n",
      "1. **Data Preprocessing:**\n",
      "    - Reading data from CSV files.\n",
      "    - Dropping missing values to ensure data quality.\n",
      "    - Text cleaning operations, including replacing newline characters, removing emojis, normalizing Amharic letter variants, extracting and removing hashtags, symbols, URLs, and user mentions.\n",
      "    - The text is further cleaned to remove excessive spaces and punctuation.\n",
      "\n",
      "2. **Text Analysis:**\n",
      "    - Implementation of a Word2Vec model using the Gensim library to generate word embeddings for tokenized text.\n",
      "    - This model is trained on the cleaned, tokenized corpus to find vectors for specific words (e.g., \"ኢትዮጵያ\") and to identify words similar to a given word.\n",
      "\n",
      "3. **Integration with Hugging Face:**\n",
      "   - Downloading components of a pre-trained model from Hugging Face's model hub.\n",
      "   - The model ID \"iocuydi/llama-2-amharic-3784m\" indicates a focus on Amharic, suggesting an application for language-specific tasks.\n",
      "   - Usage of AutoTokenizer and AutoModelForSeq2SeqLM from the transformers library to initialize and utilize the downloaded model, potentially for tasks like text generation or translation.\n",
      "\n",
      "**Evaluation:**\n",
      "\n",
      "- The document illustrates a comprehensive workflow for collecting, cleaning, and analyzing textual data, with a specific emphasis on Amharic language processing.\n",
      "- It demonstrates the application of modern NLP techniques and tools, such as word embedding and transformer models, highlighting the practical steps involved in these processes.\n",
      "- The final stages of the document showcase an advanced application of NLP models via integration with the Hugging Face platform, allowing for the exploration and deployment of pre-trained language models.\n",
      "- Overall, the document serves as an informative guide for handling and analyzing textual data, with specific insights into working with non-Latin scripts like Amharic, showcasing the versatility and breadth of applications possible with current NLP methodologies.\n",
      "('pipeline = pipeline (\"Text-Generation\", model=model, device=-1, '\n",
      " 'tokenizer=tokenizer, max_length=1000 )\\n'\n",
      " 'from langchain.text_splitter import CharacterTextSplitter\\n'\n",
      " 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n'\n",
      " 'from langchain_openai import OpenAIEmbeddings\\n'\n",
      " 'from langchain_community.vectorstores import Chroma\\n'\n",
      " 'from PyPDF2 import PdfReader\\n'\n",
      " 'from langchain_community.embeddings import HuggingFaceEmbeddings\\n'\n",
      " 'from langchain_community.embeddings.sentence_transformer import (\\n'\n",
      " '  SentenceTransformerEmbeddings,\\n'\n",
      " ')\\n'\n",
      " ' \\n'\n",
      " 'persist_directory = \"db\"\\n'\n",
      " ' \\n'\n",
      " 'class MySpecialFunctions:\\n'\n",
      " '  def __init__(self):\\n'\n",
      " '  pass\\n'\n",
      " '  def get_file_text(self, files):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for file in files:\\n'\n",
      " '  try:\\n'\n",
      " \"  with open(file, 'r', encoding='utf-8') as f:\\n\"\n",
      " '  content = f.read()\\n'\n",
      " '  text += content\\n'\n",
      " '  except Exception as e:\\n'\n",
      " '  print(f\"Error reading file {file}: {e}\")\\n'\n",
      " '  return text\\n'\n",
      " '  def get_pdf_text(self, pdf):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for doc in pdf:\\n'\n",
      " '  reader = PdfReader(doc)\\n'\n",
      " '  for page in reader.pages:\\n'\n",
      " '  text += page.extract_text()\\n'\n",
      " '  return text\\n'\n",
      " '  def get_text_chunks(self, text):\\n'\n",
      " '  text_siplitter = RecursiveCharacterTextSplitter(\\n'\n",
      " '  chunk_size = 1000,   chunk_overlap = 200,\\n'\n",
      " \"  separators=['\\\\n', '\\\\n\\\\n'],\\n\"\n",
      " '  length_function = len)\\n'\n",
      " '  chunk = text_siplitter.split_text(text)\\n'\n",
      " '  return chunk\\n'\n",
      " '  def get_vectorstore(self, chunks):\\n'\n",
      " '  hf_embedding = HuggingFaceEmbeddings()\\n'\n",
      " '  embedding_function = '\n",
      " 'SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n'\n",
      " '  vector_db = Chroma.from_documents(\\n'\n",
      " '  documents = chunks,\\n'\n",
      " '  embedding = hf_embedding,\\n'\n",
      " '  )\\n'\n",
      " '  return vector_db\\n'\n",
      " 'import streamlit as st \\n'\n",
      " 'from MyFunctions import MySpecialFunctions\\n'\n",
      " 'from dotenv import load_dotenv\\n'\n",
      " ' \\n'\n",
      " 'special_functions_instance = MySpecialFunctions()\\n'\n",
      " 'def get_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_file_text(external_data )\\n'\n",
      " 'def get_pdf_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_pdf_text(external_data )\\n'\n",
      " 'def get_chunks(text):\\n'\n",
      " '  return special_functions_instance.get_text_chunks(text)\\n'\n",
      " 'def get_vectorstore(text_chunks):\\n'\n",
      " '  return special_functions_instance.get_vectorstore(text_chunks)  \\n'\n",
      " 'def main():\\n'\n",
      " '  load_dotenv()\\n'\n",
      " '  st.set_page_config(page_title=\"Generation of Telegram Ads in Amharic\", '\n",
      " 'page_icon= \":smile\")\\n'\n",
      " '  with st.sidebar:\\n'\n",
      " '  pass\\n'\n",
      " '  st.markdown(\"   external_data = st.file_uploader(\" Upload the generative '\n",
      " 'text from fune-tuning\", accept_multiple_files= True)\\n'\n",
      " '  if st.button(\"Retrieval\"):\\n'\n",
      " '  with st.spinner(\"Processing\"):\\n'\n",
      " '  text = get_pdf_text(external_data )\\n'\n",
      " '  text_chunks = get_chunks(text)\\n'\n",
      " '  st.write(text_chunks)\\n'\n",
      " '  vectorstore_db = get_vectorstore(text_chunks)\\n'\n",
      " ' \\n'\n",
      " 'if __name__ == \"__main__\":\\n'\n",
      " '  main()\\n'\n",
      " 'import sentencepiece as spm\\n'\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m \"\n",
      " \"--vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " \"sp.load('m.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " \"print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))\\n\"\n",
      " 'print(sp.decode_ids([460, 133, 774, 1276]))\\n'\n",
      " ' \\n'\n",
      " 'print(sp.get_piece_size())\\n'\n",
      " ' \\n'\n",
      " 'print(sp.id_to_piece(460))\\n'\n",
      " \"print(sp.piece_to_id('▁በአዲስ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\\n\"\n",
      " ' \\n'\n",
      " 'for id in range(3):\\n'\n",
      " '  print(sp.id_to_piece(id), sp.is_control(id))\\n'\n",
      " ' \\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " ' \\n'\n",
      " \"serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " 'sp.load_from_serialized_proto(serialized_model_proto)\\n'\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user \"\n",
      " \"--user_defined_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_user = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_user.load('m_user.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\\n\"\n",
      " \"print(sp_user.piece_to_id('<sep>'))  \\n\"\n",
      " \"print(sp_user.piece_to_id('<cls>'))  \\n\"\n",
      " \"print('3=', sp_user.decode_ids([3]))  \\n\"\n",
      " \"print('4=', sp_user.decode_ids([4]))  \\n\"\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl \"\n",
      " \"--control_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_ctrl = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_ctrl.load('m_ctrl.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\")\n"
     ]
    }
   ],
   "source": [
    "query = \"Evaluate the document and provide an assessment of its overall content\"\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python code collection presents a moderate-to-advanced difficulty level, catering to those with some familiarity with Python, data manipulation using pandas, natural language processing (NLP), and machine learning (ML) models. Here's a rundown of the aspects contributing to its complexity:\n",
      "\n",
      "1. **Use of External Libraries**: The code extensively uses external libraries such as pandas for data manipulation, Gensim for Word2Vec models, and Hugging Face's transformers for working with pre-trained language models. Familiarity with these libraries is essential.\n",
      "\n",
      "2. **Data Processing and NLP Techniques**: Demonstrated are various preprocessing steps typical in NLP tasks, such as emoji and URL extraction and removal, text normalization, and handling of special symbols and mentions. Such operations require understanding of both Python and basic NLP principles.\n",
      "\n",
      "3. **Working with Word2Vec Models**: Part of the code involves training a Word2Vec model using the Gensim library. This involves understanding hyperparameters like embedding size, window size, and min_count, crucial for working with word embeddings.\n",
      "\n",
      "4. **Usage of Pre-Trained Language Models from Hugging Face**: The code also shows how to download and use models from Hugging Face, including adapting a sequence-to-sequence model for use. This indicates an advanced use case involving interaction with an external API and working with cutting-edge NLP models.\n",
      "\n",
      "5. **Script Structure and Modularization**: The code snippets suggest a project structure that includes separate modules and directories for different phases of data processing, from raw data handling to parsing, cleaning, and finally, analysis or model training. Understanding this structure requires some background in software engineering best practices.\n",
      "\n",
      "6. **Environment and Dependency Management**: There's an implicit need for managing Python environments and dependencies (notice the use of `sys.path.append` for local module imports), which is a key skill for any Python developer working on sizable projects.\n",
      "\n",
      "Notable Complexity Factors:\n",
      "- **Text Normalization for Non-Roman Scripts**: Specific to this collection is the handling of the Amharic language, including normalization of variant character representations. This adds a layer of complexity due to the non-Roman script and linguistic nuances of Amharic.\n",
      "- **Integration with Hugging Face Hub and API**: The code demonstrates downloading components for a pre-trained model from Hugging Face Hub using an API key, showcasing an advanced application involving APIs.\n",
      "\n",
      "Overall, the code collection is positioned towards individuals with intermediate Python knowledge, a good understanding of NLP and ML fundamentals, and familiarity with the specific libraries and tools used.\n",
      "The provided Python code snippets span across a variety of tasks primarily focused on data preprocessing, analysis, and natural language processing (NLP) with Python libraries such as Pandas, Gensim, and Hugging Face's Transformers. Here's an assessment of its complexity and notable patterns:\n",
      "\n",
      "### Complexity Level: Intermediate to Advanced\n",
      "\n",
      "1. **Data Preprocessing and Cleaning**: The code involves reading data from CSV and JSON files, handling missing values, and cleaning text data by removing or replacing unwanted characters, symbols, emojis, URLs, and mentions. This part requires a good understanding of Pandas and regular expressions, placing it at an intermediate level.\n",
      "\n",
      "2. **Word Embeddings with Gensim**: Generating word embeddings using the Gensim library's Word2Vec model increases the complexity as it involves understanding of NLP concepts like tokenization and embeddings. The parameters for the Word2Vec model such as vector size, window size, and min_count are set explicitly, suggesting an intermediate to advanced level of complexity due to the need for a solid grasp of how these parameters affect the model's performance.\n",
      "\n",
      "3. **Using Hugging Face's Transformers for NLP Tasks**: The code shows how to download a pre-trained model from Hugging Face, and set up for use in sequence-to-sequence language modeling, which is an advanced topic. It requires familiarity with the Transformers library and concepts such as tokenization and sequence-to-sequence models.\n",
      "\n",
      "### Notable Patterns and Structures:\n",
      "\n",
      "- **Modular Approach**: The code snippets demonstrate a modular approach to handling different steps of data preprocessing and modeling. For example, there's separation between reading and cleaning data, training a Word2Vec model, and utilizing a pre-trained language model from Hugging Face. This structure aids in making the code more readable and maintainable.\n",
      "\n",
      "- **Use of Lambdas and Apply Functions**: There's extensive use of lambda functions and the `.apply()` method on DataFrame columns to perform operations like extracting hashtags, emojis, and applying various text cleaning steps. This pattern is common in data manipulation with Pandas and demonstrates a more pythonic way of applying functions to DataFrame columns.\n",
      "\n",
      "- **External Utility Functions**: The code snippets include calls to functions from an imported module (`MyPreprocessing`), which suggests that some of the preprocessing logic is abstracted away into external utility functions. This design pattern helps in organizing code better and reusing common functionality.\n",
      "\n",
      "- **Environment Variable for API Key**: Using an environment variable for the Hugging Face API key (`HUGGING_FACE_API_KEY`) is a good practice for security reasons, as it keeps sensitive information out of the codebase.\n",
      "\n",
      "Overall, the code collection demonstrates a mix of data manipulation, NLP techniques, and best practices aimed at intermediate to advanced Python developers. It highlights the application of Python in real-world data preprocessing and analysis tasks, particularly in the context of text data.\n",
      "('pipeline = pipeline (\"Text-Generation\", model=model, device=-1, '\n",
      " 'tokenizer=tokenizer, max_length=1000 )\\n'\n",
      " 'from langchain.text_splitter import CharacterTextSplitter\\n'\n",
      " 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n'\n",
      " 'from langchain_openai import OpenAIEmbeddings\\n'\n",
      " 'from langchain_community.vectorstores import Chroma\\n'\n",
      " 'from PyPDF2 import PdfReader\\n'\n",
      " 'from langchain_community.embeddings import HuggingFaceEmbeddings\\n'\n",
      " 'from langchain_community.embeddings.sentence_transformer import (\\n'\n",
      " '  SentenceTransformerEmbeddings,\\n'\n",
      " ')\\n'\n",
      " ' \\n'\n",
      " 'persist_directory = \"db\"\\n'\n",
      " ' \\n'\n",
      " 'class MySpecialFunctions:\\n'\n",
      " '  def __init__(self):\\n'\n",
      " '  pass\\n'\n",
      " '  def get_file_text(self, files):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for file in files:\\n'\n",
      " '  try:\\n'\n",
      " \"  with open(file, 'r', encoding='utf-8') as f:\\n\"\n",
      " '  content = f.read()\\n'\n",
      " '  text += content\\n'\n",
      " '  except Exception as e:\\n'\n",
      " '  print(f\"Error reading file {file}: {e}\")\\n'\n",
      " '  return text\\n'\n",
      " '  def get_pdf_text(self, pdf):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for doc in pdf:\\n'\n",
      " '  reader = PdfReader(doc)\\n'\n",
      " '  for page in reader.pages:\\n'\n",
      " '  text += page.extract_text()\\n'\n",
      " '  return text\\n'\n",
      " '  def get_text_chunks(self, text):\\n'\n",
      " '  text_siplitter = RecursiveCharacterTextSplitter(\\n'\n",
      " '  chunk_size = 1000,   chunk_overlap = 200,\\n'\n",
      " \"  separators=['\\\\n', '\\\\n\\\\n'],\\n\"\n",
      " '  length_function = len)\\n'\n",
      " '  chunk = text_siplitter.split_text(text)\\n'\n",
      " '  return chunk\\n'\n",
      " '  def get_vectorstore(self, chunks):\\n'\n",
      " '  hf_embedding = HuggingFaceEmbeddings()\\n'\n",
      " '  embedding_function = '\n",
      " 'SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n'\n",
      " '  vector_db = Chroma.from_documents(\\n'\n",
      " '  documents = chunks,\\n'\n",
      " '  embedding = hf_embedding,\\n'\n",
      " '  )\\n'\n",
      " '  return vector_db\\n'\n",
      " 'import streamlit as st \\n'\n",
      " 'from MyFunctions import MySpecialFunctions\\n'\n",
      " 'from dotenv import load_dotenv\\n'\n",
      " ' \\n'\n",
      " 'special_functions_instance = MySpecialFunctions()\\n'\n",
      " 'def get_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_file_text(external_data )\\n'\n",
      " 'def get_pdf_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_pdf_text(external_data )\\n'\n",
      " 'def get_chunks(text):\\n'\n",
      " '  return special_functions_instance.get_text_chunks(text)\\n'\n",
      " 'def get_vectorstore(text_chunks):\\n'\n",
      " '  return special_functions_instance.get_vectorstore(text_chunks)  \\n'\n",
      " 'def main():\\n'\n",
      " '  load_dotenv()\\n'\n",
      " '  st.set_page_config(page_title=\"Generation of Telegram Ads in Amharic\", '\n",
      " 'page_icon= \":smile\")\\n'\n",
      " '  with st.sidebar:\\n'\n",
      " '  pass\\n'\n",
      " '  st.markdown(\"   external_data = st.file_uploader(\" Upload the generative '\n",
      " 'text from fune-tuning\", accept_multiple_files= True)\\n'\n",
      " '  if st.button(\"Retrieval\"):\\n'\n",
      " '  with st.spinner(\"Processing\"):\\n'\n",
      " '  text = get_pdf_text(external_data )\\n'\n",
      " '  text_chunks = get_chunks(text)\\n'\n",
      " '  st.write(text_chunks)\\n'\n",
      " '  vectorstore_db = get_vectorstore(text_chunks)\\n'\n",
      " ' \\n'\n",
      " 'if __name__ == \"__main__\":\\n'\n",
      " '  main()\\n'\n",
      " 'import sentencepiece as spm\\n'\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m \"\n",
      " \"--vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " \"sp.load('m.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " \"print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))\\n\"\n",
      " 'print(sp.decode_ids([460, 133, 774, 1276]))\\n'\n",
      " ' \\n'\n",
      " 'print(sp.get_piece_size())\\n'\n",
      " ' \\n'\n",
      " 'print(sp.id_to_piece(460))\\n'\n",
      " \"print(sp.piece_to_id('▁በአዲስ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\\n\"\n",
      " ' \\n'\n",
      " 'for id in range(3):\\n'\n",
      " '  print(sp.id_to_piece(id), sp.is_control(id))\\n'\n",
      " ' \\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " ' \\n'\n",
      " \"serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " 'sp.load_from_serialized_proto(serialized_model_proto)\\n'\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user \"\n",
      " \"--user_defined_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_user = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_user.load('m_user.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\\n\"\n",
      " \"print(sp_user.piece_to_id('<sep>'))  \\n\"\n",
      " \"print(sp_user.piece_to_id('<cls>'))  \\n\"\n",
      " \"print('3=', sp_user.decode_ids([3]))  \\n\"\n",
      " \"print('4=', sp_user.decode_ids([4]))  \\n\"\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl \"\n",
      " \"--control_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_ctrl = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_ctrl.load('m_ctrl.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\")\n"
     ]
    }
   ],
   "source": [
    "query = \"Evaluate the complexity level of the Python code collection and provide an assessment of its overall difficulty, ranging from beginner-friendly to advanced. Highlight any notable patterns or structures that contribute to its complexity.\"\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document outlines a multifaceted process involving data extraction, cleaning, transformation, and machine learning model training. Presented as a series of Python code snippets, it encompasses reading, cleaning, preprocessing text data, and applying Word2Vec for generating word embeddings, followed by the integration of a model from Hugging Face Hub. Below is a breakdown analysis and suggested improvements:\n",
      "\n",
      "### Content Analysis\n",
      "1. **Data Processing and Cleaning:**\n",
      "   - Reading CSV and JSON data.\n",
      "   - Handling missing values and duplicate data.\n",
      "   - Text normalization, including replacing specific characters, removing emojis, links, mentions, and non-alphanumeric symbols.\n",
      "\n",
      "2. **Feature Extraction:**\n",
      "   - Extracting hashtags, emojis, symbols, URLs, and mentions from the text data.\n",
      "\n",
      "3. **Text Preprocessing:**\n",
      "   - Standardizing certain Amharic letters.\n",
      "   - Removing unnecessary whitespace and punctuation.\n",
      "\n",
      "4. **Word2Vec Model Training:**\n",
      "   - Tokenizing text data.\n",
      "   - Training a Word2Vec model to generate word embeddings.\n",
      "\n",
      "5. **Integration with Hugging Face:**\n",
      "   - Downloading files from a Hugging Face repository.\n",
      "   - Loading a tokenizer and a Seq2Seq model for generating or modifying text data.\n",
      "\n",
      "### Structure Analysis\n",
      "- The document is structured in a linear, process-driven manner, starting from raw data handling to more complex operations like NLP model training and integration.\n",
      "- It lacks a clear separation between sections which could improve readability. For instance, categorizing blocks of code into sections like Data Cleaning, Feature Extraction, Model Training, etc., could help.\n",
      "- The process is logically ordered from data preprocessing to model application, which is a strength.\n",
      "\n",
      "### Potential Improvements\n",
      "1. **Modularization:**\n",
      "   - Breaking down the script into functions or modules based on their purposes (e.g., cleaning, preprocessing, model training) for better code organization and reusability.\n",
      "\n",
      "2. **Error Handling:**\n",
      "   - Adding error handling and logging for debugging and to handle any potential issues during file reading, data cleaning, or model training phases.\n",
      "\n",
      "3. **Comments and Documentation:**\n",
      "   - Adding more comments and documentation within the code to explain the purpose and functionality of each block or function for better understandability.\n",
      "\n",
      "4. **Optimization:**\n",
      "   - Evaluating the performance implications of the text manipulation and model training processes, potentially optimizing for speed and resource usage.\n",
      "\n",
      "5. **Validation and Testing:**\n",
      "   - Incorporating validation steps to ensure the accuracy and reliability of the data cleaning and preprocessing steps, as well as the effectiveness of the Word2Vec model.\n",
      "\n",
      "6. **Configurability:**\n",
      "   - Providing configurations (such as file paths, model parameters) in a separate config file or as command-line arguments to make the script more adaptable.\n",
      "\n",
      "In conclusion, while the document showcases a comprehensive approach towards text data handling and processing for NLP tasks, emphasizing modularization, documentation, and optimization could greatly enhance its structure, readability, and maintainability.\n",
      "The document outlines a series of data processing and modeling tasks using Python, involving several libraries such as pandas, gensim, and transformers, focused primarily on natural language processing (NLP) and machine learning tasks. It demonstrates a workflow divided into clear segments: data preparation, cleaning, modeling, and utilization of pre-trained models.\n",
      "\n",
      "**Content Overview:**\n",
      "\n",
      "1. **Data Preparation and Cleaning:**\n",
      "   - It begins with importing necessary libraries and setting up directory paths.\n",
      "   - A DataFrame is loaded from a CSV file containing text data. Basic operations include dropping missing values, replacing newline characters, extracting and removing hashtags, emojis, symbols, and links, and normalizing certain characters for consistency.\n",
      "   - There's a focus on Amharic language data, with specific character replacements to standardize various forms.\n",
      "   - Text data is cleaned to remove unnecessary symbols, URLs, mentions, and extra spaces, improving the quality of the dataset for analysis.\n",
      "\n",
      "2. **Word2Vec Model:**\n",
      "   - The text data is tokenized into words and passed into a Word2Vec model to learn word embeddings - vector representations of words.\n",
      "   - The model parameters include the size of the word vectors, the size of the window context, and the minimum count of word occurrences.\n",
      "   - It highlights querying the model for a specific word vector and finding similar words, demonstrating the model's capability to understand word semantics.\n",
      "\n",
      "3. **Utilization of Pre-trained Transformer Models:**\n",
      "   - Demonstrates downloading model files from Hugging Face using an API key.\n",
      "   - It focuses on using the `llama-2-amharic-3784m` model for Amharic text data, showcasing how to load a tokenizer and a model to be used for causal language modeling or sequence-to-sequence tasks.\n",
      "   - This segment indicates a higher-level NLP task potentially for language generation or translation.\n",
      "\n",
      "**Structure:**\n",
      "- The document is structured logically, starting from data preparation to advanced modeling. It uses a scripting approach, where each block of code is meant to be run sequentially.\n",
      "- There is a clear separation between different tasks (e.g., data preparation, Word2Vec modeling, transformer model utilization).\n",
      "\n",
      "**Potential Improvements:**\n",
      "- **Code Comments and Documentation:** Adding comments and explanations to the code would greatly improve readability and understandability, especially for readers unfamiliar with some of the libraries or tasks.\n",
      "- **Error Handling and Validation:** The script lacks error handling and validation checks (e.g., verifying successful file reads or model training outcomes).\n",
      "- **Modularization:** Separating the workflow into functions or modules for each significant step (data cleaning, modeling, etc.) could improve reusability and maintainability.\n",
      "- **Evaluation and Visualization:** Including model evaluation metrics or visualization of results (e.g., word embedding spaces) could provide insights into the effectiveness of the models and preprocessing steps.\n",
      "- **Experimental Details:** Additional context on the dataset, objectives, and expected outcomes could contextualize the code snippets and highlight their importance or applications.\n",
      "\n",
      "This analysis provides a multifaceted view of the document, recognizing its focused application in NLP while suggesting ways to enhance accessibility, robustness, and utility.\n",
      "('pipeline = pipeline (\"Text-Generation\", model=model, device=-1, '\n",
      " 'tokenizer=tokenizer, max_length=1000 )\\n'\n",
      " 'from langchain.text_splitter import CharacterTextSplitter\\n'\n",
      " 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n'\n",
      " 'from langchain_openai import OpenAIEmbeddings\\n'\n",
      " 'from langchain_community.vectorstores import Chroma\\n'\n",
      " 'from PyPDF2 import PdfReader\\n'\n",
      " 'from langchain_community.embeddings import HuggingFaceEmbeddings\\n'\n",
      " 'from langchain_community.embeddings.sentence_transformer import (\\n'\n",
      " '  SentenceTransformerEmbeddings,\\n'\n",
      " ')\\n'\n",
      " ' \\n'\n",
      " 'persist_directory = \"db\"\\n'\n",
      " ' \\n'\n",
      " 'class MySpecialFunctions:\\n'\n",
      " '  def __init__(self):\\n'\n",
      " '  pass\\n'\n",
      " '  def get_file_text(self, files):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for file in files:\\n'\n",
      " '  try:\\n'\n",
      " \"  with open(file, 'r', encoding='utf-8') as f:\\n\"\n",
      " '  content = f.read()\\n'\n",
      " '  text += content\\n'\n",
      " '  except Exception as e:\\n'\n",
      " '  print(f\"Error reading file {file}: {e}\")\\n'\n",
      " '  return text\\n'\n",
      " '  def get_pdf_text(self, pdf):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for doc in pdf:\\n'\n",
      " '  reader = PdfReader(doc)\\n'\n",
      " '  for page in reader.pages:\\n'\n",
      " '  text += page.extract_text()\\n'\n",
      " '  return text\\n'\n",
      " '  def get_text_chunks(self, text):\\n'\n",
      " '  text_siplitter = RecursiveCharacterTextSplitter(\\n'\n",
      " '  chunk_size = 1000,   chunk_overlap = 200,\\n'\n",
      " \"  separators=['\\\\n', '\\\\n\\\\n'],\\n\"\n",
      " '  length_function = len)\\n'\n",
      " '  chunk = text_siplitter.split_text(text)\\n'\n",
      " '  return chunk\\n'\n",
      " '  def get_vectorstore(self, chunks):\\n'\n",
      " '  hf_embedding = HuggingFaceEmbeddings()\\n'\n",
      " '  embedding_function = '\n",
      " 'SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n'\n",
      " '  vector_db = Chroma.from_documents(\\n'\n",
      " '  documents = chunks,\\n'\n",
      " '  embedding = hf_embedding,\\n'\n",
      " '  )\\n'\n",
      " '  return vector_db\\n'\n",
      " 'import streamlit as st \\n'\n",
      " 'from MyFunctions import MySpecialFunctions\\n'\n",
      " 'from dotenv import load_dotenv\\n'\n",
      " ' \\n'\n",
      " 'special_functions_instance = MySpecialFunctions()\\n'\n",
      " 'def get_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_file_text(external_data )\\n'\n",
      " 'def get_pdf_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_pdf_text(external_data )\\n'\n",
      " 'def get_chunks(text):\\n'\n",
      " '  return special_functions_instance.get_text_chunks(text)\\n'\n",
      " 'def get_vectorstore(text_chunks):\\n'\n",
      " '  return special_functions_instance.get_vectorstore(text_chunks)  \\n'\n",
      " 'def main():\\n'\n",
      " '  load_dotenv()\\n'\n",
      " '  st.set_page_config(page_title=\"Generation of Telegram Ads in Amharic\", '\n",
      " 'page_icon= \":smile\")\\n'\n",
      " '  with st.sidebar:\\n'\n",
      " '  pass\\n'\n",
      " '  st.markdown(\"   external_data = st.file_uploader(\" Upload the generative '\n",
      " 'text from fune-tuning\", accept_multiple_files= True)\\n'\n",
      " '  if st.button(\"Retrieval\"):\\n'\n",
      " '  with st.spinner(\"Processing\"):\\n'\n",
      " '  text = get_pdf_text(external_data )\\n'\n",
      " '  text_chunks = get_chunks(text)\\n'\n",
      " '  st.write(text_chunks)\\n'\n",
      " '  vectorstore_db = get_vectorstore(text_chunks)\\n'\n",
      " ' \\n'\n",
      " 'if __name__ == \"__main__\":\\n'\n",
      " '  main()\\n'\n",
      " 'import sentencepiece as spm\\n'\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m \"\n",
      " \"--vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " \"sp.load('m.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " \"print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))\\n\"\n",
      " 'print(sp.decode_ids([460, 133, 774, 1276]))\\n'\n",
      " ' \\n'\n",
      " 'print(sp.get_piece_size())\\n'\n",
      " ' \\n'\n",
      " 'print(sp.id_to_piece(460))\\n'\n",
      " \"print(sp.piece_to_id('▁በአዲስ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\\n\"\n",
      " ' \\n'\n",
      " 'for id in range(3):\\n'\n",
      " '  print(sp.id_to_piece(id), sp.is_control(id))\\n'\n",
      " ' \\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " ' \\n'\n",
      " \"serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " 'sp.load_from_serialized_proto(serialized_model_proto)\\n'\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user \"\n",
      " \"--user_defined_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_user = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_user.load('m_user.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\\n\"\n",
      " \"print(sp_user.piece_to_id('<sep>'))  \\n\"\n",
      " \"print(sp_user.piece_to_id('<cls>'))  \\n\"\n",
      " \"print('3=', sp_user.decode_ids([3]))  \\n\"\n",
      " \"print('4=', sp_user.decode_ids([4]))  \\n\"\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl \"\n",
      " \"--control_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_ctrl = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_ctrl.load('m_ctrl.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\")\n"
     ]
    }
   ],
   "source": [
    "query = 'Perform a comprehensive analysis of the document and provide insights on its content, structure, and potential improvements'\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided Python script spans several tasks, primarily focused on text preprocessing, data cleaning, and utilising machine learning for text vectorization. Let's break it down focusing on the Python and SQL keywords and syntax used, particularly classes, functions, and other related syntax:\n",
      "\n",
      "1. **Import Statements**:\n",
      "   - The script imports various modules and functions at different points, indicating dependencies on libraries like pandas, gensim, transformers, and a custom module (utils.preprocessing).\n",
      "\n",
      "2. **Variables and Path Settings**:\n",
      "   - Variables like `parsed_dir`, `cleaned_dir`, and `file_name` are set, which are used for specifying file paths.\n",
      "\n",
      "3. **Classes and Methods**:\n",
      "   - A class `MyPreprocessing` from the custom module `utils.preprocessing` is instantiated multiple times in different parts of the script. This class is presumably equipped with methods for text preprocessing.\n",
      "   - Various methods from pandas (`read_csv`, `dropna`, `replace`, `apply`, `str`, and `to_csv`) and from pandas series (`head`, `shape`, `tail`) are used for handling DataFrame operations.\n",
      "   - The script creates a Word2Vec model using gensim's `Word2Vec` class, indicating an instantiation and use of machine learning models for embedding.\n",
      "   - Methods from the `transformers` library are used to download, tokenize, and process text using a pre-trained model. This includes using `AutoTokenizer` and `AutoModelForSeq2SeqLM` classes.\n",
      "\n",
      "4. **Loops and Conditionals**:\n",
      "   - A complex loop structure is employed for replacing characters in the DataFrame. This loop iterates over a list of lists to perform string replacement operations.\n",
      "   - No explicit if-else conditionals are present, but lambda functions and method chaining apply conditional logic in data processing lines.\n",
      "\n",
      "5. **Data Cleaning Operations**:\n",
      "   - Extensive data cleaning steps are carried out including dropping missing values, replacing newline characters, extracting hashtags, emojis, mentions, and URLs using custom methods, and performing string replacement to clean up the text.\n",
      "\n",
      "6. **File I/O**:\n",
      "   - The script involves reading from CSV and JSON files and writing to CSV and TXT files, indicating file input/output operations for data preprocessing and saving the cleaned data.\n",
      "\n",
      "7. **Machine Learning Operations**:\n",
      "   - Training a Word2Vec model using the gensim library and embedding text data.\n",
      "   - Downloading a model from Hugging Face, and using tokenizer and model from the `transformers` library for sequence to sequence learning modeling indicates advanced machine learning and natural language processing operations.\n",
      "\n",
      "8. **No SQL Keywords**:\n",
      "   - There are no explicit SQL keywords or database manipulation operations in the script. All data manipulation appears to be done in-memory using pandas DataFrames and not persisted in an SQL database.\n",
      "\n",
      "9. **Function Definitions**:\n",
      "   - The script does not contain explicit Python function definitions (`def`) within the provided code, relying instead on library functions and methods for data manipulation and analysis.\n",
      "\n",
      "This summary focuses on the analysis of classes, functions, and key programming constructs as used within the script, with a notable absence of direct SQL operations.\n",
      "The provided Python code contains a sequence of operations mainly focused on preprocessing, analyzing, and exporting text data, with a particular emphasis on working with Ethiopian Amharic data. It also includes a segment on utilizing pre-trained models from Hugging Face. Here’s a breakdown based on the requested aspects:\n",
      "\n",
      "### Python Keywords and Constructs:\n",
      "- **Classes and Objects:** The code uses the `MyPreprocessing` class from an external utility module. Instances of this class are created and used for various text preprocessing tasks.\n",
      "- **Functions and Method Calls:** Multiple functions/methods are used, including those from pandas (e.g., `read_csv`, `dropna`, `replace`, `to_csv`) for data manipulation, those from the `MyPreprocessing` class for specific text processing tasks (e.g., `extract_hashtags`, `remove_emojis`), and Gensim's Word2Vec model functions for training word embeddings.\n",
      "- **Loops:** There's a loop to replace specific letters in the Amharic text to standardize various forms of writing. Loops are also used for downloading files from Hugging Face.\n",
      "- **Conditionals:** Specific conditionals aren't explicitly detailed in the provided snippets, but conditional logic might be present within the method calls (especially within the `MyPreprocessing` class methods).\n",
      "- **Imports and Modules:** The script makes extensive use of imports for functionality, including standard Python modules (`sys`, `os`), third-party libraries (`pandas`, `gensim`, `transformers`, `huggingface_hub`), and custom modules (`utils.preprocessing`).\n",
      "\n",
      "### SQL or Database Interactions:\n",
      "- **Direct SQL or Database Keywords:** There are no direct SQL queries or database connection keywords within the provided code. The data manipulation and storage are handled through file operations with CSV files and pandas DataFrames.\n",
      "\n",
      "### Other Syntax Points:\n",
      "- Regex is used for text manipulation and cleaning (e.g., replacing newline characters, removing URLs, mentions, emojis, and symbols from the text).\n",
      "- The code also includes loading models and tokenizers from Hugging Face and using them, which demonstrates interaction with machine learning models and APIs.\n",
      "- File I/O operations are evident in reading JSON and CSV files, writing cleaned text to CSV, and saving word embeddings models.\n",
      "- The use of environment variables to securely access the Hugging Face API.\n",
      "\n",
      "### Summary:\n",
      "The script emphasizes text preprocessing, handling Amharic text data, and leveraging natural language processing (NLP) models. It illustrates the use of Python for data manipulation, cleaning, and applying machine learning models. However, there are no SQL/database interaction elements in the provided segments.\n",
      "('pipeline = pipeline (\"Text-Generation\", model=model, device=-1, '\n",
      " 'tokenizer=tokenizer, max_length=1000 )\\n'\n",
      " 'from langchain.text_splitter import CharacterTextSplitter\\n'\n",
      " 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n'\n",
      " 'from langchain_openai import OpenAIEmbeddings\\n'\n",
      " 'from langchain_community.vectorstores import Chroma\\n'\n",
      " 'from PyPDF2 import PdfReader\\n'\n",
      " 'from langchain_community.embeddings import HuggingFaceEmbeddings\\n'\n",
      " 'from langchain_community.embeddings.sentence_transformer import (\\n'\n",
      " '  SentenceTransformerEmbeddings,\\n'\n",
      " ')\\n'\n",
      " ' \\n'\n",
      " 'persist_directory = \"db\"\\n'\n",
      " ' \\n'\n",
      " 'class MySpecialFunctions:\\n'\n",
      " '  def __init__(self):\\n'\n",
      " '  pass\\n'\n",
      " '  def get_file_text(self, files):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for file in files:\\n'\n",
      " '  try:\\n'\n",
      " \"  with open(file, 'r', encoding='utf-8') as f:\\n\"\n",
      " '  content = f.read()\\n'\n",
      " '  text += content\\n'\n",
      " '  except Exception as e:\\n'\n",
      " '  print(f\"Error reading file {file}: {e}\")\\n'\n",
      " '  return text\\n'\n",
      " '  def get_pdf_text(self, pdf):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for doc in pdf:\\n'\n",
      " '  reader = PdfReader(doc)\\n'\n",
      " '  for page in reader.pages:\\n'\n",
      " '  text += page.extract_text()\\n'\n",
      " '  return text\\n'\n",
      " '  def get_text_chunks(self, text):\\n'\n",
      " '  text_siplitter = RecursiveCharacterTextSplitter(\\n'\n",
      " '  chunk_size = 1000,   chunk_overlap = 200,\\n'\n",
      " \"  separators=['\\\\n', '\\\\n\\\\n'],\\n\"\n",
      " '  length_function = len)\\n'\n",
      " '  chunk = text_siplitter.split_text(text)\\n'\n",
      " '  return chunk\\n'\n",
      " '  def get_vectorstore(self, chunks):\\n'\n",
      " '  hf_embedding = HuggingFaceEmbeddings()\\n'\n",
      " '  embedding_function = '\n",
      " 'SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n'\n",
      " '  vector_db = Chroma.from_documents(\\n'\n",
      " '  documents = chunks,\\n'\n",
      " '  embedding = hf_embedding,\\n'\n",
      " '  )\\n'\n",
      " '  return vector_db\\n'\n",
      " 'import streamlit as st \\n'\n",
      " 'from MyFunctions import MySpecialFunctions\\n'\n",
      " 'from dotenv import load_dotenv\\n'\n",
      " ' \\n'\n",
      " 'special_functions_instance = MySpecialFunctions()\\n'\n",
      " 'def get_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_file_text(external_data )\\n'\n",
      " 'def get_pdf_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_pdf_text(external_data )\\n'\n",
      " 'def get_chunks(text):\\n'\n",
      " '  return special_functions_instance.get_text_chunks(text)\\n'\n",
      " 'def get_vectorstore(text_chunks):\\n'\n",
      " '  return special_functions_instance.get_vectorstore(text_chunks)  \\n'\n",
      " 'def main():\\n'\n",
      " '  load_dotenv()\\n'\n",
      " '  st.set_page_config(page_title=\"Generation of Telegram Ads in Amharic\", '\n",
      " 'page_icon= \":smile\")\\n'\n",
      " '  with st.sidebar:\\n'\n",
      " '  pass\\n'\n",
      " '  st.markdown(\"   external_data = st.file_uploader(\" Upload the generative '\n",
      " 'text from fune-tuning\", accept_multiple_files= True)\\n'\n",
      " '  if st.button(\"Retrieval\"):\\n'\n",
      " '  with st.spinner(\"Processing\"):\\n'\n",
      " '  text = get_pdf_text(external_data )\\n'\n",
      " '  text_chunks = get_chunks(text)\\n'\n",
      " '  st.write(text_chunks)\\n'\n",
      " '  vectorstore_db = get_vectorstore(text_chunks)\\n'\n",
      " ' \\n'\n",
      " 'if __name__ == \"__main__\":\\n'\n",
      " '  main()\\n'\n",
      " 'import sentencepiece as spm\\n'\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m \"\n",
      " \"--vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " \"sp.load('m.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " \"print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))\\n\"\n",
      " 'print(sp.decode_ids([460, 133, 774, 1276]))\\n'\n",
      " ' \\n'\n",
      " 'print(sp.get_piece_size())\\n'\n",
      " ' \\n'\n",
      " 'print(sp.id_to_piece(460))\\n'\n",
      " \"print(sp.piece_to_id('▁በአዲስ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\\n\"\n",
      " ' \\n'\n",
      " 'for id in range(3):\\n'\n",
      " '  print(sp.id_to_piece(id), sp.is_control(id))\\n'\n",
      " ' \\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " ' \\n'\n",
      " \"serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " 'sp.load_from_serialized_proto(serialized_model_proto)\\n'\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user \"\n",
      " \"--user_defined_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_user = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_user.load('m_user.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\\n\"\n",
      " \"print(sp_user.piece_to_id('<sep>'))  \\n\"\n",
      " \"print(sp_user.piece_to_id('<cls>'))  \\n\"\n",
      " \"print('3=', sp_user.decode_ids([3]))  \\n\"\n",
      " \"print('4=', sp_user.decode_ids([4]))  \\n\"\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl \"\n",
      " \"--control_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_ctrl = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_ctrl.load('m_ctrl.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\")\n"
     ]
    }
   ],
   "source": [
    "query = \"Please evaluate the supplied Python code, concentrating on the  python and sql code key words like how class are used, how many function and others related syntax.\"\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
