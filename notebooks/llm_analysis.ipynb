{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "import pickle\n",
    "env_vars = dotenv_values('.env')\n",
    "openai.api_key = env_vars.get('OPENAI_API_KEY')\n",
    "openai_client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "with open(\"../txt/pickle/cluster_lists.pkl\", 'rb') as file:\n",
    "    loaded_clusters = pickle.load(file)\n",
    "\n",
    "retrieved_documents = loaded_clusters[0]\n",
    "print(len(retrieved_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_model_answer(query, retrieved_documents):\n",
    "    # Make a request to the OpenAI API for analysis for each cluster\n",
    "    cluster_analysis = []\n",
    "\n",
    "    for cluster in retrieved_documents:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"As an assistant, your role is to be helpful and provide answers in a concise and summary-style manner. Your objective is to draw from the available document and your knowledge to deliver accurate and concise information. If you encounter a question for which you lack the answer or cannot find the relevant information, it's best to respond with a brief summary or let the user know that the answer is not available. Avoid including specific code examples and instead focus on offering assistance whenever possible. Your primary aim is to deliver clear and precise responses, ensuring that the information you provide is helpful to the user.\"},\n",
    "            {\"role\": \"user\", \"content\": '\\n'.join(cluster) + '\\n' + query}\n",
    "        ]\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            messages=messages,\n",
    "        )\n",
    "\n",
    "        analysis = response.choices[0].message.content.strip()\n",
    "        cluster_analysis.append(analysis)\n",
    "    \n",
    "    return cluster_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The provided script appears to be a complex Python application designed for '\n",
      " 'language processing and retrieval tasks, integrating various components and '\n",
      " \"libraries to accomplish its objectives. Here's a general overview of its \"\n",
      " 'components and functionalities:\\n'\n",
      " '\\n'\n",
      " '1. **Environment and Dependency Management**: The application utilizes '\n",
      " '`python-dotenv` for managing environment variables, which is a common '\n",
      " 'practice for configuring different aspects of an application outside the '\n",
      " 'code base.\\n'\n",
      " '\\n'\n",
      " \"2. **Language Model Integration**: It integrates OpenAI's language models \"\n",
      " 'through `ChatOpenAI` for generating text-based responses. This suggests the '\n",
      " 'application might be used for chatbot or conversational AI purposes.\\n'\n",
      " '\\n'\n",
      " '3. **Text Processing and Management**: It involves text processing '\n",
      " 'utilities, including a text loader and splitter, indicating the application '\n",
      " 'processes and analyzes large texts or documents, possibly to extract '\n",
      " 'relevant information or insights.\\n'\n",
      " '\\n'\n",
      " '4. **Embeddings and Vector Storage**: The application uses embeddings '\n",
      " '(specifically, OpenAI Embeddings) to convert text into numerical '\n",
      " 'representations, enabling similarity searches and other vector-based '\n",
      " 'operations. Pinecone is used as a vector database for storing and retrieving '\n",
      " 'these embeddings, which supports efficient similarity search among vectors.\\n'\n",
      " '\\n'\n",
      " '5. **Retrieval-Based Answer Generation (RAG)**: The script mentions a RAG '\n",
      " 'chain, suggesting it employs a retrieval-augmented generation approach. This '\n",
      " 'involves retrieving relevant documents or text snippets based on a query and '\n",
      " 'using them as context for generating a response, blending retrieval and '\n",
      " 'generative capabilities for answering queries more effectively.\\n'\n",
      " '\\n'\n",
      " '6. **Document and Chunk Handling**: It processes documents, breaking them '\n",
      " 'into smaller chunks for more manageable processing and embedding. This '\n",
      " 'indicates handling potentially large documents that require segmenting for '\n",
      " 'efficient processing.\\n'\n",
      " '\\n'\n",
      " '7. **Interactions with External APIs**: The script shows interactions with '\n",
      " 'external APIs like Pinecone and presumably OpenAI, showcasing its reliance '\n",
      " 'on external services for certain functionalities.\\n'\n",
      " '\\n'\n",
      " '8. **Asynchronous and Batch Processing**: Parts of the script hint at batch '\n",
      " 'processing and possibly asynchronous operations (e.g., waiting for an index '\n",
      " 'to be ready before proceeding), aiming to handle operations efficiently and '\n",
      " 'at scale.\\n'\n",
      " '\\n'\n",
      " 'In summary, this Python application is designed for processing and analyzing '\n",
      " 'text, incorporating advanced AI and NLP (Natural Language Processing) '\n",
      " 'capabilities, specifically using language models for generating responses '\n",
      " 'and vector databases for managing embeddings. It represents a sophisticated '\n",
      " 'integration of different technologies for retrieval-augmented conversational '\n",
      " 'AI or similar applications.',\n",
      " 'Based on the various snippets provided in the text, it appears that the data '\n",
      " 'encompasses a wide array of topics, primarily involving programming, data '\n",
      " 'processing, and API usage scenarios. Key highlights include:\\n'\n",
      " '\\n'\n",
      " '1. **Installation and Configuration**: Code snippets suggest setting up a '\n",
      " 'Python environment with dependencies related to AI modeling, data handling '\n",
      " '(datasets), client-server communication (Pinecone, FastAPI), and utility '\n",
      " \"libraries (dotenv for environment variables). There's an emphasis on using \"\n",
      " \"OpenAI's GPT models, suggesting a focus on natural language processing or \"\n",
      " 'generation tasks.\\n'\n",
      " '\\n'\n",
      " '2. **API Interaction**: There’s considerable focus on crafting requests to '\n",
      " 'APIs, particularly for generating text completions or evaluations using '\n",
      " \"OpenAI's GPT models. This involves constructing complex requests with \"\n",
      " 'specific parameters (e.g., temperature, max tokens) and processing the '\n",
      " 'responses.\\n'\n",
      " '\\n'\n",
      " '3. **Data Processing**: The data includes examples of complex data '\n",
      " 'manipulation tasks, including text splitting, embedding vector generation, '\n",
      " 'and similarity searches within vector databases. There’s usage of Pandas for '\n",
      " 'data manipulation and Seaborn/Matplotlib for potential data visualization.\\n'\n",
      " '\\n'\n",
      " '4. **Client-Server Applications**: Examples suggest the creation of web '\n",
      " 'applications using FastAPI, which interact with AI models to process user '\n",
      " 'inputs and generate responses. This includes handling CORS, defining routes, '\n",
      " 'and structuring request/response data models.\\n'\n",
      " '\\n'\n",
      " '5. **Evaluation and Validation**: There are instances of evaluating or '\n",
      " 'validating generated text, indicating an application context where '\n",
      " 'AI-generated output (like chatbot responses or AI-generated prompts) is '\n",
      " 'critically assessed for accuracy, relevance, or sufficiency.\\n'\n",
      " '\\n'\n",
      " '6. **React Components**: Some snippets outline the use of React for building '\n",
      " 'user interfaces, indicating a frontend application that interacts with the '\n",
      " 'server-side logic for tasks like prompt generation or AI model interaction.\\n'\n",
      " '\\n'\n",
      " '7. **Assisting Tools and Frameworks**: Usage of tools like LangChain, a '\n",
      " 'framework for connecting language models with data sources or APIs, '\n",
      " 'indicates a sophisticated approach to leveraging AI for specific tasks, '\n",
      " 'potentially aiming at enhancing productivity or enabling new capabilities in '\n",
      " 'applications.\\n'\n",
      " '\\n'\n",
      " '8. **User Interface and Interaction**: The presence of UI element '\n",
      " 'definitions and interactions in a web context suggests the final output '\n",
      " 'could be a web application enabling users to interact with AI models, '\n",
      " 'receive processed data, or visualize insights.\\n'\n",
      " '\\n'\n",
      " 'In summary, the data presents an advanced programming context where AI, '\n",
      " 'particularly natural language generation and processing, plays a central '\n",
      " 'role in developing applications or tools that involve API interactions, data '\n",
      " 'processing, and providing user interfaces for interaction and output '\n",
      " 'visualization.',\n",
      " 'Based on the information provided, you loaded a dataset named '\n",
      " '\"llama-2-arxiv-papers-chunked\" using a library for handling datasets, '\n",
      " 'potentially for machine learning or data analysis purposes. This dataset, '\n",
      " 'derived from \"jamescalam\" (likely a user or organization on a platform where '\n",
      " 'the dataset is hosted), seems to involve chunks of text related to arXiv '\n",
      " 'papers, which suggests it might contain scientific publications or extracts '\n",
      " 'from them.\\n'\n",
      " '\\n'\n",
      " 'The choice of dataset and the mention of the \"train\" split indicate you '\n",
      " 'might be working on a task involving machine learning, where the \"train\" '\n",
      " \"split is primarily used to train models. The dataset's naming convention \"\n",
      " '(\"llama-2\") implies it could be related to or intended for use with models '\n",
      " 'akin to LLaMA (Language Model at Meta AI), which are designed for '\n",
      " 'understanding or generating human-like text. \\n'\n",
      " '\\n'\n",
      " 'Given that the dataset specifically mentions being \"chunked,\" it suggests '\n",
      " 'that the original text of arXiv papers has been divided into smaller, '\n",
      " 'manageable pieces. This approach is typically used to handle large documents '\n",
      " 'by breaking them down into smaller segments to make them more digestible for '\n",
      " 'machine learning models, especially for tasks like text summarization, '\n",
      " 'question answering, or context retrieval.\\n'\n",
      " '\\n'\n",
      " \"However, without directly accessing the dataset's content, specific details \"\n",
      " 'about the nature of the text, the number of documents, the average length of '\n",
      " 'each chunk, or the topics covered in these arXiv papers remain unknown. For '\n",
      " 'in-depth analysis or to provide a more comprehensive overview, inspecting '\n",
      " 'some samples from the dataset or its metadata would be essential.',\n",
      " 'The provided data spans across multiple domains including Python scripting, '\n",
      " 'FastAPI backend development, document processing, AI model integration, '\n",
      " 'React frontend development, and handling web requests. Here’s a breakdown:\\n'\n",
      " '\\n'\n",
      " '1. **Python Scripting and Environment Setup:** It starts with leveraging the '\n",
      " '`dotenv` library to load environmental variables, indicating a setup where '\n",
      " 'configuration might be stored outside the script for security or '\n",
      " 'convenience.\\n'\n",
      " '\\n'\n",
      " \"2. **Document Processing using LangChain:** There's significant use of a \"\n",
      " 'library or module named `langchain`, dealing with document loading, '\n",
      " 'splitting, embedding, and querying using AI models like `OpenAI`. It '\n",
      " 'suggests a workflow where documents are being processed, split into '\n",
      " 'manageable parts, embedded into vector space for similarity search or '\n",
      " 'retrieval tasks, and then integrated with language models for generating or '\n",
      " 'querying content.\\n'\n",
      " '\\n'\n",
      " '3. **Integration with OpenAI Models:** The script integrates OpenAI models '\n",
      " 'for various tasks, likely involving natural language understanding or '\n",
      " 'generation. This integration is evident in multiple parts of the script, '\n",
      " 'indicating a focus on leveraging AI for content generation, question '\n",
      " 'answering, or similar tasks.\\n'\n",
      " '\\n'\n",
      " '4. **FastAPI Backend Development:** Portions of the codebase are dedicated '\n",
      " 'to setting up a FastAPI application, handling CORS (Cross-Origin Resource '\n",
      " 'Sharing) settings, and defining route handlers. This is indicative of a '\n",
      " 'backend service designed to expose APIs, possibly for document processing, '\n",
      " 'AI model interactions, or serving data to a frontend application.\\n'\n",
      " '\\n'\n",
      " \"5. **React Frontend Development:** There's a significant chunk of code \"\n",
      " 'related to React development, suggesting a user interface that interacts '\n",
      " 'with the backend services. It includes form handling, triggering API calls '\n",
      " 'upon user actions, and rendering tables dynamically based on fetched data.\\n'\n",
      " '\\n'\n",
      " '6. **AI-Powered Analysis and Interaction:** The script outlines a workflow '\n",
      " 'where user inputs are evaluated by AI models to generate responses or '\n",
      " 'analyses. This includes setting up a chain of operations where documents are '\n",
      " 'retrieved, processed by AI models, and the resulting data is parsed for '\n",
      " 'end-user presentation or further analysis.\\n'\n",
      " '\\n'\n",
      " '7. **Data Representation and Management:** Usage of models and schemas for '\n",
      " 'structuring data responses, alongside handling of tabular data within a '\n",
      " 'React application, points towards efforts to manage and represent data '\n",
      " 'effectively both on the backend and frontend.\\n'\n",
      " '\\n'\n",
      " 'In summary, the provided data spans the development of a complex application '\n",
      " 'involving document processing, AI model integration for natural language '\n",
      " 'tasks, a backend service setup using FastAPI for API management, and a '\n",
      " 'frontend application built with React for user interaction. This setup '\n",
      " 'indicates a solution aimed at leveraging AI for analyzing and processing '\n",
      " 'documents, with a user-friendly interface for interaction and possibly for '\n",
      " 'educational or business analysis purposes.',\n",
      " 'The data provided in the script snippets spans several operations and '\n",
      " 'functionalities related to handling datasets, text processing, embeddings, '\n",
      " 'and database interactions, as well as interactions with the OpenAI API. Here '\n",
      " 'is a summary and categorization of the various operations and concepts '\n",
      " 'covered:\\n'\n",
      " '\\n'\n",
      " '### Data Loading and Processing\\n'\n",
      " '- **Datasets**: Loading datasets for further processing, notably involving '\n",
      " 'operations like splitting, chunking, and mapping. The datasets seem to be '\n",
      " 'specifically geared towards some form of text analysis or NLP tasks.\\n'\n",
      " '- **Chunking Text**: Text is split into smaller chunks based on specified '\n",
      " 'criteria such as chunk size and overlap, making it easier to handle or '\n",
      " 'process large texts incrementally.\\n'\n",
      " '- **Data Transformation**: Using operations like `map` and `filter` to '\n",
      " 'transform datasets and extract necessary information for further processing '\n",
      " 'or analysis.\\n'\n",
      " '\\n'\n",
      " '### Embeddings\\n'\n",
      " '- **Embedding Generation**: Leveraging both OpenAI and Hugging Face '\n",
      " 'embeddings to convert text into numerical vectors that represent the '\n",
      " 'semantic meaning of the text.\\n'\n",
      " '- **Semantic Search**: A demonstration of a semantic search functionality '\n",
      " 'which seems to use embeddings to find and rank documents based on their '\n",
      " 'relevance to a given query.\\n'\n",
      " '\\n'\n",
      " '### Database Interactions\\n'\n",
      " '- **Storing Data**: Snippets showing how to connect to a PostgreSQL database '\n",
      " 'using `psycopg2`, insert data, and perform custom queries. This includes '\n",
      " 'registering vector types for advanced search functionalities.\\n'\n",
      " '- **Semantic Search with Database**: Another showcase of semantic search, '\n",
      " 'this time likely utilizing stored embeddings in a database to fetch '\n",
      " 'semantically relevant documents based on a query.\\n'\n",
      " '\\n'\n",
      " '### Environment and API Interaction\\n'\n",
      " '- **Environment Configuration**: Using `dotenv` to load environment '\n",
      " 'variables, which likely include API keys and database connection strings for '\n",
      " 'secure and configurable use.\\n'\n",
      " '- **OpenAI API Usage**: Demonstrates generating responses to prompts using '\n",
      " 'the OpenAI API, which involves making API calls with specific parameters '\n",
      " 'like models, tokens, and completion settings.\\n'\n",
      " '\\n'\n",
      " '### Evaluation and Analysis\\n'\n",
      " '- **Text Evaluation**: Scripts intended to evaluate or classify text based '\n",
      " 'on certain criteria, possibly to detect the relevancy or quality of '\n",
      " 'generated text or user inputs.\\n'\n",
      " '- **Utility and Evaluation Functions**: Custom evaluation logic is shown, '\n",
      " 'including interaction with user-provided text and the generation of scores '\n",
      " 'or classifications based on model outputs.\\n'\n",
      " '\\n'\n",
      " '### General Observations\\n'\n",
      " '- **Modular Code**: The code snippets showcase different aspects of working '\n",
      " 'with text data, including loading, processing, embedding, evaluating, and '\n",
      " 'storing the data, indicating a modular approach to handling NLP tasks.\\n'\n",
      " '- **Interactivity**: Parts of the script involve getting user input and '\n",
      " 'providing feedback, indicating an interactive component or application where '\n",
      " 'users can query or interact with the system in real time.\\n'\n",
      " '\\n'\n",
      " 'Overall, the data provided encapsulates a complex workflow for processing '\n",
      " 'and analyzing text data, leveraging modern NLP techniques and models, '\n",
      " 'interacting with APIs for data enrichment, and handling data persistence '\n",
      " 'through databases.',\n",
      " 'The provided data contains multiple segments showcasing different aspects of '\n",
      " 'programming, software implementation, and system interactions, particularly '\n",
      " 'focusing on web development, machine learning model interactions, and vector '\n",
      " \"embeddings within the context of a web application. Here's a breakdown of \"\n",
      " 'the main components:\\n'\n",
      " '\\n'\n",
      " '1. **Web Development with React:** It starts with a React functional '\n",
      " 'component named `App` that renders a simple \"Hello world!\" message styled '\n",
      " 'with Tailwind CSS. This demonstrates the use of React for building user '\n",
      " 'interfaces along with Tailwind CSS for styling. It also includes boilerplate '\n",
      " 'code for using React with a Browser Router for routing and Strict Mode for '\n",
      " 'highlighting potential problems in an application.\\n'\n",
      " '\\n'\n",
      " \"2. **Environment Setup for OpenAI's API:** The text includes code for \"\n",
      " \"setting up and using OpenAI's API, indicating the utilization of machine \"\n",
      " 'learning models (specifically GPT-3.5-turbo) to perform tasks or answer '\n",
      " 'queries. This segment shows the initialization of the API key from the '\n",
      " \"environment variables and making requests to OpenAI's service.\\n\"\n",
      " '\\n'\n",
      " '3. **Message Handling in a Chat Application:** There’s an example of '\n",
      " 'handling messages within a chat system, showcasing how system messages, '\n",
      " 'human messages, and AI-generated messages can be structured and processed in '\n",
      " 'a conversational interface.\\n'\n",
      " '\\n'\n",
      " '4. **LLMChain in LangChain:** Information is provided on the concept of a '\n",
      " 'LLMChain within the context of LangChain, a framework aimed at developing '\n",
      " 'applications powered by language models. The LLMChain is described as a '\n",
      " 'series of input variables, a model (LLM or Chat Model), and an optional '\n",
      " 'output parser, working together to produce a formatted prompt and process '\n",
      " 'outputs from a language model.\\n'\n",
      " '\\n'\n",
      " '5. **Vector Embeddings and Similarity Search with Pinecone:** There is a '\n",
      " 'detailed example of using Pinecone to create a vector index for storing and '\n",
      " 'retrieving vector embeddings. It involves initializing Pinecone, creating an '\n",
      " 'index, embedding textual documents using OpenAI’s embedding models, and '\n",
      " 'performing similarity search for queries. This demonstrates integrating '\n",
      " 'vector search into applications for tasks like semantic search or '\n",
      " 'recommendation systems.\\n'\n",
      " '\\n'\n",
      " '6. **Data Handling and Embedding with LangChain and Datasets:** The text '\n",
      " 'touches on loading and processing dataset chunks, presumably for embedding '\n",
      " 'and indexing purposes. It uses tools like `datasets` to load data and '\n",
      " '`langchain` for working with language models and vector stores.\\n'\n",
      " '\\n'\n",
      " 'In summary, the provided text spans from front-end web development practices '\n",
      " 'using React and Tailwind CSS, through utilizing AI for chatbot-like '\n",
      " 'interactions and queries, to advanced data processing and handling with '\n",
      " 'embeddings for similarity search within applications. It reflects a complex '\n",
      " 'and comprehensive example of integrating modern web development techniques '\n",
      " 'with cutting-edge AI and machine learning capabilities to build intelligent '\n",
      " 'and interactive applications.',\n",
      " \"The data you've provided appears to be Python code snippets that serve \"\n",
      " 'multiple purposes, mainly revolving around working with text data, '\n",
      " 'interacting with API services, processing PDF documents, managing text '\n",
      " \"embeddings, and handling web requests. Here's a rundown of the distinct \"\n",
      " 'functionalities demonstrated in your data:\\n'\n",
      " '\\n'\n",
      " '1. **Using Extensions for Development Efficiency**: It begins with '\n",
      " 'activating the `autoreload` extension in an interactive Python environment '\n",
      " '(likely Jupyter notebooks), which allows for automatic reloading of modules '\n",
      " 'before executing user code.\\n'\n",
      " '\\n'\n",
      " \"2. **Interacting with OpenAI's API**: The code imports necessary libraries \"\n",
      " \"for connecting to OpenAI's API, demonstrating how to set up and use the API \"\n",
      " 'key from an environment variable. An instance of a `ChatOpenAI` class is '\n",
      " 'created to interact with chat models, specifically `gpt-3.5-turbo`.\\n'\n",
      " '\\n'\n",
      " '3. **Handling Environment Variables**: Utilizes `dotenv` to load environment '\n",
      " 'variables, which is a common practice to manage secrets or API keys '\n",
      " 'securely.\\n'\n",
      " '\\n'\n",
      " '4. **Message Schema Definition and Interaction**: The implementation uses a '\n",
      " 'custom schema (`SystemMessage`, `HumanMessage`, `AIMessage`) to structure '\n",
      " 'messages involving AI interactions. This part of the code is used to '\n",
      " 'simulate a conversation flow with an AI, appending and processing messages.\\n'\n",
      " '\\n'\n",
      " '5. **Working with PDFs using PyPDF2**: Demonstrates how to read a PDF '\n",
      " 'document, extract text, and break the extracted text into chunks for further '\n",
      " 'processing. This could be useful in data extraction and preprocessing '\n",
      " 'tasks.\\n'\n",
      " '\\n'\n",
      " '6. **Vector Embeddings and Similarity**: The provided code also outlines a '\n",
      " 'workflow for generating embeddings from text chunks using a model from '\n",
      " 'OpenAI, presumably to convert text data into a vectorized form that can be '\n",
      " 'used for similarity comparisons or further machine learning tasks.\\n'\n",
      " '\\n'\n",
      " '7. **Using Pinecone for Vector Search**: Shows the setup for connecting to '\n",
      " 'Pinecone, a vector database, highlighting how to insert data and perform '\n",
      " 'vector similarity searches. This is commonly used in applications like '\n",
      " 'recommender systems or semantic search engines.\\n'\n",
      " '\\n'\n",
      " '8. **Utility Functions**: Additional utility functions are provided for '\n",
      " 'chunking text, generating unique IDs based on content, and calculating '\n",
      " 'cosine similarity between vectors, which are helpful in various data '\n",
      " 'processing and machine learning pipelines.\\n'\n",
      " '\\n'\n",
      " '9. **Flask/Django-like Web Request Handling**: Lastly, it models a web '\n",
      " 'request handling scenario, possibly within a Flask or Django application, '\n",
      " 'where a prompt is generated based on the most relevant chunk of text to a '\n",
      " \"user's query.\\n\"\n",
      " '\\n'\n",
      " 'Overall, the data encompasses a broad spectrum of tasks related to text '\n",
      " 'processing, API interaction, vector space modeling, and web application '\n",
      " 'development, showcasing an advanced example of integrating various '\n",
      " 'technologies and services for handling and analyzing text data '\n",
      " 'programmatically.',\n",
      " 'The provided data appears to be a mix of code snippets, error logs, and CSS '\n",
      " 'styles, along with some helper functions and pseudo code for various '\n",
      " 'operations such as evaluating prompts, comparing them, and generating test '\n",
      " 'cases. The data spans across multiple contexts, including web design (CSS), '\n",
      " 'environmental variables setup, API interaction, chat interface rendering, '\n",
      " 'and machine learning tasks like prompt evaluation and vector embeddings.\\n'\n",
      " '\\n'\n",
      " 'Key aspects and operations included are:\\n'\n",
      " '\\n'\n",
      " '1. **Error Handling**: Multiple logs indicating errors like incorrect API '\n",
      " 'keys, unexpected NoneType, invalid request errors, and string indices must '\n",
      " 'be integers. These suggest issues with API access and data format handling '\n",
      " 'in a software environment.\\n'\n",
      " '\\n'\n",
      " '2. **Environment Variable Management**: Codes for loading and retrieving '\n",
      " 'OpenAI API keys from environment variables, implying interaction with '\n",
      " \"OpenAI's services.\\n\"\n",
      " '\\n'\n",
      " '3. **Chat Interface Rendering**: HTML and JavaScript (React) snippets for '\n",
      " 'rendering a chat interface, message lists, input, and typing indicators, '\n",
      " 'suggesting development of a chat application.\\n'\n",
      " '\\n'\n",
      " '4. **Evaluation Functions**: Script outline for comparing and evaluating '\n",
      " 'prompts based on criteria like relevance and using machine learning '\n",
      " 'techniques. It mentions using sentence transformers for encoding and '\n",
      " 'calculating similarity scores between prompts.\\n'\n",
      " '\\n'\n",
      " '5. **Error Logging and Debugging**: Error capture and logging for '\n",
      " 'operations, suggesting the presence of a logging system for debugging '\n",
      " 'purposes.\\n'\n",
      " '\\n'\n",
      " '6. **Prompt Generation and Evaluation**: The pseudo code and functions hint '\n",
      " 'at generating prompts from a document, evaluating these prompts using a '\n",
      " 'custom pipeline, possibly in the context of NLP or chatbot functionalities. '\n",
      " 'It involves creating retrievers from document chunks and evaluating prompts '\n",
      " 'with various metrics.\\n'\n",
      " '\\n'\n",
      " '7. **Environment Configuration**: Snippets for configuring and accessing '\n",
      " 'environment variables for API keys, indicating interaction with external '\n",
      " 'APIs.\\n'\n",
      " '\\n'\n",
      " '8. **CSS Styling**: CSS code for basic web styling, including animations and '\n",
      " 'media queries for responsiveness, suggests the presence of a web interface.\\n'\n",
      " '\\n'\n",
      " '9. **Utility Scripts**: Mention of utility scripts and operations like data '\n",
      " 'loading, sentence embedding, prompt comparison, and ranking. It shows an '\n",
      " 'application involving text processing, likely in a machine learning or NLP '\n",
      " 'context.\\n'\n",
      " '\\n'\n",
      " '10. **Machine Learning Pipelines**: Descriptions for creating pipelines '\n",
      " 'using embeddings, retrievers, and evaluating generated prompts imply the use '\n",
      " 'of modern machine learning techniques to process text data.\\n'\n",
      " '\\n'\n",
      " 'Overall, the data represents a complex software project involving API '\n",
      " 'interaction, web development, environment configuration, and advanced '\n",
      " 'machine learning or NLP techniques for prompt processing and evaluation. The '\n",
      " 'errors logged and the code snippets suggest both frontend and backend '\n",
      " 'development processes with a focus on handling textual content, likely for '\n",
      " 'chat applications or NLP tasks.',\n",
      " 'The data provided spans several different contexts, including code snippets '\n",
      " 'for JavaScript (React), CSS, Python, and some environmental setup for API '\n",
      " 'keys. It seems to be a mixture of code examples, function definitions, and '\n",
      " 'program logic, likely from a software development or educational project. '\n",
      " 'Here’s a generalized overview of the different segments within the data:\\n'\n",
      " '\\n'\n",
      " '1. **React JavaScript**: Portions of the data include React component '\n",
      " 'definitions, styling with CSS-in-JS, and usage of React hooks (`useState` '\n",
      " 'and `useEffect`). Specifically, it outlines a simple React app structure '\n",
      " 'with components like `InputComponent` and `OutputComponent` and styling for '\n",
      " 'these components.\\n'\n",
      " '\\n'\n",
      " '2. **CSS**: There are CSS snippets for styling a web application. This '\n",
      " 'includes styles for the app body, headers, animations for logo spinning, and '\n",
      " 'media queries for responsive design.\\n'\n",
      " '\\n'\n",
      " '3. **Python - General Programming**: The data includes Python snippets for '\n",
      " 'various purposes, such as importing modules (`os`, `sys`, `json`), API '\n",
      " 'interaction, processing environmental variables, and asynchronously fetching '\n",
      " \"data. There's a focus on interacting with the OpenAI API, specifically \"\n",
      " 'around chat completion functionalities. \\n'\n",
      " '\\n'\n",
      " '4. **Python - OpenAI GPT-3.5-turbo Integration**: It shows the usage of '\n",
      " 'GPT-3.5-turbo model from OpenAI for generating text completions based on '\n",
      " 'prompts and other provided information. There are also examples of loading '\n",
      " 'environmental variables, setting up a client for OpenAI’s API, and making '\n",
      " 'requests to get completions.\\n'\n",
      " '\\n'\n",
      " '5. **Python - FastAPI Implementation**: Parts of the data include examples '\n",
      " 'of setting up and using FastAPI to create a web service. This includes '\n",
      " 'endpoint definitions, request handling, CORS middleware configuration, and '\n",
      " 'JSON manipulation.\\n'\n",
      " '\\n'\n",
      " '6. **Environmental Setup and Dependency Management**: Instructions and '\n",
      " 'examples for setting up environmental variables (using `.env` files) and '\n",
      " 'loading them within the Python script. There’s also a mention of adding '\n",
      " 'paths to `sys.path` for module import resolution.\\n'\n",
      " '\\n'\n",
      " '7. **Data Handling and Analysis**: Examples of handling, loading, embedding, '\n",
      " 'and indexing datasets (including usage of Pinecone for vector search and '\n",
      " \"embeddings). Additionally, there's logic for processing user input, \"\n",
      " 'constructing augmented prompts, and analyzing textual data with NLP (Natural '\n",
      " 'Language Processing) techniques.\\n'\n",
      " '\\n'\n",
      " '8. **LangChain and Pinecone Integration**: Some snippets discuss how to '\n",
      " 'integrate LangChain and Pinecone for advanced language processing and text '\n",
      " 'retrieval tasks.\\n'\n",
      " '\\n'\n",
      " 'From a high-level perspective, the provided data seems to be part of a '\n",
      " 'larger project or series of tutorials covering web development with React, '\n",
      " 'server-side programming with FastAPI, interaction with machine learning '\n",
      " 'models via the OpenAI API, and handling complex data structures. It exhibits '\n",
      " 'a comprehensive view of developing an application that leverages modern '\n",
      " 'JavaScript frameworks on the frontend and Python for backend and AI model '\n",
      " 'interfacing.',\n",
      " 'The provided data represents a sequence of Python import statements and code '\n",
      " 'snippets that collectively demonstrate how to set up and use a complex '\n",
      " 'application involving natural language processing (NLP) tasks. Specifically, '\n",
      " 'the application seems to be leveraging various components of the `langchain` '\n",
      " 'library and the OpenAI API to perform operations like document loading, text '\n",
      " 'splitting, embedding computation, retrieval from a vector database, and '\n",
      " 'generating responses based on prompts. \\n'\n",
      " '\\n'\n",
      " 'Key functionalities demonstrated through the code snippets include:\\n'\n",
      " '\\n'\n",
      " '1. **Document Loading:** Utilizing different loaders such as '\n",
      " '`HuggingFaceDatasetLoader`, `PyPDFLoader`, and `TextLoader` to load various '\n",
      " 'types of documents.\\n'\n",
      " '\\n'\n",
      " '2. **Text Splitting:** Employing `RecursiveCharacterTextSplitter`, '\n",
      " '`TokenTextSplitter`, and `CharacterTextSplitter` for breaking down the text '\n",
      " 'into more manageable pieces, possibly for more efficient processing or '\n",
      " 'analysis.\\n'\n",
      " '\\n'\n",
      " '3. **Storage:** Mention of `LocalFileStore` suggests a way to cache or store '\n",
      " 'data locally, possibly for embeddings or processed documents.\\n'\n",
      " '\\n'\n",
      " \"4. **OpenAI and Embeddings:** Utilizing OpenAI's API to generate embeddings \"\n",
      " 'and leveraging cached embeddings for more efficient retrieval and processing '\n",
      " 'of text data. The `OpenAIEmbeddings` class specifically indicates use of '\n",
      " \"OpenAI's models for generating text embeddings.\\n\"\n",
      " '\\n'\n",
      " '5. **Vector Store with Chroma:** The code hints at using `Chroma` for '\n",
      " 'storing and retrieving document embeddings in a vector database, '\n",
      " 'facilitating operations like similarity search or document clustering.\\n'\n",
      " '\\n'\n",
      " '6. **Interaction Model Programming with Chat Prompts:** It utilizes '\n",
      " '`ChatPromptTemplate`, `SystemMessagePromptTemplate`, and '\n",
      " '`HumanMessagePromptTemplate` for structuring conversational AI interactions. '\n",
      " 'This setup is employed in creating various interaction models mimicking '\n",
      " 'different roles like a Linux terminal, an English translator, a JavaScript '\n",
      " 'console, etc.\\n'\n",
      " '\\n'\n",
      " '7. **Retrieval and Response Generation:** Setting up a retrieval-based '\n",
      " 'question-answering model with `LLMChain` and `RetrievalQA` followed by the '\n",
      " 'execution of a model chain that includes retrieving context, generating a '\n",
      " 'prompt, querying an OpenAI model, and parsing string outputs.\\n'\n",
      " '\\n'\n",
      " '8. **Environment Setup:** Loading environment variables (e.g., OpenAI API '\n",
      " 'key) using `dotenv`, indicating the use of external configuration for API '\n",
      " 'keys and possibly other sensitive information.\\n'\n",
      " '\\n'\n",
      " 'The overall purpose of this setup seems to be building a multifaceted NLP '\n",
      " 'application capable of handling various tasks such as executing specific '\n",
      " 'actions based on user prompts, generating embeddings for texts, performing '\n",
      " 'document retrieval based on semantic similarity, and creating interactive, '\n",
      " 'role-based chatbots. The integration of OpenAI API indicates leveraging '\n",
      " 'state-of-the-art language models for generating responses, embeddings, or '\n",
      " 'performing language understanding tasks.',\n",
      " 'The provided data comprises multiple independent Python scripts and React '\n",
      " 'components for a project that appears to involve several key '\n",
      " 'functionalities, including text processing, machine learning model '\n",
      " 'interaction, web backend and frontend development, document handling, and '\n",
      " 'environmental variable management. Below is a breakdown of the '\n",
      " 'functionalities and components included in the data:\\n'\n",
      " '\\n'\n",
      " '1. **Python Backend Scripts**:\\n'\n",
      " '   - Importing necessary libraries and setting up environment variables, '\n",
      " 'including interacting with the OpenAI API.\\n'\n",
      " \"   - A function to insert a specific path into the system's PATH variable \"\n",
      " 'for module resolution.\\n'\n",
      " '   - Loading environmental variable configurations using a custom utility '\n",
      " 'module.\\n'\n",
      " '   - A template for handling text input, performing operations with an '\n",
      " 'OpenAI API client, and managing file reading and writing. This involves '\n",
      " 'generating text completions using OpenAI models, saving JSON data, and '\n",
      " 'generating prompt data based on user inputs and predefined contexts.\\n'\n",
      " '   - Code snippets for loading and preprocessing PDF documents from a '\n",
      " 'directory, splitting text into manageable chunks, and vectorizing these '\n",
      " 'chunks using OpenAI embeddings. Additionally, it interfaces with Weaviate, a '\n",
      " 'vector database, to store and query vectors.\\n'\n",
      " '   - Initialization and interaction with the Weaviate vector database '\n",
      " 'include defining a schema, adding vectorized text data, and performing '\n",
      " 'similarity searches based on user input to retrieve relevant documents.\\n'\n",
      " '\\n'\n",
      " '2. **React Frontend Components**:\\n'\n",
      " \"   - 'App' component managing application state and handling user input and \"\n",
      " 'file uploads. It interacts with a backend endpoint to submit data and '\n",
      " 'receive responses.\\n'\n",
      " \"   - 'Input' component for capturing user objectives, expected outputs, and \"\n",
      " \"file uploads. It communicates user input back to the parent 'App' \"\n",
      " 'component.\\n'\n",
      " \"   - 'Output' component for displaying processed data, such as prompts, \"\n",
      " 'scores, or other results returned from the backend.\\n'\n",
      " '\\n'\n",
      " '3. **General Overview and Interaction**:\\n'\n",
      " '   - The project likely aims to provide a solution for generating and '\n",
      " 'evaluating text prompts based on user input. This involves processing user '\n",
      " 'requests, analyzing documents, vectorizing text, and querying a vector '\n",
      " 'database for similar content.\\n'\n",
      " '   - The backend scripts reveal a sophisticated setup for handling and '\n",
      " 'analyzing text data, leveraging the OpenAI API for text completions and '\n",
      " 'embeddings, and using Weaviate for vector storage and retrieval.\\n'\n",
      " '   - The React frontend components suggest a web application interface for '\n",
      " 'user interaction, providing forms for input submission and displaying the '\n",
      " 'processed outputs.\\n'\n",
      " '\\n'\n",
      " 'Overall, the data represents parts of a robust system integrating machine '\n",
      " 'learning, natural language processing, vector databases, and web development '\n",
      " 'to create an interactive application, possibly for automated text generation '\n",
      " 'and analysis.',\n",
      " 'The provided text appears to be Python code outlining components and '\n",
      " 'functionalities likely intended for a web application, possibly involving '\n",
      " 'document processing and natural language understanding. The code snippets '\n",
      " 'suggest the incorporation of several key features:\\n'\n",
      " '\\n'\n",
      " '1. **PDF Processing:** The `PyPDF2` library is used with a custom `MyPDF` '\n",
      " 'utility to process PDF documents. A function `get_pdf_text` extracts text '\n",
      " 'from PDF files.\\n'\n",
      " '\\n'\n",
      " '2. **Text Splitting:** A custom text splitter, `MyTextSplitter`, leveraging '\n",
      " 'a `RecursiveCharacterTextSplitter`, splits the extracted text into '\n",
      " 'manageable chunks based on certain criteria like separator (`\\\\n`), chunk '\n",
      " 'size, chunk overlap, and a length function.\\n'\n",
      " '\\n'\n",
      " '3. **Language Understanding:** The code involves embedding and retrieving '\n",
      " 'text chunks possibly for understanding or processing the language data. It '\n",
      " 'uses `Chroma` from `langchain_community.vectorstores` and `OpenAIEmbeddings` '\n",
      " 'from `langchain_openai` to convert text chunks into a certain form of vector '\n",
      " 'storage for further processing.\\n'\n",
      " '\\n'\n",
      " '4. **Retrieval System:** A retriever system is mentioned, suggesting a '\n",
      " 'functionality to retrieve information or responses based on the processed '\n",
      " 'and stored vector representations, with a specific number of prompts to '\n",
      " 'generate (`\"k\": 2`).\\n'\n",
      " '\\n'\n",
      " \"5. **Interaction Flow:** There's a clear indication of interaction with the \"\n",
      " 'user, involving file uploads, processing buttons, and displaying prompts or '\n",
      " 'answers. This interaction likely happens in a web application powered by '\n",
      " '`streamlit`.\\n'\n",
      " '\\n'\n",
      " '6. **Environment and Utilities:** The code hints at a structured project '\n",
      " 'with environment variable management (`dotenv`), HTML templates '\n",
      " '(`htmlTemplates`), and utility modules for PDF processing, text splitting, '\n",
      " 'vector storage, and language chain utilities.\\n'\n",
      " '\\n'\n",
      " '7. **Conversation Handling:** A significant portion of the code is dedicated '\n",
      " 'to handling conversational input from users and generating conversational '\n",
      " 'prompts based on the processed documents and questions. This suggests an AI '\n",
      " 'or bot-like interface where users can interact, ask questions, and receive '\n",
      " 'relevant prompts or answers based on the uploaded documents.\\n'\n",
      " '\\n'\n",
      " 'In summary, the code seems to be part of a Python-based web application that '\n",
      " 'allows users to upload documents, process them for text extraction, split '\n",
      " 'the text for better management, and then use language understanding '\n",
      " 'techniques to embed this information for retrieval or conversational '\n",
      " 'purposes. The application likely aims to provide users with optimized '\n",
      " 'prompts or answers based on their input and the content of their documents, '\n",
      " 'leveraging AI or machine learning models for natural language processing.',\n",
      " 'The text you provided outlines a series of Python code snippets illustrating '\n",
      " 'the setup and usage of an OpenAI API for generating AI-powered content based '\n",
      " 'on given prompts. This includes setting up environmental variables for '\n",
      " \"OpenAI's API key, constructing templates for generating textual content, and \"\n",
      " 'demonstrating how to create and execute chains of AI-driven tasks. The '\n",
      " 'examples focus on financial advice generation, test case generation for '\n",
      " 'validating web login functionality, and code snippet generation for Python '\n",
      " 'programming tasks.\\n'\n",
      " '\\n'\n",
      " 'Key components and their purposes are:\\n'\n",
      " '\\n'\n",
      " '1. **Setting OpenAI API Key**: This involves assigning the API key to an '\n",
      " 'environment variable and using it to authenticate API requests. This key is '\n",
      " \"crucial for interacting with OpenAI's services.\\n\"\n",
      " '\\n'\n",
      " '2. **PromptTemplate and LLMChain from LangChain**: These are used for '\n",
      " 'defining a template-based prompt that fills in user-specified variables '\n",
      " \"(like 'financial_concept') and chaining together language model (LLM) tasks \"\n",
      " 'for AI to perform.\\n'\n",
      " '\\n'\n",
      " '3. **OpenAI object initialization and task execution**: Shows initializing '\n",
      " 'an OpenAI object with specific parameters (like temperature) for content '\n",
      " 'generation and executing tasks based on this configuration to generate '\n",
      " 'explanations on topics such as GDP, economics, deficits, and collateral.\\n'\n",
      " '\\n'\n",
      " '4. **Function `generate_ai_powered_test_cases`**: Demonstrates automating '\n",
      " \"the generation of test cases for a specified prompt using OpenAI's API, \"\n",
      " 'specifically targeting the creation of diverse and comprehensive test cases '\n",
      " 'for software testing.\\n'\n",
      " '\\n'\n",
      " '5. **Function `generate_test_cases_from_dataset`**: Illustrates generating '\n",
      " 'alternative code snippets based on a given dataset of examples, showcasing '\n",
      " 'how AI can provide different approaches to solving the same problem, such as '\n",
      " 'doubling a number in Python.\\n'\n",
      " '\\n'\n",
      " \"Overall, the data represents a practical application of OpenAI's API for \"\n",
      " 'content generation and automation tasks, leveraging AI to create meaningful, '\n",
      " 'contextual, and varied outputs based on user-defined templates and examples.',\n",
      " 'The given data encompasses a variety of Python functions and classes '\n",
      " 'designed for different purposes including random data generation for '\n",
      " 'evaluations, API interactions, environment variable management, natural '\n",
      " 'language processing (NLP), and more. The key components and their '\n",
      " 'functionalities are as follows:\\n'\n",
      " '\\n'\n",
      " '1. **Random Data Generation for Evaluations**: Functions like '\n",
      " '`monte_carlo_eval` and `elo_eval` generate simulated outcomes based on '\n",
      " 'random selections to evaluate scenarios or items. The Monte Carlo method '\n",
      " 'simulates different outcomes to estimate the average score, while the Elo '\n",
      " 'rating system updates based on the outcomes of hypothetical matchups.\\n'\n",
      " '\\n'\n",
      " '2. **Elo Rating System Simulation**: This part involves functions to '\n",
      " 'simulate and update Elo ratings based on the outcomes of simulated events. '\n",
      " 'Elo ratings are a method for calculating the relative skill levels of '\n",
      " 'players in competitor-versus-competitor games.\\n'\n",
      " '\\n'\n",
      " '3. **API and Environment Variable Management**: The script demonstrates how '\n",
      " 'to load API keys and other settings from environment variables, ensuring '\n",
      " 'secure and flexible configuration management. Classes like `OPENAI_KEYS` and '\n",
      " '`VECTORDB_KEYS` encapsulate the logic for accessing specific environment '\n",
      " 'variables.\\n'\n",
      " '\\n'\n",
      " '4. **Natural Language Processing (NLP) and Retrieval**: The script includes '\n",
      " 'setups for loading data, splitting text into manageable chunks, embedding it '\n",
      " 'into vector space for NLP tasks, and retrieving information relevant to '\n",
      " 'given queries. This part utilizes libraries and APIs like OpenAI for '\n",
      " 'embeddings and a theoretical vector database client for document retrieval '\n",
      " 'based on vector space models.\\n'\n",
      " '\\n'\n",
      " '5. **Question-Answering Pipeline**: It outlines a framework for answering '\n",
      " 'questions using a retrieve-and-generate (RAG) approach. This involves '\n",
      " 'retrieving relevant context using vector-based search and then generating '\n",
      " 'answers with models like GPT-3.5-turbo in a templated format that limits '\n",
      " 'responses to two sentences for conciseness.\\n'\n",
      " '\\n'\n",
      " '6. **Evaluation of NLP Outcomes**: The last section details how to evaluate '\n",
      " 'the performance of NLP tasks using metrics like precision, recall, '\n",
      " 'faithfulness, and relevance. It suggests the use of a dataset containing '\n",
      " 'questions, generated answers, contexts, and ground truths to assess the '\n",
      " 'quality of the NLP system.\\n'\n",
      " '\\n'\n",
      " 'Overall, the data demonstrates a comprehensive approach to building and '\n",
      " 'evaluating NLP systems, from managing configurations securely to generating '\n",
      " 'and evaluating responses to natural language queries. It showcases the '\n",
      " 'integration of different tools and methodologies, including environment '\n",
      " 'management, data retrieval and processing, natural language understanding '\n",
      " 'and generation, and performance evaluation.']\n"
     ]
    }
   ],
   "source": [
    "query = \"Analyse the data passed to you and give a general overview on it?\"\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Evaluate the document and provide an assessment of its overall content\"\n",
    "response = openai_model_answer_list(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python code collection presents a moderate-to-advanced difficulty level, catering to those with some familiarity with Python, data manipulation using pandas, natural language processing (NLP), and machine learning (ML) models. Here's a rundown of the aspects contributing to its complexity:\n",
      "\n",
      "1. **Use of External Libraries**: The code extensively uses external libraries such as pandas for data manipulation, Gensim for Word2Vec models, and Hugging Face's transformers for working with pre-trained language models. Familiarity with these libraries is essential.\n",
      "\n",
      "2. **Data Processing and NLP Techniques**: Demonstrated are various preprocessing steps typical in NLP tasks, such as emoji and URL extraction and removal, text normalization, and handling of special symbols and mentions. Such operations require understanding of both Python and basic NLP principles.\n",
      "\n",
      "3. **Working with Word2Vec Models**: Part of the code involves training a Word2Vec model using the Gensim library. This involves understanding hyperparameters like embedding size, window size, and min_count, crucial for working with word embeddings.\n",
      "\n",
      "4. **Usage of Pre-Trained Language Models from Hugging Face**: The code also shows how to download and use models from Hugging Face, including adapting a sequence-to-sequence model for use. This indicates an advanced use case involving interaction with an external API and working with cutting-edge NLP models.\n",
      "\n",
      "5. **Script Structure and Modularization**: The code snippets suggest a project structure that includes separate modules and directories for different phases of data processing, from raw data handling to parsing, cleaning, and finally, analysis or model training. Understanding this structure requires some background in software engineering best practices.\n",
      "\n",
      "6. **Environment and Dependency Management**: There's an implicit need for managing Python environments and dependencies (notice the use of `sys.path.append` for local module imports), which is a key skill for any Python developer working on sizable projects.\n",
      "\n",
      "Notable Complexity Factors:\n",
      "- **Text Normalization for Non-Roman Scripts**: Specific to this collection is the handling of the Amharic language, including normalization of variant character representations. This adds a layer of complexity due to the non-Roman script and linguistic nuances of Amharic.\n",
      "- **Integration with Hugging Face Hub and API**: The code demonstrates downloading components for a pre-trained model from Hugging Face Hub using an API key, showcasing an advanced application involving APIs.\n",
      "\n",
      "Overall, the code collection is positioned towards individuals with intermediate Python knowledge, a good understanding of NLP and ML fundamentals, and familiarity with the specific libraries and tools used.\n",
      "The provided Python code snippets span across a variety of tasks primarily focused on data preprocessing, analysis, and natural language processing (NLP) with Python libraries such as Pandas, Gensim, and Hugging Face's Transformers. Here's an assessment of its complexity and notable patterns:\n",
      "\n",
      "### Complexity Level: Intermediate to Advanced\n",
      "\n",
      "1. **Data Preprocessing and Cleaning**: The code involves reading data from CSV and JSON files, handling missing values, and cleaning text data by removing or replacing unwanted characters, symbols, emojis, URLs, and mentions. This part requires a good understanding of Pandas and regular expressions, placing it at an intermediate level.\n",
      "\n",
      "2. **Word Embeddings with Gensim**: Generating word embeddings using the Gensim library's Word2Vec model increases the complexity as it involves understanding of NLP concepts like tokenization and embeddings. The parameters for the Word2Vec model such as vector size, window size, and min_count are set explicitly, suggesting an intermediate to advanced level of complexity due to the need for a solid grasp of how these parameters affect the model's performance.\n",
      "\n",
      "3. **Using Hugging Face's Transformers for NLP Tasks**: The code shows how to download a pre-trained model from Hugging Face, and set up for use in sequence-to-sequence language modeling, which is an advanced topic. It requires familiarity with the Transformers library and concepts such as tokenization and sequence-to-sequence models.\n",
      "\n",
      "### Notable Patterns and Structures:\n",
      "\n",
      "- **Modular Approach**: The code snippets demonstrate a modular approach to handling different steps of data preprocessing and modeling. For example, there's separation between reading and cleaning data, training a Word2Vec model, and utilizing a pre-trained language model from Hugging Face. This structure aids in making the code more readable and maintainable.\n",
      "\n",
      "- **Use of Lambdas and Apply Functions**: There's extensive use of lambda functions and the `.apply()` method on DataFrame columns to perform operations like extracting hashtags, emojis, and applying various text cleaning steps. This pattern is common in data manipulation with Pandas and demonstrates a more pythonic way of applying functions to DataFrame columns.\n",
      "\n",
      "- **External Utility Functions**: The code snippets include calls to functions from an imported module (`MyPreprocessing`), which suggests that some of the preprocessing logic is abstracted away into external utility functions. This design pattern helps in organizing code better and reusing common functionality.\n",
      "\n",
      "- **Environment Variable for API Key**: Using an environment variable for the Hugging Face API key (`HUGGING_FACE_API_KEY`) is a good practice for security reasons, as it keeps sensitive information out of the codebase.\n",
      "\n",
      "Overall, the code collection demonstrates a mix of data manipulation, NLP techniques, and best practices aimed at intermediate to advanced Python developers. It highlights the application of Python in real-world data preprocessing and analysis tasks, particularly in the context of text data.\n",
      "('pipeline = pipeline (\"Text-Generation\", model=model, device=-1, '\n",
      " 'tokenizer=tokenizer, max_length=1000 )\\n'\n",
      " 'from langchain.text_splitter import CharacterTextSplitter\\n'\n",
      " 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n'\n",
      " 'from langchain_openai import OpenAIEmbeddings\\n'\n",
      " 'from langchain_community.vectorstores import Chroma\\n'\n",
      " 'from PyPDF2 import PdfReader\\n'\n",
      " 'from langchain_community.embeddings import HuggingFaceEmbeddings\\n'\n",
      " 'from langchain_community.embeddings.sentence_transformer import (\\n'\n",
      " '  SentenceTransformerEmbeddings,\\n'\n",
      " ')\\n'\n",
      " ' \\n'\n",
      " 'persist_directory = \"db\"\\n'\n",
      " ' \\n'\n",
      " 'class MySpecialFunctions:\\n'\n",
      " '  def __init__(self):\\n'\n",
      " '  pass\\n'\n",
      " '  def get_file_text(self, files):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for file in files:\\n'\n",
      " '  try:\\n'\n",
      " \"  with open(file, 'r', encoding='utf-8') as f:\\n\"\n",
      " '  content = f.read()\\n'\n",
      " '  text += content\\n'\n",
      " '  except Exception as e:\\n'\n",
      " '  print(f\"Error reading file {file}: {e}\")\\n'\n",
      " '  return text\\n'\n",
      " '  def get_pdf_text(self, pdf):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for doc in pdf:\\n'\n",
      " '  reader = PdfReader(doc)\\n'\n",
      " '  for page in reader.pages:\\n'\n",
      " '  text += page.extract_text()\\n'\n",
      " '  return text\\n'\n",
      " '  def get_text_chunks(self, text):\\n'\n",
      " '  text_siplitter = RecursiveCharacterTextSplitter(\\n'\n",
      " '  chunk_size = 1000,   chunk_overlap = 200,\\n'\n",
      " \"  separators=['\\\\n', '\\\\n\\\\n'],\\n\"\n",
      " '  length_function = len)\\n'\n",
      " '  chunk = text_siplitter.split_text(text)\\n'\n",
      " '  return chunk\\n'\n",
      " '  def get_vectorstore(self, chunks):\\n'\n",
      " '  hf_embedding = HuggingFaceEmbeddings()\\n'\n",
      " '  embedding_function = '\n",
      " 'SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n'\n",
      " '  vector_db = Chroma.from_documents(\\n'\n",
      " '  documents = chunks,\\n'\n",
      " '  embedding = hf_embedding,\\n'\n",
      " '  )\\n'\n",
      " '  return vector_db\\n'\n",
      " 'import streamlit as st \\n'\n",
      " 'from MyFunctions import MySpecialFunctions\\n'\n",
      " 'from dotenv import load_dotenv\\n'\n",
      " ' \\n'\n",
      " 'special_functions_instance = MySpecialFunctions()\\n'\n",
      " 'def get_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_file_text(external_data )\\n'\n",
      " 'def get_pdf_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_pdf_text(external_data )\\n'\n",
      " 'def get_chunks(text):\\n'\n",
      " '  return special_functions_instance.get_text_chunks(text)\\n'\n",
      " 'def get_vectorstore(text_chunks):\\n'\n",
      " '  return special_functions_instance.get_vectorstore(text_chunks)  \\n'\n",
      " 'def main():\\n'\n",
      " '  load_dotenv()\\n'\n",
      " '  st.set_page_config(page_title=\"Generation of Telegram Ads in Amharic\", '\n",
      " 'page_icon= \":smile\")\\n'\n",
      " '  with st.sidebar:\\n'\n",
      " '  pass\\n'\n",
      " '  st.markdown(\"   external_data = st.file_uploader(\" Upload the generative '\n",
      " 'text from fune-tuning\", accept_multiple_files= True)\\n'\n",
      " '  if st.button(\"Retrieval\"):\\n'\n",
      " '  with st.spinner(\"Processing\"):\\n'\n",
      " '  text = get_pdf_text(external_data )\\n'\n",
      " '  text_chunks = get_chunks(text)\\n'\n",
      " '  st.write(text_chunks)\\n'\n",
      " '  vectorstore_db = get_vectorstore(text_chunks)\\n'\n",
      " ' \\n'\n",
      " 'if __name__ == \"__main__\":\\n'\n",
      " '  main()\\n'\n",
      " 'import sentencepiece as spm\\n'\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m \"\n",
      " \"--vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " \"sp.load('m.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " \"print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))\\n\"\n",
      " 'print(sp.decode_ids([460, 133, 774, 1276]))\\n'\n",
      " ' \\n'\n",
      " 'print(sp.get_piece_size())\\n'\n",
      " ' \\n'\n",
      " 'print(sp.id_to_piece(460))\\n'\n",
      " \"print(sp.piece_to_id('▁በአዲስ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\\n\"\n",
      " ' \\n'\n",
      " 'for id in range(3):\\n'\n",
      " '  print(sp.id_to_piece(id), sp.is_control(id))\\n'\n",
      " ' \\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " ' \\n'\n",
      " \"serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " 'sp.load_from_serialized_proto(serialized_model_proto)\\n'\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user \"\n",
      " \"--user_defined_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_user = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_user.load('m_user.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\\n\"\n",
      " \"print(sp_user.piece_to_id('<sep>'))  \\n\"\n",
      " \"print(sp_user.piece_to_id('<cls>'))  \\n\"\n",
      " \"print('3=', sp_user.decode_ids([3]))  \\n\"\n",
      " \"print('4=', sp_user.decode_ids([4]))  \\n\"\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl \"\n",
      " \"--control_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_ctrl = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_ctrl.load('m_ctrl.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\")\n"
     ]
    }
   ],
   "source": [
    "query = \"Evaluate the complexity level of the Python code collection and provide an assessment of its overall difficulty, ranging from beginner-friendly to advanced. Highlight any notable patterns or structures that contribute to its complexity.\"\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document outlines a multifaceted process involving data extraction, cleaning, transformation, and machine learning model training. Presented as a series of Python code snippets, it encompasses reading, cleaning, preprocessing text data, and applying Word2Vec for generating word embeddings, followed by the integration of a model from Hugging Face Hub. Below is a breakdown analysis and suggested improvements:\n",
      "\n",
      "### Content Analysis\n",
      "1. **Data Processing and Cleaning:**\n",
      "   - Reading CSV and JSON data.\n",
      "   - Handling missing values and duplicate data.\n",
      "   - Text normalization, including replacing specific characters, removing emojis, links, mentions, and non-alphanumeric symbols.\n",
      "\n",
      "2. **Feature Extraction:**\n",
      "   - Extracting hashtags, emojis, symbols, URLs, and mentions from the text data.\n",
      "\n",
      "3. **Text Preprocessing:**\n",
      "   - Standardizing certain Amharic letters.\n",
      "   - Removing unnecessary whitespace and punctuation.\n",
      "\n",
      "4. **Word2Vec Model Training:**\n",
      "   - Tokenizing text data.\n",
      "   - Training a Word2Vec model to generate word embeddings.\n",
      "\n",
      "5. **Integration with Hugging Face:**\n",
      "   - Downloading files from a Hugging Face repository.\n",
      "   - Loading a tokenizer and a Seq2Seq model for generating or modifying text data.\n",
      "\n",
      "### Structure Analysis\n",
      "- The document is structured in a linear, process-driven manner, starting from raw data handling to more complex operations like NLP model training and integration.\n",
      "- It lacks a clear separation between sections which could improve readability. For instance, categorizing blocks of code into sections like Data Cleaning, Feature Extraction, Model Training, etc., could help.\n",
      "- The process is logically ordered from data preprocessing to model application, which is a strength.\n",
      "\n",
      "### Potential Improvements\n",
      "1. **Modularization:**\n",
      "   - Breaking down the script into functions or modules based on their purposes (e.g., cleaning, preprocessing, model training) for better code organization and reusability.\n",
      "\n",
      "2. **Error Handling:**\n",
      "   - Adding error handling and logging for debugging and to handle any potential issues during file reading, data cleaning, or model training phases.\n",
      "\n",
      "3. **Comments and Documentation:**\n",
      "   - Adding more comments and documentation within the code to explain the purpose and functionality of each block or function for better understandability.\n",
      "\n",
      "4. **Optimization:**\n",
      "   - Evaluating the performance implications of the text manipulation and model training processes, potentially optimizing for speed and resource usage.\n",
      "\n",
      "5. **Validation and Testing:**\n",
      "   - Incorporating validation steps to ensure the accuracy and reliability of the data cleaning and preprocessing steps, as well as the effectiveness of the Word2Vec model.\n",
      "\n",
      "6. **Configurability:**\n",
      "   - Providing configurations (such as file paths, model parameters) in a separate config file or as command-line arguments to make the script more adaptable.\n",
      "\n",
      "In conclusion, while the document showcases a comprehensive approach towards text data handling and processing for NLP tasks, emphasizing modularization, documentation, and optimization could greatly enhance its structure, readability, and maintainability.\n",
      "The document outlines a series of data processing and modeling tasks using Python, involving several libraries such as pandas, gensim, and transformers, focused primarily on natural language processing (NLP) and machine learning tasks. It demonstrates a workflow divided into clear segments: data preparation, cleaning, modeling, and utilization of pre-trained models.\n",
      "\n",
      "**Content Overview:**\n",
      "\n",
      "1. **Data Preparation and Cleaning:**\n",
      "   - It begins with importing necessary libraries and setting up directory paths.\n",
      "   - A DataFrame is loaded from a CSV file containing text data. Basic operations include dropping missing values, replacing newline characters, extracting and removing hashtags, emojis, symbols, and links, and normalizing certain characters for consistency.\n",
      "   - There's a focus on Amharic language data, with specific character replacements to standardize various forms.\n",
      "   - Text data is cleaned to remove unnecessary symbols, URLs, mentions, and extra spaces, improving the quality of the dataset for analysis.\n",
      "\n",
      "2. **Word2Vec Model:**\n",
      "   - The text data is tokenized into words and passed into a Word2Vec model to learn word embeddings - vector representations of words.\n",
      "   - The model parameters include the size of the word vectors, the size of the window context, and the minimum count of word occurrences.\n",
      "   - It highlights querying the model for a specific word vector and finding similar words, demonstrating the model's capability to understand word semantics.\n",
      "\n",
      "3. **Utilization of Pre-trained Transformer Models:**\n",
      "   - Demonstrates downloading model files from Hugging Face using an API key.\n",
      "   - It focuses on using the `llama-2-amharic-3784m` model for Amharic text data, showcasing how to load a tokenizer and a model to be used for causal language modeling or sequence-to-sequence tasks.\n",
      "   - This segment indicates a higher-level NLP task potentially for language generation or translation.\n",
      "\n",
      "**Structure:**\n",
      "- The document is structured logically, starting from data preparation to advanced modeling. It uses a scripting approach, where each block of code is meant to be run sequentially.\n",
      "- There is a clear separation between different tasks (e.g., data preparation, Word2Vec modeling, transformer model utilization).\n",
      "\n",
      "**Potential Improvements:**\n",
      "- **Code Comments and Documentation:** Adding comments and explanations to the code would greatly improve readability and understandability, especially for readers unfamiliar with some of the libraries or tasks.\n",
      "- **Error Handling and Validation:** The script lacks error handling and validation checks (e.g., verifying successful file reads or model training outcomes).\n",
      "- **Modularization:** Separating the workflow into functions or modules for each significant step (data cleaning, modeling, etc.) could improve reusability and maintainability.\n",
      "- **Evaluation and Visualization:** Including model evaluation metrics or visualization of results (e.g., word embedding spaces) could provide insights into the effectiveness of the models and preprocessing steps.\n",
      "- **Experimental Details:** Additional context on the dataset, objectives, and expected outcomes could contextualize the code snippets and highlight their importance or applications.\n",
      "\n",
      "This analysis provides a multifaceted view of the document, recognizing its focused application in NLP while suggesting ways to enhance accessibility, robustness, and utility.\n",
      "('pipeline = pipeline (\"Text-Generation\", model=model, device=-1, '\n",
      " 'tokenizer=tokenizer, max_length=1000 )\\n'\n",
      " 'from langchain.text_splitter import CharacterTextSplitter\\n'\n",
      " 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n'\n",
      " 'from langchain_openai import OpenAIEmbeddings\\n'\n",
      " 'from langchain_community.vectorstores import Chroma\\n'\n",
      " 'from PyPDF2 import PdfReader\\n'\n",
      " 'from langchain_community.embeddings import HuggingFaceEmbeddings\\n'\n",
      " 'from langchain_community.embeddings.sentence_transformer import (\\n'\n",
      " '  SentenceTransformerEmbeddings,\\n'\n",
      " ')\\n'\n",
      " ' \\n'\n",
      " 'persist_directory = \"db\"\\n'\n",
      " ' \\n'\n",
      " 'class MySpecialFunctions:\\n'\n",
      " '  def __init__(self):\\n'\n",
      " '  pass\\n'\n",
      " '  def get_file_text(self, files):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for file in files:\\n'\n",
      " '  try:\\n'\n",
      " \"  with open(file, 'r', encoding='utf-8') as f:\\n\"\n",
      " '  content = f.read()\\n'\n",
      " '  text += content\\n'\n",
      " '  except Exception as e:\\n'\n",
      " '  print(f\"Error reading file {file}: {e}\")\\n'\n",
      " '  return text\\n'\n",
      " '  def get_pdf_text(self, pdf):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for doc in pdf:\\n'\n",
      " '  reader = PdfReader(doc)\\n'\n",
      " '  for page in reader.pages:\\n'\n",
      " '  text += page.extract_text()\\n'\n",
      " '  return text\\n'\n",
      " '  def get_text_chunks(self, text):\\n'\n",
      " '  text_siplitter = RecursiveCharacterTextSplitter(\\n'\n",
      " '  chunk_size = 1000,   chunk_overlap = 200,\\n'\n",
      " \"  separators=['\\\\n', '\\\\n\\\\n'],\\n\"\n",
      " '  length_function = len)\\n'\n",
      " '  chunk = text_siplitter.split_text(text)\\n'\n",
      " '  return chunk\\n'\n",
      " '  def get_vectorstore(self, chunks):\\n'\n",
      " '  hf_embedding = HuggingFaceEmbeddings()\\n'\n",
      " '  embedding_function = '\n",
      " 'SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n'\n",
      " '  vector_db = Chroma.from_documents(\\n'\n",
      " '  documents = chunks,\\n'\n",
      " '  embedding = hf_embedding,\\n'\n",
      " '  )\\n'\n",
      " '  return vector_db\\n'\n",
      " 'import streamlit as st \\n'\n",
      " 'from MyFunctions import MySpecialFunctions\\n'\n",
      " 'from dotenv import load_dotenv\\n'\n",
      " ' \\n'\n",
      " 'special_functions_instance = MySpecialFunctions()\\n'\n",
      " 'def get_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_file_text(external_data )\\n'\n",
      " 'def get_pdf_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_pdf_text(external_data )\\n'\n",
      " 'def get_chunks(text):\\n'\n",
      " '  return special_functions_instance.get_text_chunks(text)\\n'\n",
      " 'def get_vectorstore(text_chunks):\\n'\n",
      " '  return special_functions_instance.get_vectorstore(text_chunks)  \\n'\n",
      " 'def main():\\n'\n",
      " '  load_dotenv()\\n'\n",
      " '  st.set_page_config(page_title=\"Generation of Telegram Ads in Amharic\", '\n",
      " 'page_icon= \":smile\")\\n'\n",
      " '  with st.sidebar:\\n'\n",
      " '  pass\\n'\n",
      " '  st.markdown(\"   external_data = st.file_uploader(\" Upload the generative '\n",
      " 'text from fune-tuning\", accept_multiple_files= True)\\n'\n",
      " '  if st.button(\"Retrieval\"):\\n'\n",
      " '  with st.spinner(\"Processing\"):\\n'\n",
      " '  text = get_pdf_text(external_data )\\n'\n",
      " '  text_chunks = get_chunks(text)\\n'\n",
      " '  st.write(text_chunks)\\n'\n",
      " '  vectorstore_db = get_vectorstore(text_chunks)\\n'\n",
      " ' \\n'\n",
      " 'if __name__ == \"__main__\":\\n'\n",
      " '  main()\\n'\n",
      " 'import sentencepiece as spm\\n'\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m \"\n",
      " \"--vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " \"sp.load('m.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " \"print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))\\n\"\n",
      " 'print(sp.decode_ids([460, 133, 774, 1276]))\\n'\n",
      " ' \\n'\n",
      " 'print(sp.get_piece_size())\\n'\n",
      " ' \\n'\n",
      " 'print(sp.id_to_piece(460))\\n'\n",
      " \"print(sp.piece_to_id('▁በአዲስ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\\n\"\n",
      " ' \\n'\n",
      " 'for id in range(3):\\n'\n",
      " '  print(sp.id_to_piece(id), sp.is_control(id))\\n'\n",
      " ' \\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " ' \\n'\n",
      " \"serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " 'sp.load_from_serialized_proto(serialized_model_proto)\\n'\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user \"\n",
      " \"--user_defined_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_user = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_user.load('m_user.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\\n\"\n",
      " \"print(sp_user.piece_to_id('<sep>'))  \\n\"\n",
      " \"print(sp_user.piece_to_id('<cls>'))  \\n\"\n",
      " \"print('3=', sp_user.decode_ids([3]))  \\n\"\n",
      " \"print('4=', sp_user.decode_ids([4]))  \\n\"\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl \"\n",
      " \"--control_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_ctrl = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_ctrl.load('m_ctrl.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\")\n"
     ]
    }
   ],
   "source": [
    "query = 'Perform a comprehensive analysis of the document and provide insights on its content, structure, and potential improvements'\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided Python script spans several tasks, primarily focused on text preprocessing, data cleaning, and utilising machine learning for text vectorization. Let's break it down focusing on the Python and SQL keywords and syntax used, particularly classes, functions, and other related syntax:\n",
      "\n",
      "1. **Import Statements**:\n",
      "   - The script imports various modules and functions at different points, indicating dependencies on libraries like pandas, gensim, transformers, and a custom module (utils.preprocessing).\n",
      "\n",
      "2. **Variables and Path Settings**:\n",
      "   - Variables like `parsed_dir`, `cleaned_dir`, and `file_name` are set, which are used for specifying file paths.\n",
      "\n",
      "3. **Classes and Methods**:\n",
      "   - A class `MyPreprocessing` from the custom module `utils.preprocessing` is instantiated multiple times in different parts of the script. This class is presumably equipped with methods for text preprocessing.\n",
      "   - Various methods from pandas (`read_csv`, `dropna`, `replace`, `apply`, `str`, and `to_csv`) and from pandas series (`head`, `shape`, `tail`) are used for handling DataFrame operations.\n",
      "   - The script creates a Word2Vec model using gensim's `Word2Vec` class, indicating an instantiation and use of machine learning models for embedding.\n",
      "   - Methods from the `transformers` library are used to download, tokenize, and process text using a pre-trained model. This includes using `AutoTokenizer` and `AutoModelForSeq2SeqLM` classes.\n",
      "\n",
      "4. **Loops and Conditionals**:\n",
      "   - A complex loop structure is employed for replacing characters in the DataFrame. This loop iterates over a list of lists to perform string replacement operations.\n",
      "   - No explicit if-else conditionals are present, but lambda functions and method chaining apply conditional logic in data processing lines.\n",
      "\n",
      "5. **Data Cleaning Operations**:\n",
      "   - Extensive data cleaning steps are carried out including dropping missing values, replacing newline characters, extracting hashtags, emojis, mentions, and URLs using custom methods, and performing string replacement to clean up the text.\n",
      "\n",
      "6. **File I/O**:\n",
      "   - The script involves reading from CSV and JSON files and writing to CSV and TXT files, indicating file input/output operations for data preprocessing and saving the cleaned data.\n",
      "\n",
      "7. **Machine Learning Operations**:\n",
      "   - Training a Word2Vec model using the gensim library and embedding text data.\n",
      "   - Downloading a model from Hugging Face, and using tokenizer and model from the `transformers` library for sequence to sequence learning modeling indicates advanced machine learning and natural language processing operations.\n",
      "\n",
      "8. **No SQL Keywords**:\n",
      "   - There are no explicit SQL keywords or database manipulation operations in the script. All data manipulation appears to be done in-memory using pandas DataFrames and not persisted in an SQL database.\n",
      "\n",
      "9. **Function Definitions**:\n",
      "   - The script does not contain explicit Python function definitions (`def`) within the provided code, relying instead on library functions and methods for data manipulation and analysis.\n",
      "\n",
      "This summary focuses on the analysis of classes, functions, and key programming constructs as used within the script, with a notable absence of direct SQL operations.\n",
      "The provided Python code contains a sequence of operations mainly focused on preprocessing, analyzing, and exporting text data, with a particular emphasis on working with Ethiopian Amharic data. It also includes a segment on utilizing pre-trained models from Hugging Face. Here’s a breakdown based on the requested aspects:\n",
      "\n",
      "### Python Keywords and Constructs:\n",
      "- **Classes and Objects:** The code uses the `MyPreprocessing` class from an external utility module. Instances of this class are created and used for various text preprocessing tasks.\n",
      "- **Functions and Method Calls:** Multiple functions/methods are used, including those from pandas (e.g., `read_csv`, `dropna`, `replace`, `to_csv`) for data manipulation, those from the `MyPreprocessing` class for specific text processing tasks (e.g., `extract_hashtags`, `remove_emojis`), and Gensim's Word2Vec model functions for training word embeddings.\n",
      "- **Loops:** There's a loop to replace specific letters in the Amharic text to standardize various forms of writing. Loops are also used for downloading files from Hugging Face.\n",
      "- **Conditionals:** Specific conditionals aren't explicitly detailed in the provided snippets, but conditional logic might be present within the method calls (especially within the `MyPreprocessing` class methods).\n",
      "- **Imports and Modules:** The script makes extensive use of imports for functionality, including standard Python modules (`sys`, `os`), third-party libraries (`pandas`, `gensim`, `transformers`, `huggingface_hub`), and custom modules (`utils.preprocessing`).\n",
      "\n",
      "### SQL or Database Interactions:\n",
      "- **Direct SQL or Database Keywords:** There are no direct SQL queries or database connection keywords within the provided code. The data manipulation and storage are handled through file operations with CSV files and pandas DataFrames.\n",
      "\n",
      "### Other Syntax Points:\n",
      "- Regex is used for text manipulation and cleaning (e.g., replacing newline characters, removing URLs, mentions, emojis, and symbols from the text).\n",
      "- The code also includes loading models and tokenizers from Hugging Face and using them, which demonstrates interaction with machine learning models and APIs.\n",
      "- File I/O operations are evident in reading JSON and CSV files, writing cleaned text to CSV, and saving word embeddings models.\n",
      "- The use of environment variables to securely access the Hugging Face API.\n",
      "\n",
      "### Summary:\n",
      "The script emphasizes text preprocessing, handling Amharic text data, and leveraging natural language processing (NLP) models. It illustrates the use of Python for data manipulation, cleaning, and applying machine learning models. However, there are no SQL/database interaction elements in the provided segments.\n",
      "('pipeline = pipeline (\"Text-Generation\", model=model, device=-1, '\n",
      " 'tokenizer=tokenizer, max_length=1000 )\\n'\n",
      " 'from langchain.text_splitter import CharacterTextSplitter\\n'\n",
      " 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n'\n",
      " 'from langchain_openai import OpenAIEmbeddings\\n'\n",
      " 'from langchain_community.vectorstores import Chroma\\n'\n",
      " 'from PyPDF2 import PdfReader\\n'\n",
      " 'from langchain_community.embeddings import HuggingFaceEmbeddings\\n'\n",
      " 'from langchain_community.embeddings.sentence_transformer import (\\n'\n",
      " '  SentenceTransformerEmbeddings,\\n'\n",
      " ')\\n'\n",
      " ' \\n'\n",
      " 'persist_directory = \"db\"\\n'\n",
      " ' \\n'\n",
      " 'class MySpecialFunctions:\\n'\n",
      " '  def __init__(self):\\n'\n",
      " '  pass\\n'\n",
      " '  def get_file_text(self, files):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for file in files:\\n'\n",
      " '  try:\\n'\n",
      " \"  with open(file, 'r', encoding='utf-8') as f:\\n\"\n",
      " '  content = f.read()\\n'\n",
      " '  text += content\\n'\n",
      " '  except Exception as e:\\n'\n",
      " '  print(f\"Error reading file {file}: {e}\")\\n'\n",
      " '  return text\\n'\n",
      " '  def get_pdf_text(self, pdf):\\n'\n",
      " '  text = \"\"\\n'\n",
      " '  for doc in pdf:\\n'\n",
      " '  reader = PdfReader(doc)\\n'\n",
      " '  for page in reader.pages:\\n'\n",
      " '  text += page.extract_text()\\n'\n",
      " '  return text\\n'\n",
      " '  def get_text_chunks(self, text):\\n'\n",
      " '  text_siplitter = RecursiveCharacterTextSplitter(\\n'\n",
      " '  chunk_size = 1000,   chunk_overlap = 200,\\n'\n",
      " \"  separators=['\\\\n', '\\\\n\\\\n'],\\n\"\n",
      " '  length_function = len)\\n'\n",
      " '  chunk = text_siplitter.split_text(text)\\n'\n",
      " '  return chunk\\n'\n",
      " '  def get_vectorstore(self, chunks):\\n'\n",
      " '  hf_embedding = HuggingFaceEmbeddings()\\n'\n",
      " '  embedding_function = '\n",
      " 'SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n'\n",
      " '  vector_db = Chroma.from_documents(\\n'\n",
      " '  documents = chunks,\\n'\n",
      " '  embedding = hf_embedding,\\n'\n",
      " '  )\\n'\n",
      " '  return vector_db\\n'\n",
      " 'import streamlit as st \\n'\n",
      " 'from MyFunctions import MySpecialFunctions\\n'\n",
      " 'from dotenv import load_dotenv\\n'\n",
      " ' \\n'\n",
      " 'special_functions_instance = MySpecialFunctions()\\n'\n",
      " 'def get_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_file_text(external_data )\\n'\n",
      " 'def get_pdf_text(external_data ):\\n'\n",
      " '  return special_functions_instance.get_pdf_text(external_data )\\n'\n",
      " 'def get_chunks(text):\\n'\n",
      " '  return special_functions_instance.get_text_chunks(text)\\n'\n",
      " 'def get_vectorstore(text_chunks):\\n'\n",
      " '  return special_functions_instance.get_vectorstore(text_chunks)  \\n'\n",
      " 'def main():\\n'\n",
      " '  load_dotenv()\\n'\n",
      " '  st.set_page_config(page_title=\"Generation of Telegram Ads in Amharic\", '\n",
      " 'page_icon= \":smile\")\\n'\n",
      " '  with st.sidebar:\\n'\n",
      " '  pass\\n'\n",
      " '  st.markdown(\"   external_data = st.file_uploader(\" Upload the generative '\n",
      " 'text from fune-tuning\", accept_multiple_files= True)\\n'\n",
      " '  if st.button(\"Retrieval\"):\\n'\n",
      " '  with st.spinner(\"Processing\"):\\n'\n",
      " '  text = get_pdf_text(external_data )\\n'\n",
      " '  text_chunks = get_chunks(text)\\n'\n",
      " '  st.write(text_chunks)\\n'\n",
      " '  vectorstore_db = get_vectorstore(text_chunks)\\n'\n",
      " ' \\n'\n",
      " 'if __name__ == \"__main__\":\\n'\n",
      " '  main()\\n'\n",
      " 'import sentencepiece as spm\\n'\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m \"\n",
      " \"--vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " \"sp.load('m.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " \"print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))\\n\"\n",
      " 'print(sp.decode_ids([460, 133, 774, 1276]))\\n'\n",
      " ' \\n'\n",
      " 'print(sp.get_piece_size())\\n'\n",
      " ' \\n'\n",
      " 'print(sp.id_to_piece(460))\\n'\n",
      " \"print(sp.piece_to_id('▁በአዲስ'))\\n\"\n",
      " ' \\n'\n",
      " \"print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\\n\"\n",
      " ' \\n'\n",
      " 'for id in range(3):\\n'\n",
      " '  print(sp.id_to_piece(id), sp.is_control(id))\\n'\n",
      " ' \\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " ' \\n'\n",
      " \"serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\\n\"\n",
      " ' \\n'\n",
      " 'sp = spm.SentencePieceProcessor()\\n'\n",
      " 'sp.load_from_serialized_proto(serialized_model_proto)\\n'\n",
      " ' \\n'\n",
      " \"print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\\n\"\n",
      " ' \\n'\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user \"\n",
      " \"--user_defined_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_user = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_user.load('m_user.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\\n\"\n",
      " \"print(sp_user.piece_to_id('<sep>'))  \\n\"\n",
      " \"print(sp_user.piece_to_id('<cls>'))  \\n\"\n",
      " \"print('3=', sp_user.decode_ids([3]))  \\n\"\n",
      " \"print('4=', sp_user.decode_ids([4]))  \\n\"\n",
      " \"spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_ctrl \"\n",
      " \"--control_symbols=<sep>,<cls> --vocab_size=2000')\\n\"\n",
      " ' \\n'\n",
      " 'sp_ctrl = spm.SentencePieceProcessor()\\n'\n",
      " \"sp_ctrl.load('m_ctrl.model')\\n\"\n",
      " ' \\n'\n",
      " \"print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\")\n"
     ]
    }
   ],
   "source": [
    "query = \"Please evaluate the supplied Python code, concentrating on the  python and sql code key words like how class are used, how many function and others related syntax.\"\n",
    "response = openai_model_answer(query, retrieved_documents)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullopenai_model_answer(query, retrieved_documents, model=\"gpt-4-turbo-preview\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"As an assistant, your role is to be helpful and provide answers in a concise and summary-style manner. Your objective is to draw from the available document and your knowledge to deliver accurate and concise information. If you encounter a question for which you lack the answer or cannot find the relevant information, it's best to respond with a brief summary or let the user know that the answer is not available. Avoid including specific code examples and instead focus on offering assistance whenever possible. Your primary aim is to deliver clear and precise responses, ensuring that the information you provide is helpful to the user.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
