{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "allweek_data = pd.read_csv(\"../data_files/submission.csv\")\n",
    "allweek_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'submission_id', 'payload', 'rubric_id', 'rubric_type',\n",
       "       'next_action', 'request_origin', 'params', 'code_content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allweek_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 15, 32, 38, 44, 61, 65, 72])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allweek_data['rubric_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week1 (53, 9)\n",
      "week2 (40, 9)\n",
      "week3 (8, 9)\n",
      "week5 (36, 9)\n",
      "week6 (35, 9)\n",
      "week7 (31, 9)\n",
      "week9 (31, 9)\n",
      "week10 (32, 9)\n"
     ]
    }
   ],
   "source": [
    "rubric_id_10 = allweek_data[allweek_data[\"rubric_id\"] == 10].reset_index(drop=True)\n",
    "rubric_id_15 = allweek_data[allweek_data[\"rubric_id\"] == 15].reset_index(drop=True)\n",
    "rubric_id_32 = allweek_data[allweek_data[\"rubric_id\"] == 32].reset_index(drop=True)\n",
    "rubric_id_38 = allweek_data[allweek_data[\"rubric_id\"] == 38].reset_index(drop=True)\n",
    "rubric_id_44 = allweek_data[allweek_data[\"rubric_id\"] == 44].reset_index(drop=True)\n",
    "rubric_id_61 = allweek_data[allweek_data[\"rubric_id\"] == 61].reset_index(drop=True)\n",
    "rubric_id_65 = allweek_data[allweek_data[\"rubric_id\"] == 65].reset_index(drop=True)\n",
    "rubric_id_72 = allweek_data[allweek_data[\"rubric_id\"] == 72].reset_index(drop=True)\n",
    "# Print the resulting rows\n",
    "print(\"week1\",rubric_id_10.shape) #telecom dataset\n",
    "print(\"week2\",rubric_id_15.shape) #dropped_airflowdb\n",
    "print(\"week3\",rubric_id_32.shape) #dropped_fewfiles\n",
    "print(\"week5\",rubric_id_38.shape) #algorand\n",
    "print(\"week6\",rubric_id_44.shape) #rag\n",
    "print(\"week7\",rubric_id_61.shape) #amharic finetuning\n",
    "print(\"week9\",rubric_id_65.shape) #hardhat\n",
    "print(\"week10\",rubric_id_72.shape) #image generation\n",
    "#ruburic_id_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week1 (53, 9)\n",
      "week5 (36, 9)\n",
      "week6 (35, 9)\n",
      "week7 (31, 9)\n",
      "week9 (31, 9)\n",
      "week10 (32, 9)\n"
     ]
    }
   ],
   "source": [
    "week1_python = allweek_data[allweek_data[\"rubric_id\"] == 10].reset_index(drop=True)\n",
    "# rubric_id_15 = data[data[\"rubric_id\"] == 15].reset_index(drop=True)\n",
    "# rubric_id_32 = data[data[\"rubric_id\"] == 32].reset_index(drop=True)\n",
    "week5_algo = allweek_data[allweek_data[\"rubric_id\"] == 38].reset_index(drop=True)\n",
    "week6_rag = allweek_data[allweek_data[\"rubric_id\"] == 44].reset_index(drop=True)\n",
    "week7_finetune = allweek_data[allweek_data[\"rubric_id\"] == 61].reset_index(drop=True)\n",
    "week9_eth = allweek_data[allweek_data[\"rubric_id\"] == 65].reset_index(drop=True)\n",
    "week10_img_generate = allweek_data[allweek_data[\"rubric_id\"] == 72].reset_index(drop=True)\n",
    "# Print the resulting rows\n",
    "print(\"week1\",week1_python.shape) #telecom dataset\n",
    "# print(\"week2\",rubric_id_15.shape) #dropped_airflowdb\n",
    "# print(\"week3\",rubric_id_32.shape) #dropped_fewfiles\n",
    "print(\"week5\",week5_algo.shape) #algorand\n",
    "print(\"week6\",week6_rag.shape) #rag\n",
    "print(\"week7\",week7_finetune.shape) #amharic finetuning\n",
    "print(\"week9\",week9_eth.shape) #hardhat\n",
    "print(\"week10\",week10_img_generate.shape) #image generation\n",
    "#ruburic_id_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week1_python.to_csv('../data_files/week1_python.csv', index=False)\n",
    "# week5_algo.to_csv('../data_files/week5_algo.csv', index=False)\n",
    "# week6_rag.to_csv('../data_files/week6_rag.csv', index=False)\n",
    "# week7_finetune.to_csv('../data_files/week7_finetune.csv', index=False)\n",
    "# week9_eth.to_csv('../data_files/week9_eth.csv', index=False)\n",
    "# week10_img_generate.to_csv('../data_files/week10_img_generate.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Uneccessary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace np.nan values with an empty string\n",
    "# week6_rag['code_content'].fillna('', inplace=True)\n",
    "def drop_column(df):\n",
    "    # Drop a column\n",
    "    columns_to_drop = ['submission_id','payload', 'rubric_id', 'rubric_type', 'next_action', 'request_origin', 'params']\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    print(\"DataFrame after dropping unwanted column\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_packages(text):\n",
    "    patterns = [\n",
    "    r\"### BEGIN File: \\w+\\.txt ###.*?END File: \\w+\\.txt ###\",\n",
    "    r\"### BEGIN File: (\\w+/\\w+.txt) ###.*?END File: \\1 ###\",\n",
    "    r\"### BEGIN File: (\\w+/\\w+.txt) ###.*?END File: (\\w+/\\w+.txt) ###\",\n",
    "    r\"### BEGIN File: ([\\w./-]+\\.txt) ###.*?END File: ([\\w./-]+\\.txt) ###\",\n",
    "    r\"### BEGIN File: (\\w+/\\w+.pgerd) ###.*?END File: (\\w+/\\w+.pgerd) ###\",\n",
    "    r\"### BEGIN File: \\w+\\.lock ###.*?END File: \\w+\\.lock ###\",\n",
    "    r\"### BEGIN File: \\w+\\.yml ###.*?END File: \\w+\\.yml ###\",\n",
    "    r\"### BEGIN File: \\w+\\.yaml ###.*?END File: \\w+\\.yaml ###\",\n",
    "    r\"### BEGIN File: \\w+\\.toml ###.*?END File: \\w+\\.toml ###\",\n",
    "    r\"### BEGIN File: \\w+\\.log ###.*?END File: \\w+\\.log ###\",\n",
    "    #r\"### BEGIN File: model_schema\\/model_schema_design\\.pgerd ###.*?END File: model_schema\\/model_schema_design\\.pgerd ###\",\n",
    "    r\"### BEGIN File: package\\.json ###.*?END File: package\\.json ###\",\n",
    "    r\"### BEGIN File: setup\\.py ###.*?END File: setup\\.py ###\",\n",
    "    r\"### BEGIN File: tailwind.config\\.js ###.*?END File: tailwind.config\\.js ###\",\n",
    "    r\"### BEGIN File: tailwind.config\\.ts ###.*?END File: tailwind.config\\.ts ###\",\n",
    "    r\"### BEGIN File: Dockerfile ###.*?END File: Dockerfile ###\",\n",
    "    ]\n",
    "    cleaned_text = re.sub(\"|\".join(patterns), '', text, flags=re.DOTALL | re.MULTILINE)\n",
    "    return cleaned_text\n",
    "#textt = clean_packages(data['code_content'][151])\n",
    "# week6_rag['code_content'] = week6_rag['code_content'].apply(clean_packages)\n",
    "\n",
    "def clean_content(text):\n",
    "    pattern = r\"### BEGIN File: README\\.md ###.*?END File: README\\.md ###\"\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "#data['code_content'][182] = clean_content(data['code_content'][182])\n",
    "# Assuming your DataFrame is named 'data' and the column containing the text is named 'code_content'\n",
    "# week6_rag['code_content'] = week6_rag['code_content'].apply(clean_content)\n",
    "\n",
    "def clean_License(text):\n",
    "    pattern = r'BEGIN File: LICENSE.*?END File: LICENSE'\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "# data['code_content'][182] = clean_License(data['code_content'][182])\n",
    "# Assuming your DataFrame is named 'data' and the column containing the text is named 'code_content'\n",
    "# week6_rag['code_content'] = week6_rag['code_content'].apply(clean_License)\n",
    "\n",
    "def remove_comments(text):\n",
    "    pattern = r'#.*?\\n'\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "# data['code_content'][182] = remove_comments(data['code_content'][182])\n",
    "# Assuming your DataFrame is named 'data' and the column containing the text is named 'code_content'\n",
    "# week6_rag['code_content'] = week6_rag['code_content'].apply(remove_comments)\n",
    "\n",
    "def clean_perged_files(text):\n",
    "    pattern = r'### BEGIN File:.*?model_schema/model_schema_design\\.pgerd.*?### END File'\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "# data['code_content'][182] = clean_perged_files(data['code_content'][182])\n",
    "# Assuming your DataFrame is named 'data' and the column containing the text is named 'code_content'\n",
    "# week6_rag['code_content'] = week6_rag['code_content'].apply(clean_perged_files)\n",
    "\n",
    "def extract_content(text):\n",
    "    pattern = r'Content:\\s*(.*?)(?=\\s*Commit History:)'\n",
    "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
    "    if matches:\n",
    "        return '\\n'.join(matches)\n",
    "    else:\n",
    "        return text\n",
    "# data['code_content'][182] = extract_content(data['code_content'][182])\n",
    "# Assuming your DataFrame is named 'df' and the column containing the text is named 'text_column'\n",
    "# week6_rag['code_content'] = week6_rag['code_content'].apply(extract_content)\n",
    "\n",
    "def clean_installs(text):\n",
    "    pattern = r\"pip install \\w+\\n\"\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "# data['code_content'][182] = clean_perged_files(data['code_content'][182])\n",
    "# Assuming your DataFrame is named 'data' and the column containing the text is named 'code_content'\n",
    "# week6_rag['code_content'] = week6_rag['code_content'].apply(clean_installs)\n",
    "\n",
    "def remove_extra_spaces(strings):\n",
    "    updated_strings = []\n",
    "    pattern = r\"(?<!\\S)\\s+(?!\\S)\"\n",
    "\n",
    "    for string in strings:\n",
    "        updated_string = re.sub(pattern, \" \", string)\n",
    "        updated_strings.append(updated_string)\n",
    "\n",
    "    return updated_strings\n",
    "\n",
    "def remove_extra_newlines(strings):\n",
    "    updated_strings = []\n",
    "    pattern = r\"\\n+\"\n",
    "\n",
    "    for string in strings:\n",
    "        updated_string = re.sub(pattern, \"\\n\", string.strip())\n",
    "        updated_strings.append(updated_string)\n",
    "\n",
    "    return updated_strings\n",
    "\n",
    "def extract(text):\n",
    "    pattern = r'Content:\\s*(.*?)$'\n",
    "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
    "    if matches:\n",
    "        return '\\n'.join(matches)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_emptycontent_rows(df):\n",
    "    # Iterate over the rows\n",
    "    for index, row in df.iterrows():\n",
    "        if row['code_content'].startswith('Repository Structure:'):\n",
    "            # Drop the row with the specified index\n",
    "            df = df.drop(index)\n",
    "    # Reset the index of the DataFrame\n",
    "    data = df.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def checking_for_empty_value(df):\n",
    "    column_name = 'code_content'\n",
    "\n",
    "    # Check if the column contains empty strings\n",
    "    has_empty_values = any(df[column_name] == '')\n",
    "\n",
    "    if has_empty_values:\n",
    "        return \"The column contains empty string values.\"\n",
    "    else:\n",
    "        return \"The column does not contain empty string values.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract basic sample form week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "file_path =\"../data_files/week7_finetune.csv\"\n",
    "data = read_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code_content\n",
       "False    21\n",
       "True     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['code_content'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop none rows from code_content column\n",
    "data.dropna(subset=['code_content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code_content\n",
       "False    21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['code_content'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after dropping unwanted column\n"
     ]
    }
   ],
   "source": [
    "data = drop_column(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['code_content'] = data['code_content'].apply(clean_packages)\n",
    "data['code_content'] = data['code_content'].apply(clean_content)\n",
    "data['code_content'] = data['code_content'].apply(clean_License)\n",
    "#data['code_content'] = data['code_content'].apply(remove_comments)\n",
    "data['code_content'] = data['code_content'].apply(clean_perged_files)\n",
    "data['code_content'] = data['code_content'].apply(extract_content)\n",
    "data['code_content'] = data['code_content'].apply(clean_installs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>code_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>617</td>\n",
       "      <td>import unittest\\nimport sys, os\\nsys.path.appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>746</td>\n",
       "      <td>## #### Import modules\\n%import sys, os, json,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>642</td>\n",
       "      <td>import unittest\\nimport sys, os\\nsys.path.appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>659</td>\n",
       "      <td>## ### Remove:\\n\\n## * Null values, new_line(\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>603</td>\n",
       "      <td>## ### Remove:\\n\\n## * Null values, new_line(\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>705</td>\n",
       "      <td>import json\\nimport re\\n\\n\\nclass Util:\\n    d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>682</td>\n",
       "      <td>! pip install transformers bitsandbytes peft t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>585</td>\n",
       "      <td>## ### Remove:\\n\\n## * Null values, new_line(\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>763</td>\n",
       "      <td>## ### Remove:\\n\\n## * Null values, new_line(\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id                                       code_content\n",
       "10      617  import unittest\\nimport sys, os\\nsys.path.appe...\n",
       "12      746  ## #### Import modules\\n%import sys, os, json,...\n",
       "13      642  import unittest\\nimport sys, os\\nsys.path.appe...\n",
       "15      659  ## ### Remove:\\n\\n## * Null values, new_line(\"...\n",
       "16      603  ## ### Remove:\\n\\n## * Null values, new_line(\"...\n",
       "17      705  import json\\nimport re\\n\\n\\nclass Util:\\n    d...\n",
       "18      682  ! pip install transformers bitsandbytes peft t...\n",
       "19      585  ## ### Remove:\\n\\n## * Null values, new_line(\"...\n",
       "20      763  ## ### Remove:\\n\\n## * Null values, new_line(\"..."
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 2)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop a row from the dataframe\n",
    "# data = data.drop(8)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['code_content'][10] = extract(data['code_content'][10])\n",
    "data['code_content'] = data['code_content'].apply(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['code_content'] = remove_extra_spaces(data['code_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_emptycontent_rows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The column does not contain empty string values.'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checking_for_empty_value(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 2)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# output_directory = '../content/'\n",
    "# os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# # Iterate over each row in the DataFrame\n",
    "# for index, row in data.iterrows():\n",
    "#     # Get the code content and create the file path\n",
    "#     code_content = row['code_content']\n",
    "#     file_path = os.path.join(output_directory, f'file_{index}.txt')\n",
    "\n",
    "#     # Write the code content to a text file\n",
    "#     with open(file_path, 'w') as file:\n",
    "#         file.write(code_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email</th>\n",
       "      <th>User ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abrhamaddis32@gmail.com</td>\n",
       "      <td>619</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>merronmuche@gmail.com</td>\n",
       "      <td>766</td>\n",
       "      <td>Basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>jaraguta03@gmail.com</td>\n",
       "      <td>665</td>\n",
       "      <td>Basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>abelbekele.addise@gmail.com</td>\n",
       "      <td>606</td>\n",
       "      <td>Advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>amitchew@gmail.com</td>\n",
       "      <td>604</td>\n",
       "      <td>Basic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        Email  User ID         label\n",
       "0           0      abrhamaddis32@gmail.com      619  Intermediate\n",
       "1           1        merronmuche@gmail.com      766         Basic\n",
       "2           2         jaraguta03@gmail.com      665         Basic\n",
       "3           3  abelbekele.addise@gmail.com      606      Advanced\n",
       "4           4           amitchew@gmail.com      604         Basic"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data = pd.read_csv(\"../data_files/email_id_files.csv\")\n",
    "user_data = pd.DataFrame(user_data)\n",
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = user_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.rename(columns={'User ID': 'user_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames based on matching user_id values\n",
    "combined_data = pd.merge(data, user_data, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>code_content</th>\n",
       "      <th>Email</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>import json\\nimport re\\n \\nclass Util():\\n  de...</td>\n",
       "      <td>abrhamaddis32@gmail.com</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>766</td>\n",
       "      <td>import json\\nimport re\\n \\nclass Util:\\n  def ...</td>\n",
       "      <td>merronmuche@gmail.com</td>\n",
       "      <td>Basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>761</td>\n",
       "      <td>! pip install transformers bitsandbytes peft t...</td>\n",
       "      <td>lillian.alehegn@aait.edu.et</td>\n",
       "      <td>Basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>765</td>\n",
       "      <td>## ### Remove:\\n\\n## * Null values, new_line(\"...</td>\n",
       "      <td>mtadesse813@gmail.com</td>\n",
       "      <td>Basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>652</td>\n",
       "      <td>! pip install transformers bitsandbytes peft t...</td>\n",
       "      <td>fanuelabebe@gmail.com</td>\n",
       "      <td>Advanced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                       code_content  \\\n",
       "0      619  import json\\nimport re\\n \\nclass Util():\\n  de...   \n",
       "1      766  import json\\nimport re\\n \\nclass Util:\\n  def ...   \n",
       "2      761  ! pip install transformers bitsandbytes peft t...   \n",
       "3      765  ## ### Remove:\\n\\n## * Null values, new_line(\"...   \n",
       "4      652  ! pip install transformers bitsandbytes peft t...   \n",
       "\n",
       "                         Email         label  \n",
       "0      abrhamaddis32@gmail.com  Intermediate  \n",
       "1        merronmuche@gmail.com         Basic  \n",
       "2  lillian.alehegn@aait.edu.et         Basic  \n",
       "3        mtadesse813@gmail.com         Basic  \n",
       "4        fanuelabebe@gmail.com      Advanced  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('../data_files/all_week_with_comment_labeled/all_week7_finetune.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
