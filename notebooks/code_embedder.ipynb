{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import json \n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "from collections import Counter\n",
    "\n",
    "# from __future__ import print_function\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data_clean.csv\")\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "env_vars = dotenv_values('.env')\n",
    "openai.api_key = env_vars.get('OPENAI_API_KEY')\n",
    "client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"function add(a, b) { return a + b; }\",\n",
    "model=\"text-embedding-ada-002\"\n",
    "\n",
    "def get_embedding(text, model):\n",
    "   #text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "get_embedding(input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "embeddings_2d = embed_model.embed_documents(\"function add(a, b) { return a + b; }\")\n",
    "len(embeddings_2d), len(embeddings_2d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.         0.53986259 0.3341897  0.55777129]\n",
      " [0.53986259 1.         0.37462465 0.43740911]\n",
      " [0.3341897  0.37462465 1.         0.31579159]\n",
      " [0.55777129 0.43740911 0.31579159 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Define a collection of code snippets\n",
    "code_snippets = [\n",
    "  \"\"\"\n",
    "    def factorial(n):\n",
    "    result = 1\n",
    "    for i in range(1, n + 1):\n",
    "        result *= i\n",
    "    return result\n",
    "  \"\"\",\n",
    "  \"\"\"\n",
    "  def fibonacci(n, memo={}):\n",
    "    if n in memo:\n",
    "        return memo[n]\n",
    "    if n <= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo)\n",
    "        memo[n] = result\n",
    "        return result\n",
    "  \"\"\",\n",
    "  \"\"\"\n",
    "    unsigned long long emi(unsigned int n) {\n",
    "      if (n == 0 || n == 1) {\n",
    "          return 1;\n",
    "      } else {\n",
    "          return n * emi(n - 1);\n",
    "      }\n",
    "    }\n",
    "  \"\"\",\n",
    "  \"\"\"\n",
    "    def looping(n):    \n",
    "          print(n) \n",
    "  \"\"\",\n",
    "]\n",
    "\n",
    "embeddings=[]\n",
    "for code in code_snippets:\n",
    "    response = client.embeddings.create(input=code, model=\"text-embedding-3-small\")\n",
    "    embedding = response.data[0].embedding  # Access the 'embedding' attribute within the 'data' attribute\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Generate embeddings for each code snippet\n",
    "# embeddings = [client.embeddings.create(input=code, model=\"text-embedding-ada-002\")['embedding'] for code in code_snippets]\n",
    "\n",
    "# Convert the embeddings to a format suitable for cosine similarity\n",
    "embeddings_matrix = [embedding for embedding in embeddings]\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Print the similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "# data['code_embedding'] = data['code_content'].apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
    "\n",
    "# def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
    "#    embedding = get_embedding(code_query, model='text-embedding-3-small')\n",
    "#    df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "#    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "#    return res\n",
    "# res = search_functions(data, 'Completions API tests', n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<00:00, 13.4kB/s]\n",
      "config.json: 100%|██████████| 498/498 [00:00<00:00, 1.40MB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:04<00:00, 212kB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:01<00:00, 283kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 150/150 [00:00<00:00, 568kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 499M/499M [02:29<00:00, 3.34MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Snippet: def factorial(n):\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "Embedding: tensor([-2.7771e-01,  1.5763e-01,  2.9554e-01, -1.4439e-02, -2.2769e-01,\n",
      "        -7.4887e-01, -3.6330e-02,  1.8782e-01,  5.6236e-01,  3.8150e-01,\n",
      "        -2.9504e-01,  5.7466e-01, -3.3646e-01,  6.0460e-02,  7.8014e-01,\n",
      "        -2.2050e-01,  2.0446e-01,  2.4504e-01, -8.1589e-02,  4.3083e-01,\n",
      "        -1.5995e-01, -4.3422e-01,  5.6799e-01, -1.8546e-01,  5.5998e-01,\n",
      "         3.4823e-01, -3.2295e-02,  8.2829e-01, -4.7591e-01,  5.8467e-01,\n",
      "        -1.3403e-01, -1.9524e-02,  1.4167e+00,  1.7739e-01,  3.5241e-01,\n",
      "        -2.5652e-01, -3.4333e-01,  2.7224e-01,  2.0292e-01, -2.4290e-01,\n",
      "        -5.0367e-02,  7.0001e-01, -9.2676e-01, -2.6064e-01,  5.5162e-01,\n",
      "         2.3583e-01,  4.3142e-01,  1.0256e-01, -3.9705e-02,  4.6246e-01,\n",
      "         5.4635e-01,  2.8289e-01, -4.7521e-01, -2.1048e-01,  4.3169e-01,\n",
      "         4.9456e-01, -1.0646e+00, -7.2688e-01, -1.3995e-01, -4.6614e-01,\n",
      "        -1.1544e-01, -5.7775e-01, -2.4178e-01, -1.3490e-01,  1.2202e+00,\n",
      "         1.8436e-01,  4.4942e-01,  7.0245e-01, -1.1733e-01,  3.1560e-01,\n",
      "        -1.2647e-01, -4.4317e-01, -7.8812e-03, -5.9655e-01, -7.3452e-01,\n",
      "         7.2372e-01, -2.3006e-01, -9.0895e+00, -1.4940e-02,  4.2227e-01,\n",
      "         3.5349e-01, -4.8109e-01,  1.2406e+00,  8.4637e-01, -5.4802e-01,\n",
      "         1.2661e-01,  3.4765e-02,  1.0474e-01, -5.5207e-01,  5.4786e-02,\n",
      "         3.2785e-01, -8.9257e-02,  4.4992e-01,  6.2726e-01, -2.2522e-01,\n",
      "         9.1673e-01,  5.8525e-01, -2.0377e-01,  1.0218e-02, -4.2714e-01,\n",
      "         3.0138e-02, -6.1555e-01,  3.2452e-01,  5.3409e-01,  2.0074e-01,\n",
      "        -4.9272e-01,  2.9573e-01, -6.6512e-01,  4.9446e-01, -2.6070e-01,\n",
      "        -5.6426e-02, -4.1784e-01,  8.2299e-01,  2.0595e-01,  2.4258e-01,\n",
      "        -2.6380e-01,  2.1651e-01,  3.7560e-01, -1.3872e-01,  2.2304e-01,\n",
      "        -7.8196e-01,  3.3682e-01, -3.9482e-01,  8.6448e-01, -1.9388e-01,\n",
      "         2.8273e-01, -1.9847e-01,  1.2126e-01,  3.5556e-02,  1.9203e-01,\n",
      "        -1.0062e+00, -6.3364e-01, -3.6485e-01,  4.7904e-01,  5.2829e-01,\n",
      "        -5.4789e-01,  3.2805e-01,  1.3898e-01, -1.9227e-01,  2.9902e-01,\n",
      "        -4.8842e-01, -6.1575e-01, -2.7932e-01,  1.9758e-01,  1.0892e+00,\n",
      "        -8.7863e-02, -1.0678e-01,  4.3761e-01,  1.2324e-02, -4.3722e-01,\n",
      "        -7.1780e-01,  1.4930e-01,  1.2157e+00, -7.6278e-02,  9.8478e-02,\n",
      "        -1.8707e+00,  1.2325e-01,  1.6465e-01,  3.5895e-01, -1.8284e-01,\n",
      "         5.6869e-01, -1.8606e-01,  6.4122e-02,  2.9714e-01,  5.4717e-01,\n",
      "         1.4004e-01, -2.8557e-01, -2.1109e-02,  4.2382e-01,  8.8430e-01,\n",
      "        -4.1697e-01, -5.1676e-01, -5.7754e-01, -5.4955e-02,  3.6460e-01,\n",
      "         7.5084e-01,  4.0618e-01, -3.9068e-04, -2.6694e-01,  1.0413e+00,\n",
      "         5.0394e-01, -3.0148e-01,  2.7148e-01, -6.3345e-01, -4.3464e-01,\n",
      "         3.2653e-01, -3.0156e-01,  7.4526e-01, -2.0819e-01, -1.0930e-02,\n",
      "        -3.5555e-01, -4.8773e-01,  2.6697e-01,  3.7918e-01,  8.5840e-02,\n",
      "         1.4039e-02,  1.4752e-01,  3.5171e-01,  1.0904e+00,  1.9494e-01,\n",
      "        -3.0437e-01,  8.4771e-01,  4.7102e-01,  4.4346e-01,  1.2005e-01,\n",
      "         1.8047e-01, -4.0007e-01,  5.4156e-01,  6.9690e-01,  1.2219e+00,\n",
      "         1.7236e+00,  5.9442e-03,  2.6669e-01, -2.0804e-01, -1.0457e+00,\n",
      "         7.2203e-02, -6.7318e-01, -1.6061e+00, -5.2900e-01, -2.6786e-01,\n",
      "        -1.2554e+00,  2.9647e-01, -2.7809e-01, -2.3685e-01, -2.1253e-01,\n",
      "        -7.9915e-02,  3.5991e-01, -2.8381e-01, -2.9329e-01,  2.4026e-01,\n",
      "        -3.3547e-01, -2.6505e-01, -9.9487e-01, -2.9788e-01,  7.2001e-02,\n",
      "        -3.7186e-01, -6.2395e-01,  4.4200e-01,  3.1451e-01, -9.6206e-01,\n",
      "        -4.5937e-01,  2.5394e-01,  6.6282e-01,  1.0663e+00,  4.3915e-03,\n",
      "        -6.3456e-01,  2.9452e-01,  7.8271e-01,  6.8524e-01,  6.4591e-01,\n",
      "         3.3070e-01,  6.0548e-01,  3.0466e-01, -2.8438e-01,  8.5383e-02,\n",
      "        -4.0841e-01, -2.3725e-01, -4.1800e-01,  6.0786e-01,  2.4633e+00,\n",
      "         3.2128e-01, -9.8469e-03,  7.1049e-01, -6.9267e-01,  3.5826e-01,\n",
      "         1.6093e-01,  2.9745e-01,  4.2775e-01,  6.7011e-01,  2.4791e-01,\n",
      "         1.3891e+00,  3.6593e-01, -3.4090e-01,  1.5327e-01,  1.9008e-01,\n",
      "         2.3979e-01,  2.4941e-01,  1.0576e-01, -1.0314e+00, -2.6144e-01,\n",
      "        -5.9138e-01, -4.9894e-01,  4.8171e-01, -5.9408e-02, -1.5590e-01,\n",
      "        -2.7969e-01, -1.4779e-01,  6.7185e-01, -7.2581e-02, -1.9563e-01,\n",
      "         2.6995e-01, -4.8714e-01,  1.0245e+00,  2.2782e-01, -8.9908e-02,\n",
      "         3.4049e-01,  4.3070e-01,  4.2914e-01,  3.2739e-01, -5.1654e-01,\n",
      "         3.7543e-01,  5.5839e-01, -3.8615e-02,  1.1384e-01, -1.8070e-01,\n",
      "        -2.8003e-01, -1.9845e-01,  2.5866e-01,  3.9947e-01, -3.4904e-01,\n",
      "         3.3462e-01, -8.5488e-01, -2.3926e-01, -9.7620e-02,  2.9795e-01,\n",
      "        -2.0992e-01,  2.5021e-02,  2.6507e-01, -3.2670e-01,  2.1304e-01,\n",
      "         5.0688e-01, -6.5505e-02,  9.4753e-01, -1.0890e+00,  4.9345e-01,\n",
      "         8.0629e-01, -8.0036e-01, -5.4234e-01, -1.3802e+00, -1.2330e-01,\n",
      "        -1.1139e+00,  1.4175e+00,  5.6419e-01,  1.4912e+00, -5.9604e-01,\n",
      "        -3.3572e-01,  2.6544e-01, -4.7282e-01,  1.3515e-01, -4.7483e-01,\n",
      "        -1.0695e+00,  6.3152e-01,  3.5711e-01, -2.9847e-01,  4.1191e-01,\n",
      "         1.0603e+00, -3.2348e-01, -1.3662e-01,  1.0742e+00,  2.4466e-01,\n",
      "        -3.6762e-01, -8.0402e-01, -2.3461e-01, -2.7209e-01,  4.5828e-01,\n",
      "         1.9522e+00,  5.5903e-01,  1.6272e-01,  7.4364e-02, -4.4534e-01,\n",
      "        -1.9904e-01, -3.1381e-01,  4.1322e-01,  1.8593e+00,  5.1699e-01,\n",
      "        -2.2312e-01, -5.5593e-01,  2.7230e-01, -2.7594e-01,  3.7698e-01,\n",
      "         4.7639e-02,  2.9034e-02,  4.9052e-02,  5.9225e-01,  1.2164e-01,\n",
      "        -1.4014e-01,  3.6896e-01, -2.8110e-01,  2.4048e-01, -6.8196e-02,\n",
      "        -6.3084e-01,  1.6795e-01,  2.2686e-01, -3.3838e-02,  4.0705e-01,\n",
      "        -1.9290e+00,  5.5930e-01, -3.8204e-01,  1.4370e+00, -1.7558e-01,\n",
      "         1.6714e-01,  3.3921e-01,  2.0974e-01, -4.4868e-01, -3.6757e-01,\n",
      "         3.4461e-01,  2.9257e-01, -1.3499e-01, -1.2416e-01, -4.7620e-01,\n",
      "        -3.9208e-01,  1.2896e-01, -1.4421e-01, -2.3921e-01, -3.4628e-01,\n",
      "        -2.7821e-02, -3.4353e-01, -2.5169e-01, -5.7542e-01,  7.4048e-01,\n",
      "        -4.0107e-01,  1.4589e+00, -2.0712e-01, -2.9161e-02,  2.7654e-02,\n",
      "         3.4811e-01,  4.5896e-01, -9.5203e-01,  3.1428e-01,  4.0210e-01,\n",
      "         2.6514e-01,  1.2525e-01,  4.9927e-01, -4.0537e-01,  9.4993e-01,\n",
      "        -3.5059e-01,  2.1115e-01,  2.2576e-02, -8.0218e-01, -4.8720e-01,\n",
      "        -4.9171e-01,  2.6592e-01, -3.7862e-03,  1.5206e-01, -5.7816e-01,\n",
      "        -3.0444e-01, -5.7631e-01, -3.1681e-01,  3.5382e-01,  4.5734e-01,\n",
      "         6.1150e-01,  3.2934e-01,  4.7816e-01,  1.1902e-01,  1.1195e-01,\n",
      "         4.2863e-01, -5.3996e-01,  1.4226e+00,  3.7693e-01, -7.6196e-01,\n",
      "         2.5379e-01,  1.1038e-02,  4.4457e-01, -1.0273e+00,  4.8127e-01,\n",
      "        -5.0815e-01,  1.6804e-01,  2.3897e-02,  5.3605e-01,  4.4559e-01,\n",
      "        -1.2862e-01,  1.0372e-01,  4.9282e-01, -1.6609e-01,  4.6105e-01,\n",
      "        -1.3581e+00,  4.6927e-01, -2.7039e-02,  6.7829e-01,  1.7400e-01,\n",
      "        -7.1846e-01,  1.5007e-01, -2.6772e-01, -5.7106e-01, -1.8229e-01,\n",
      "        -1.0637e-01, -2.3747e-01,  1.7966e+00,  4.2783e-01,  1.4650e+00,\n",
      "        -1.3176e-01, -4.1822e-01, -2.3752e-01, -7.2293e-01,  6.2078e-01,\n",
      "         2.5609e-01, -3.5370e-02, -3.8762e-01,  5.7544e-02, -5.2382e-01,\n",
      "        -4.0392e-01, -4.3631e-01, -6.0128e-01,  7.2970e-01, -6.3839e-01,\n",
      "         1.0807e-01, -5.3916e-01,  7.8495e-01, -7.5307e-01,  6.0654e-01,\n",
      "         1.6789e-02,  2.0932e-01,  7.2544e-01,  1.1414e+00, -1.8635e-01,\n",
      "         6.6701e-01, -3.6162e-01,  9.7993e-01,  4.4397e-01,  7.6800e-02,\n",
      "        -2.2688e-01,  5.8911e-01,  1.2327e+00,  8.4518e-02, -7.2097e-02,\n",
      "         3.5509e-01, -4.6030e-01, -5.8085e-01, -8.7105e-01,  2.2352e+00,\n",
      "         6.8822e-01,  1.9582e-01, -1.3614e-01,  1.7421e-02,  1.6783e+00,\n",
      "        -1.6214e-01, -6.4247e-01, -9.8634e-01,  1.9217e-01, -6.0076e-01,\n",
      "        -2.4595e-01, -1.2333e-01,  4.9589e-01,  4.0808e-03,  1.0864e-01,\n",
      "        -3.6542e-01, -2.8331e-01,  1.7847e-01, -1.7668e-01,  5.2332e-01,\n",
      "        -4.7303e-01, -2.1339e-01,  1.6914e+00,  2.6062e-01,  3.7736e-01,\n",
      "        -3.2903e-01, -3.7527e-01, -1.4176e+00, -8.7858e-02, -2.0083e-01,\n",
      "         5.8110e-01,  3.8004e-02, -3.8328e-01, -4.9171e-01,  2.7518e-01,\n",
      "        -4.3562e-02, -1.1375e-01,  3.3039e-01, -2.0640e-01, -1.6463e-01,\n",
      "        -5.4064e-01,  1.0799e+00,  2.7611e-01, -3.8186e-01,  5.5114e-01,\n",
      "         2.0453e-01, -1.1626e-02, -1.9482e-01,  4.3099e-01, -2.5325e-01,\n",
      "        -1.2288e+00,  3.4413e-01,  3.2862e-01, -2.5032e-01,  2.1882e-01,\n",
      "        -1.3480e-01,  8.9631e-01, -4.4156e-01, -2.5241e-01, -1.3447e-01,\n",
      "         5.1278e-01,  1.4516e+00,  5.6699e-01,  3.9863e-01, -2.7369e-01,\n",
      "         8.4784e-01,  3.6295e-01,  5.4070e-01,  5.5860e+00, -3.7204e-01,\n",
      "         1.1839e+00,  4.4057e-01,  2.7494e-01, -1.1404e-01, -1.1085e+00,\n",
      "        -3.1628e-01, -2.3313e-02, -1.1043e-01, -3.1243e-01,  7.2804e-01,\n",
      "        -2.1010e-01, -4.5214e-02,  4.8468e-01, -7.0550e-01, -4.6823e-01,\n",
      "         2.9716e-01, -1.4632e-02, -2.3938e-01,  6.0772e-02,  7.7307e-02,\n",
      "         4.4137e-01, -1.9305e-01,  3.7212e-01,  1.5693e-02,  4.0534e-01,\n",
      "         4.1069e-01, -3.8887e-01, -2.0420e-01, -1.9502e-01,  2.2840e-01,\n",
      "        -2.9106e-01, -3.9180e-01, -4.8700e-01,  1.7528e+00, -1.0508e-01,\n",
      "        -5.2206e-01, -1.4058e-01, -1.2645e+00,  3.2740e-01, -3.2167e-01,\n",
      "         2.8059e-01, -4.4613e-01, -1.0208e-01, -1.7289e-01, -5.9601e-01,\n",
      "         5.0840e-01, -1.9310e-01, -2.8642e-01,  2.2954e-01,  7.4852e-01,\n",
      "        -3.6005e-01,  4.9321e-02,  3.7250e-01, -1.0688e-01, -9.7044e-01,\n",
      "        -6.7187e-01, -5.6845e-01, -1.7085e-01,  1.3704e-01,  5.1609e-01,\n",
      "         2.0320e-01, -9.3297e-01, -1.8704e-01, -1.0023e-01, -1.8438e-01,\n",
      "        -2.8976e-01,  3.8966e-01,  4.6741e-01,  1.8317e+00,  8.8672e-01,\n",
      "         2.2843e-01, -2.0831e-01, -4.7911e-01, -5.6594e-01,  1.0785e+00,\n",
      "        -6.9382e-01,  8.9322e-02, -8.8120e-01, -4.3102e-01,  5.4050e-01,\n",
      "        -4.4447e-01, -5.3130e-01,  2.4479e-01,  4.2577e-01,  7.9077e-01,\n",
      "         4.9663e-01,  1.4807e+00, -1.9052e-01,  2.0111e-01,  3.3286e-02,\n",
      "        -9.0340e-01,  3.4025e-02, -2.7193e-01, -5.3828e-01, -1.1311e+00,\n",
      "        -5.0657e-02,  6.3255e-01,  6.5773e-02,  6.8788e-01, -6.0718e-01,\n",
      "         2.1897e-01,  2.4234e-01,  1.4548e-02, -6.5879e-01,  6.5237e-01,\n",
      "        -2.5993e-01,  7.8346e-01,  1.5329e-01, -1.7463e-01, -2.5392e-01,\n",
      "         1.0426e-01,  3.4631e-01,  1.6465e+00, -4.2448e-01,  1.8556e-02,\n",
      "         6.0129e-01,  1.7069e-01, -8.6292e-02,  2.2861e-01,  1.1820e+00,\n",
      "        -1.9666e-01, -2.9463e-01,  2.9386e-01,  2.3312e-02, -2.0909e-01,\n",
      "        -1.1357e-01, -1.7108e-02,  6.8936e-01, -3.5022e-01,  3.7114e-01,\n",
      "         9.7309e-01,  1.2047e+00,  6.4466e-01, -2.5050e-01, -7.8151e-01,\n",
      "        -5.8987e-02,  6.1890e-01,  2.9128e-01, -7.7788e-02, -3.3332e-01,\n",
      "         3.9764e-01, -8.8528e-01, -9.4216e-01, -3.4500e-01,  7.1214e-01,\n",
      "         3.6431e-01, -5.7143e-02, -2.8408e-01, -3.4049e-01,  5.4375e-01,\n",
      "        -9.5523e-03,  4.0796e-01,  3.3946e-01, -3.6030e-01,  1.6685e-01,\n",
      "         1.2280e+00,  1.1045e-01, -5.5671e-01,  2.0616e-01, -1.4182e+00,\n",
      "         1.3136e-01, -4.0480e-01,  4.5116e-01,  3.5223e-01,  4.8403e-01,\n",
      "        -2.5821e-01,  4.9714e-01, -6.4169e-02,  4.3010e-01, -2.1185e-01,\n",
      "        -3.8228e-01,  5.5841e-01,  1.9710e-01,  1.0387e-01,  7.2754e-01,\n",
      "        -2.4480e-01, -3.8472e-01,  6.5048e-01])\n",
      "\n",
      "Code Snippet: def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
      "Embedding: tensor([-3.4161e-01,  1.5280e-01,  2.1255e-01, -2.9312e-03, -5.1721e-01,\n",
      "        -7.1108e-01, -1.3339e-01,  1.5582e-01,  6.2156e-01,  4.2254e-01,\n",
      "        -3.1645e-01,  8.1835e-01, -2.0056e-01, -1.5176e-01,  8.3777e-01,\n",
      "        -1.7627e-01,  1.3474e-01,  3.5670e-01, -4.2228e-02,  3.6208e-01,\n",
      "        -1.8197e-01, -3.4554e-01,  6.1880e-01, -3.4894e-01,  5.4795e-01,\n",
      "         3.7067e-01, -4.4333e-02,  9.1201e-01, -5.5441e-01,  7.2795e-01,\n",
      "        -2.5473e-01, -6.2549e-02,  1.4096e+00,  1.8612e-01,  3.7436e-01,\n",
      "        -2.8351e-01, -4.2933e-01,  1.8333e-01,  5.8856e-02, -2.7784e-01,\n",
      "        -3.1088e-02,  6.6387e-01, -9.9736e-01, -2.4981e-01,  5.1599e-01,\n",
      "         2.2166e-01,  4.0598e-01,  4.5089e-02,  9.9616e-03,  5.7714e-01,\n",
      "         5.1565e-01,  3.3916e-01, -5.2240e-01, -2.7706e-01,  4.4299e-01,\n",
      "         5.2644e-01, -1.0904e+00, -7.9311e-01, -1.6904e-01, -4.0192e-01,\n",
      "        -1.1711e-01, -3.9234e-01, -2.8358e-01, -7.6770e-02,  1.2508e+00,\n",
      "         2.3755e-01,  5.8093e-01,  5.9684e-01, -1.3255e-01,  2.7729e-01,\n",
      "        -1.3557e-01, -4.7916e-01, -1.7572e-01, -6.8119e-01, -7.5785e-01,\n",
      "         7.6770e-01, -2.2440e-01, -8.8114e+00,  7.1057e-02,  4.9341e-01,\n",
      "         3.5031e-01, -5.9699e-01,  1.2849e+00,  7.6000e-01, -5.4268e-01,\n",
      "         3.3629e-02,  4.2003e-02,  6.6286e-02, -5.7467e-01, -9.7553e-02,\n",
      "         3.5486e-01, -7.9989e-03,  5.4047e-01,  7.7605e-01, -2.3316e-01,\n",
      "         8.7742e-01,  6.1701e-01, -3.7054e-01,  8.9619e-03, -3.9045e-01,\n",
      "         8.6925e-02, -6.5580e-01,  3.2346e-01,  6.0423e-01,  3.3154e-01,\n",
      "        -5.9734e-01,  4.3954e-01, -7.1606e-01,  4.9551e-01, -1.9060e-01,\n",
      "         2.1598e-02, -3.9582e-01,  7.7515e-01,  1.5661e-01,  2.1821e-01,\n",
      "        -2.4479e-01,  2.3577e-01,  3.7426e-01, -5.6000e-02,  1.7727e-01,\n",
      "        -8.0850e-01,  3.5599e-01, -4.6076e-01,  8.0698e-01, -2.4402e-01,\n",
      "         3.4697e-01, -1.6762e-01,  6.6389e-02,  8.0945e-02,  1.8599e-01,\n",
      "        -1.0697e+00, -4.7914e-01, -3.7956e-01,  5.1205e-01,  5.4846e-01,\n",
      "        -5.4545e-01,  3.0638e-01,  9.9090e-02, -2.2944e-01,  3.0337e-01,\n",
      "        -5.4912e-01, -5.7839e-01, -1.8425e-01,  2.0296e-01,  9.6140e-01,\n",
      "        -8.9631e-02,  2.5027e-03,  5.2017e-01,  1.4675e-01, -4.6975e-01,\n",
      "        -7.8064e-01,  1.9987e-01,  1.2304e+00, -8.1559e-02,  2.9258e-02,\n",
      "        -2.0162e+00,  1.9498e-01,  3.4048e-01,  3.5371e-01, -2.9174e-01,\n",
      "         4.8539e-01, -1.5560e-02,  3.7535e-02,  4.7430e-01,  4.9230e-01,\n",
      "         1.0510e-01, -4.1016e-01,  4.8404e-03,  3.4472e-01,  9.0497e-01,\n",
      "        -3.9135e-01, -5.8911e-01, -5.5587e-01, -3.6617e-02,  2.4856e-01,\n",
      "         7.7407e-01,  3.6094e-01, -1.3964e-02, -2.9929e-01,  1.1144e+00,\n",
      "         6.0084e-01, -2.2387e-01,  2.1657e-01, -6.2241e-01, -3.7397e-01,\n",
      "         3.6387e-01, -2.9335e-01,  6.1768e-01, -9.1753e-02, -4.0626e-02,\n",
      "        -2.5750e-01, -4.3724e-01,  2.2876e-01,  5.2135e-01,  7.1142e-02,\n",
      "         3.0120e-02,  1.9375e-01,  2.9672e-01,  9.7871e-01,  1.3103e-01,\n",
      "        -3.4397e-01,  8.3555e-01,  4.9743e-01,  4.3776e-01,  4.3636e-03,\n",
      "         3.0997e-01, -4.0069e-01,  5.6410e-01,  5.8783e-01,  1.2513e+00,\n",
      "         1.7055e+00, -1.3111e-02,  1.8634e-01, -2.3736e-01, -8.6307e-01,\n",
      "         1.3573e-01, -7.5045e-01, -1.6650e+00, -5.0692e-01, -2.3201e-01,\n",
      "        -1.3288e+00,  2.4349e-01, -3.6406e-01, -2.0471e-01, -1.9032e-01,\n",
      "        -1.4634e-01,  3.5290e-01, -2.5320e-01, -3.1969e-01,  2.8820e-01,\n",
      "        -3.5656e-01, -2.6383e-01, -9.9320e-01, -2.4461e-01, -2.6366e-02,\n",
      "        -4.3127e-01, -6.1841e-01,  4.2295e-01,  3.6490e-01, -1.0286e+00,\n",
      "        -5.4111e-01,  3.0566e-01,  6.6062e-01,  1.0742e+00,  1.0421e-01,\n",
      "        -8.3827e-01,  2.4156e-01,  6.3420e-01,  7.1550e-01,  7.0710e-01,\n",
      "         3.2074e-01,  5.5041e-01,  3.3237e-01, -1.8915e-01,  5.4121e-02,\n",
      "        -4.3481e-01, -2.7781e-01, -4.8648e-01,  6.0069e-01,  2.0548e+00,\n",
      "         4.1637e-01, -2.4716e-02,  7.1850e-01, -7.3055e-01,  3.3582e-01,\n",
      "         9.1754e-02,  2.3647e-01,  4.0973e-01,  7.2856e-01,  3.2368e-01,\n",
      "         1.4682e+00,  4.0694e-01, -3.5353e-01,  2.2030e-01,  1.5899e-01,\n",
      "         2.3173e-01,  3.8651e-01,  7.7367e-02, -1.0947e+00, -2.6514e-01,\n",
      "        -5.2969e-01, -4.8587e-01,  4.6463e-01, -1.2370e-01, -1.7654e-01,\n",
      "        -3.6126e-01, -1.2814e-01,  6.1321e-01, -6.0818e-02, -1.9608e-01,\n",
      "         2.9831e-01, -4.8720e-01,  1.1425e+00,  1.8900e-01, -1.1324e-01,\n",
      "         3.3179e-01,  4.8846e-01,  4.1920e-01,  3.5657e-01, -6.1471e-01,\n",
      "         3.3608e-01,  6.1107e-01,  1.2185e-01,  1.3184e-01, -3.1667e-01,\n",
      "        -3.1800e-01, -3.1109e-01,  3.1170e-01,  3.1862e-01, -3.6492e-01,\n",
      "         3.8408e-01, -9.6715e-01, -1.3674e-01, -1.1019e-02,  3.3131e-01,\n",
      "        -1.4706e-01,  2.6354e-02,  3.5592e-01, -3.6623e-01,  3.0643e-01,\n",
      "         4.8566e-01, -1.3443e-01,  9.8897e-01, -1.0082e+00,  4.8229e-01,\n",
      "         8.4810e-01, -7.6495e-01, -6.6424e-01, -1.5025e+00, -9.6021e-02,\n",
      "        -1.0842e+00,  1.3563e+00,  5.8755e-01,  1.6519e+00, -7.3098e-01,\n",
      "        -4.4072e-01,  3.2790e-01, -5.0176e-01,  1.3023e-01, -4.9786e-01,\n",
      "        -1.0347e+00,  6.6422e-01,  3.9076e-01, -2.4633e-01,  3.5043e-01,\n",
      "         1.0599e+00, -3.5753e-01, -1.2462e-01,  1.2317e+00,  3.0910e-01,\n",
      "        -5.2280e-01, -8.5317e-01, -1.5250e-01, -2.3510e-01,  4.4380e-01,\n",
      "         1.9774e+00,  5.8151e-01,  1.0861e-01,  3.2139e-02, -4.6423e-01,\n",
      "        -1.4897e-01, -3.2144e-01,  3.5003e-01,  1.9261e+00,  6.1922e-01,\n",
      "        -1.7013e-01, -6.8806e-01,  2.4750e-01, -2.2577e-01,  2.9412e-01,\n",
      "         7.9348e-02, -9.4473e-02,  5.7189e-02,  6.9731e-01,  1.6267e-01,\n",
      "        -1.4301e-01,  3.7341e-01, -2.5590e-01,  2.4431e-01, -4.4833e-04,\n",
      "        -6.7412e-01,  2.5589e-01,  2.4304e-01,  1.7725e-03,  3.7533e-01,\n",
      "        -1.8733e+00,  6.0133e-01, -4.1099e-01,  1.5679e+00, -1.6838e-01,\n",
      "         2.3054e-01,  3.7374e-01,  1.9622e-01, -4.1491e-01, -3.1042e-01,\n",
      "         3.2333e-01,  2.8249e-01, -1.3070e-01, -2.1372e-01, -4.6319e-01,\n",
      "        -3.7116e-01,  2.0659e-01, -1.5384e-01, -2.4935e-01, -3.9838e-01,\n",
      "        -5.8410e-02, -3.0308e-01, -1.6223e-01, -5.5122e-01,  8.0699e-01,\n",
      "        -4.6587e-01,  1.5132e+00, -3.0000e-01, -4.7580e-02,  6.9394e-02,\n",
      "         3.4194e-01,  5.1175e-01, -8.9938e-01,  4.4209e-01,  4.1926e-01,\n",
      "         1.8262e-01,  1.6322e-01,  5.5403e-01, -4.7159e-01,  8.4781e-01,\n",
      "        -3.0381e-01,  2.1299e-01, -2.4315e-02, -6.7454e-01, -5.4877e-01,\n",
      "        -5.3424e-01,  1.5129e-01, -1.3161e-02,  8.3669e-02, -6.2492e-01,\n",
      "        -2.8126e-01, -6.1919e-01, -3.3726e-01,  3.7118e-01,  4.3016e-01,\n",
      "         6.7554e-01,  3.8438e-01,  5.3095e-01,  2.4577e-01,  1.0088e-01,\n",
      "         4.3058e-01, -5.2064e-01,  1.3843e+00,  4.6438e-01, -7.8087e-01,\n",
      "         2.5593e-01, -3.8449e-02,  3.9550e-01, -5.0730e-01,  4.5817e-01,\n",
      "        -3.7322e-01,  1.4570e-01, -1.5625e-03,  4.9734e-01,  4.4385e-01,\n",
      "        -1.7374e-01,  5.9859e-02,  4.5181e-01, -1.7108e-01,  4.9308e-01,\n",
      "        -1.4372e+00,  3.9998e-01, -1.1644e-01,  5.8467e-01,  1.7445e-01,\n",
      "        -8.3335e-01,  1.9311e-01, -2.2741e-01, -6.0797e-01, -2.2947e-01,\n",
      "        -4.5942e-02, -3.1234e-01,  1.8961e+00,  4.9084e-01,  1.6095e+00,\n",
      "        -1.7398e-01, -5.1586e-01, -2.5751e-01, -7.2136e-01,  6.7639e-01,\n",
      "         3.5682e-01, -5.5079e-02, -4.1055e-01,  5.4177e-02, -5.4348e-01,\n",
      "        -5.3298e-01, -3.3826e-01, -5.8935e-01,  7.2855e-01, -6.5329e-01,\n",
      "         8.0680e-02, -6.1484e-01,  8.0639e-01, -6.8871e-01,  5.8807e-01,\n",
      "         1.5401e-02,  2.0849e-01,  5.9586e-01,  1.2804e+00, -1.5927e-01,\n",
      "         7.2289e-01, -3.4043e-01,  1.0243e+00,  4.4179e-01,  9.2488e-02,\n",
      "        -1.2987e-01,  5.2783e-01,  1.2908e+00,  1.0100e-01, -6.3544e-02,\n",
      "         3.0309e-01, -4.1045e-01, -6.4281e-01, -7.7586e-01,  2.1881e+00,\n",
      "         7.1856e-01,  1.8142e-01, -2.2926e-01,  6.0983e-02,  1.6606e+00,\n",
      "        -1.2995e-01, -8.5057e-01, -1.0110e+00,  1.3688e-01, -5.3564e-01,\n",
      "        -2.8777e-01, -1.4882e-01,  5.2876e-01, -3.2650e-03,  2.5670e-01,\n",
      "        -3.1479e-01, -2.3903e-01,  1.8822e-01, -2.7580e-01,  4.8983e-01,\n",
      "        -5.8652e-01, -3.0879e-01,  1.6839e+00,  3.6028e-01,  3.4646e-01,\n",
      "        -2.2813e-01, -3.3655e-01, -1.5535e+00, -9.4403e-02, -2.6994e-01,\n",
      "         6.6252e-01, -1.4728e-01, -3.9783e-01, -5.1512e-01,  3.4038e-01,\n",
      "         1.6367e-02, -7.0387e-02,  3.8829e-01, -2.0181e-01, -1.0682e-02,\n",
      "        -5.4319e-01,  8.7009e-01,  2.2517e-01, -4.5387e-01,  5.3837e-01,\n",
      "         1.6728e-01, -6.7117e-02, -2.5123e-01,  3.8443e-01, -2.3665e-01,\n",
      "        -1.3919e+00,  4.3528e-01,  4.4461e-01, -3.3300e-01,  2.8509e-01,\n",
      "        -2.2125e-01,  8.2496e-01, -4.6218e-01, -1.7698e-01, -2.6998e-02,\n",
      "         5.0608e-01,  1.4918e+00,  4.4758e-01,  3.8456e-01, -2.4651e-01,\n",
      "         8.6650e-01,  3.0506e-01,  6.0289e-01,  5.1089e+00, -3.3424e-01,\n",
      "         1.2343e+00,  4.2371e-01,  2.8064e-01, -1.0903e-01, -1.1783e+00,\n",
      "        -3.1725e-01, -1.8075e-01, -1.3904e-01, -3.2706e-01,  7.2746e-01,\n",
      "        -3.2936e-01, -4.3660e-02,  4.5496e-01, -6.6980e-01, -5.5136e-01,\n",
      "         3.1417e-01,  1.3045e-01, -2.2425e-01,  5.9335e-02,  9.9148e-02,\n",
      "         4.5852e-01, -7.4856e-02,  4.5879e-01,  1.5277e-02,  3.3506e-01,\n",
      "         4.1007e-01, -4.4873e-01, -1.7671e-01, -1.4308e-01,  2.8202e-01,\n",
      "        -1.9036e-01, -3.9048e-01, -5.4971e-01,  1.8727e+00, -1.8667e-01,\n",
      "        -5.6236e-01, -1.3137e-01, -1.2278e+00,  3.2265e-01, -3.6998e-01,\n",
      "         3.3531e-01, -4.1706e-01, -1.6812e-01, -1.7324e-01, -5.9798e-01,\n",
      "         5.6762e-01, -1.6046e-01, -1.3866e-01,  2.7110e-01,  7.4279e-01,\n",
      "        -4.3668e-01,  1.2987e-01,  4.5151e-01, -4.3757e-02, -1.0567e+00,\n",
      "        -6.8896e-01, -5.4453e-01, -2.6973e-01,  1.7400e-01,  5.9526e-01,\n",
      "         2.4812e-01, -1.0078e+00, -3.7504e-01, -2.2583e-01, -1.5617e-01,\n",
      "        -2.9697e-01,  6.0822e-01,  4.6048e-01,  1.9481e+00,  8.8686e-01,\n",
      "         2.7938e-01, -1.0550e-01, -3.1012e-01, -4.6745e-01,  1.0894e+00,\n",
      "        -6.3627e-01,  1.2454e-01, -9.0190e-01, -4.6606e-01,  5.3622e-01,\n",
      "        -4.8023e-01, -5.1085e-01,  3.3895e-01,  4.1162e-01,  9.1764e-01,\n",
      "         4.2570e-01,  1.4562e+00, -2.6549e-01,  2.1135e-01,  9.1256e-02,\n",
      "        -9.2381e-01,  4.5983e-03, -2.0325e-01, -4.7744e-01, -1.3109e+00,\n",
      "        -8.5286e-02,  6.6770e-01,  4.2419e-02,  7.0438e-01, -4.7869e-01,\n",
      "         1.9072e-01,  2.5890e-01,  2.0584e-02, -7.4755e-01,  6.5433e-01,\n",
      "        -2.2444e-01,  7.9480e-01,  1.0644e-01, -1.2039e-01, -2.8181e-01,\n",
      "         1.0180e-01,  3.5791e-01,  1.8127e+00, -3.9728e-01, -4.9969e-02,\n",
      "         7.1733e-01,  1.6849e-01, -8.3528e-02,  1.7019e-01,  1.2211e+00,\n",
      "        -1.8308e-01, -2.2090e-01,  3.1748e-01,  3.5626e-02, -2.3283e-01,\n",
      "        -1.7622e-01,  1.3646e-01,  7.3491e-01, -3.8489e-01,  4.5583e-01,\n",
      "         9.6144e-01,  1.2602e+00,  6.2013e-01, -2.9969e-01, -7.8016e-01,\n",
      "        -9.0690e-02,  5.8461e-01,  2.9354e-01, -4.3826e-02, -2.4978e-01,\n",
      "         3.2568e-01, -8.8534e-01, -1.0001e+00, -4.0223e-01,  6.3499e-01,\n",
      "         3.5957e-01, -1.4216e-01, -2.6349e-01, -4.1851e-01,  6.0397e-01,\n",
      "        -8.8257e-02,  4.6952e-01,  4.2657e-01, -4.1969e-01,  9.7922e-02,\n",
      "         1.1776e+00,  2.2681e-01, -6.4870e-01,  1.6822e-01, -1.0870e+00,\n",
      "         9.4214e-02, -3.9756e-01,  5.3893e-01,  2.8286e-01,  5.4125e-01,\n",
      "        -2.8882e-01,  5.5490e-01, -9.2566e-02,  3.3105e-01, -1.9218e-01,\n",
      "        -3.6489e-01,  5.4878e-01,  1.6935e-01,  9.1414e-02,  6.6677e-01,\n",
      "        -3.9326e-01, -4.3105e-01,  6.5573e-01])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pre-trained CodeBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Example Python code snippets\n",
    "code_snippets = [\n",
    "    \"def factorial(n):\\n    if n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\",\n",
    "    \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fibonacci(n - 1) + fibonacci(n - 2)\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the code snippets\n",
    "encoded_inputs = tokenizer(code_snippets, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate code embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    embeddings_bert = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Print the code embeddings\n",
    "for i, snippet in enumerate(code_snippets):\n",
    "    print(\"Code Snippet:\", snippet)\n",
    "    print(\"Embedding:\", embeddings_bert[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2777,  0.1576,  0.2955,  ..., -0.2448, -0.3847,  0.6505],\n",
       "        [-0.3416,  0.1528,  0.2126,  ..., -0.3933, -0.4310,  0.6557]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the embeddings to a format suitable for cosine similarity\n",
    "embeddings_matrix = [embedding for embedding in embeddings]\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\"This is the first document.\",\n",
    "\t\t\t\"This document is the second document.\",\n",
    "\t\t\t\"And this is the third one.\",\n",
    "\t\t\t\"Is this the first document?\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary (Feature Names):\", feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vboxuser/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 2.44MB/s]\n",
      "model.safetensors: 100%|██████████| 548M/548M [03:19<00:00, 2.75MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 57.9kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 90.4kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 830kB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.94MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.66MB/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'  # Change to the desired model, e.g., 'gpt2-medium'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate output based on the prompt\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, json\n",
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "import llm_model as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pre-trained CodeBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Example Python code snippets\n",
    "code_snippets = [\n",
    "    \"def factorial(n):\\n    if n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\",\n",
    "    \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fibonacci(n - 1) + fibonacci(n - 2)\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the code snippets\n",
    "encoded_inputs = tokenizer(code_snippets, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate code embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Print the code embeddings\n",
    "for i, snippet in enumerate(code_snippets):\n",
    "    print(\"Code Snippet:\", snippet)\n",
    "    print(\"Embedding:\", embeddings[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = utils.silhouette_to_find_optimal_k(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels, job_embeddings = utils.kmeans_clustering(optimal_k, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Fetch_Each_clustering_Content(texts, cluster_labels):\n",
    "    # Write text data for each cluster to separate text files\n",
    "    for cluster_label in range(optimal_k):\n",
    "        cluster_texts = np.array(texts)[np.array(cluster_labels) == cluster_label]\n",
    "        save_path = f'./txt/hon_{cluster_label}_texts.txt'\n",
    "\n",
    "        with open(save_path, 'w') as file:\n",
    "            for text in cluster_texts:\n",
    "                file.write(text + '\\n')\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "# Specify the optimal number of clusters and the path for saving the text file\n",
    "texts = data['code_content'].splitlines()\n",
    "# Perform k-means clustering and save the cluster labels\n",
    "cluster_labels = Fetch_Each_clustering_Content(texts, cluster_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
