{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, sys, json\n",
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "import chroma as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_reader(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        txt_contents = file.read()\n",
    "    txt_contents = txt_contents.strip()\n",
    "    return [txt_contents]\n",
    "file_path = './txt/python.txt'\n",
    "pdf_texts = txt_reader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 22\n"
     ]
    }
   ],
   "source": [
    "character_split_texts = utils.text_splitter_chunks(pdf_texts)\n",
    "#print(character_split_texts[10])\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 31\n"
     ]
    }
   ],
   "source": [
    "token_split_texts = utils.sentence_transfomer_textsplitter(character_split_texts)\n",
    "#print(token_split_texts[10])\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = utils.embedding(token_split_texts)\n",
    "#print(embedding_function([token_split_texts[10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = 'python0'\n",
    "chroma_collection = utils.connect_with_chromadb(embedding_function, token_split_texts, collection_name)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = chroma_collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retrieved_documents['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_documents = retrieved_documents[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "env_vars = dotenv_values('.env')\n",
    "openai.api_key = env_vars.get('OPENAI_API_KEY')\n",
    "openai_client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "def openai_model_answer(query, retrieved_documents, model=\"gpt-3.5-turbo-16k\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant, your role is to provide assistance and answer questions based on the available document and your own knowledge. If you encounter a question for which you don't have the answer or cannot find the relevant information in the document, please respond with `I don't know` or `I don't see the answer on the document.` Your goal is to provide accurate and relevant information from the document and offer assistance whenever possible. \"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document provided consists of code snippets written in Python. Here are some insights on its content, structure, and potential improvements:\n",
      "\n",
      "Content:\n",
      "- The document includes several classes and functions, each serving a specific purpose, such as storing results, analyzing user satisfaction, handling missing information, embedding chunks of text, analyzing dataframes, cleaning data, generating test data, and more.\n",
      "- The code utilizes external libraries such as psycopg, os, langchain, tqdm, pandas, sklearn, sqlalchemy, numpy, scipy, and weaviate.\n",
      "\n",
      "Structure:\n",
      "- The document starts with the definition of a class called `storeresults`, which has a method `_ _ call _ _` that inserts data into a PostgreSQL database.\n",
      "- Next, there is a section for importing libraries and setting up environment variables.\n",
      "- Following that, there is a section for defining the classes `usersatisfactionanalysis`, `missinginformation`, `embedchunks`, `dataframeinformation`, and `cleandata`, each with their respective methods.\n",
      "- The bottom part of the document contains code that calls the defined functions and classes.\n",
      "- The document seems to be missing some necessary imports, such as `import pandas` and `import numpy`.\n",
      "\n",
      "Potential improvements:\n",
      "- The code could benefit from proper commenting and documentation to make it easier for others to understand and use.\n",
      "- The document could include examples or use cases for each function or class to provide more context and clarity.\n",
      "- The code could be organized into separate files or modules for better maintainability.\n",
      "- Adding exception handling and error messages would improve the robustness of the code.\n",
      "- Some parts of the code seem to have redundant or unused variables, which could be removed to improve readability.\n",
      "- The document could be formatted more consistently, with proper indentation and spacing, to enhance readability.\n",
      "\n",
      "Please note that without further context or specific requirements, it is challenging to provide more detailed insights or specific improvements.\n"
     ]
    }
   ],
   "source": [
    "query = 'Perform a comprehensive analysis of the document and provide insights on its content, structure, and potential improvements'\n",
    "output_answer = openai_model_answer(query=query, retrieved_documents=retrieved_documents)\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided Python code snippets, here are some insights, potential optimizations, and best practices:\n",
      "\n",
      "1. Storing Results:\n",
      "   - The `storeresults` class defines a method named `__call__`, which is used to store batch data in the PostgreSQL database. The method uses psycopg2 to establish a connection to the database and inserts the data into the \"document\" table.\n",
      "   - An improvement could be made by using prepared statements instead of dynamically building the SQL query for each iteration. Prepared statements can improve performance and security by pre-compiling the query and reusing it for multiple executions.\n",
      "\n",
      "2. Data Loading and Analysis:\n",
      "   - The `load_data_from_database` method in the `usersatisfactionanalysis` class retrieves data from a PostgreSQL database and loads it into a pandas DataFrame.\n",
      "   - The `perform_user_satisfaction_analysis` method calculates engagement and experience scores based on relevant columns in the dataset. It then derives a satisfaction score, identifies the top 10 satisfied customers, and performs clustering on users.\n",
      "   - One potential optimization could be to use database-specific optimizations, such as index creation on relevant columns, to improve query performance. Additionally, the data loading process could be optimized by loading only the required columns from the database instead of selecting all columns.\n",
      "\n",
      "3. Missing Data Analysis:\n",
      "   - The `missinginformation` class provides methods to analyze missing data in a DataFrame. The `missing_values_table` method calculates the number and percentage of missing values for each column.\n",
      "   - The `dataframeinformation` class provides methods to analyze skewness and missing data in a DataFrame. The `get_skewness` method calculates the skewness of numeric columns, and the `get_skewness_missing_count` method combines skewness and missing data analysis for further exploration.\n",
      "   - It is considered a best practice to handle missing data appropriately. Depending on the analysis requirements, missing data can be imputed using techniques like mean, median, or other statistical methods. Additionally, preprocessing steps like data normalization or scaling can be applied.\n",
      "\n",
      "4. Data Cleaning and Transformation:\n",
      "   - The `cleandata` class provides methods to clean and transform data in a DataFrame. Methods like `convert_dtype`, `format_float`, `convert_bytes_to_megabytes`, `convert_ms_to_sec`, `fix_missing_ffill`, `fix_missing_bfill`, `drop_column`, `fill_mode`, `handle_outliers` can be used for various data cleaning and transformation tasks.\n",
      "   - It's important to handle outliers and skewed data appropriately based on the analysis requirements. This can include techniques like winsorizing, logarithmic transformation, or scaling.\n",
      "\n",
      "5. Chat Models and Prompts:\n",
      "   - The code includes the use of chat models and prompts for natural language processing tasks.\n",
      "   - The `generate_test_data` function generates test prompts by completing a given prompt with context and other input parameters.\n",
      "   - The `main` function reads context and prompt messages from files and generates test data by calling the `generate_test_data` function. The resulting test data is saved to a file.\n",
      "   - It's important to carefully design and customize chat prompts based on the specific requirements of the task at hand. Fine-tuning chat models with task-specific data can also lead to improved performance.\n",
      "\n",
      "Overall, the provided code snippets cover various aspects related to data storage, retrieval, analysis, and cleaning. However, further optimization and customization based on specific requirements may be necessary.\n"
     ]
    }
   ],
   "source": [
    "query = \"Analyze a collection of Python code snippets and provide insights on common programming patterns, potential optimizations, and best practices.\"\n",
    "output_answer = openai_model_answer(query=query, retrieved_documents=retrieved_documents)\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided Python code, we can make the following assessment:\n",
      "\n",
      "Overall Assessment: Intermediate\n",
      "\n",
      "Factors influencing the intermediate category:\n",
      "\n",
      "1. Complexity of operations: The code performs several operations such as connecting to a PostgreSQL database, executing SQL queries, loading data into a pandas dataframe, performing user satisfaction analysis, clustering, data cleaning, and text generation. These operations require a moderate level of understanding and skill.\n",
      "\n",
      "2. Knowledge and experience required: The code requires knowledge of Python programming, SQL, pandas library, scikit-learn library, and various NLP (Natural Language Processing) concepts such as embeddings and text generation. It also requires experience in working with databases and data analysis.\n",
      "\n",
      "3. Code structure and organization: The code is structured into classes and functions, which indicates a higher level of organization compared to beginner-level code. However, there are some areas where the code could be further improved in terms of modularity and reusability.\n",
      "\n",
      "4. External dependencies: The code relies on external libraries and APIs such as psycopg2 for database connection, Weaviate for vector storage and retrieval, OpenAI for embeddings and text generation, and scikit-learn for clustering. Dealing with these dependencies requires familiarity with integrating external libraries and APIs into a Python project.\n",
      "\n",
      "Overall, the code demonstrates a moderate level of complexity in terms of the tasks it performs and requires intermediate-level knowledge and experience in Python programming, data analysis, and NLP concepts.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Please evaluate the supplied Python code, concentrating on the complexity of the tasks it performs and the level of understanding and skill required to grasp and carry out these operations. Instead of categorizing specific sections, provide a general assessment framework based on the code's content as a whole. Assign it to beginner, intermediate, or expert categories based on overall criteria. Detail the factors influencing each category, including the complexity of operations, and the knowledge and experience needed to effectively engage with the code.\n",
    "\"\"\"\n",
    "output_answer = openai_model_answer(query=query, retrieved_documents=retrieved_documents)\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complexity level of this Python code collection can be considered intermediate to advanced. It includes the use of classes, database connections, SQL queries, data manipulation and analysis using pandas, embedding models, clustering algorithms, handling missing values, and text generation using OpenAI's API.\n",
      "\n",
      "Notable patterns and structures that contribute to its complexity include:\n",
      "\n",
      "1. Class-based structure: The code includes multiple classes such as `storeresults`, `usersatisfactionanalysis`, `missinginformation`, `embedchunks`, `dataframeinformation`, `cleandata`, which encapsulate related functionality and promote code organization and reusability.\n",
      "\n",
      "2. Database interaction: The code establishes connections to a PostgreSQL database using the `psycopg` library and performs data insertion and retrieval operations using SQL queries.\n",
      "\n",
      "3. Parallel processing: The code utilizes parallel processing techniques with the `tqdm` library and the `actorpoolstrategy` to process data in batches faster.\n",
      "\n",
      "4. Data analysis and manipulation: The code uses pandas for data analysis tasks, including reading data from a database, calculating engagement and experience scores, performing clustering with k-means, handling missing values, and fixing outliers.\n",
      "\n",
      "5. Text generation: The code leverages OpenAI's API for text generation tasks, such as generating test prompts and completing prompts.\n",
      "\n",
      "Overall, this code collection requires a good understanding of Python programming concepts and libraries, database operations, data analysis, and text generation techniques. It may pose a moderate to high level of difficulty for beginners but is suitable for intermediate to advanced Python developers.\n"
     ]
    }
   ],
   "source": [
    "query = \"Evaluate the complexity level of the Python code collection and provide an assessment of its overall difficulty, ranging from beginner-friendly to advanced. Highlight any notable patterns or structures that contribute to its complexity.\"\n",
    "output_answer = openai_model_answer(query=query, retrieved_documents=retrieved_documents)\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't see the answer to your question in the provided code. Would you like me to propose some general strategies for enhancing error handling and ensuring code reliability in Python?\n"
     ]
    }
   ],
   "source": [
    "query = \"Examine the error handling and exception management practices employed in the Python code collection. Evaluate the robustness of error handling, identify potential vulnerabilities, and propose strategies for enhancing error handling and ensuring code reliability.\"\n",
    "output_answer = openai_model_answer(\n",
    "    query=query, retrieved_documents=retrieved_documents\n",
    ")\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given Python code collection, the testing approach adopted appears to be limited. There are no explicit unit tests or integration tests present in the code snippet.\n",
      "\n",
      "To improve the overall test coverage and reliability of the codebase, here are some suggestions:\n",
      "\n",
      "1. Implement Unit Tests: Write unit tests for individual functions or methods within the codebase. This will allow for the testing of individual units of code in isolation to ensure they behave as expected. Unit tests can be written using testing frameworks such as pytest or unittest.\n",
      "\n",
      "2. Add Integration Tests: Develop integration tests that verify the interaction and collaboration between different components or modules within the codebase. Integration tests can be used to ensure that different parts of the system work together correctly. \n",
      "\n",
      "3. Use Test Automation: Implement automated test frameworks or tools to execute tests automatically, allowing for regular and reliable testing. This can help catch bugs and regressions quickly.\n",
      "\n",
      "4. Expand Test Coverage: Evaluate the current test coverage and identify any critical areas that are not covered. Add additional test cases to cover these areas and ensure that all functionalities are thoroughly tested.\n",
      "\n",
      "5. Incorporate Test Data Generation: Generate test data programmatically to create a diverse and comprehensive test suite. This can help cover a wide range of scenarios and edge cases, improving the effectiveness of testing.\n",
      "\n",
      "6. Use Testing Libraries or Frameworks: Consider leveraging testing libraries or frameworks specifically designed for Python code testing, such as PyTest or unittest. These tools provide built-in functionalities for organizing and executing tests. \n",
      "\n",
      "7. Implement Continuous Integration (CI) and Continuous Deployment (CD) Pipelines: Set up CI/CD pipelines to automatically build, test, and deploy the code changes. This ensures that changes are thoroughly tested before being deployed, reducing the likelihood of introducing bugs into the production environment.\n",
      "\n",
      "By adopting these suggestions, you can improve the overall test coverage and reliability of the codebase, minimizing the risk of bugs and issues.\n"
     ]
    }
   ],
   "source": [
    "query = \"Analyze the testing approach adopted within the Python code collection. Assess the presence of unit tests, integration tests, or other testing methodologies, and provide suggestions for improving the overall test coverage and reliability of the codebase.\"\n",
    "output_answer = openai_model_answer(\n",
    "    query=query, retrieved_documents=retrieved_documents\n",
    ")\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the available code, here are some observations regarding the coding approach in terms of professionalism:\n",
      "\n",
      "1. Code Organization: The code is organized into classes and functions, which follows a modular approach and promotes code reusability.\n",
      "\n",
      "2. Variable Naming: Most of the variable names are descriptive and meaningful, which improves code readability.\n",
      "\n",
      "3. Error Handling: There are some basic error handling mechanisms in place, such as printing error messages and handling exceptions. However, there is room for improvement in terms of providing more informative error messages.\n",
      "\n",
      "4. Use of Libraries: The code utilizes various libraries for different functionalities, such as psycopg, pandas, sklearn, and sqlalchemy. This demonstrates the use of standard libraries and the ability to leverage existing code.\n",
      "\n",
      "5. Data Processing: The code includes functions for loading data from a PostgreSQL database, performing analysis on user satisfaction data, and handling missing values in a dataframe. This indicates a systematic and well-structured approach to data processing.\n",
      "\n",
      "6. Code Comments: There are some inline comments explaining the purpose of certain code blocks or functions. However, more comments could be added to improve code documentation and make it easier for other developers to understand the code.\n",
      "\n",
      "Overall, the coding approach appears to be professional, with proper organization and consideration for code readability and maintainability. However, without additional information or context, it is difficult to assess other aspects such as performance optimization or adherence to specific coding standards.\n"
     ]
    }
   ],
   "source": [
    "query = \"Analysis the code based on its coding approach interms of level of proffesionalism?\"\n",
    "output_answer = openai_model_answer(\n",
    "    query=query, retrieved_documents=retrieved_documents\n",
    ")\n",
    "print(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code provided is a mix of different classes and functions. It is not clear how the efficiency and performance of the code can be analyzed based on the information provided. To analyze the code efficiency for better performance, you would need to profile the code, identify any bottlenecks or areas of improvement, and optimize them accordingly.\n",
      "\n",
      "Here are a few general tips for optimizing code efficiency:\n",
      "\n",
      "1. Use data structures and algorithms that have better time complexity.\n",
      "2. Minimize unnecessary computations and operations.\n",
      "3. Avoid redundant or repetitive code.\n",
      "4. Use appropriate indexing and slicing techniques.\n",
      "5. Optimize database queries by indexing and optimizing SQL queries.\n",
      "6. Use parallel processing or multi-threading for computationally intensive tasks.\n",
      "7. Profile the code using tools like profilers to identify performance bottlenecks.\n",
      "8. Optimize memory usage and minimize unnecessary memory allocations.\n",
      "9. Use efficient data loading and handling techniques.\n",
      "10. Optimize code execution by minimizing I/O operations.\n",
      "\n",
      "These are just general suggestions, and the specific optimizations depend on the context and requirements of the code. It would be helpful to have more specific information about the code and its purpose to provide more targeted optimizations.\n",
      "\n",
      "Let me know if you need help with any specific part of the code or if there's anything else I can assist you with.\n"
     ]
    }
   ],
   "source": [
    "query = \"Analysis the code efficieny for better performance?\"\n",
    "output_answer = openai_model_answer(\n",
    "    query=query, retrieved_documents=retrieved_documents\n",
    ")\n",
    "print(output_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
